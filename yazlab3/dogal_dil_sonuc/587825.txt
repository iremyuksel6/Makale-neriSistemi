fulli asynchron multifront solver use distribut dynam schedul paper analyz main featur discuss tune algorithm direct solut spars linear system distribut memori comput develop context long term european research project algorithm use multifront approach especi design cover larg class problem problem symmetr posit definit gener symmetr unsymmetr matric possibl rank defici provid user sever format algorithm achiev high perform exploit parallel come sparsiti problem avail dens matric algorithm use dynam distribut task schedul techniqu accommod numer pivot allow migrat comput task lightli load processor larg comput task divid subtask enhanc parallel asynchron commun use throughout solut process effici overlap commun computationw illustr design choic experiment result obtain sgi origin 2000 ibm sp2 test matric provid industri partner parasol project b introduct consid direct solut larg spars linear system distribut memori comput system form n theta n symmetr posit definit gener symmetr unsymmetr spars matrix possibl rank defici b righthand side vector x solut vector comput work present articl perform work packag 21 within parasol project parasol esprit iv long term research project 20160 integr environ parallel spars matrix solver main goal project start januari 1996 finish june 1999 build test portabl librari solv larg spars system equat distribut memori system final librari public domain contain routin direct iter solut symmetr unsymmetr system context parasol produc multifront massiv parallel solver 27 28 refer mump remaind paper sever aspect algorithm use mump combin give approach uniqu among spars direct solver includ ffl classic partial numer pivot numer factor requir use dynam data structur ffl abil automat adapt comput load variat numer phase ffl high perform exploit independ comput due sparsiti avail dens matric ffl capabl solv wide rang problem includ symmetr unsymmetr rankdefici system use either lu ldl factor address factor design fulli asynchron algorithm base multifront approach distribut dynam schedul task current version packag provid larg rang option includ possibl input matrix assembl format either singl processor distribut processor addit matrix input element format current one processor mump also determin rank nullspac basi rankdefici matric return schur complement matrix contain classic pre postprocess facil exampl matrix scale iter refin error analysi among work distribut memori spars direct solver awar 7 10 12 22 23 24 know capabl mump solver difficulti handl dynam data structur effici distribut memori approach perform numer pivot factor phase instead base static map task data allow task migrat numer factor numer pivot clearli avoid symmetr posit definit matric unsymmetr matric duff koster 18 19 design algorithm permut larg entri onto diagon shown significantli reduc numer pivot demmel li 12 shown one preprocess matrix use code duff koster static pivot possibl modifi diagon valu follow iter refin normal provid reason accur solut observ preprocess combin appropri scale input matrix issu numer stabil approach rest paper organ follow first introduc main term use multifront approach section 2 throughout paper studi perform obtain set test problem describ section 3 discuss section 4 main parallel featur approach section 5 give initi perform figur show influenc order variabl perform mump section 6 describ work accept input matric element form section 7 briefli describ main properti algorithm use distribut assembl matric section 8 comment memori scalabl issu section 9 describ analys distribut dynam schedul strategi analys section 10 show modifi assembl tree introduc parallel present summari result section 11 result present paper obtain 35 processor ibm sp2 locat gmd bonn germani node comput 66 mhertz processor 128 mbyte physic memori 512 mbyte virtual memori sgi cray origin 2000 parallab univers bergen norway also use run largest test problem parallab comput consist 64 node share 24 gbyte physic distribut memori node two r10000 mip risc 64bit processor share 384 mbyte local memori processor run frequenc 195 mhertz peak perform littl 400 mflop per second experi report paper use version 40 mump softwar written fortran 90 requir mpi messag pass make use bla 14 15 lapack 6 blac 13 scalapack 9 subroutin ibm sp2 current use nonoptim portabl local instal scalapack ibm optim librari pessl v2 avail multifront method intent describ detail multifront method rather defin term use later paper refer reader earlier public detail descript exampl 3 17 20 multifront method elimin oper take place within dens submatric call frontal matric frontal matrix partit shown figur 1 matrix pivot chosen within block f 11 schur complement matrix comput use updat later row column overal matrix call updat matrix contribut block fulli sum row partli sum row fulli sum column partli sum column figur 1 partit frontal matrix overal factor spars matrix use multifront scheme describ assembl tree node correspond comput schur complement describ edg repres transfer contribut block son node father node tree father node assembl sum contribut block son node entri origin matrix origin matrix given assembl format complet row column input matrix assembl facilit input matrix order accord pivot order store collect arrowhead permut matrix entri exampl column row column arrowhead list associ variabl g symmetr case entri lower triangular part matrix store say store matrix arrowhead form arrowhead unassembl matric complet element matric assembl frontal matric input matrix need preprocess implement assembl tree construct symmetr pattern matrix given sparsiti order symmetr pattern mean pattern matrix aa summat symbol note allow matrix unsymmetr numer pivot possibl variabl elimin frontal matrix fulli sum row column correspond variabl ad contribut block sent father node assembl fulli sum row column frontal matrix father node mean correspond elimin oper delay repeat elimin step later frontal matric introduc stabl pivot delay fulli sum part delay elimin step correspond posteriori modif origin assembl tree structur gener introduc addit numer fillin factor import aspect assembl tree oper pair node neither ancestor independ give possibl obtain parallel tree socal tree parallel exampl work commenc parallel leaf node tree fortun near root node tree tree parallel poor frontal matric usual much larger techniqu exploit parallel dens factor use exampl block use higher level bla call node parallel discuss aspect parallel multifront method later section paper work base experi design implement multifront scheme share virtual share memori comput exampl 2 3 4 initi prototyp distribut memori multifront version 21 describ design result distribut memori multifront algorithm rest paper 3 test problem throughout paper use set test problem illustr perform algorithm describ set section tabl 1 2 list unassembl assembl test problem respect except one come industri partner parasol project remain matrix bbmat forthcom rutherfordbo spars matrix collect 16 symmetr matric show number entri lower triangular part matrix typic parasol test case follow major applic area comput fluid dynam cfd structur mechan model compound devic model ship mobil offshor platform industri process complex nonnewtonian liquid model car bodi engin compon test problem provid assembl format element format suffix rsa rse use differenti element format origin matrix repres sum element matric nonzero entri row column correspond variabl ith element element matric may overlap number entri matrix element format usual larger matrix assembl compar matric det norsk verita norway tabl 1 2 typic twice number entri unassembl element format real symmetr element rse matrix name order element entri origin t1rse 97578 5328 6882780 det norsk verita ship 001rse 34920 3431 3686133 det norsk verita ship 003rse 121728 45464 9729631 det norsk verita shipsec1rs 140874 41037 8618328 det norsk verita shipsec5rs 179860 52272 11118602 det norsk verita shipsec8rs 114919 35280 7431867 det norsk verita threadrs 29736 2176 3718704 det norsk verita x104rse 108384 6019 7065546 det norsk verita tabl 1 unassembl symmetr test matric parasol partner element format tabl 3 4 5 present statist factor variou test problem use mump tabl show number entri factor number floatingpoint oper flop elimin unsymmetr problem show estim number assum pivot actual number numer pivot use statist clearli depend order use two class order consid paper first approxim minimum degre order refer amd see 1 second class base hybrid nest dissect minimum degre techniqu refer nd hybrid order gener use onmeti 26 combin graph partit tool scotch 29 variant amd halo amd see 30 matric avail assembl unassembl format use nest dissect base order provid det norsk verita denot mfr note paper intent compar packag use obtain order discuss influenc type order perform mump section 5 amd order algorithm tightli integr within mump code order pass mump extern comput order tight integr observ tabl 3 analysi time smaller use amd real unsymmetr assembl rua matrix name order entri origin mixingtank 29957 1995041 polyflow sa bbmat 38744 1771722 rutherfordbo cfd real symmetr assembl rsa matrix name order entri origin oilpan 73752 1835470 inpro b5tuer 162610 4036144 inpro crankseg 1 52804 5333507 macnealschwendl bmw7st 1 141347 3740507 macnealschwendl ship 001rsa 34920 2339575 det norsk verita ship 003rsa 121728 4103881 det norsk verita shipsec1rsa 140874 3977139 det norsk verita shipsec5rsa 179860 5146478 det norsk verita shipsec8rsa 114919 3384159 det norsk verita threadrsa 29736 2249892 det norsk verita x104rsa 108384 5138004 det norsk verita tabl 2 assembl test matric parasol partner except matrix bbmat amd order entri flop time matrix factor theta10 6 estim actual estim actual second mixingtank 385 391 641 644 49 invextrusion1 303 312 343 358 46 bbmat 460 462 413 416 81 nd order entri flop time matrix factor theta10 6 estim actual estim actual second mixingtank 189 196 130 132 128 bbmat 357 358 255 257 113 tabl 3 statist unsymmetr test problem ibm sp2 userdefin precomput order paper nd mfr order addit cost comput extern order includ tabl amd order nd order entri flop time entri flop matrix factor analysi factor b5tuer 26 13 15 24 12 bmw7st 1 tabl 4 statist symmetr test problem ibm sp2 entri flop matrix factor ship 003 57 73 shipsec1 37 shipsec5 51 52 shipsec8 34 34 thread tabl 5 statist symmetr test problem avail assembl rsa unassembl rse format mfr order 4 parallel implement issu paper assum onetoon map process processor distribut memori environ process thu implicitli refer uniqu processor say exampl task alloc process mean task also map onto correspond processor share memori environ 4 exploit parallel aris sparsiti tree parallel dens factor kernel node parallel avoid limit due central schedul host process charg schedul work process chosen distribut schedul strategi implement pool work task distribut among process particip numer factor host process still use perform analysi phase identifi pool work task distribut righthand side vector collect solut implement allow host process particip comput factor solut phase allow user run code singl processor avoid one processor idl factor solut phase code solv system three main step 1 analysi host perform approxim minimum degre order base symmetr matrix pattern carri symbol factor order also provid user host also comput map node assembl tree processor map keep commun cost factor solut minimum balanc memori comput requir process comput cost approxim number floatingpoint oper assum pivot perform storag cost number entri factor comput map host send symbol inform process use inform process estim work space requir part factor solut estim work space larg enough handl comput task assign process analysi time plu possibl task may receiv dynam factor assum excess amount unexpect fillin occur due numer pivot 2 factor origin matrix first preprocess exampl convert arrowhead format matrix assembl distribut process particip numer factor process alloc array contribut block factor numer factor frontal matrix perform process determin analysi phase potenti one process determin dynam factor must kept solut phase 3 solut righthand side vector b broadcast host process comput solut vector x use distribut factor comput factor phase solut vector assembl host 41 sourc parallel consid condens assembl tree figur 2 leav repres subtre assembl tree subtre type 2 type 3 type 2 type 2 type 1 figur 2 distribut comput multifront assembl tree four processor p0 p1 p2 p3 consid tree parallel transfer contribut block node assembl tree father node requir local data movement node assign process commun requir node assign differ process reduc amount commun factor solut phase map comput analysi phase assign subtre assembl tree singl process gener map algorithm choos leaf subtre process map subtre care onto process achiev good overal load balanc comput bottom tree describ detail 5 howev exploit tree parallel speedup disappoint obvious depend problem typic maximum speedup 3 5 illustr tabl 6 poor perform caus fact tree parallel decreas go toward root tree moreov observ see exampl 4 often 75 comput perform top three level assembl tree thu necessari obtain parallel within larg node near root tree addit parallel base parallel block version algorithm use factor frontal matric node assembl tree treat one process refer node type 1 parallel assembl tree refer type 1 parallel parallel obtain onedimension 1d block partit row frontal matrix node larg contribut block see figur 2 node refer node type 2 correspond parallel type 2 parallel final frontal matrix root node larg enough partit twodimension 2d block cyclic way parallel root node refer node type 3 correspond parallel type 3 parallel 42 type 2 parallel analysi phase node determin type 2 number row contribut block suffici larg node type 2 one process call master hold fulli sum row perform pivot factor block process call slave perform updat partli sum row see figur 1 slave determin dynam factor process may select abl assembl origin matrix entri quickli frontal matrix type 2 node duplic correspond origin matrix entri store arrowhead element matric onto process factor way master slave process type 2 node immedi access entri need assembl local part frontal matrix duplic origin data enabl effici dynam schedul comput task requir extra storag studi detail section 8 note type 1 node origin matrix entri need present process handl node execut time master type 2 node first receiv symbol inform describ structur contribut block son node tree inform sent master process handl son base inform master determin exact structur frontal matrix decid slave process particip factor node send inform process handl son enabl send entri contribut block directli appropri process involv type 2 node assembl node subsequ perform parallel master slave process perform elimin oper frontal matrix parallel macropipelin base block factor fulli sum row use overlap commun comput effici algorithm thu depend block size use factor fulli sum row number row alloc slave process detail differ implement symmetr unsymmetr matric describ 5 43 type 3 parallel root node must factor dens matrix use standard code scalabl reason use 2d block cyclic distribut root node use scalapack 9 vendor equival implement routin pdgetrf gener matric routin pdpotrf symmetr posit definit matric actual factor current maximum one root node chosen analysi process parallel node chosen largest root provid size larger comput depend paramet otherwis factor one processor one process also call master hold indic describ structur root frontal matrix call root node determin analysi phase estim root node factor structur frontal matrix estim root node static map onto 2d grid process map fulli determin process entri estim root node assign henc assembl origin matrix entri contribut block process hold inform easili comput exactli process must send data factor phase origin matrix entri part contribut block son correspond estim root assembl soon avail master root node collect index inform delay variabl due numer pivot son build final structur root frontal matrix symbol inform broadcast process particip factor contribut correspond delay variabl sent son appropri process 2d grid assembl contribut directli assembl local destin process note requir scalapack local copi root node requir sinc lead dimens chang delay pivot 44 parallel triangular solut solut phase also perform parallel use asynchron commun forward elimin back substitut case forward elimin tree process leav root similar factor back substitut requir differ algorithm process tree root leav pool readytobeactiv task use chang distribut factor gener factor phase henc type 2 3 parallel also use solut phase root node use scalapack routin pdgetr gener matric routin pdpotr symmetr posit definit matric 5 basic perform influenc order earlier studi exampl 25 know order may serious impact uniprocessor time parallel behaviour method illustr report tabl 6 perform obtain use type 1 parallel result show use type 1 parallel produc good speedup result also show see column speedup usual get better parallel nest dissect base order minimum degre base order thu gain use nest dissect reduct number floatingpoint oper see tabl 3 4 better balanc assembl tree discuss perform obtain mump matric assembl format use refer paper perform obtain matric provid element format discuss section 6 tabl 7 8 show perform mump use nest dissect minimum degre order ibm sp2 sgi origin 2000 respect note speedup difficult comput ibm sp2 memori page often occur small number processor henc better perform nest dissect order small number processor ibm sp2 due part reduct memori requir processor sinc less entri factor get better idea true algorithm speedup without memori page effect give tabl 7 uniprocessor cpu time one processor instead elaps time matrix time speedup amd nd amd nd oilpan 126 73 291 445 bmw7st 1 556 213 255 487 bbmat 784 494 408 400 b5tuer 334 255 347 422 tabl influenc order time second speedup factor phase use type 1 parallel 32 processor ibm sp2 memori larg enough run one processor estim megaflop rate use comput uniprocessor cpu time estim also use necessari comput speedup tabl 6 small number processor still memori page effect may significantli increas elaps time howev speedup elaps time one processor given consider matrix order number processor oilpan amd 37 136 90 68 59 58 b5tuer amd 116 1555 241 168 161 131 crankseg 1 amd 456 5083 1624 784 633 bmw7st 1 amd 142 1534 465 213 184 167 nd 104 1057 367 202 129 117 mixingtank amd 495 2885 707 645 613 nd 104 3280 261 174 144 148 bbmat amd 320 2764 683 478 440 398 nd 198 1064 767 352 346 309 tabl 7 impact order time second factor ibm estim cpu time one processor mean enough memori tabl 8 also show elaps time solut phase observ speedup phase quit good remaind paper use nest dissect base order unless state otherwis factor phase matrix order number processor bmw7st 1 amd 857 560 282 185 151 142 nd 3066 1827 809 529 412 355 nd 1521 938 525 330 221 170 solut phase matrix order number processor crankseg 2 amd 68 58 44 29 24 23 nd 43 27 18 15 11 18 bmw7st 1 amd 42 24 23 19 14 16 nd 33 21 17 14 16 15 nd 83 47 27 21 18 20 nd 63 38 29 24 20 24 tabl 8 impact order time second factor solv phase sgi origin 2000 6 element input matrix format section discuss main algorithm chang handl effici problem provid element format assum origin matrix repres sum element matric nonzero entri row column correspond variabl ith element usual held dens matrix matrix symmetr lower triangular part store multifront approach element matric need assembl one frontal matrix elimin process due fact frontal matrix structur contain definit variabl adjac fulli sum variabl front consequ element matric need split assembl process note classic fanin fanout approach 7 properti hold sinc posit element matric assembl restrict fulli sum row column main modif make algorithm assembl matric accommod unassembl matric lie analysi distribut matrix assembl process describ detail analysi phase exploit element format matrix detect supervari defin supervari set variabl list adjac element illustr figur 3 matrix compos two overlap element three supervari note definit supervari differ usual definit see exampl 11 supervari use success similar context compress graph associ assembl matric structur engin prior multipl minimum degre order 8 assembl matric howev observ 1 use supervari combin approxim minimum degre algorithm effici graph size matrix supervari detect t1rse 9655992 299194 ship 003rse 7964306 204324 shipsec1rs shipsec5rs 9933236 256976 shipsec8rs 6538480 171428 threadrs 4440312 397410 x104rse 10059240 246950 tabl 9 impact supervari detect length adjac list given order phase tabl 9 show impact use supervari size graph process order phase amd order graph size length adjac list variablessupervari given input order phase without supervari detect initi graph variabl initi matrix742678 45 graph supervari sum two overlap element figur 3 supervari detect matric element format graph size twice number offdiagon entri correspond assembl matrix work space requir analysi phase use amd order domin space requir order phase graph size plu overhead small multipl order matrix sinc order perform singl processor space requir comput order memori intens part analysi phase supervari detect complet uncompress graph need built sinc order phase oper directli compress graph tabl 9 show larg graph compress reduc memori requir analysi phase dramat tabl show impact use supervari time complet analysi phase includ graph compress order see reduct time due reduc time order significantli less time also need build much smaller adjac graph supervari time analysi matrix supervari detect t1rse 46 18 15 03 ship 003rse 74 28 32 07 shipsec1rs 60 22 26 06 shipsec5rs 101 46 39 08 shipsec8rs 57 20 26 05 threadrs 26 09 12 02 x104rse 64 35 15 03 tabl 10 impact supervari detect time second analysi phase sgi origin 2000 time spent amd order parenthes overal time spent assembl process matric element format differ overal time spent assembl process equival assembl matrix obvious matric element format often significantli data assembl usual twice number entri matrix assembl format howev assembl process matric element format perform effici assembl process assembl matric first potenti assembl larger regular structur full matrix second input data assembl near leaf node assembl tree two consequ assembl perform distribut way assembl origin element matric done type 1 node henc less duplic origin matrix data necessari detail analysi duplic issu link matric element format address section 8 experi shown observ despit differ assembl process perform mump assembl unassembl problem similar provid order use reason extra amount assembl origin data unassembl problem rel small compar total number flop experiment result tabl 11 12 obtain sgi origin 2000 show good scalabl code factor solut phase set unassembl matric matrix number processor ship 003rse 392 242 156 120 92 shipsec1rs 174 128 shipsec5rs 281 176 114 63 43 shipsec8rs 187 127 68 36 threadrs 186 120 69 46 37 x104rse 56 34 20 tabl 11 time second factor unassembl matric sgi origin 2000 mfr order use matrix number processor t1rse 35 21 11 12 08 ship 003rse 69 36 33 25 20 shipsec1rs 38 31 21 16 15 shipsec5rs 55 42 29 22 19 shipsec8rs 38 31 20 14 13 threadrs 23 19 13 10 08 x104rse 26 19 14 10 11 tabl 12 time second solut phase unassembl matric sgi origin 2000 mfr order use 7 distribut assembl matrix distribut input matrix avail processor main preprocess step numer factor phase step input matrix organ arrowhead format distribut accord map provid analysi phase symmetr case first arrowhead frontal matrix also sort enabl effici assembl 5 assembl matrix initi held central host observ time distribut real entri origin matrix sometim compar time perform actual factor exampl matrix oilpan time distribut input matrix 16 processor ibm sp2 averag 6 second wherea time factor matrix 68 second use amd order see tabl 7 clearli larger problem arithmet requir actual factor time factor domin time redistribut distribut input matrix format expect reduc time redistribut phase parallel reformat sort task use asynchron alltoal instead onetoal commun furthermor expect solv larger problem sinc store complet matrix one processor limit size problem solv distribut memori comput thu improv memori time scalabl approach allow input matrix distribut base static map task process comput analyi phase one priori distribut input data remap requir begin factor distribut refer mump map limit commun duplic origin matrix correspond type 2 node studi section 8 show influenc initi matrix distribut time redistribut compar figur 4 three way provid input matrix 1 central map input matrix held one process host 2 mump map input matrix distribut process accord static map comput analysi phase 3 random map input matrix uniformli distribut process random manner correl map comput analysi phase figur clearli show benefit use asynchron alltoal commun requir mump random map compar use onetoal commun central map even interest observ distribut input matrix accord mump map significantli reduc time redistribut attribut good overlap commun comput mainli data reformat sort redistribut algorithm number processors1357 distribut time second central matrix use mump map random map figur 4 impact initi distribut matrix oilpan time redistribut ibm sp2 8 memori scalabl issu section studi memori requir memori scalabl algorithm figur 5 illustr mump balanc memori load processor figur show two matric maximum memori requir processor averag processor function number processor observ vari number processor valu quit similar number processors50150250 size total space mbyte maximum averag 28 number processors100300500700s total space mbyte maximum averag figur 5 total memori requir per processor maximum averag factor nd order tabl 13 show averag size per processor main compon work space use factor matrix bmw3 2 compon ffl factor space reserv factor processor know analysi phase type 2 node particip therefor reserv enough space abl particip type 2 node ffl stack area space use stack contribut block factor ffl initi matrix space requir store initi matrix arrowhead format ffl commun buffer space alloc send receiv buffer ffl size remain workspac alloc per processor ffl total total memori requir per processor line ideal tabl 13 obtain divid memori requir one processor number processor compar actual ideal number get idea mump scale term memori compon number processor factor 423 211 107 58 ideal 211 106 53 26 stack area 502 294 172 initi matrix 69 345 173 89 50 40 35 ideal 345 173 86 43 29 22 commun buffer 0 45 34 14 6 6 5 20 20 20 20 20 20 20 total 590 394 243 135 82 69 67 ideal 295 147 74 37 25 tabl 13 analysi memori use factor matrix bmw3 2 nd order size mbyte per processor see even total memori sum local workspac increas averag memori requir per processor significantli decreas 24 processor also see size factor stack area much larger ideal part differ due parallel unavoid anoth part howev due overestim space requir main reason map type 2 node processor known analysi processor potenti particip elimin type 2 node therefor processor alloc enough space abl particip type 2 node work space actual use smaller larg number processor could reduc estim factor stack area exampl success factor matrix bmw3 2 32 processor stack area 20 smaller report tabl 13 averag work space use commun buffer also significantli decreas 16 processor mainli due type 2 node parallel contribut block split among processor minimum granular reach therefor increas number processor decreas reach minimum granular size contribut block sent processor note larger problem averag size per processor commun buffer continu decreas larger number processor see expect line scale sinc correspond data array size need alloc process see space significantli affect differ total ideal especi larger number processor howev rel influenc fix size area smaller larg matric 3d simul therefor affect asymptot scalabl algorithm imperfect scalabl initi matrix storag come duplic origin matrix data link type 2 node assembl tree studi detail remaind section want stress howev user point view number report context relat total memori use mump packag usual domin larg problem size stack area altern duplic data relat type 2 node would alloc origin data associ frontal matrix master process respons matrix number processor oilpan type 2 node 0 total entri 1835 1845 1888 2011 2235 2521 bmw7st total entri 3740 3759 3844 4031 4308 4793 total entri 5758 5767 5832 6239 6548 7120 shipsec1rsa type 2 node 0 0 4 11 19 21 total entri 3977 3977 4058 4400 4936 5337 shipsec1rs type 2 node total entri 8618 8618 8618 8627 8636 8655 threadrsa type 2 node total entri 2250 2342 2901 4237 6561 8343 threadrs type 2 node total entri 3719 3719 3719 3719 3719 3719 tabl 14 amount duplic due type 2 node total entri sum number origin matrix entri processor theta10 3 number node also given type 2 node assembl process master process would charg redistribut origin data slave process strategi introduc extra commun cost assembl type 2 node thu chosen approach base duplic master process respons type 2 node flexibl choos collabor process dynam sinc involv data migrat origin matrix howev extra cost strategi base decis analysi node type 2 partial duplic origin matrix must perform order keep processor busi need suffici node parallel near root assembl tree mump use heurist increas number type 2 node number processor use influenc number processor amount duplic shown tabl 14 repres subset test problem show total number type 2 node sum process number origin matrix entri duplic one processor type 2 node use data duplic figur 6 show four matric number origin matrix entri duplic processor rel total number entri origin matrix sinc origin data unassembl matric gener assembl earlier assembl tree data matrix assembl format number duplic often rel much smaller unassembl matric assembl matric matrix threadrs element format extrem exampl sinc even 16 processor type 2 node parallel requir duplic see tabl 14 conclud section want point code scale well term memori usag virtual share memori comput total memori sum local workspac processor requir mump sometim excess therefor current investig reduc current overestim local stack area number processors515percentag bmw32 threadrsa figur percentag entri origin matrix duplic processor due type 2 node reduc total memori requir possibl solut might limit dynam schedul type 2 node correspond data duplic subset processor 9 dynam schedul strategi avoid drawback central schedul distribut memori comput implement distribut dynam schedul strategi remind reader type 1 node static map process analysi time type 2 task repres larg part comput parallel method involv dynam schedul strategi abl choos dynam process collabor process type 2 node design twophas assembl process let inod node type 2 let pmaster process inod initi map first phase master process son inod map send symbol data integ list pmaster structur frontal matrix determin pmaster decid partit frontal matrix choos slave process phase pmaster collect inform concern load processor help decis process slave process inform new task alloc pmaster send descript distribut frontal matrix collabor process son inod send contribut block real valu piec directli correct process involv comput inod assembl process thu fulli parallel maximum size messag sent process reduc see section 8 pool task privat process use implement dynam schedul task readi activ given process store pool task local process process execut follow algorithm algorithm 1 node process local pool empti block receiv messag process messag elseif messag avail receiv process messag els extract work pool process endif note algorithm give prioriti messag recept main reason choic first messag receiv might sourc addit work parallel second send process might block send buffer full see 5 actual implement use routin mpi iprob check whether messag avail implement two schedul strategi first strategi refer cyclic schedul master type 2 node take account load processor perform simpl cyclic map task processor second strategi refer dynam flopsbas schedul master process use inform load processor alloc type 2 task least load processor load processor defin amount work flop associ activ readytobeactiv task process charg maintain local inform associ current load simpl remot memori access procedur use exampl onesid commun routin mpi get includ mpi2 process access load processor necessari howev mpi2 avail target comput overcom design modul base symmetr commun tool mpi asynchron send receiv process charg updat broadcast local load control frequenc broadcast updat load broadcast significantli differ last load broadcast initi static map balanc work well expect dynam flopsbas schedul improv perform respect cyclic schedul tabl 15 16 show signific perform gain obtain use dynam flopsbas schedul 24 processor gain less signific test problem small keep processor busi thu lessen benefit good dynam schedul algorithm also expect featur improv behaviour parallel algorithm multius distribut memori comput anoth possibl use dynam schedul improv memori usag seen section 8 size stack area overestim dynam schedul base matrix number processor schedul 28 cyclic 791 479 407 413 389 flopsbas 611 456 419 417 404 cyclic 524 318 262 292 230 flopsbas 294 278 251 253 226 tabl 15 comparison cyclic flopsbas schedul time second factor ibm sp2 nd order matrix number processor schedul 4 8 ship 003rse cyclic 1561 1199 919 flopsbas 1403 1102 838 shipsec5rs cyclic 1135 631 428 flopsbas 999 613 370 shipsec8rs cyclic 683 363 299 flopsbas 650 350 251 tabl comparison cyclic flopsbas schedul time second factor sgi origin 2000 mfr order memori load instead comput load could use address issu type 2 task map least load processor term memori use stack area memori estim size stack area base static map task split node assembl tree process parallel type 2 node symmetr unsymmetr case factor pivot row perform singl processor processor help updat row contribut block use 1d decomposit present section 4 elimin fulli sum row repres potenti bottleneck scalabl especi frontal matric larg fulli sum block near root tree type 1 parallel limit overcom problem subdivid node larg fulli sum block illustr figur 73571 assembl tree pivot block contribut block assembl tree splitting2nfront5npiv npiv father son son figur 7 tree subdivis frontal matrix larg pivot block figur consid initi node size nfront npiv pivot replac node son node size nfront npiv son pivot father node size son npiv father npiv gammanpiv son pivot note split node increas number oper factor add assembl oper nevertheless expect benefit split increas parallel experi simpl algorithm postprocess tree symbol factor algorithm consid node near root tree split larg node far root suffici tree parallel alreadi exploit would lead addit assembl commun cost node consid split distanc root number edg root node let inod node tree dinod distanc inod root node inod dinod dmax appli follow algorithm algorithm 2 split node larg enough 1 comput number flop perform master inod 2 comput number flop perform slave assum nproc gamma 1 slave particip 3 w master w slave 31 split inod node son father npiv son 32 appli algorithm 2 recurs node son father endif endif algorithm 2 appli node nfront npiv2 larg enough want make sure son split node type 2 size contribut block son nfront npiv son node split amount work master w master larg rel amount work slave w slave reduc amount split away root add step 3 algorithm rel factor w slave factor depend machin depend paramet increas distanc node root paramet p allow us control gener amount split final algorithm recurs may divid initi node two new node effect split illustr tabl 17 symmetr matrix crankseg 2 unsymmetr matrix invextrusion1 ncut correspond number type 2 node cut valu use flag indic split flopsbas dynam schedul use run section best time obtain given number processor indic bold font see signific perform improv 40 reduct time obtain use node split best time gener obtain rel larg valu p split occur smaller valu p correspond time chang much p number processor 28 200 time 379 314 304 295 254 150 time 418 313 310 289 272 100 time 398 323 284 286 267 ncut 9 11 13 14 15 50 time 367 336 314 296 274 ncut 28 p number processor 200 time 255 167 134 121 124 150 time 249 163 135 134 124 100 time 249 162 137 131 136 50 time 249 170 135 136 166 tabl 17 time second factor number node cut differ valu paramet p ibm sp2 nest dissect order flopsbas dynam schedul use summari tabl 19 show result obtain mump 40 use dynam schedul node split default valu paramet control effici packag use therefor time alway correspond fastest possibl execut time comparison result present tabl 7 8 11 summar well benefit come work present section 9 10 matrix number processor oilpan 33 111 75 52 48 46 b5tuer 108 821 519 134 131 105 bmw7st 1 104 298 137 117 113 mixingtank 104 308 216 164 147 148 bbmat 198 2554 852 348 328 309 tabl 18 time second factor use mump 40 default option ibm sp2 nd order use estim cpu time mean swap enough memori matrix number processor bmw7st 1 62 36 ship 003rse 392 237 124 108 51 shipsec1rs 174 125 63 shipsec5rs 281 181 103 62 37 shipsec8rs threadrs 186 125 70 38 24 x104rse 56 34 19 12 11 tabl 19 time second factor use mump 40 default option sgi origin 2000 nd mfr order use largest problem solv date symmetr matrix order 943695 39 million entri number entri factor 14 theta 10 9 number oper factor 59 theta 10 12 one processor sgi origin 2000 factor phase requir 89 hour two nonded processor 62 hour requir total amount memori estim reserv mump could solv 2 processor issu address improv scalabl global address memori comput analysi perform pure distribut memori comput larger number processor possibl solut mention paper limit dynam schedul andor memori base dynam schedul develop futur acknowledg grate jennif scott john reid comment earli version paper r approxim minimum degre order algorithm linear algebra calcul virtual share memori comput vector multiprocessor multifront code memori manag issu spars multifront method multiprocessor multifront parallel distribut symmetr unsymmetr solver fanboth famili columnbas distribut choleski factoris algorithm compress graph minimum degre algorithm scalapack user guid parallel solut method larg spars system equat supernod approach spars partial pivot make spars gaussian elimin scalabl static pivot work note 94 user guid blac v1 algorithm 679 algorithm 679 rutherfordbo spars matrix collect direct method spars matric design use algorithm permut larg entri diagon spars matric algorithm permut larg entri diagon spars matrix multifront solut indefinit spars symmetr linear system developp dune approch multifrontal pour machin memoir distribue et reseau heterogen de station de travail spars choleski factor local memori multiprocessor highli scalabl parallel algorithm spars matrix factor parallel algorithm spars linear system improv runtim qualiti nest dissect order scotch 31 user guid hybrid nest dissect halo approxim minimum degre effici spars matrix order tr ctr kai shen parallel spars lu factor differ messag pass platform journal parallel distribut comput v66 n11 p13871403 novemb 2006 omer meshar dror ironi sivan toledo outofcor spars symmetricindefinit factor method acm transact mathemat softwar tom v32 n3 p445471 septemb 2006 patrick r amestoy iain duff jeanyv lexcel xiaoy li impact implement mpi pointtopoint commun perform two gener spars solver parallel comput v29 n7 p833849 juli kai shen parallel spars lu factor secondclass messag pass platform proceed 19th annual intern confer supercomput june 2022 2005 cambridg massachusett hong zhang barri smith michael sternberg peter zapol sip shiftandinvert parallel spectral transform acm transact mathemat softwar tom v33 n2 p9e june 2007 mark baertschi xiaoy li solut threebodi problem quantum mechan use spars linear algebra parallel comput proceed 2001 acmiee confer supercomput cdrom p4747 novemb 1016 2001 denver colorado iain duff jennif scott parallel direct solver larg spars highli unsymmetr linear system acm transact mathemat softwar tom v30 n2 p95117 june 2004 adapt grid refin model two confin interact atom appli numer mathemat v52 n23 p235250 februari 2005 abdou guermouch jeanyv lexcel gil utard impact reorder memori multifront solver parallel comput v29 n9 p11911218 septemb vladimir rotkin sivan toledo design implement new outofcor spars choleski factor method acm transact mathemat softwar tom v30 n1 p1946 march 2004 dror ironi gil shklarski sivan toledo parallel fulli recurs multifront spars choleski futur gener comput system v20 n3 p425440 april 2004 abdou guermouch jeanyv lexcel construct memoryminim schedul multifront method acm transact mathemat softwar tom v32 n1 p1732 march 2006 olaf schenk klau grtner twolevel dynam schedul pardiso improv scalabl share memori multiprocess system parallel comput v28 n2 p187197 februari 2002 patrick r amestoy abdou guermouch jeanyv lexcel stphane pralet hybrid schedul parallel solut linear system parallel comput v32 n2 p136156 februari 2006 patrick r amestoy iain duff jeanyv lexcel xiaoy li analysi comparison two gener spars solver distribut memori comput acm transact mathemat softwar tom v27 n4 p388421 decemb 2001 xiaoy li jame w demmel superludist scalabl distributedmemori spars direct solver unsymmetr linear system acm transact mathemat softwar tom v29 n2 p110140 june olaf schenk klau grtner solv unsymmetr spars system linear equat pardiso futur gener comput system v20 n3 p475487 april 2004 michel benzi precondit techniqu larg linear system survey journal comput physic v182 n2 p418477 novemb 2002 patrick r amestoy iain duff stphane pralet christof vmel adapt parallel spars direct solver architectur cluster smp parallel comput v29 n1112 p16451668 novemberdecemb anshul gupta recent advanc direct method solv unsymmetr spars system linear equat acm transact mathemat softwar tom v28 n3 p301324 septemb 2002 timothi davi column preorder strategi unsymmetricpattern multifront method acm transact mathemat softwar tom v30 n2 p165195 june 2004 nichola gould jennif scott yifan hu numer evalu spars direct solver solut larg spars symmetr linear system equat acm transact mathemat softwar tom v33 n2 p10e june 2007 n f klimowicz mihajlovi heil deploy parallel direct spars linear solver within parallel finit element code proceed 24th iast intern confer parallel distribut comput network p310315 februari 1416 2006 innsbruck austria bendali boubendir fare fetilik domain decomposit method coupl finit element boundari element larges problem acoust scatter comput structur v85 n9 p526535 may 2007