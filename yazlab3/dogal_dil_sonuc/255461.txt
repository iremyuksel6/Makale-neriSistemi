prune algorithm rule learn preprun postprun two standard techniqu handl nois decis tree learn preprun deal nois learn postprun address problem overfit theori learn first review sever adapt pre postprun techniqu separateandconqu rule learn algorithm discuss fundament problem primari goal paper show solv problem two new algorithm combin integr pre postprun b introduct separateandconqu rulelearn system gain popular recent success induct logic program algorithm foil quinlan 1990 analyz differ prune method type induct rule learn algorithm discuss problem main contribut paper two new algorithm topdown prune tdp approach combin pre postprun increment reduc prune irep effici integr preand postprun preprun decis combin pre postprun integr pre postprun postprun preprun liter postprun decis figur 1 prune method separateandconqu rule learn algorithm prune common framework avoid problem overfit noisi data basic idea incorpor bia toward gener simpler theori order avoid overli specif theori tri find explan noisi exampl preprun method deal nois learn instead tri find theori complet consist given train data heurist socal stop criteria use relax constraint stop learn process although posit exampl may yet explain neg exampl may still cover current theori final theori learn one pass see figur 1 separateandconqu rule learner like cn2 clark niblett 1989 foil quinlan 1990 fossil furnkranz 1994 use form nois handl anoth famili algorithm deal nois learn postprun algorithm typic first induc theori complet consist train data theori examin rule condit discard seem explain characterist particular train set thu reflect true regular domain figur 1 show schemat depict process qualiti found rule condit commonli evalu separ set train exampl seen learn postprun algorithm includ reduc error prune rep brunk pazzani 1991 grow cohen 1993 shown effect noisehandl howev also ineffici wast time learn overfit concept descript subsequ prune signific portion rule condit one remedi problem combin pre postprun purpos preprun heurist use reduc entir prevent amount overfit learn prune effici sketch third part figur 1 particular implement approach topdown prune tdp furnkranz 1994 use simpl algorithm gener set theori prune differ degre top generaltospecif order accuraci theori evalu separ set data specif theori accuraci compar accuraci best theori far submit subsequ postprun phase experi show initi topdown search better start theori effici overfit phase classic postprun algorithm search typic return theori closer final theori postprun phase also sped less prune oper need get final theori motiv success method develop rigor approach tightli integr pre postprun instead learn entir theori prune thereaft increment reduc error prune irep furnkranz widmer 1994 prune singl claus right learn new algorithm entir avoid learn overfit theori use postprun method preprun stop criterion shown figur 1 method signific speedup achiev noisi domain avoid problem approach incorpor postprun irep also learn accur theori learn algorithm mani rule learn algorithm tri construct rule socal separateandconqu strategi method root earli day machin learn cover algorithm famou aq famili michalski 1980 michalski mozet hong lavrac 1986 cn2 clark niblett 1989 clark boswel 1991 combin aq cover strategi greedi informationbas test select id3 quinlan 1983 yield power rule learn algorithm term separateandconqu coin pagallo haussler 1990 context learn decis list final separateandconqu learn basic control structur foil algorithm effici induc logic program quinlan 1990 pioneer signific research field relat learn induct logic program figur 2 show basic separateandconqu rule learn algorithm input algorithm set posit neg exampl target concept output set rule abl prove given posit exampl none neg exampl repres rule form prolog claus gener separateandconqu learn algorithm foil procedur separateandconquerexampl negativecov returntheori figur 2 separateandconqu rule learn algorithm concept literal1 proposit learn cn2 condit test valu certain attribut concept relat learn foil one also specifi relat attribut head condit rule gener prolog liter consid set rule prolog program ie rule check order one fire exampl fulfil condit rule consequ classifi instanc learn concept rule fire instanc consid member concept separateandconqu construct rule success ad condit righthand side current rule process repeat enough condit found rule neg exampl posit exampl cover rule separ train set next rule learn remain exampl henc name separateandconqu rule learn way posit exampl left method guarante posit exampl cover least one rule complet rule cover neg exampl consist simpl separateandconqu algorithm figur 2 sever drawback realworld data may noisi noisi data problem mani learn algorithm hard distinguish rare except erron exampl fundament algorithm figur 2 form complet consist theori e tri explain posit exampl none neg exampl presenc nois therefor attempt find explan neg exampl erron classifi posit tri exclud posit exampl neg classif train set explan noisi exampl typic complic exhibit low predict accuraci classifi unseen exampl problem known overfit nois one remedi problem tri increas predict accuraci consid complet consist theori also simpl theori may overgener train exampl final theori allow deliber cover neg train exampl leav posit train exampl uncov order learn simpler predict theori usual achiev via prune heurist procedur prepruningexampl negativecov stoppingcriteriontheorynewclausecov exit exit returntheori figur 3 rule learn algorithm use preprun 3 preprun figur 3 show adapt simpl separateandconqu algorithm order address noisi data preprun heurist algorithm ident one figur 2 except inner loop contain call subroutin stoppingcrit rion stop criterion heurist determin stop ad condit rule stop ad rule concept descript current rule new condit ad fulfil stop criterion inner loop termin incomplet claus ad concept descript claus contain liter assum claus found explain remain posit exampl theori without claus return remain posit exampl thu consid noisi classifi neg return theori separateandconqu algorithm employ stop criteria nois handl commonli use among encod length restrict heurist use induct logic program algorithm foil quinlan 1990 base minimum descript length principl rissanen 1978 tri avoid learn complic rule cover exampl make sure number bit need encod current claus less number bit need encod instanc cover 1 ffl signific test first use proposit cn2 induct algorithm clark niblett 1989 later relat learner mfoil dzeroski bratko 1992 test signific differ distribut posit neg 1 number bit need encod train instanc log 2 nlog 2 number train instanc p posit instanc cover current claus liter encod specifi relat log 2 relat bit variabl log 2 variabil bit whether negat 1 bit sum term liter reduc log 2 n sinc order liter within claus gener irrelev exampl cover rule overal distribut posit neg exampl compar likelihood ratio statist 2 2 distribut 1 degre freedom desir signific level insignific rule rejected103050709011013015017010 09 cutoff complex complexity20406080100 cutoff accuraci accuraci figur 4 accuraci complex vs cutoff ffl cutoff stop criterion use separateandconqu learn system fossil 1994 fossil use search heurist base statist corr lation enabl judg relev liter uniform scale 0 1 thu user requir condit consid claus construct certain minimum correl valu cutoff paramet properti use simpl robust criterion filter nois expect tupl origin nois data low correl predic background knowledg differ set valu caus differ amount preprun set result learn theori complet consist current train set everi liter correl 00 hand gener empti theori learn trivial learn problem background liter correl 10 figur 4 show typic plot accuraci rule complex vs differ valu cutoff paramet commonli use krk endgam classif task 10 nois ad 3 accur rule found cutoff valu approxim 025 035 higher cutoff valu result overgener theori lower set cutoff obvious result overfit data thu fossil cutoff paramet may view mean directli control overfit avoid bia schaffer log pn pn pn pn 3 short descript krk domain along experiment setup found begin section 71 procedur postpruningexampl splitratio exampl growingset pruningset loop exit loop returntheori figur 5 postprun algorithm 1993 wolpert 1993 set cutoff 03 good gener heurist seem independ nois level data furnkranz 1994 4 postprun preprun approach tri avoid overfit rule gener postprun approach first ignor problem overfit nois learn complet consist concept descript result theori subsequ analyz necessari simplifi gener order increas predict accuraci unseen data postprun approach commonli use decis tree learn algorithm cart breiman friedman olshen stone 1984 id3 quinlan 1987 assist niblett bratko 1986 overview comparison variou approach found minger 1989 esposito malerba semeraro 1993 41 reduc error prune common among method reduc error prune rep simpl algorithm adapt decis tree learn quinlan 1987 separateand conquer rule learn framework pagallo haussler 1990 brunk pazzani 1991 begin train data split two subset grow set usual 23 prune set 13 first phase attent paid nois data concept descript explain posit none neg exampl learn grow set result theori simplifi greedili delet condit rule theori delet would result decreas predict accuraci measur prune set pseudocod version algorithm shown figur 5 subroutin prunetheori simplifi current theori delet condit rule usual one time result set theori select one highest accuraci prune set continu prune theori repeat accuraci best prune theori predecessor rep shown learn accur theori preprun algorithm foil krk domain sever level nois brunk pazzani 1991 42 problem reduc error prune although rep quit effect rais predict accuraci noisi domain brunk pazzani 1991 sever shortcom discuss section particular suggest postprun incompat separateandconqu learn strategi effici cohen 1993 shown worstcas time complex rep bad random data n number exampl grow initi concept hand n therefor long run cost prune far outweigh cost gener initi concept descript alreadi higher cost use preprun algorithm entir avoid overfit hillclimb rep employ greedi hillclimb strategi liter claus delet concept definit predict accuraci prune set greedili maxim possibl oper lead decreas predict accuraci search process stop local maximum howev noisi domain theori gener grow phase much specif see figur 4 rep prune signific portion theori ampl opportun err way therefor also expect rep specifictogener search slow also inaccur noisi data separateandconqu strategi postprun algorithm origin research decis tree learn usual wellknown divideandconqu learn strategi use node current train set divid disjoint set accord outcom chosen test algorithm recurs appli set independ although separateandconqu approach share mani similar divid andconqu strategi one import differ prune branch decis tree never affect neighbour branch wherea prune liter rule affect subsequ rule figur 6 illustr postprun decis tree learn work right half initi grown tree cover set c train instanc prune algorithm decid prune two leav ancestor node becom leaf cover exampl cd left branch decis tree influenc oper hand prune liter claus mean claus gener ie cover posit instanc along neg instanc consequ addit posit neg instanc remov train set influenc learn subsequ claus exampl figur 6 b first three rule simplifi cover exampl origin version cover also exampl third rule cover sever exampl second rule cover third rule could easili remov postprun algorithm necessarili case exampl guarante prune train exampl prune train exampl train exampl c b figur postprun divideandconqu b separateandconqu learn algorithm second rule one prune version good explan remain set exampl b2 b2 subset origin set b prune oper gener concept ie increas set cover exampl might well good explan b2 need total differ set liter explan superset b thu learner may lead garden path unprun claus begin theori may chang evalu candid liter subsequ claus wrong choic liter undon prune 43 grow algorithm solv problem section 42 particular effici topdown postprun algorithm base techniqu use pagallo haussler 1990 propos cohen 1993 like rep grow algorithm first find theori overfit data instead prune intermedi theori delet result decreas accuraci prune set first step intermedi theori augment gener claus second step claus expand theori iter select form final concept descript claus improv predict accuraci prune set found gener claus intermedi theori form repeatedli delet final sequenc condit claus error grow set goe least thu grow improv upon rep replac bottomup hillclimb search rep topdown approach instead remov useless claus liter specif theori add promis gener rule initi empti theori result signific gain effici along slight gain accuraci experi cohen 1993 show howev asymptot time complex grow postprun method still complex initi rule grow phase recent shown cameronjon 1994 explan speedup gain topdown strategi start empti theori mani noisi domain much closer final theori overfit theori also seen figur 4 look complex specif theori complex optim theori cutoff 025 035 thu surpris grow shown outperform rep varieti dataset cohen 1993 howev still suffer ineffici caus need gener overli specif theori first pass combin pre postprun section 4 seen intermedi theori result initi overfit phase much complex final theori postprun ineffici case work perform learn phase undon prune phase natur solut problem would start prune phase simpler theori idea first investig cohen 1993 effici postprun algorithm grow see section 43 combin weak preprun heurist speed learn phase goal preprun context entir prevent overfit reduc amount thu subsequ postprun phase less work less like go wrong howev alway danger predefin stop criterion overgener theori section therefor discuss altern approach search appropri start point postprun phase 51 topdown prune one advantag fossil simpl effici cutoff stop criterion furnkranz 1994 close search heurist fossil need mere comparison heurist valu best candid liter cutoff valu order decid whether add candid liter claus hand properti use gener theori could learn fossil set cutoff paramet see figur 7 basic idea behind algorithm follow assum fossil tri learn theori cutoff 10 unless one liter background knowledg perfectli discrimin posit neg exampl case trivial exampl parentab childba find liter correl 10 thu learn empti theori procedur alltheoriesexampl cutoff 00 theori theori returntheori figur 7 algorithm gener theori learnabl fossil howev rememb liter maximum correl use inform follow way make anoth call fossil cutoff set exactli maximum correl valu least one liter one produc maximum correl ad theori typic follow sever liter correl valu higher new cutoff result new theori usual littl specif predecessor maximum correl liter cut rememb obvious valu old cutoff new maximum theori would learn thu choos valu cutoff next run also expect new theori specif previou one process repeat certain set cutoff liter prune maximumprunedcorrel 00 thu specif theori reach figur 8 show complet seri theori gener fossil 1000 noisefre exampl domain distinguish legal illeg posit kingrook chess endgam set cutoff paramet would yield one six theori train set seen theori gener less gener specif order topdown simpler theori expect accur noisi domain best theori learn iter therefor may possibl stop gener theori soon reason good theori found order avoid expens learn mani overlyspecif theori may save lot work figur 4 indic besid also possibl reus part previou theori point highest cutoff occur total cost gener complet seri concept descript may much higher cost gener mere specif theori least case cutoff occur near end learn theori frequent case base idea conceiv algorithm shown figur 9 use basic algorithm figur 7 find best theori order avoid overgener tri find specif among reason good theori learn fossil use theori start point reduc error prune precis gener theori generaltospecif order evalu design test set data usual 13 stop measur accuraci one theori fall measur accuraci best theori far minu one standard error classif 4 last theori within 1se margin hope littl specif 4 base idea cart breiman friedman olshen stone 1984 gener 6704 correct 0 posit 100 neg e e 8842 correct 6553 posit 9967 neg 9760 correct 9339 posit 9967 neg 9936 correct 9848 posit 9979 neg 9932 correct 9860 posit 9967 neg 9742 correct 9260 posit 9979 neg figur 8 gener seri theori krk domain procedur tdpexampl splitratio exampl growingset pruningset repeat loop exit loop returntheori figur 9 combin pre postprun topdown prune gener subsequ gener use reduc error prune initi generaltospecif search good theori name method topdown prune tdp algorithm succe find start theori close final theori expect algorithm faster basic rep initi search good start theori ffl speed grow phase expens theori gener 5 ffl speed prune phase prune start simpler theori thu number possibl prune oper smaller preliminari experi turn sometim cutoff happen point small fraction avail posit exampl cover clearli theori useless therefor ad constraint theori cover 50 posit exampl grow set evalu prune set theori fulfil criterion improv ad claus achiev lower cutoff valu would need start new claus 6 prune decis tree within one se best select standard classif error comput n p probabl misclassif estim prune set n number exampl prune set 5 argument cours appli noisi domain nonnoisi domain specif theori gener precis thu algorithm slower gener theori cutoff 00 6 note method may yield theori learnabl origin fossil valu cutoff paramet chang gener theori 52 experiment result compar topdown prune tdp reduc error prune rep term accuraci runtim krk endgam domain 10 artifici nois ad setup experi describ detail begin section 71 algorithm split suppli data set grow ca 23 prune set ca 13 algorithm use reduc error prune describ brunk pazzani 1991 postprun phase order exclud possibl influenc underli learn algorithm ran rep use fossil basic learn modul 7 averag accuraci 10 run 100 250 500 750 rep prune 8484 8688 8711 8921 prune 9467 9672 9780 9851 tdp prune 8915 9102 9589 9585 prune 9514 9593 9829 9870 tabl 1 accuraci krk domain 10 nois tabl 1 show tdp wors rep term predict accuraci rep better train set size 250 tdp heavili overprun one case tdp start theori 9842 correct unfortun one liter support prune set consequ prune thu yield theori mere 8134 happen rep got caught 9136 correct theori even get 9842 theori increas train set size tdp seem slightli superior rep although differ probabl small statist signific compar accuraci intermedi theori show tdp start significantli better theori rep see first line tabl 1 obvious topdown search better start theori success particular higher train set size rep sometim get stuck local optimum return bad theori howev seen rep may profit rare case tdp less like get stuck local optimum prune start initi theori alreadi quit close final theori problem local optima greedi hillclimb also like appear tdp topdown search start theori least domain intermedi theori usual appear iter tdp toplevel loop compar runtim rep tdp tabl 2 confirm tdp significantli faster rep fact even faster rep initi phase overfit alon tdp find fairli gener theori rep gener huge theori fit noisi exampl expectedli increas train set size cost rep domin prune process tdp hand even manag decreas prune time grow train set size 250 500 signific runtim increas 500 750 exampl mainli due one 10 set much specif theori learn 85594 cpu sec grow 139935 cpu sec prune time remain 9 set averag runtim 11674 cpu sec grow 1288 cpu sec prune 7 version rep use fossil better version use foil section 71 show result obtain use implement foil gener initi theori rep averag runtim 10 run 100 250 500 750 rep grow 666 7522 39717 84576 prune 293 9146 124848 292266 total 959 16668 164565 376842 tdp grow 723 5137 8017 19066 prune 124 2249 1639 15152 total 847 7386 9656 34218 tabl 2 runtim krk domain 10 nois result confirm tdp exhibit fast converg toward good theori faster rep learn prune start theori learn fossil becom increasingli accur train set grow mean learn faster also less less prune done 6 integr pre postprun algorithm present section motiv observ postprun incompat separateandconqu learn strategi discuss section 42 problem attempt solv postprun approach take account prune claus gener eventu cover exampl train set may influenc evalu candid liter subsequ claus 61 increment reduc error prune basic idea increment reduc error prune irep instead first grow complet concept descript prune thereaft individu claus prune right gener ensur algorithm remov train exampl cover prune claus subsequ claus learn thu avoid exampl influenc learn follow claus figur show pseudocod version algorithm usual current set train exampl split grow usual 23 prune set usual 13 howev entir theori one claus learn grow set liter delet claus greedi fashion delet would decreas accuraci claus prune set result rule ad concept descript cover posit neg exampl remov train grow prune set remain train instanc redistribut new grow new prune set ensur two set contain predefin percentag remain exampl set next claus learn predict accuraci prune claus predict accuraci empti claus ie claus bodi fail claus ad concept descript irep return learn claus thu accuraci prune claus prune set also serv stop criterion postprun method use preprun heurist procedur irep exampl splitratio exampl growingset pruningset negativecov loop exit loop accuracyclausepruningset accuracyfailpruningset exit returntheori figur 10 integr pre post prune increment reduc error prune algorithm prune entir set claus prune one success name increment reduc error prune irep expect irep improv upon postprun algorithm aim solv problem discuss section 42 effici irep asymptot complex size train set significantli lower complex grow overfit theori shown omegagamma n 2 log n assumpt cohen 1993 rep grow one claus pure random data cost n log n approxim logn liter test n exampl irep consid everi liter claus prune ie log n liter evalu n exampl final claus found ie log n time thu cost prune one claus n log 2 n assum size final theori constant overal cost hillclimb similarli grow irep use topdown approach instead rep bottomup final program found remov unnecessari claus liter overli specif theori repeatedli ad claus initi empti theori howev grow still gener intermedi overli specif concept descript irep directli construct final theori separateandconqu strategi irep learn claus order use prolog interpret subsequ rule learn claus complet learn prune cover exampl remov reason problem incompat learn strategi prune strategi appear irep 62 experiment result tabl 3 show comparison runtim postprun algorithm irep krk domain 10 artifici nois ad algorithm use foil inform gain criterion search heurist column initi rule growth refer initi grow phase rep grow common column rep grow give result prune phase total runtim rep grow runtim initi rule growth plu runtim rep grow irep phase tightli integr total valu runtim given domain initi rule growth rep grow irep krk1000 10 212989 2312534 80689 11535 tabl 3 averag runtim obviou irep significantli faster postprun algorithm fact alway faster rep grow initi grow phase alon irep avoid learn intermedi overfit theori also seen grow prune algorithm much faster rep confirm result cohen 1993 order get idea asymptot complex variou algorithm perform loglog analysi cameronjon 1994 estim asymptot complex divid differ logarithm two runtim differ logarithm correspond train set size thu estim slope loglogplot tabul slope adjac train set size tabl 4 domain initi rule growth rep grow irep 100250 261 411 271 154 500750 226 378 315 146 7501000 216 400 279 112 tabl 4 loglog analysi runtim noisi krk data fact tabl suggest irep subquadrat time complex consist conjectur irep time complex omegagamma n log 2 n gener result get consist analysi perform cameronjon 1994 random data surpris view noiselevel degre random data particular evid support result rep complex omegagamma n 4 initi rule grow phase 2 log n shown cohen 1993 also confirm main result cameronjon 1994 name asymptot complex grow asymptot complex initi rule grow phase origin suggest cohen 1993 howev experi absolut valu runtim grow prune phase neglig compar initi overfit phase rep often get caught local maxima abl gener right level interestingli observ despit topdown search strategi grow also occasion overfit nois data phenomenon also predict cameronjon 1994 irep hand stop gener claus whenev found claus support prune set therefor irep expect fast runtim pure random data rep grow expens high chanc first claus fit exampl prune set stop algorithm immedi without accept singl claus thu effect avoid overfit domain initi rule growth rep grow irep krk500 10 8429 9762 9817 9848 krk1000 10 8565 9801 9830 9955 tabl 5 averag accuraci term accuraci tabl 5 irep also superior postprun algorithm although seem sensit small train set size reason bad distribut grow prune exampl may caus irep stop criterion prematur stop learn redistribut exampl new grow prune set learn new claus help littl redund data small sampl size howev larger exampl set size irep outperform algorithm 7 experiment evalu test algorithm present paper varieti domain algorithm implement sicstu prolog major part implement common particular share interfac data use procedur split train set mode type symmetri inform background relat use restrict search space wherev applic inform gain use search heurist rep grow irep fossil correl heurist use fossil tdp runtim measur cpu second sun sparcstat elc 71 summari experi krk domain first summar experi domain recogn illeg chess posit krk endgam muggleton bain hayesmichi michi 1989 domain becom standard benchmark problem relat learn system solv trivial way proposit learn algorithm background knowledg contain relat like sign 10 train instanc deliber revers gener artifici nois data learn concept evalu test set 5000 noisefre ex ampl use stateoftheart relat learner foilquinlan 1990 benchmark 8 foil 61 implement c use default set except v 0 option set avoid introduct new variabl necessari task algorithm argument mode declar input effect prevent recurs algorithm train ident set size 100 1000 exampl report result averag 10 run except train set size 1000 6 run perform complex task algorithm figur show curv accuraci runtim 5 differ train set size irep bad start 8455 accuraci 100 exampl achiev highest accuraci predict accuraci foil poorli stop criterion encod length depend train set size thu weak effect prevent overfit nois 1000 exampl foil learn concept 20 rule incomprehens furnkranz 1994 irep hand consist produc 9957 correct understand 4rule approxim correct concept descript theori correctli identifi illeg posit except one white king black king white rook thu block check would make posit illeg white move postprun approach rep grow equal tdp lose accuraci compar three howev rare find 4th rule specifi white king white rook must squar also seen preprun approach taken fossil need mani exampl order make heurist prune decis reliabl fossil hand fastest algorithm foil although implement c slower increas train set size learn claus fossil see also 1994 rep prove prune method ineffici grow effici prune algorithm still suffer expens overfit phase tdp faster rep grow abl start postprun much better theori rep grow irep howev learn much better theori faster grow prune phase tdp fact irep postprun integr preprun criterion littl slower fossil much accur thu said truli combin merit postprun accuraci preprun effici becom also appar figur 12 accuraci standard deviat observ differ run plot logarithm runtim 8 current version foil avail anonym ftp ftpcssuozau 1297881 file name pubfoilnsh integ n experi perform version 61 train train irep grow fossil foil 6150015002500 100 200 300 400 500 600 700 800 900 1000 run time cpu sec train runtim vs train irep grow fossil foil 61 figur 11 krk domain 10 nois differ train set size runtim cpu sec irep grow fossil foil 61 figur 12 krk domain 10 nois 1000 exampl 72 mesh domain also test algorithm finit element mesh design problem first studi describ detail dolsak muggleton 1992 problem mesh design break complex object number finit element order abl comput pressur deform forc appli object basic problem manual mesh design select optim number finit element edg structur sever author tri ilp method problem dolsak muggleton 1992 dzeroski bratko 1992 quinlan 1994 avail background knowledg consist attributebas descript edg topolog relat edg setup experi quinlan 1994 ie learn rule four five object data set test learn concept fifth object learn theori test quinlan 1994 littl differ setup use dzeroski bratko 1992 instead actual predict valu number finit element edg mere check possibl valu whether valu could deriv learn rule basic differ test ground instanc wherea dzeroski bratko 1992 test target predic unbound valu number finit element posit exampl set also test learn theori neg exampl make sure overgener tabl 6 two number given five set first number accuraci posit exampl second number show accuraci test neg exampl well given runtim total runtim learn prune irep clearli faster postprun algorithm without lose predict accuraci tdp find accur start theori rep shorter time span consequ prune time much shorter rep learn theori littl accur howev tdp faster grow although start prune phase algorithm accuraci fossil 9097 000 1599 initi theori rep grow 8742 3147 635569 grow 8927 2375 988032 initi theori tdp 8899 2889 376294 irep 9014 1281 47125 tabl experi mesh domain simpler theori reason implement tdp use rep prune theori result initi search good start theori might worthwhil improv tdp use grow algorithm postprun phase also indic domain tdp initi topdown search effect krk domain work left postprun phase algorithm faster accur irep fossil cutoff 03 howev fossil couldnt discov signific regular data thu consist learn empti theori liter background knowledg correl 03 nevertheless still best algorithm term accuraci show poorli algorithm domain hope abl improv result domain tri faster algorithm new data set dolsak bratko jezernik 1994 contain total 10 object thu hope provid redund howev compar studi new data set big interest phenomenon although prune liter gener claus posit exampl cover prune theori whole cover fewer posit exampl obvious mani learn rule gener improv accuraci much remov entir rule therefor overal accuraci theori primarili optim delet mani rule cover posit exampl also equal greater number neg exampl also taken evid regular detect basic separateandconqu induct modul reliabl 73 proposit data set also experi data set uci repositori machin learn databas previous use compar proposit learn algorithm appendix holt 1993 give summari result achiev variou algorithm commonli use data set uci repositori short descript set select 9 experi remain set use either descript data set unclear two class could handl implement learn algorithm lymphographi data set remov 6 exampl class normal find fibrosi order get 2class problem data use describ holt 1993 data set task learn definit minor class dataset background knowledg consist relat one variabl breast cancer accuraci stnd dev rang time fossil 7333 456 1766 1968 grow 6846 472 1539 18367 tdp 7174 379 1243 17331 irep 7089 523 1958 2897 hepat accuraci stnd dev rang time fossil 7607 577 2343 21740 rep 7696 393 1080 10228 grow 7645 424 1114 10239 tdp 7942 388 1187 11624 irep 7866 280 734 6040 sick euthyroid accuraci stnd dev rang time fossil 9758 040 135 89140 rep 9755 032 106 504023 grow 9752 047 164 463526 irep 9748 050 170 97070 tabl 7 result breast cancer hepat sick euthyroid domain one constant argument wherev appropri comparison two differ variabl data type allow well experi valu fossil cutoff paramet set 03 runtim dataset measur cpu second sun sparcstat elc except mushroom krkpa7 dataset quit big thu run consider faster sparcstat s10 experi follow setup use holt 1993 ie algorithm train 23 data test remain 13 howev 10 run perform algorithm data set result found tabl 7 8 9 line show averag accuraci 10 set standard deviat rang differ maximum minimum accuraci encount runtim algorithm result c45 decis tree learn system extens noisehandl capabl quinlan 1993 taken experi perform holt 1993 meant indic perform stateoftheart decis tree learn algorithm data set short look show result vari term accuraci quit consist glass g2 accuraci stnd dev rang time fossil 7732 479 1596 21642 rep 7776 431 1473 9331 grow 7563 469 1697 9311 irep 7631 489 1595 6301 vote accuraci stnd dev rang time fossil 9535 117 334 10522 rep 9584 139 392 5741 grow 9563 136 392 5384 tdp 9522 154 449 6217 irep 9475 175 695 2243 vote vi accuraci stnd dev rang time fossil 8907 264 813 8894 rep 8672 346 1078 16326 grow 8749 335 1093 13749 irep 8725 327 1075 3878 tabl 8 result glass vote domain runtim irep fastest algorithm 6 9 test problem secondbest 2 remain 3 tabl also confirm grow usual faster rep tdp result consist faster rep grow case indic initi topdown search good start theori overfit data much initi rule grow phase rep grow fossil runtim unstabl fastest algorithm dataset far slowest data set differ accuraci statist signific 9 signific differ found krkpa7 chess endgam domain tdp fossil perform significantli 1 wors algorithm fossil significantli 5 better tdp vote vi domain outperform 5 sometim 1 algorithm 9 use rang test use quickli determin signific differ medium valu small n 20 sampl size mitteneck 1977 valu 0152 signific level 5 0210 signific level 1 medium valu r rang found tabl 7 9 krkpa7 accuraci stnd dev rang time fossil 9517 266 863 238361 rep 9784 054 201 424308 grow 9748 041 106 421900 irep 9774 036 132 178550 lymphographi 2 class accuraci stnd dev rang time c45 4 class 7752 446 fossil 8722 439 1723 2079 grow 8210 528 1753 1842 mushroom accuraci stnd dev rang time fossil 9996 003 011 353819 grow 9957 066 156 208881 tabl 9 result chess krkpa7 lymphographi mushroom domain lymphographi domain gener c45 seem littl superior algorithm one count result lymphographi rule learn algorithm presum easier 2class task howev rule learn algorithm seem competit allow structur analysi group 9 domain 3 subclass tabl 7 contain domain overfit seem harm ie rep postprun phase significantli least 5 improv upon concept learn initi overfit phase 10 tabl 8 contain domain prune make signific differ final tabl 9 contain domain prune recommend exemplifi mushroom data overfit phase learn 100 correct concept descript significantli better 5 learn prune might justifi argu use separ run prune data comparison main purpos howev compar differ prune approach evalu merit prune result initi overfit phase rep grow tdp may nevertheless indic latter come addit cost algorithm mushroom krkpa7 domain known free nois medic domain tabl 7 noisi therefor assum group domain correspond amount nois contain data 8 conclus paper discuss differ prune techniqu separateandconqu rule learn algorithm convent preprun method effici alway accur postprun method latter howev tend expens learn overspeci theori first addit ineffici point fundament incompat postprun method separateand conquer rule learn system solut investig two method combin integr pre postprun algorithm tdp perform initi topdown search hypothesi space find theori overfit train data still fairli gener theori use start theori subsequ postprun phase tri gener theori appropri level systemat algorithm vari cutoff paramet preprun algorithm fossil provid effici way gener theori generaltospecif order good start theori often found consider less time would need gener specif theori fit train exampl cours prune phase simpler theori also shorter prune phase specif theori irep integr pre postprun one algorithm instead postprun entir theori rule prune right learn experi show approach effect combin effici preprun accuraci postprun domain high redund realworld databas typic larg noisi thu requir learn algorithm effici noisetoler irep seem appropri choic purpos irep tdp deliber design close resembl basic postprun algorithm rep instanc alreadi point tdp improv use grow instead rep tdp postprun phase case irep chosen accuraci prune set basic prune stop criterion order get fair comparison rep concentr methodolog differ postprun irep effici integr pre postprun import advantag postprun method way evalu theori rule irep case entir independ basic learn algorithm prune stop criteria improv perform elimin weak instanc point cohen 1995 accuraci estim lowcoverag rule high varianc therefor irep like stop prematur overgener domain suscept small disjunct problem holt acker porter 1989 cohen 1995 also point defici accuracybas prune criterion show stop criterion base descript length better prune criterion significantli improv irep accuraci without loss effici anoth way improv irep tri furnkranz 1995 irep tri improv upon rep prune rule level instead theori level investig way take tri improv upon irep algorithm prune liter level result algorithm 2 rep seem littl stabl low train set size signific differ runtim could observ appear littl slower irep although asymptot algorithm clearli subquadrat current investig merit avoid loss inform caus need split train set separ grow prune set particular techniqu base wellknown minim descript length principl could provid valuabl altern acknowledg research sponsor austrian fond zur forderung der wissenschaftlichen forschung grant number p10489mat financi support austrian research institut artifici intellig provid austrian feder ministri scienc research would like thank gerhard widmer patient read improv numer version paper r classif regress tree investig noisetoler relat concept learn algorithm rule induct cn2 recent improv cn2 induct algorithm effici prune method separateandconqu rule learn system fast effect rule induct decis tree prune search state space fossil robust relat learner tight integr prune learn tight integr prune learn extend abstract increment reduc error prune concept learn problem small disjunct simpl classif rule perform well commonli use dataset pattern recognit ruleguid infer empir comparison prune method decis tree induc tion planung und statistisch auswertung von experimenten 8th experiment comparison human machin learn formal learn decis rule noisi domain boolean featur discoveri empir learn learn effici classif procedur applic chess end game learn logic definit relat minimum descript length principl categor ori model shortest data descript overfit avoid bia overfit avoid bia tr learn decis rule noisi domain simplifi decis tree boolean featur discoveri empir learn experiment comparison human machin learn formal rule induct cn2 learn nonrecurs definit relat linu c45 program machin learn overfit avoid bia simpl classif rule perform well commonli use dataset fossil learn logic definit relat empir comparison prune method decis tree induct cn2 induct algorithm learn noisi exampl decis tree prune search state space tight integr prune learn extend abstract complex batch approach reduc error rule set induct ctr jo ranilla oscar luac antonio bahamond heurist learn decis tree prune classif rule ai commun v16 n2 p7187 jo ranilla oscar luac antonio bahamond heurist learn decis tree prune classif rule ai commun v16 n2 p7187 april johann frnkranz peter flach roc n rule learn toward better understand cover algorithm machin learn v58 n1 p3977 januari 2005 johann frnkranz round robin classif journal machin learn research 2 p721747 312002 marco muselli diego liberati binari rule gener via ham cluster ieee transact knowledg data engin v14 n6 p12581268 novemb 2002 johann frnkranz separateandconqu rule learn artifici intellig review v13 n1 p354 jan 1999 b kotsianti zaharaki p e pintela machin learn review classif combin techniqu artifici intellig review v26 n3 p159190 novemb 2006