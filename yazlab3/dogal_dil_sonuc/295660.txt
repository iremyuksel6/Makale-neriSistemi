spacetimeeffici schedul execut parallel irregular comput articl investig tradeoff time space effici schedul execut parallel irregular comput distributedmemori machin employ acycl task depend graph model irregular parallel mix granular use direct remot memori access support fast commun propos new schedul techniqu runtim activ memori manag scheme improv memori util retain good time effici provid theoret analysi correct perform work implement context rapid system use inspectorexecutor approach parallel irregular comput runti demostr effect propos techniqu sever irregular applic spars matrix code fast multipol method particl simul experiment result crayt3 show problem larg size solv limit space capac loss execut effici caus extra memori manag overhead reason b introduct consider effort parallel system research spent timeeffici par allel articl investig tradeoff time space effici execut irregular comput memori capac processor limit sinc timeeffici parallel may lead extra space requir definit irregular comput literatur actual clear normal scientif comput code chaotic adapt comput commun pattern consid irregular provid effect system support low overhead irregular problem difficult identifi one key issu parallel system research kennedi 1996 number project address softwar techniqu parallel differ class irregular code wen et al 1995 da et al 1994 lain banerje 1994 gerasouli author address yang depart comput scienc univers california santa barbara ca 93106 c fu siemen pyramid mailstop sj1210 san jose ca 95134 work support part nsf ccr9409695 cda9529418 int9513361 ccr9702640 darpa subcontract umd z883603 preliminari version articl appear 6th acm symposium principl practic parallel program ppopp97 yang c fu et al 1995 fink et al 1996 articl address schedul issu parallel model direct acycl depend graph dag sarkar 1989 mix granular model found use perform predict code optim parallel applic static slowli chang depend pattern spars matrix problem particl simul chong et al 1995 fu yang 1996b gerasouli et al 1995 jiao 1996 problem commun comput irregularli interleav vari granular asynchron schedul fast commun techniqu need exploit data local balanc load low synchron cost howev techniqu may impos extra space requir exampl direct memori access normal much lower softwar overhead highlevel messagepass primit remot address howev need known time perform data access thu access remot space must alloc advanc schedul optim techniqu may prefetch presend data object overlap comput commun may requir extra temporari space hold data object therefor use advanc optim techniqu add difficulti design softwar support achiev high util processor memori resourc articl present two dag schedul algorithm minim space usag retain good parallel time effici basic idea use volatil object earli possibl space releas reus design activ memori manag scheme increment alloc necessari space processor effici execut dag schedul use direct remot memori access provid analysi perform correct techniqu propos techniqu implement rapid program tool fu yang 1996a parallel irregular applic runtim origin schedul execut scheme rapid support increment memori allo cation suffici space rapid deliv good perform sever test irregular program spars choleski factor spars triangular solver fu yang 1997 fast multipol method nbodi simul fu 1997 particular show rapid use parallel spars lu gaussian elimin dynam partial pivot import open parallel problem literatur deliv high megaflop crayt3dt3 fu yang 1996b anoth usag rapid perform predict sinc static schedul rapid predict potenti speedup given dag reason accuraci exampl rapid spars lu code achiev 70 predict speedup fu yang 1996b found size problem rapid solv restrict avail amount memori motiv us studi space optim note form space overhead exist space oper system kernel hash tabl index irregular object task graph articl focus optim space usag dedic store content data object rest articl organ follow section 2 summar relat work section 3 describ comput memori model section 4 present spaceeffici schedul algorithm section 5 discuss use yang c fu delta 3 schedul algorithm rapid activ memori manag scheme execut schedul section 6 give experiment result 2 relat work previou research static dag schedul sarkar 1989 wolski feo 1993 yang gerasouli 1992 1994 address memori issu schedul algorithm dynam dag propos blelloch et al 1995 requir space usag processor 1 sequenti space quirement p total number processor depth dag work provid solid theoret ground spaceeffici schedul space model differ sinc assum global share memori pool model space upper bound impos individu processor stronger constraint cilk runtim system blumof et al 1995 address space effici issu space complex os 1 per processor good gener high solv larg problem space limit memoryoptim techniqu propos articl space usag close 1 p per processor anoth distinct differ work blelloch et al 1995 blumof et al 1995 rapid dag obtain runtim inspector stage execut thu make schedul scheme static model dag grow onthefli comput proce dynam schedul prefer two reason use static schedul 1 practic difficult minim runtim control overhead dynam schedul parallel spars code mix granular distribut memori machin 2 applic problem consid iter natur optim static schedul use mani iter work use hardwar support directli access remot memori avail sever modern parallel architectur workstat cluster ibel et al 1996 stricker et al 1995 schauser scheiman 1995 advantag direct remot memori access identifi fast commun research activ messag von eicken et al 1992 thu expect research benefit result use fast commun support design softwar layer 3 comput memori model comput model consist set task set distinct data object task read write subset data object data depend graph ddg deriv partit code normal three type depend task true depend antidepend output depend poli chronopoulo 1988 ddg anti output depend edg may redund subsum true data depend edg antioutput depend edg elimin program transform articl deal transform depend graph contain acycl true depend extens classic task graph model commut task mark task graph captur parallel aris commut oper detail parallel model describ fu yang 1996a1997 defin term use task model follow yang c fu dag contain set task direct depend edg task data object access task let rt x set object task x read w x set object task x write data object assign uniqu owner processor processor may alloc temporari space object processor p x own call perman object p x call volatil object p x defin permp x set perman data object processor p x v olat ilep x set volatil data object processor p x perman object stay alloc execut owner processor static schedul give processor assign task execut order task processor defin tap x set task execut processor p x let term x execut processor schedul let x denot task x execut immedi given dag g static schedul g schedul graph deriv mark execut edg g eg add edg x g x schedul legal correspond schedul graph acycl x depend edg g task x weight denot predict comput time call x complet task edg x weight denot predict commun delay call c xy send correspond messag two processor use weight inform static schedul algorithm optim expect parallel time execution3711158t 1 b c fig 1 dag b schedul dag 2 processor c anoth schedul yang c fu delta 5 figur 1a show dag 20 task access 11 data object task repres either ij read j write j j read write j tabl list read write set task part b c figur two schedul dag perman object evenli distribut two processor g g owner comput rule use assign task processor exampl assum task messag cost one unit time messag sent asynchron processor overhead send receiv messag ignor tabl read write set task figur 1a notic task graph model use ssa form static singl assign cytron et al 1991 notic differ task modifi data object main reason target applic task graph deriv runtim normal data depend ssa form use would mani object manag spacetimeeffici schedul algorithm need balanc two conflict goal shorten length schedul minim space requir memori suffici hold object execut stage space recycl volatil data necessari follow two strategi use releas space volatil data object certain execut point 1 valu object longer use 2 object longer access discuss section 5 second strategi reduc overhead data manag therefor choos approach strategi space requir estim follow definit 1 given task execut order processor volatil object processor call live task access task access still access futur formal follow hold live 9t x 9t z x otherwis call dead definit 2 let sizem size data object task tw processor p x ie tw 2 tap x comput volatil space requir tw p x live volatil object tw yang c fu memori requir schedul 8px f schedul figur 1b volatil object 3 processor p 1 dead task 310 5 dead 510 assum data object unit size easi calcul mem 9 howev schedul figur 1c mem req 8 lifetim volatil object 7 3 disjoint p 1 space share possibl 4 space time effici schedul given dag p processor schedul approach contain two stage owner comput rule use assign task processor task modifi data object map cluster cluster evenli assign processor exampl given dag figur 1a two processor set task assign processor p 0 g set task processor p 1 ft g base task assign ment determin perman volatil data object processor task processor order follow dag depend edg focu optim task order previous order algorithm call yang gerasouli 1992 propos time effici may requir extra space hold volatil object order aggress execut timecrit task section propos two new order algorithm call mpo dt idea volatil object referenc earli possibl avail local memori shorten lifetim volatil object potenti reduc memori requir processor 41 rcp algorithm briefli discuss algorithm detail descript found yang gerasouli 1992 heurist order task simul execut task let exit task task children given dag time prioriti task x call tp x length longest path task exit task name tp x x exit task otherwis ty child simul execut task call readi parent select execut need data object receiv point schedul cycl processor select readi task highest time prioriti yang c fu delta 7 1 least one unschedul task 2 find processor px earliest idl time 3 schedul readi task tx highest prioriti processor px updat readi task list processor endwhil fig 2 rcp algorithm rcp algorithm summar figur 2 line 4 5 exist descript want illustr differ rcp mpo 42 mpo memorypriorityguid order mpo order algorithm list figur 3 differ mpo rcp 1 prioriti use mpo select readi task size total object space alloc divid size total object space need execut task tie rcp time prioriti tp use break tie 2 mpo need estim total object space alloc schedul cycl updat memori prioriti task line 4 5 1 least one unschedul task 2 find processor px earliest idl time 3 schedul readi task tx highest prioriti processor px alloc volatil object tx use alloc yet processor px 5 updat prioriti tx children sibl processor px updat readi task list processor endwhil fig 3 mpo algorithm exampl figur 1c show schedul produc mpo b produc rcp order differ figur 1b c start time 6 processor 1 time 78 select rcp 310 chosen mpo illustr figur 4 three readi task processor 1 time select 78 longest path exit task path length 4 mpo 310 highest space prioriti 1 3 10 avail local time 6 78 space prioriti 05 space 7 alloc time 6 assum object unit size result mpo schedul requir less memori longer parallel time algorithm complex potenti time consum part updat space prioriti unschedul task line 5 suffici updat space prioriti children sibl newli schedul task task possibl candid readi task next round space prioriti children candid task updat later candid task schedul use e x e x denot number yang c fu 05 readi task list proc 1 b fig 4 schedul scenario time 6 number parenthes next readi task mpo space prioriti rcp time prioriti time remain unschedul task b partial schedul time 6 incom edg outgo edg task x respect complex line 5 number task e number depend edg term e x v use v bound number children parent x may addit complex line 2 3 4 6 v log p v log v vp log v respect p total number processor total number data object sinc p usual small compar v total time complex mpo ove 43 dt dataaccessdirect time slice dt aggress space optim thu intend case memori usag primari import design base fact memori usag processor improv volatil object short life span word time period alloc dealloc short basic idea dt slice comput base data access pattern task task within slice access small group volatil object task schedul physic processor slice slice task within slice order use depend criticalpath inform algorithm given dag set task oper set data object describ step dt algorithm follow 1 construct data connect graph dcg node dcg repres distinct data object edg repres tempor order data access comput cycl may occur access two data object interleav simplic use name data object correspond data node unless caus confus construct dcg follow rule appli base origin dag task x 2 v use modifi data object x associ data object node yang c fu delta 9 x modifi one object may use object compu tation use object x associ modifi object possibl task associ multipl data node case doubli direct edg ad among data node make strongli connect direct edg ad data node data node j exist task depend edg x x associ data node associ data node j last two rule reflect tempor order data access comput step 2 deriv strongli connect compon dcg edg compon constitut dag task appear one compon compon associ set task usemodifi data object compon defin one slice task slice consid schedul togeth runtim processor execut task slice slice follow topolog order slice impos depend among correspond strongli connect compon note topolog order slice impos constraint task order processor assign task follow owner comput rule must suppli use dt produc actual schedul step 3 use prioriti base preced schedul approach gener dt schedul slice deriv step 2 prioriti assign task base slice belong two readi task slice task higher criticalpath prioriti schedul first readi task slice prioriti lower unschedul task processor task schedul task higher slice prioriti processor schedul use prioriti guarante task execut accord deriv slice order exampl figur 5 show exampl dt order dag figur 1a part dcg mark node correspond data name task within rectangl associ correspond data object sinc dcg acycl data node maxim strongli connect compon treat one slice topolog order node produc slice processor execut task follow slice order shown part b slice mark numer illustr execut order memori requir mem req 7 compar 9 figur 1b produc rcp 8 figur 1c mpo hand schedul length increas rcp mpo dt less less critic path inform use algorithm complex step 1 complex deriv access pattern task ie read andor write access data object oe log v complex map task data node ov complex gener edg data node oe log check need prevent duplic edg ad thu total complex step 1 v step 2 deriv strongli connect compon cost yang c fu3711158t1416 d357 slice 1 slice 2 slice 4 slice 5 slice 6 slice 1 slice 2 slice 3 slice 4 slice 5 slice 6 slice 7 b fig 5 sampl dcg deriv dag b dt schedul dag 2 processor gener preced edg among slice cost om log topolog sort slice cost om e therefor total complex step 2 om cost step 3 ov log v e give overal complex dt oelog v number task e number edg number data object origin dag space effici dt lead good memori util follow theorem give memori bound dt first definit introduc definit 3 given processor assign r task volatil space requir slice l processor p x denot vpx r l defin amount space need alloc volatil object use execut task l p x maximum volatil space requir l r defin assum processor assign r task use owner comput rule produc even distribut data space perman data object among processor show follow result theorem 1 given processor assign r task dt schedul processor slice order l 1 schedul execut space usag per processor 1 sequenti space complex proof first sinc r lead even distribut perman object perman data space need processor 1 p suppos task yang c fu delta 11 x slice l need alloc space volatil object enough space accord definit h 1 claim space alloc volatil data object associ slice l 1 freed therefor extra h space processor enough execut task l need show claim correct suppos volatil data object 0 dealloc slice l igamma1 0 associ l least one task 2 l j j use 0 modifi 0 0 perman data object modifi 0 accord dt algorithm belong slice l instead l j thu contradict 2 dcg acycl data node dcg constitut strongli connect compon therefor slice associ one data object impli h defin theorem 1 size largest data object thu follow corollari corollari 1 dcg task graph acycl maximum size object unit 1 dt produc schedul execut processor use 1 per processor 1 sequenti space complex appli theorem 1 corollari 1 import applic graph 1d columnblockbas spars lu dag fu yang 1996b matrix partit set spars column block task kj use one spars column block k modifi column block j figur 1 actual spars lu graph dt produc acycl dcg 1d columnblockbas spars lu task graph illustr figur 5a let w maximum space need store column block accord corollari 1 processor need w volatil space execut dt schedul spars lu 2d blockbas spars choleski approach describ fu yang 1997 matrix divid n spars column block column block divid n submatric submatrix index j mark ij choleski task graph structur n layer layer repres elimin process use column block k modifi column block k 1 n specif choleski factor comput diagon block kk use scale nonzero submatric within kth column block ie ik nonzero submatric use updat rest matrix ie ij updat task step k belong slice associ data object ik henc extra space need execut slice summat submatric column block k accord theorem 1 dt schedul choleski execut 1 p w space processor w maximum space need store column block result summar follow corollari normal w dt schedul space effici two problem corollari 2 1d columnblockbas spars lu task graph 2d blockbas spars choleski graph dt schedul execut use 1 yang c fu space 1 i2 k merg l l 0 space els space fig 6 dt slicemerg algorithm per processor w size largest column block partit input matrix optim avail memori space processor known say av ail mem time effici dt algorithm optim merg sever consecut slice memori suffici slice appli prioritybas schedul algorithm merg slice assum k slice valid slice order given task assign r merg strategi summar figur 6 set new slice gener sinc calcul memori requir slice take oe log time complex merg process ove log shown merg algorithm produc optim solut given slice order theorem 2 given order slice sequenc slicemerg algorithm figur 6 produc solut minimum number slice proof theorem proven contradict let new slice sequenc produc algorithm figur 6 optim sequenc u e f slice contain set consecut l slice l 1 merg algorithm figur 6 group mani first l slice possibl hre 1 take origin l slice slice f 2 add f 1 new f 1 ident e 1 let new f 2 call 2 thu produc optim sequenc appli transform f 0 2 compar final transform sequenc f 1 anoth optim sequenc contradict sinc new sequenc complet cover l slice unless slice empti 2 note optim merg algorithm restrict given slice order interest topic studi exist slice sequenc algorithm follow partial order impli given dcg lead minimum number slice minimum parallel time shown tang 1998 heurist use binpack techniqu develop number slice within factor two optimum yang c fu delta 13 5 memori manag schedul execut section first briefli describ rapid runtim system schedul techniqu appli discuss necessari runtim support effici execut dag schedul deriv propos schedul algorithm 51 rapid system depend transform analysi depend task schedul cluster user specif task data object data access pattern data depend graph ddg complet task graph iter asynchron task assign data object owner schedul execut fig 7 process runtim parallel rapid figur 7 show runtim parallel process rapid circl action perform system box either side circl repres input output action api rapid includ set librari function specifi irregular data object task access object inspector stage depict left three circl figur 7 rapid extract dag data access pattern produc execut schedul executor stage rightmost circl schedul comput execut er sinc target applic iter natur exampl spars matrix factor use extens solv set nonlinear differenti equat numer method newtonraphson iter spars depend graph deriv jacobian matrix spars pattern jacobian matrix remain one iter anoth nine applic studi karmarkar 1991 typic number iter execut comput graph rang 708 16069 averag 5973 thu optim cost spent inspector stage pay long simul problem shown fu 1997 rapid inspector stage test spars matrix factor triangular solver fast multipol method fmm rel larg problem size take 12 total time schedul reus 100 iter inspector idea found previou scientif comput research georg liu 1981 inspector optim call preprocess compar previou spectorexecutor system irregular comput da et al 1994 executor phase rapid deal complic depend structur executor stage rapid use direct remot memori access rma execut schedul deriv inspector stage rma avail modern multiprocessor architectur crayt3 shmem meiko cs2 dma sci cluster memori map rma processor write memori processor given remot address rma allow data transfer directli sourc destin locat without buffer impos much lower overhead higherlevel commun layer mpi use rma complic design runtim execut control data consist yang c fu howev find dag gener rapid satisfi follow properti simplifi design distinct data object task x receiv data object identif differ parent readwrit depend path either x x depend path either x x d4 dag sequenti task execut consist sequenti execut follow topolog sort dag name 2 rt x valu x read memori execut produc one x parent gener dag satisfi d1 d2 d3 may alway sequentializ yang 1993 dag properti call dependencecomplet exampl dag discuss figur 1 section 6 dependencecomplet ddg deriv sequenti code transform dependencecomplet dag fu yang 1997 52 execut scheme activ memori manag maintain reus data space execut new research topic complic use lowoverhead rmabas commun sinc remot data address must known advanc discuss two issu relat execut dag schedul rma 1 address consist address data object processor becom stale valu data object longer use space object releas use classic cach coher protocol maintain address consist introduc substanti amount overhead taken simpl approach volatil object consid dead object name access processor way volatil object name alloc processor strategi lead slightli larger memori requir reduc complex maintain address consist memori requir estim section 3 follow design strategi 2 address buffer also use rma transfer address sinc address packag sent infrequ use address buffer processor send new address inform unless destin processor read previou address packag reduc manag overhead execut model use activ memori manag scheme present map memori alloc point insert dynam two consecut task execut processor first map alway begin execut processor map follow dealloc space dead volatil object dead inform static calcul perform data flow analysi given dag complex proport size graph yang c fu delta 15 map alloc d8 map alloc d1 d3 d5 addr d1 d3 d5 d8 addr d3 suspend send d3 d5 suspend d7 map alloc d7 addr d7 send d7 send d8 map stop data object readi map ye rec end ra cq next task send addr complet comput b fig 8 map execut schedul figur 2c bthe control flow processor alloc volatil space task execut current point execut chain assum remain task processor alloc stop k enough space execut k1 next map right k1 assembl address packag processor address packag may differ depend object access processor figur 8a illustr map address notif execut schedul figur 1c avail amount memori 8 processor 2 unit memori volatil object p 1 addit map begin task chain anoth map right task 510 1 space 3 5 freed space 7 alloc address 7 p 1 sent p 0 p 0 send content 7 p 1 receiv address 7 figur 8b show control flow execut scheme system five differ state execut 1 rec wait receiv desir data object processor rec state proceed object current task need avail local 2 exe execut task 3 snd send messag remot address messag avail messag enqueu map processor could block map state attempt send address packag processor previou address packag consum destin processor end state processor execut task still need clear send queue might block address suspend yang c fu messag still unavail three block state ie state selfcycl figur 8b follow two oper must conduct frequent order avoid deadlock make execut evolv quickli ra read new address packag deliv suspend messag address avail 53 analysi deadlock consist theorem 3 given dag g legal schedul execut activ memori manag deadlock free name system eventu execut task proof assum commun processor reliabl prove theorem induct use follow two fact proof fact 1 deadlock situat happen processor block wait cycl eg circular chain state rec map end eventu processor circular chain two thing ra cq space alloc releas activ complet fact 2 processor wait receiv data object local address data object must alreadi notifi processor processor alway alloc space send address object use object let g 0 schedul graph g g 0 acycl given schedul legal without loss gener assum topolog sort g 0 produc linear task order induct follow order induct base 1 must entri task g 0 ie task without parent execut 1 processor call map execut complet space alloc p x send newli creat address deadlock occur processor involv circular wait chain p x block state map await avail address buffer destin processor ra cq sinc destin processor must circular chain also ra cq accord fact 1 address buffer eventu free newli creat p x sent p x abl leav map state parent data 1 need avail local henc 1 complet success induct assumpt assum task x 1 x execut k parent g 0 complet execut show k execut success suppos ie deadlock occur let p x k processor state p x either map rec state end discuss follow two case case 1 p x state rec induct assumpt reason p x receiv data object k object sent yang c fu delta 17 remot processor p sinc k parent finish caus p send data object unavail remot address p x accord fact 2 address must alreadi sent p p x wait receiv object henc p eventu read address oper ra fact 1 deliv messag p x therefor p x execut k case 2 p x state map situat one discuss induct base processor abl leav map state 2 theorem 4 given dependencecomplet dag g legal schedul execut activ memori manag consist name task read data object produc parent specifi g proof theorem 3 task execut runtim scheme depend edg x check inde read object produc x execut illustr figur 9 two case could read inconsist copi prove contradict imposs assum schedul processor p x schedul p x time time depend pathedg writesend object case 1 case 2 fig 9 illustr two case prove theorem 4 case 1 sendersid inconsist p 6 p x execut x p x tri send p messag may suspend destin address may avail sinc buffer use content p x may modifi actual sent p time assum case true let u task overwrit processor sent p u must intend produc anoth task v p x sinc execut u x happen v accord properti d2 must exist depend path x v u accord properti d3 must exist depend path u x yang c fu exist depend path x u order among x u sequenti execut must x would abl read copi produc x contradict properti d4 exist depend path u x similarli show v would abl read produc u sequenti execut contradict properti d4 case 2 receiversit inconsist object produc x success deliv local memori p content p may overwritten anoth task call u time assum case true let v task assign p task suppos read produc u accord properti d1 v 6 illustr figur 9 accord properti d2 d3 depend structur among case 1 similarli find contradict 2 6 experiment result implement propos schedul heurist activ memori manag scheme rapid crayt3dt3 meiko cs2 section report perform approach t3e three irregular program spars choleski factor 2d block data map rothberg 1992 rothberg schreiber 1994 fu yang 1997 task graph static depend structur long nonzero pattern spars matrix given runtim preprocess stage spars gaussian elimin lu factor partial pivot problem unpredict depend storag structur due dynam piv ote parallel sharedmemori platform address li 1996 howev effici parallel distributedmemori machin still remain open problem scientif comput literatur use static factor approach estim worstcas depend structur storag need fu yang 1996b show approach overestim space much test matric rapid code deliv breakthrough perform t3dt3e 1 fast multipol method fmm simul movement nonuniformli distribut particl given irregular particl distribut spatial domain divid box differ level repres hierarch tree leaf contain number particl iter particl simul fmm comput consist upward downward pass tree end iter particl may move one leaf anoth comput commun weight dag repres fmm comput may chang slightli sinc particl movement normal slow dag repres fmm comput reus mani iter found jiao 1996 static schedul reus approxim 100 iter without much perform degrad recent optim code use special schedul mechan elimin rapid control overhead set new perform record shen et al 1998 yang c fu delta 19 detail descript fmm parallel use rapid found fu 1997 first examin memorymanag scheme impact parallel perform space limit studi effect schedul heurist reduc memori requir reason present order propos schedul algorithm effect without proper runtim memori manag present order also allow us separ impact runtim memori manag new schedul algorithm t3e machin use 128mb memori per node blas3 gemm routin dongarra et al 1988 achiev 388 megaflop rma primit shmem put achiev 052 overhead 500mbsec peak bandwidth test matric use articl harwellbo matrix bcsstk29 aris structur engin analysi spars choleski goodwin matrix fluid mechan problem spars lu matric medium size 2 solvabl one three schedul heurist compar perform fmm use distribut 64k particl experi test case reach similar conclus report parallel time differ memori constraint manual control avail memori space processor 75 50 40 25 tot tot total memori space need given task schedul without space recycl obtain tot calcul sum space perman volatil object access processor let tot maximum valu among processor 61 rapid without activ memori manag 7051525perform spars choleski factor t3e processor 70103050perform fast multipol method t3e processor b fig 10 speedup without memori optim spars choleski b fmm 2 bcsstk29 dimens 13992 18 million nonzero includ fillin goodwin dimens 7320 35 million nonzero includ fillin yang c fu tabl ii absolut perform megaflop spars lu partial pivot matrix p2 p4 p8 p16 p32 p64 goodwin 736 1357 2380 3737 5226 6558 rapid without memori manag figur 10 tabl ii show overal perform rapid t3e without use memori optim three test program version rapid recycl space executor stage result serv comparison basi assess perform memori manag scheme note speedup choleski fmm compar highqual sequenti code result consist previou work rothberg 1992 jiao 1996 speedup choleski reason sinc deal spars matric speedup fmm high leaf node fmm hierarch tree normal computationintens suffici parallel spars lu sinc approach use static symbol factor overestim comput list megaflop perform calcul megaflop use accur oper count superlu li 1996 divid correspond numer factor time rapid activ memori manag tabl iii examin perform degrad use activ memori manag rcp still use task order show later much improv space effici obtain use dt mpo result tabl spars choleski bcsstk29 spars lu goodwin fmm differ space constraint column pt inc ratio parallel time increas use memori manag scheme comparison base parallel time rcp schedul 100 memori avail without memori manag overhead entri mark 1 impli correspond schedul nonexecut memori constraint result basic show trend perform degrad increas number processor increas avail memori space decreas overhead contribut address notif space recycl howev degrad reason consid amount memori save exampl memori manag scheme save 60 space choleski parallel time degrad 6493 observ schedul like execut reduc memori capac number processor increas processor lead volatil object processor give memori manag scheme flexibl alloc dealloc space even 40 maximum memori requir schedul activ memori manag still execut 16 32 processor rapid without support fail execut tabl iii also list averag number map requir one processor processor use fewer map requir sinc less space need store perman object processor note fmm execut time activ memori manag sometim even shorter without memori manag explan yang c fu delta 21 tabl iii perform degrad use activ memori manag map pt inc map pt inc map pt inc p8 200 381 300 421 500 641 p32 200 492 294 727 322 943 lu 75 50 40 map pt inc map pt inc map pt inc fmm 75 50 40 map pt inc map pt inc map pt inc p32 200 115 300 115 500 185 although comput associ leaf node particl partit tree intens mix much intens commun incur downward upward pass compar choleski lu interprocessor messag fmm downward upward pass insert memorymanag activ enlarg gap consecut commun messag lead less network content overhead map three type memori manag activ result time increas ra cq map experi found deliveri address packag map never hinder wait previou content address buffer consum tabl iv report overhead impos map clear overhead insignific compar total time increas studi tabl iii howev activ frequent address checkingdeliv oper prolong messag send caus execut delay task critic path 62 effect comparison memoryschedul heurist subsect compar memori time effici rcp mpo dt memori scalabl first examin much memori save use mpo dt defin memori scalabl memori reduct ratio 1 sequenti space requir p space requir per processor schedul produc algorithm p processor 22 delta yang c fu tabl iv map overhead term percentag total execut time 75 50 40 75 50 40 75 50 40 p32 154 120 100 102 68 50 43 32 33 comparison memori requir spars choleski processor memori requir reduct ratio x mpo comparison memori requir spars lu processor memori requir reduct ratio x mpo b comparison memori requir fmm processor memori requir reduct ratio x mpo c fig 11 memori scalabl comparison three schedul heurist spars choleski b spars lu c fmm figur 11 show memori reduct ratio three schedul algorithm choleski lu fmm uppermost curv graph 1 p perfect memori scalabl figur show mpo dt significantli reduc memori requir dt memori requir close optimum choleski lu case consist corollari 1 2 hand rcp time effici memori scalabl particularli spars lu fmm find dt algorithm result singl slice ie task belong slice reason lot yang c fu delta 23 depend among task dt actual reduc rcp thu experi show allow complex increas oelogvmv log v oev mpo appli schedul task within slice instead rcp improv space effici time differ rcp mpo dt also compar parallel time differ among three heurist tabl v vi differ memori constraint two tabl algorithm compar b ie vs b entri mark indic correspond b schedul execut memori constraint schedul mark indic b schedul nonexecut tabl v increas parallel time rcp mpo rcp vs mpo ratio p4 96 110 111 lu 100 75 50 40 25 fmm 100 75 50 40 25 tabl show actual parallel time increas switch rcp mpo averag increas reason sometim mpo schedul outperform rcp schedul even though predict parallel time rcp shorter although mpo use much criticalpath inform rcp reduc number map need improv execut effici furthermor reus object soon possibl potenti improv cach perform factor mix togeth make actual execut time mpo schedul competit rcp dt aggress memori save util criticalpath inform comput slice tabl vi show time slowdown use dt instead mpo clear mpo substanti outperform dt term execut time even though dt effici memori usag differ especi signific larg number processor mpo optim memori usag parallel time howev time need dt instanc lu case 25 avail memori dt schedul yang c fu tabl vi increas parallel time mpo dt mpo vs dt ratio lu 100 75 50 40 25 p8 435 374 322 55 execut 16 processor mpo schedul space costli run note dt space effici improv use mpo schedul slice slice merg dt avail amount memori space known dt schedul optim slicemerg process call dtsm discuss section 43 list time reduct ratio use slice merg tabl vii dt vs dtsm result encourag case substanti improv obtain result parallel time dt schedul slice merg get close rcp schedul merg slice give schedul flexibl util criticalpath inform dt also effect improv cach perform thu dt algorithm slice merg valuabl problem size big avail amount space known tabl vii reduct parallel time dt dtsm ratio p4 613 485 290 729 lu 100 75 50 40 25 p32 5055 3996 3885 3456 2395 impact solvabl problem size new schedul algorithm help solv problem unsolv origin rapid system optim space usag exampl previous biggest matrix could solv yang c fu delta 25 use rapid lu code e40r0100 contain 958 million nonzero fillin use runtim activ memori manag dt schedul algorithm rapid abl solv larger matrix call ex11 268 million nonzero achiev 9785 megaflop 64 t3e node term singlenod perform get 387 megaflop per node 16 node 137 megaflop per node 64 node consid code parallel softwar tool number good t3e 7 conclus optim memori usag import solv larg parallel scientif problem softwar support becom complex applic irregular comput data access pattern main contribut work develop schedul optim techniqu effici memori manag scheme support use fast commun primit avail modern processor architectur propos techniqu integr rapid runtim system achiev good time space effici theoret analysi correct memori perform corrobor design techniqu experi spars matrix fmm code show overhead introduc memori manag activ reason mpo heurist competit criticalpath schedul algorithm deliv good memori time effici dt aggress memori save achiev competit time effici slice merg conduct space effici improv incorpor mpo slice schedul note propos techniqu use semiautomat program tool rapid still challeng develop fulli automat system futur interest studi automat gener coarsegrain dag sequenti code cosnard loi 1995 extend result complic depend structur chakrabarti et al 1995 girkar polychronopoulo 1992 ramaswami et al 1994 investig use propos techniqu perform engin parallel system darpa 1998 massiv parallel distributedmemori machin still valuabl highend largescal applic problem futur eg doe asci program extens smp cluster use dt schedul actual also improv cach perform use result data placement smp memori hierarchi need studi acknowledg would like thank apostolo gerasouli keshav pingali ed rothberg vivek sarkar rob schreiber kathi yelick comment work anonym refere siddhartha chatterje vegard holmedahl valuabl feedback improv present theorem 2 point one refere also thank xiangmin jiao help implement rapid jia jiao provid us fmm code test case xiaoy li provid lu test matric r provabl effici schedul cilk effici multithread runtim system model benefit mix data task parallel multiprocessor runtim support finegrain irregular dag automat task graph gener techniqu effici comput static singl assign form control depend graph httpwww commun optim irregular scientif comput distribut memori architectur extend set basic linear algebra subroutin flexibl commun mechanismsfor dynam structur applic schedul runtim support parallel irregular comput spars lu factor partial pivot distribut memori machin also ucsb technic report trcs9703 comput solut larg spars posit definit system schedul structur unstructur comput automat extract functin parallel ordinari program implement activ messag splitc sci cluster architectur implic softwar support parallel process irregular dynam comput new parallel architectur spars matrix comput base finit project geometri high perform fortran problem progress spars gaussian elimin high perform comput parallel program compil exploit memori hierarchi sequenti parallel spars choleski factor improv load distribut parallel spars choleski factor partit schedul parallel program execut multiproc sor experi activ messag meiko cs2 elimin forest guid 2d spars lu factor decoupl synchron data transfer messag pass system parallel comput person commun activ messag mechan integr commun comput runtim support portabl distribut data structur program parititon numa multiprocessor comput sy tem schedul code gener parallel architectur comput scienc list schedul without commun delay parallel comput dsc schedul parallel task unbound number processor revis juli tr algorithm 656 extend set basic linear algebra subprogram model implement test program effici comput static singl assign form control depend graph new parallel architectur spars matrix comput base finit project geometri activ messag program partit numa multiprocessor comput system list schedul without commun delay techniqu overlap comput commun irregular iter applic commun optim irregular scientif comput distribut memori architectur schedul code gener parallel architectur improv load distribut parallel spars choleski factor provabl effici schedul languag finegrain parallel model benefit mix data task parallel decoupl synchron data transfer messag pass system parallel comput runtim compil parallel spars matrix comput runtim techniqu exploit irregular task parallel distribut memori architectur elimin forest guid 2d spars lu factor spars lu factor partial pivot distribut memori machin partit schedul parallel program multiprocessor parallel program compil comput solut larg spars posit definit automat extract function parallel ordinari program experi activ messag meiko cs2 flexibl commun mechan dynam structur applic softwar support parallel process irregular dynam comput spars gaussian elimin highperform comput schedul runtim support parallel irregular comput ctr roxan adl marc aiguier franck delaplac toward automat parallel spars matrix comput journal parallel distribut comput v65 n3 p313330 march 2005 heejo lee jong kim sung je hong sunggu lee task schedul use block depend dag blockori spars choleski factor proceed 2000 acm symposium appli comput p641648 march 2000 como itali heejo lee jong kim sung je hong sunggu lee task schedul use block depend dag blockori spars choleski factor parallel comput v29 n1 p135159 januari