fileaccess characterist parallel scientif workload abstractphenomen improv comput perform multiprocessor match compar gain io system perform imbal result io becom signific bottleneck mani scientif applic one key overcom bottleneck improv perform multiprocessor file system design highperform multiprocessor file system requir comprehens understand expect workload unfortun recent gener workload studi multiprocessor file system conduct goal charisma project remedi problem character behavior sever product workload differ machin level individu read write first set result charisma project describ workload observ intel ipsc860 think machin cm5 paper intend compar contrast two workload understand essenti similar differ isol common trend platformdepend varianc use comparison abl gain insight gener principl guid multiprocessor filesystem design b introduct grow imbal comput perform io subsystem perform multiprocessor imbal result io becom signific bottleneck mani scientif applic thu clear need improv design highperform parallel file system enabl meet io need applic success system design must possess thorough understand system like use understand system polici mechan optim case expect common system work load design far forc reli specul parallel file system would use extrapol filesystem character generalpurpos workload uniprocessor distribut system scientif workload vector supercomput address limit initi charisma project june 1993 character io scientif multiprocessor applic varieti product parallel comput platform site 1 work done studi io need parallel scientif applic typic examin small number select appli cation charisma project uniqu record individu read write request live multiprogram parallel workload far complet character studi intel ipsc860 nasa ame research center 1 think machin cm5 nation center supercomput applic 2 system address similar set question ffl job mix look like mani job run concurr mani processor job use ffl mani file read written 1 charisma may found httpwwwcsdartmoutheduresearchcharismahtml ffl typic read write request size space file access sequenti way ffl overal implic parallel filesystem design paper address final question integr result observ across multipl platform end use result two machinespecif studi tri identifi observ hold across variou multiprocessor platform pinpoint characterist appear specif singl platform environ next section describ previou studi multiprocessor file system filesystem workload describ two platform examin studi section 3 outlin research method section 4 present result section 5 draw overal conclus background section review mani previou studi filesystem workload outlin basic design current multiprocessor file system final describ design two platform trace intel ipsc860 think machin cm5 21 workload character classifi previou filesystem workload studi character generalpurpos workstat workstat network scientif vector applic scientif parallel applic generalpurpos workstat uniprocessor file access pattern measur mani time floyd elli 3 4 ousterhout et al 5 measur isol unix work station baker et al measur distribut unix system sprite 6 ramakrishnan et al 7 studi access pattern commerci comput environ vaxvm platform studi cover generalpurpos engin offic workload uniprocessor applic studi identifi sever characterist common among uniprocessor filesystem workload file tend small kilobyt tend access small request tend access complet sequenti ie byte file access order begin end scientif vector applic studi specif examin scientif workload vector machin del rosario choudhari provid inform character grandchalleng applic 8 powel measur set static characterist file size cray1 file system 9 miller katz trace specif iointens cray applic determin perfil access pattern 10 focus primarili access rate miller katz also measur secondarytertiari file migrat pattern cray 11 give good pictur longterm wholefil access pattern pasqual polyzo studi io intens cray applic focus pattern io rate 12 13 studi limit singleprocess applic vector supercomput studi identifi sever characterist common among supercomput filesystem workload unlik workstat filesystem workload file tend larg mani megabyt gigabyt tend access larg request like workstat workload file typic access complet sequenti scientif parallel applic experiment studi io parallel scientif program run multiprocessor rare crockett 14 kotz 15 hypothes charact parallel scientif filesystem workload reddi banerje chose five sequenti scientif applic perfect benchmark parallel eightprocessor alliant find sequenti fileaccess pattern 16 studi interest far need sampl size small program parallel sequenti program parallel program per se io parallel cypher et al 17 studi individu parallel scientif applic measur tempor pattern io rate galbreath et al 18 present use highlevel character base anecdot evid bagrodia et al 19 propos use pablo analyz character specif applic crandal et al perform analysi three scientif applic 20 part charisma project trace parallel io request live product mix user program intel ipsc 1 cm5 2 studi includ one machin program platform 22 exist parallel file system singl coher model parallel fileaccess yet emerg parallelio model often close tie particular machin architectur well program model nonetheless common characterist increas parallel parallel file system declust block file across mani disk access parallel extend tradit file abstract growabl address linear sequenc byte parallel fileaccess method common provid io mode specifi whether parallel process share common file pointer 14 21 22 23 24 25 system base memorymap interfac 26 27 two provid way user specifi perprocess logic view file 28 29 provid simd style transfer 30 31 25 18 final addit share file pointer mpiio allow applic describ map linear file comput node run applic term higherlevel data structur 32 clearli industri research commun yet settl singl new model file access thu aspect parallel filesystem workload depend particular model provid user implic fact studi discuss throughout paper whenev depend appar 23 system studi use system design workload character must base realist workload similar expect use futur purpos meant trace multiprocessor file system use product scientif comput intel ipsc860 nasa ame numer aerodynam simul facil met criterion think machin cm5 nation center supercomput applic ncsa 231 intel ipsc860 concurr file system ipsc860 distributedmemori messagepass mimd machin comput node base intel i860 processor connect hypercub network io handl dedic io node connect singl comput node rather directli hypercub interconnect io node base intel i386 processor control singl scsi disk drive may also one servic node handl thing ethernet connect interact shell 33 time studi ipsc860 na 128 comput node 10 io node comput node 8 mb memori io node 4 mb memori singl 760 mb disk drive 34 also singl servic node handl 10mbit ethernet connect host comput total io capac 76 gb total bandwidth less 10 mb concurr file system cf stripe file across disk 4 kb block request sent directli comput node issu request appropri io node servic sinc ipsc mimd machin comput node oper independ one anoth assist programm coordin access independ comput node singl share file cf provid four io mode mode 0 default mode give process file pointer mode 1 share singl file pointer among process mode 2 like mode 1 enforc roundrobin order access across node mode 3 like mode 2 restrict access size ident detail cf perform found 21 35 36 232 think machin cm5 scalabl file system cm5 distributedmemori machin mani ten thousand sparcbas process node small number control processor cp process node logic group partit manag cp job execut singl partit gener process node partit execut pro gram although may execut differ instruct spmdstyle within individu partit job timeshar process node commun via two scalabl interprocessor commun network 37 although possibl user job run differ partit commun one anoth rare done practic cm5 support varieti io devic 37 38 studi focus scalabl array sda primari highvolum highbandwidth storag devic cm5 ncsa sda expand raid3 disk system typic provid io bandwidth 33264 mbsec scalabl file system sf enhanc unix file system extens support parallel io larg file although fulli gener file system sf optim parallel highvolum transfer trace project cm5 ncsa 512 node gener divid 5 static partit size 32 32 64 128 256 node partit cm5 reconfigur time machin reconfigur singl 512node partit node singl cpu network interfac 4 vector unit collect memori size 32 mbnode sda singl file system distribut across 118 data disk 1 pariti disk total capac 138 gb logic block size file system 295 kb physic disk block size 59 kb cm5 support two primari program model dataparallel controlparallel io model paper character io program written cmf dataparallel fortran dialect cmmd controlparallel messag librari cmf program model present singl thread control user node appear execut ident code though may oper differ data cmf io librari support routin layer top sf allow user read write array portion thereof sda via either special librari call normal fortran read write statement sinc singl thread control everi io request collect whenev applic issu io request everi node applic must particip request issu data distribut io parallel hidden user cmmd librari may use varieti familiar program languag eg c c f77 like ipsc provid user independ thread control process node cmmd io also layer top sf like cf provid varieti io mode 39 23 cmmd localindepend mode like mode 0 cf give process view file allow process make arbitrari request file globalindepend mode process privat file pointer state share exampl one process perform ioctl chang block mode block mode chang everi process cmmd synchronoussequenti mode like cfss mode 2 everi node must particip io request may request differ amount data data read written contigu region file node request satisfi roundrobin order final mode synchronousbroadcast everi node access exact region file possibl write data mode like use read header inform share configur file ncsa cmf user outnumb cmmd user factor 7 3 40 3 method given divers multiprocessor file system possibl construct architectureindepend workload studi thu import studi varieti platform compar contrast result product workload multipl platform may deriv sever benefit first strong common trend one confid make gener use parallel filesystem design second studi variou platform pinpoint platform environmentdepend characterist may use design new file system similar platform environ section describ method collect analyz data two differ platform 31 trace collect charisma trace file begin header record contain enough inform make file selfdescript continu seri event record one per event ipsc860 one trace file collect entir file system trace io involv concurr file system mean io done standard input output host file system limit sequenti ethernet speed record collect data 156 hour period 3 week februari 1994 trace continu whole 3 week tri get realist pictur whole workload trace differ time day week includ night weekend period cover singl trace file rang minut 22 hour longest continu trace period 625 hour trace usual initi machin idl case job run began trace job trace trace stop one two way manual full system crash machin usual idl trace manual stop ipsc860 highlevel cf call implement runtim librari link user program instrument librari call gener event record time call sinc instrument almost entir within userlevel librari job whose file access trace includ system program eg cp ftp well user program relink period trace instrument librari default user wish applic trace option link uninstru librari regardless whether applic trace abl record job start end separ mechan trace 3016 job run 2237 run singl node actual trace least 429 779 multinod job least 41 singlenod job tremend number singlenod job system program surpris necessarili undesir mani untrac particular one singlenod job run period account 800 singlenod job simpli check statu machin way distinguish untrac job trace job cf io number trace job lower bound one primari concern minim degre measur perturb workload reduc network content local percal overhead buffer event record node sent central trace collector buffer full sinc larg messag ipsc broken 4 kb block chose buffer size buffer allow us reduc number messag sent collector well 90 without steal much memori user job trace record written file system trace care minim effect perform well creat larg buffer data collector write data cf larg sequenti block sinc data collector link noninstru librari use file system record simpl benchmark instrument librari reveal overhead ad instrument virtual undetect case worst case found 7 increas execut time one run na nht1 applicationio benchmark 41 instrument librari put product use anecdot evid suggest notic perform loss although collect 700 mb data trace file account less 1 total cf traffic sinc node buffer 4 kb data send central data collector raw trace file contain partial order list event record order record complic lack synchron clock ipsc860 node maintain clock clock synchron system startup drift significantli 42 partial compens asynchroni timestamp block record left comput node receiv data collector differ two attempt adjust event order compens node clock drift rel collector clock techniqu result better estim actual event order still approxim much analysi base spatial rather tempor inform 32 cm5 trace collect cm5 trace program two differ program model dataparallel cm fortran cmf program controlparallel cmmd program cm5 program model cf applic perform io via runtim librari paper examin discuss io done scalabl disk array cmf cf instrument runtim cmf io librari collect trace gather data singl file ipsc cm5 applic trace data written separ file trace nearli cmf applic ran 23day period june 28 1994 juli 20 1994 instrument mechan user disabl trace particular job set environ variabl user exampl industri partner ncsa request featur made use therebi applic trace separ mechan allow us count total number cmf job run trace period even suppress trace gener 1943 job period 1760 trace neither figur includ program compil trace librari instal 1760 job trace repres 434 distinct applic run 384 distinct user ipsc attempt reduc effect trace user popul wrote perjob trace file onto serial unix file system avoid content sda io buffer trace record memori wrote disk larg block minim trace overhead perform measur taken betatest indic instrument increas total applic execut time less 5 cmmd classifi cmf workload gener workload cmmd workload selfselect develop cmmd trace librari think machin corpor inhous version cmmd sinc develop offsit ncsa system staff reluct make default librari reli user voluntarili link program cmmd trace librari us gather trace trace period two week summer 1994 obtain trace 127 job repres 29 distinct applic run 11 distinct user volunt tend heavi user sda rel sophist programm interest parallelio behavior perhap classifi workload iointens workload compar gener cmf workload differ consid interpret cmmd data cmmd io implement clientserv architectur privileg cm 5 host process respons run server loop monitor cmmd io piggyback trace record clientserv protocol actual trace record produc cm5 comput node commun host server written local unix file system sinc commun trace record embed normal clientserv io protocol believ perturb minim section compar contrast ipsc cm5 workload tri identifi common trend isol reason differ behavior character workload top begin examin number job machin number use file job examin individu io request addit studi size io request look sequenti regular among examin request higher level tri identifi specif kind regular access pattern final examin file share variou granular summari statist three set trace may seen tabl 1 classifi file whether actual read written read written within singl open period rather mode use open file file open neither read written close trace megabyt number file system job read written open read written neither tabl 1 summari data collect ipsc cm5 41 job fig 1 show amount time machin spent run given number job 2 sinc cm5 much larger user base surpris spent less time idl 2 data overal number job cm5 collect 2 week may 1995 trace period sinc attempt correl inform result paper lack contemporan view signific percent total time number job figur 1 amount time machin spent given number job run data includ job even file access could trace ipsc unlik ipsc cm5 timeshar partit allow job run time although ipsc idl nearli 28 time trace cm5 idl less 2 time machin activ execut job ipsc spent 35 time run singl job cm5 spent 6 time run singl job mean ipsc use run multipl applic simultan 25 time cm5 execut multipl job 92 time although job use file system file system clearli must provid highperform access mani concurr presum unrel job uniprocessor file system tune situat multiprocessor filesystem research ignor issu focus optim singlejob perform fig 2 show distribut number comput node use job machin although singlenod job appear domin job popul ipsc job caus daemon run period check statu machin multiplenod job fairli evenli distribut among remain size 64 node although ipsc allow job small singl node cm5 minimum partit size 32 node 60 cmf job cm5 use percent job number comput node ipsc b cm5 51204080percent job number comput node cmmd figur 2 distribut number comput node use job workload machin limit choic power 2 cm5 minimum partit size 32 node smallest partit size hand sinc cmmd workload selfselect includ fairli larg iointens applic observ bia toward larg number node 50 trace cmmd job use 256 node clearli file system success must allow effici access small sequenti job larg highli parallel job varieti condit system load 42 file two system studi two differ manner file may open local global file said local open node access file issu independ request open file file local open node open file privat view file oper file directli affect node use file contrast file said open global node applic collect issu request open file file global open node share view file cf io model support notion global open file cf must open local discuss section 231 cf provid sever access mode allow file treat global open discuss perjob file statist coalesc local open issu cf singl global open multipl node cf applic issu local open file count local open singl global open sinc cmf dataparallel languag provid singl thread control everi file oper cmf collect file open cmf global cmmd allow programm open file either local global sinc cmmd applic wish open file global may explicitli sinc cmmd file open local attempt coalesc local open global open cf tabl note mani file written read inde 2 3 time mani specul programm trace cf applic often found easier open separ output file comput node rather coordin write common output file hypothesi support substanti smaller averag number byte written per file 12 mb averag byte read per file 33 mb ipsc differ averag number byte access appear cm5 workload cmf job read averag 278 mbfile wrote averag 252 mbfile cmmd applic read 1175 mbfile wrote 1102 mbfile domin writeonli file cm5 appear come partli checkpoint activ partli output file written sda later visual number byte read written per file cmf substanti smaller cmmd amount data transfer per file still order magnitud larger observ cf user seem made use higher disk capac bandwidth cm5 offer anoth common trend across three platform file read written 35 cf 54 cmmd 58 cmf behavior also common unix file system 3 may accentu difficulti coordin concurr read write file tabl 2 show job open file cours execut number number job file cf cmf cmmd tabl 2 among trace job number file open job often small 14 although open mani file one cf job open 2217 file although cmf requir file open node cf job open larg number file open one file per node although shown tabl nearli 25 job use cmf use file sda applic probabl comput intens io via nf number file open per job higher cmmd cmf perhap due selfselect natur user despit differ absolut number file open appear clear use multipl file per job common therefor although file open concurr filesystem design must optim access sever file within job found 061 open cf workload temporari file defin file delet job creat rariti temporari file file read written indic applic chose use file extens memori core solut mani cf applic comput fluid dynam code found outofcor method gener slow workload cm5 exhibit larger number temporari file 38 cmf job 49 cmmd job differ may indic ofcor method common cm5 may caus delet checkpoint file job ran complet fraction file file size byte cmf figur 3 cumul distribut function cdf number file size close file size x cdfx repres fraction file x fewer byte fig 3 show wide rang size file system system 3 file access cf although file larger generalpurpos file system 6 smaller would expect see scientif supercomput environ 10 file cm5 significantli larger ipsc size much evenli distribut one like reason file cm5 larger avail 20 time disk space 43 io request size figur 4 5 show ipsc cm5 vast major access small byte transfer larg access inde 96 read cf request fewer 100 byte read transfer 2 data read similarli 90 write cf fewer 100 byte write transfer 3 data written number small request surpris due poor perform cf 36 cmmd interfac 3 mani small file well sever distinct peak across whole rang size constant granular captur detail felt import histogram chose plot file size logarithm scale pseudologarithm bucket size bucket size 10 100 byte 10 byte bucket size 100 1000 100 byte read fraction data fraction cf cmmd cmf cmf read size byte cf figur 4 cdf number read request size amount data read request size fraction fraction data write cmmd cmf write size byte cf cf cmmd figur 5 cdf number write request size amount data written request size similar cf comput node issu request data independ comput node request cmmd somewhat larger cf 87 read 96 write 1000 byte cmf provid collect model io request issu comput node accordingli would expect see much larger request cmf either cmmd cf found howev even cmf 97 read 87 write 1000 byte ipsc small request cm5 known perform poorli although access cm5 larger observ ipsc still significantli smaller ten hundr kilobyt use typic perform analys system 36 43 studi shown larg io request common scientif applic run supercomput seen small request common scientif applic run parallel comput inde trend hold across two differ parallel machin use three parallel filesystem interfac two parallel program model therefor believ preponder small request size observ scientif workload natur result parallel fundament larg class parallel applic conclud futur parallel file system must focu provid low latenc small request well high bandwidth larg request 44 sequenti one common characterist previou file system workload studi particularli scientif workload file typic access sequenti 5 6 10 defin sequenti request one begin higher file offset point previou request comput node end looser definit sequenti use studi refer previou studi call sequenti call consecut consecut request sequenti request begin precis previou request end figur 6 7 show amount sequenti consecut access file observ workload figur look pernod access pattern cf cmmd perjob access pattern cmf three interfac nearli access writeonli file 100 sequenti access readonli file also predominantli sequenti cmf cmmd sever file read nonsequenti sever applic cm5 wrote data file forward order read back revers order behavior account least nonsequenti access machin unsurprisingli readwrit file access nonsequenti cmmd cmf cf cmmd cmf cf cmmd cmf cf readonli c readwrit fraction file fraction file fraction file figur cdf sequenti access file point line indic fraction file workload contain indic percentag sequenti access look graph consecut access fig 7 find behavior vari system interfac cf cmf nearli 90 writeonli file access 100 consecut cmmd hand 60 writeonli file access complet consecut three interfac readonli file much less like access consecut writeonli file least consecut access found cf 65 readonli file consecut access case access readwrit file primarili nonconsecut one signific reason rel high percentag consecut access writeonli file ipsc tendenc applic assign differ file node write data singl node access file frequent cf readonli c readwrit fraction file fraction file fraction file consecut cmmd cmf cmmd cmf cmmd cmf cf cf figur 7 cdf consecut access file point line indic fraction file workload contain indic percentag consecut access reason node access file nonconsecut multipl node access file happen frequent readonli file cf file cmmd larg number sequenti nonconsecut access often result interleav access interleav access aris success record file access differ node perspect individu node byte must skip one request next high percentag consecut access file cmf program expect look collect joblevel pattern rather individu nodelevel pattern sinc io request cmf applic issu individu node sort interleav unlik appear percent file number differ request size cmf cmmd number differ interv percent file cmf cmmd figur 8 number differ interv request size use file across particip node file zero interv size one access node file zero request size open close without access 45 regular workload mani small nonconsecut request differ previous observ workload tradit uniprocessor supercomput attempt gain better understand observ workload tri identifi point regular interv first look interv request number byte end one request begin next consecut access interv size 0 number interv size use file across node access file shown fig 8 surpris number file around 13 case read written one request per node ie interv file 99 cf 79 cmf 51 cmmd access singl interv size access consecut ie one interv size 0 remaind 1intervals file along 2intervals file repres remain file suggest exist anoth form highli regular access pattern file 3 differ interv size regular complex request get better feel regular fig 8 also show number differ request size use file cf exhibit highest degre regular 90 file access one two request size cmmd next 75 file access one two differ request size cmf least regular half file access two fewer request size may indic cmf user use file store differ data structur eg differ matric even cmf 80 file access three fewer request size combin regular request size regular interv size mani applic clearli use regular structur access pattern possibl much data matrix form 46 stride access better understand structur caus regular nonconsecut access pattern examin trace file evid stride access pattern 44 461 simplestrid refer seri io request simplestrid access pattern request number byte file pointer increment amount request pattern would occur exampl process parallel applic read column data matrix store rowmajor order could also correspond pattern gener applic distribut column matrix across processor cyclic pattern column could distribut evenli matrix store rowmajor order sinc stride pattern less like occur singlenod file sinc could occur file one two access look file three request multipl node fig 9 show mani access select subset cf cmmd file appear part simplestrid access pattern sinc consecut access could fraction file cmmdwithout cmfwith cmmdwith cfswithout cfswith fraction file figur 9 cumul distribut file accord fraction access involv simplestrid pattern plot show frequenc stride access consecut access count stride without consid trivial form stride access interv 0 fig 9 show frequenc stride access without consecut access includ either case 80 file examin cf appar access entir stride pattern stride access also common cmmd 60 file access entir stride nonconsecut pattern exclud consecut access appear almost stride access cmf 20 request file take part stride pattern lack stride access cmf surpris sinc stride access typic caus explicit express data distribut controlparallel program accordingli remaind discuss focu cf cmmd defin stride segment group request appear part singl simplestrid pattern fig 9 show percentag request involv stride segment tell us whether file access singl filelong stride segment mani shorter segment fig 10 show file stride segment file access mani stride segment sinc interest case file clearli access stride pattern figur includ consecut access segment fewer request number request segment vari machin fig 11 number file number stride segment cf number stride segments10003000500070000 50 100 150 200 number file figur 10 number differ stride segment file ignor segment request note two plot use differ scales1000002000000 number access number segment number accesses20060010001400 cf b cmmd number segment figur 11 head segmentlength distribut plot show number segment given length includ short segment 10 fewer access show segment cf fell rang 20 30 request segment cmmd 55 fig 12 show file access much longer segment machin exist simplestrid pattern interest potenti use fact mani file access multipl short segment suggest level structur beyond describ simplestrid pattern 462 nest pattern nestedstrid access pattern similar simplestrid access pattern rather compos simpl request separ regular stride file compos cf number accesses50150250 200 400 600 800 1000 1200 number segment number accesses50015002500500 1000 1500 2000 2500 number segment figur 12 tail segmentlength distribut stride segment separ regular stride file simplestrid pattern examin last section could call singlynest pattern doublynest pattern could correspond pattern gener applic distribut column matrix store rowmajor order across processor cyclic pattern column could distribut evenli across processor simplestrid subpattern correspond request gener within row matrix toplevel pattern correspond distanc one row next access pattern could also gener applic read singl column data threedimension matrix higher level nest could occur applic map multidimension matrix onto set processor maximum level number number nest cf file cmmd file tabl 3 number file use given maximum level nest tabl 3 show frequent nest pattern occur cf cmmd file appar stride access zero level nest file access simplestrid pattern singl level nest interestingli machin far common file exhibit three level nest two tendenc suggest use multidimension matric common system 47 synchron access mode although notion synchron access file built semant dataparallel cmf case cf cmmd provid synchron access file cf cmmd provid user option use file pointer share among node also provid sever mode provid user differ semant govern file pointer share given regular request interv size ipsc cfss mode see section 231 would seem help trace show howev 99 file use mode 0 provid node independ file pointer fig 8 give one hint although differ request size interv size often one someth easili support sharedpoint mode mode 0 also known fastest four mode offer cf contrast cf cmmd localindepend mode known slow use 088 total io instead cmmd applic use synchronoussequenti mode 78 io synchronousbroadcast mode use 87 total io globalindepend mode use 119 total io specul use one node subset node use file among data one may inclin conclud cm5 applic need fast synchron io anecdot evid suggest howev user frequent want independ io will pay perform penalti cmmd cf user adopt differ io strategi achiev end result high perform illustr capabl exist machin may influenc user behavior writeblock readbyt readblocks02061 fraction file percent share writebyt fraction file percent share writebyt writeblock readbyt readblocks02061 fraction file percent share readblock readbyt writeblock writebyt cf c cmmd figur 13 cdf file share node readonli writeonli file byte block granular block size ipsc 4 kbyte block size cm5 kbyte 48 file share within job file concurr share two process open time uniprocessor distributedsystem workload concurr share known uncom mon write concurr share file almost unheard 6 parallel file system cours concurr file share among process within job presum norm concurr file share job like rare inde trace saw great deal file share within job concurr file share job interest question individu byte block file share fig 13 show frequenc byte blockshar system three case share readonli file writeonli file surpris given complex coordin write share inde cf 70 readonli file byte share multipl comput node 90 writeonli file byte share found similar result cmmd 61 readonli file byte share 93 writeonli file none byte share cmf least share three system 95 writeonli file byte share 60 readonli file 1 fewer byte share multipl comput node lack share like artifact cmf dataparallel program model processor static assign nonoverlap portion matrix even lot byteshar usual larg amount blockshar overal amount block share impli strong interprocess spatial local suggest cach io node may improv system perform 1 5 conclus recommend across two machin two program model cover paper found import similar differ compar uniprocessor workload three parallel workload use much larger file domin write although variat magnitud found small request size common three parallel workload uniprocessor workload compar vectorsupercomput workload observ much smaller request tendenc toward nonconsecut sequenti file access final parallel lead new interleav access pattern high interprocess spatial local io node detail result may specif two system studi workload two site believ gener conclus wide applic scientif workload run looselycoupl mimd multiprocessor categori includ mani current multiprocessor ultim believ filesystem interfac must chang current interfac forc programm break larg parallel io activ small nonconsecut request believ controlparallel model support stride io request programm interfac comput node comput node io node 24 44 stride request effect increas request size lower overhead introduc opportun lowlevel optim 45 futur work believ lowlevel workload analys conduct import first step toward develop parallel file system meet need parallel scientif applic still great deal work done ffl trace platform reduc likelihood result specif architectur environ ffl studi specif applic greater detail workload studi describ parallel file system use studi individu applic allow us understand use fashion better understand applic programm fundament need ffl design implement new interfac file system base workload analys acknowledg mani thank na divis nasa ame especi jeff becker russel carter fineberg art lazanoff bill nitzberg leigh ann tanner mani thank also orran krieger bernard traversat rest charisma group thank michael welg curti canada ncsa mani thank ncsa user includ greg bryan dian cook tom cortes kathryn johnston chri kuszmaul fadi najjar robert sugar also thank kapil mathur david phillimor think machin corpor doreen revi duke final thank mani user agre applic trace r filesystem workload scientif multiprocessor character parallel fileaccess pattern largescal multiprocessor shortterm file refer pattern unix environ directori refer pattern hierarch file system trace driven analysi unix 42 bsd file system measur distribut file system analysi file io trace commerci comput environ high perform io parallel comput problem prospect demo file system inputoutput behavior supercomput appli cation analysi file migrat unix supercomput environ static analysi io characterist scientif applic product workload case studi scientif applic io behavior file concept parallel io prefetch file system mimd multi processor studi io behavior perfect benchmark multiprocessor architectur requir parallel scientif applic explicit commun applicationsdriven parallel io inputoutput instru mentat character model manag polici putoutput characterist scalabl parallel applic concurr file system highli parallel mass storag system unix file access cach multicomput environ cmmd io parallel unix io multiprocessor file system interfac hf flexibl file system largescal mul tiprocessor parallel access file vesta file system ncube parallel io softwar connect machin model cm2 technic summari parallel file io routin mpiio parallel file io interfac mpi ipsc2 ipsc860 user guid nasa ame research center perform measur concurr file system intel ipsc2 hypercub perform ipsc860 concurr file system cmmd refer manual version 30 person commun global time refer hypercub multiprocessor perform cm5 scalabl file system lowlevel interfac highlevel parallel io diskdirect io mimd multiprocessor tr ctr dean hildebrand peter honeyman directpnf scalabl transpar versatil access parallel file system proceed 16th intern symposium high perform distribut comput june 2529 2007 monterey california usa len wisniewski brad smisloff nil nieuwejaar sun mpiio effici io parallel applic proceed 1999 acmiee confer supercomput cdrom p14e novemb 1419 1999 portland oregon unit state dean hildebrand lee ward peter honeyman larg file small write pnf proceed 20th annual intern confer supercomput june 28juli 01 2006 cairn queensland australia thakur william gropp ewe lusk case use mpi deriv datatyp improv io perform proceed 1998 acmiee confer supercomput cdrom p110 novemb 0713 1998 san jose ca satyanarayanan carla schlatter elli adapt key mobil io acm comput survey csur v28 n4e dec 1996 jarek nieplocha holger dachsel ian foster implement noncollect parallel io cluster environ use activ messag commun cluster comput v2 n4 p271279 1999 myra b cohen charl j colbourn order disk doubl erasur code proceed thirteenth annual acm symposium parallel algorithm architectur p229236 juli 2001 crete island greec hong scott brandt darrel e long ethan l miller ying lin use memsbas storag comput systemsdevic model manag acm transact storag to v2 n2 p139160 may 2006 gokhan memik mahmut kandemir alok choudhari exploit interfil access pattern use multicollect io proceed 1st usenix confer file storag technolog januari 2830 2002 monterey ca yijian wang david kaeli profileguid io partit proceed 17th annual intern confer supercomput june 2326 2003 san francisco ca usa ron oldfield david kotz improv data access comput grid applic cluster comput v9 n1 p7999 januari 2006 toni cort sergi girona jess labarta design issu cooper cach coher problem proceed fifth workshop io parallel distribut system p3746 novemb 1717 1997 san jose california unit state florin isaila guido malpohl vlad olaru gabor szeder walter tichi integr collect io cooper cach clusterfil parallel file system proceed 18th annual intern confer supercomput june 26juli 01 2004 malo franc thakur william gropp ewe lusk implement mpiio portabl high perform proceed sixth workshop io parallel distribut system p2332 may 0505 1999 atlanta georgia unit state thoma sterl conclus beowulf cluster comput linux mit press cambridg 2001 gokhan memik mahmut kandemir weikeng liao alok choudhari multicollect io techniqu exploit interfil access pattern acm transact storag to v2 n3 p349369 august 2006 emilia rosti giusepp serazzi evgenia smirni mark squillant model parallel applic larg comput io requir ieee transact softwar engin v28 n3 p286307 march 2002 paolo cremonesi claudio gennaro integr perform model spmd applic mimd architectur ieee transact parallel distribut system v13 n12 p13201332 decemb 2002 paolo cremonesi claudio gennaro integr perform model spmd applic mimd architectur ieee transact parallel distribut system v13 n7 p745757 juli 2002 stergio v anastasiadi kenneth c sevcik michael stumm scalabl faulttoler support variabl bitrat data exedra stream server acm transact storag to v1 n4 p419456 novemb 2005 david kotz diskdirect io mimd multiprocessor acm transact comput system toc v15 n1 p4174 feb 1997 jack dongarra ian foster geoffrey fox william gropp ken kennedi linda torczon andi white refer sourcebook parallel comput morgan kaufmann publish inc san francisco ca