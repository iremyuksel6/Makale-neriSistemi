predict analysi wavefront applic use loggp paper develop highli accur loggp model complex wavefront applic use mpi commun ibm sp2 key featur model includ 1 elucid princip wavefront synchron structur 2 explicit highfidel model mpisend mpirec primit mpisendrec model use deriv l g simpl twonod microbenchmark model paramet obtain measur small applic problem size four sp node result show loggp model predict second high degre accuraci measur applic execut time larg problem run 128 node detail perform project provid larg futur processor configur expect avail applic develop result indic scale beyond one two thousand node yield greatli diminish improv execut time synchron delay princip factor limit scalabl applic b introduct paper investig use parallel machin model call loggp analyz perform larg complex applic stateoftheart commerci parallel platform applic known sweep3d interest threedimension particl transport problem identifi asci benchmark evalu high perform parallel architectur applic also interest fairli complex synchron structur synchron structur must captur analyt model order model accur predict applic execut time thu provid accur perform project larger system new architectur modif applic one question address research variant logp model 4 best suit analyz perform sweep3d ibm sp system sinc version sweep3d use mpi commun primit loggp model 2 includ addit paramet g accur model commun cost larg pipelin messag turn provid requisit accuraci possibl due block natur mpi primit content messag process resourc neglig thu recent extens logp captur impact content 712 need previou work 467 logp model appli import fairli simpl kernel algorithm fft lu sort algorithm spars matrix multipli two experiment studi appli model complex full applic splash benchmark 9 11 howev studi effect synchron applic perform scalabl measur empir rather estim model mani previou analyt model analyz applic perform restrict simpler synchron structur sweep3d eg 8 one except determinist task graph analysi model 1 shown accur predict perform applic complex synchron structur loggp model repres synchron structur abstractli task graph key question address research whether abstract represent suffici analyz full complex applic sweep3d construct loggp model captur synchron structur also elucid basic synchron structur sweep3d similar approach 2 use commun microbenchmark deriv input paramet l g howev show section 3 deriv paramet somewhat complex mpi commun sp2 meiko cs2 thu explicit model mpisend mpirec primit develop although loggp input paramet deriv fourprocessor run sweep3d loggp model project perform quit accur 128 processor sever fix total problem size sever case fix problem size per processor model also quickli easili project perform larg futur processor configur expect avail applic develop research support part darpaito contract n6600197c8533 comput scienc technic report 1392 univers wisconsinmadison februari 1999 appear proc 7 th acm sigplan symp principl practic parallel program ppopp 99 atlanta ga may 1999 show sever interest result deriv analysi section 2 provid brief overview sweep3d applic section 3 deriv model mpisend mpirec paramet valu character commun cost section 4 present loggp equat sweep3d well modif need applic util multiprocessor smp node sp2 latter case two type commun cost intraclust inter cluster section 5 provid model valid result well perform project futur system section 6 provid conclus work 2 sweep3d sweep3d describ 10 detail task graph show complex synchron among task version code analyz paper given 5 give simpl overview version sweep3d includ aspect relev loggp model structur algorithm appar loggp model present section 4 name impli sweep3d transport calcul implement seri pipelin sweep three dimension grid let dimens denot ijk 3d grid map onto twodimension array processor size mn processor perform calcul partit j dimens size itjtk shown figur 1 note due problem map figur 1 processor processor grid figur 2 number p ij vari 1 n indic horizont posit processor singl iter consist seri pipelin sweep 3d grid start 8 corner octant grid map sweep two dimension processor grid illustr figur 2 mo denot number angl consid problem processor perform itjtkmo calcul sweep octant creat finer granular pipelin thu increas parallel comput block data comput given processor partit angl block factor mmi kplane block factor mk paramet specifi number angl number plane kdimens respect comput boundari data forward next processor pipelin processor interior processor grid receiv boundari data two neighbor processor comput block base valu send result calcul two neighbor destin processor determin direct sweep optim version sweep3d analyz block given processor calcul sweep given pair octant processor free start calcul block sweep next pair octant exampl lower left corner processor start comput first block sweep octant 7 comput last block sweep origin octant 6 shown greater detail loggp model sweep3d section 4 pipelin sweep octant 8 complet one iter algorithm one energi group code analyz twelv iter execut one time step target problem interest asci program involv order group 10000 time step grid size order 10 9 twenti million eg 280280255 scale model project problem size shown section 5 3 commun paramet l g present loggp model sweep3d sp2 deriv model mpisend mpirec commun primit use applic mpisendrec model need loggp model sweep3d also need deriv two commun paramet valu name network latenc l process overhead send receiv messag commun structur sweep3d ignor gap g paramet time consecut messag transmiss greater minimum allow valu intermessag transmiss time give roundtrip commun time mpi commun ibm sp measur use simpl commun microbenchmark valu g gap per byte deriv directli measur discuss model sp2 mpisend mpirec primit use l g paramet follow descript valu l deriv signific result deriv valu l g differ valu fortran c microbenchmark measur greatli increas confid valid mpi commun model figur 1 partit 3d grid j dimens figur 2 sweep octant jt processor grid 31 measur commun time roundtrip commun time function messag size simpl fortran commun microbenchmark given figur 3 b data point messag given size sent processor processor b receiv process processor b immedi sent back roundtrip time measur subtract time call mpisend time mpirec oper complet figur also includ result model roundtrip commun use deriv l discuss seen figur measur commun time increas significantli messag size henc g paramet requir accur model commun cost two point worth note figur commun cost chang abruptli messag size equal 4kb due handshak mechan implement messag larger 4kb handshak model slope curv g chang messag size equal 1kb messag process overhead also differ messag larger 1 kb messag smaller 1kb due maximum ip packet size thu deriv separ valu g g l messag 32 model mpisend mpirec model develop reflect fairli detail understand mpisend mpirec primit implement sp2 abl obtain author mpi softwar might necessari modifi model futur version mpi librari sweep3d run differ messagepass architectur modifi use nonblock mpi primit model illustr gener approach captur impact system modif sinc sp2 system use poll receiv messag assum overhead send messag approxim overhead receiv messag messag smaller 4kb handshak requir total endtoend cost send receiv messag model simpli valu g depend whether messag size larger smaller 1kb messag larger 4kb endtoend commun requir handshak header initi sent destin processor destin processor must repli short acknowledg correspond receiv post receiv post header messag sent endtoend cost model follow note process overhead receiv ack model subsum process overhead send data correspond receiv yet post addit synchron delay incur delay model next section addit total cost commun given loggp model sweep3d requir separ cost send receiv messag messag size less 4kb valu depend messag size messag size greater equal 4kb receiv cost includ time inform send processor receiv post delay messag arriv 33 commun paramet valu use equat totalcomm measur roundtrip commun time deriv valu l given tabl 1 valu g g comput directli slope curv figur respect rang messag size deriv l solv three equat totalcomm messag size less 1kb 14kb greater 4kb respect three unknown l appli method roundtrip time measur obtain c microbenchmark yield valu l g measur figur 3 mpi round trip commun messag size time usec1003005000 2000 4000 6000 8000 10000 messag size time usec measur model obtain fortran benchmark although valu differ shown tabl 1 greatli increas confid valid model mpi commun primit use paramet valu deriv way measur model commun cost differ less 4 messag 64256kb shown figur 3 note although measur model valu seem diverg messag size equal 8kb figur 2a figur 2b show valu messag size 8kb good agreement 4 loggp model sweep3d section develop loggp model sweep3d use model mpi commun cost develop section 3 first present model assum processor mn processor grid map differ smp node sp2 case network latenc commun give modifi equat case 22 region processor grid map singl fourprocessor smp node sp2 roundtrip time paramet valu comput section 3 commun processor differ smp node equat use comput intranod commun paramet 41 basic model loggp model take advantag symmetri sweep perform execut thu calcul estim execut time sweep one octant pair use execut time obtain total execut time sweep explain sweep describ section 2 processor wait input two neighbor processor comput valu portion grid size mmi mk jt processor send boundari valu two neighbor processor wait receiv new input use cost associ activ develop loggp model summar tabl 2 directli express preced sendrec synchron constraint implement algorithm time comput one block data model equat 5 tabl 2 equat w g measur time comput one grid point mmi mk jt input paramet defin section 2 specifi number angl grid point per block per processor consid octant pair 56 sweep begin processor upperleft corner processor grid shown figur 2 recal upperleft processor number p 11 account pipelin wavefront sweep use recurs formula equat 6 tabl 2 comput time processor p ij begin calcul sweep denot horizont posit processor grid first term equat 6 correspond case messag west last arriv processor p ij case messag north alreadi sent receiv messag west process due block natur mpi commun second term equat 6 model case messag north last arriv note startp appropri one two term equat 6 delet processor east north edg processor grid sweep3d applic make sweep across processor direct octant pair critic path time two rightdownward sweep comput equat 7 tabl 2 time lowerleft corner processor p 1m finish commun result last block sweep octant 6 point sweep octant 7 8 upper right start processor p 1m proceed toward note subscript send receiv term equat 7 includ indic direct commun event make easier understand term includ equat send receiv cost deriv section 32 critic path sweep octant 7 8 time processor grid complet calcul sweep sinc sweep octant 1 2 next iter wont begin processor p n1 finish due symmetri sweep3d algorithm mention time sweep northeast total time sweep octant 5 6 start processor p 00 move southeast processor p nm thu comput critic path time octant 7 8 shown equat 8 tabl 2 equat 8 repres time processor p nm finish last calcul second octant pair processor tabl loggp model sweep3d messag size 1024 1024 tabl 1 sp2 mpi commun paramet directli east p n1m must start comput calcul commun need result block octant wait processor p nm receiv result last block calcul comput result base block due symmetri sweep octant 1 4 sweep octant 5 8 total execut time one iter comput equat 9 tabl 2 equat 56 contain one term m1l equat 78 contain two term m1l n2l account synchron cost synchron term motiv observ measur commun time within sweep3d greater measur mpi commun cost discuss section 3 m1l term 56 78 captur delay caus send block destin processor post correspond receiv delay accumul j direct thu total delay 1m depend number processor north m1 furthermor synchron cost zero problem messag size smaller 4kb sinc case processor send messag whether correspond receiv post second synchron delay 78 n2l repres differ receiv post messag actual receiv send processor sinc processor receiv north west southeast sweep like wait messag west sinc delay cumul processor dimens processor p n1m model delay n2l notic receiv synchron term 0 processor west edg processor grid sinc processor west receiv messag includ 56 express 42 model cluster smp node modif model need 22 region processor grid map singl fourprocessor smp cluster ibm sp2 rather map processor grid separ smp node chang outlin anticip next gener mpi softwar sp support full use cluster processor let l local denot network latenc intraclust messag remot denot latenc interclust messag l local l remot 2 follow discuss assum intraclust interclust messag equat easili modifi case let l r subscript denot model variabl eg totalcomm send receiv comput use l local l remot respect use notat modifi equat comput execut time sweep3d given tabl 3 describ recal processor number start 1 j dimens also recal processor p ij denot horizon posit processor grid j even incom messag intraclust outgo messag interclust vice versa true j odd mean startp ij comput totalcomm l receiv l send l incom messag former case totalcomm r receiv r send r latter case odd j even variabl first term startp ij interclust commun commun variabl second term intraclust commun vice versa true even j odd send receiv variabl equat 56 78 intraclust variabl assum number processor j dimens even map 22 processor region smp cluster synchron term 56 78 comput use l avg chang requir model modifi model valid detail simul 3 howev sinc yet valid system measur effici mpi softwar intraclust commun doesnt yet exist result case processor map separ smp node given paper nevertheless chang model full cluster use simpl illustr model versatil furthermor equat use project system perform next gener mpi softwar 43 measur work w valu work per grid point w g obtain measur valu 2x2 grid processor fact obtain accuraci result paper measur w g perprocessor grid size account differ 20 aris cach miss effect sinc sweep3d program contain extra calcul fixup five twelv iter measur w g valu iter type although detail creator logploggp may intend increas accuraci substanti need larg scale project section 5 furthermor recurs model sweep3d repres tabl 3 modifi loggp equat intraclust commun sp2 sweep sweep3d code addit measur comput time main bodi code ie iter time step comput time denot w w measur singl processor run specif problem size model paramet thu measur use simpl code instrument rel short one two fourprocessor run next section investig accur model predict measur execut time sweep3d applic 5 experiment result section present result obtain loggp model valid loggp project sweep3d run time measur run time 128 processor use loggp model predict evalu scalabl sweep3d thousand processor two differ problem size interest applic develop unless otherwis state report execut time one energi group one time step twelv iter time step figur 4 compar execut time predict loggp model measur execut time fortran version sweep3d 128 sp2 processor fix total problem size 150150150 505050 kblock mk equal 10 number processor increas messag size comput time per processor decreas overhead synchron increas problem size processor configur messag size vari 16kb 1kb remark high agreement model estim measur system perform across entir rang figur 5 show larger problem size achiev reason good speedup ie low commun synchron overhead 128 processor smaller problem size note model highli accur case figur 6 show predict measur applic execut time function number processor sp2 two differ case fix problem size per processor figur 6a processor partit threedimension grid size 20201000 figur 6b processor partit size 45x45x1000 experi total problem size increas number processor increas agreement model estim problem size 150150150 b problem size 505050 figur 4 valid loggp model fix total problem size 128 processor b 2500 processor figur 5 sweep3d speedup fix total problem size figur 4 code mk10 mmi3501500 50 100 150 processor time sec model processor time processor processor measur execut time gener excel level abstract model howev result show model less quantit accur mk1 verifi mani configur loggp model qualit accur determin whether execut time mk1 higher lower execut time mk10 also verifi model quantit accur valu mk larger 10 result 45x45x1000 also illustr c version code creat fortran version use f2c somewhat slower fortran code although absolut perform c differ perform trend report paper fortran code also observ c code model project figur 7 show project execut time sweep3d fix problem size per processor system scale thousand processor expect avail asci site near futur two fix perprocessor problem size consid 661000 1414255 case model predict valid 2500 processor use simul shown measur execut time 661000 case illustr unexplain system anomali measur execut time suddenli increas given small increas number processor anomali occur coupl fix perprocessor grid size examin note anomali occur even though problem size per processor fix thu seem unlik explain cach behavior messag size one hazard model analyt simul anomal system behavior predict howev model estim show jump execut time due expect commun synchron cost detail examin system implement requir discov hope correct caus anomali figur 6 figur 7a b predict excel scale case memori usag per processor kept constant nevertheless solv 10 9 problem size 661000 grid point per processor requir 27000 processor result figur 7a suggest execut time scale group 10000 time step prohibit problem configur 20201000 b 45451000 figur valid loggp model fix problem size per processor 661000 figur 7 project sweep3d execut time fix problem size per processor mmi3 mk1040012000 100 200 300 400 500 number processor time sec c measur mk10 c loggp mk10 c measur mk1 c loggp mk1 measur mk10 loggp mk10 measur mk1 loggp mk150150250 number processor time number processor time number processor time sec measur loggp figur 8 give project execut time sweep3d system scale 20000 processor two differ total problem size interest applic develop case project execut time singl time step involv 12 iter scale factor 30 reflect fact comput interest scientist involv energi group rather one note problem size per processor decreas number processor increas thu sweep3d configur larger mk higher perform loggp model use determin valu sweep3d configur paramet ie mmi yield lowest execut time given processor configur problem size one key observ result figur 8 point greatli diminish improv execut time number processor increas beyond one two thousand second key observ figur 8b even optim valu sweep3d configur paramet unlimit number processor solv billion grid point problem time step appear requir prohibit execut time use current algorithm investig caus limit scalabl figur 7 8 figur 9 show breakdown execut time problem size figur 8 breakdown show much critic path execut time due comput nonoverlap synchron nonoverlap commun key observ system scale synchron delay becom signific domin factor execut time synchron delay model m1l n1l term equat 7 8 tabl 2 modif reduc synchron cost would highli desir solv larg problem interest exampl simpl modif might explor use nonblock form mpisend howev fundament algorithm chang reduc synchron delay may need figur 10 show could yield greater benefit improv processor technolog due difficulti speed commun latenc 20 million grid point b 1 billion grid point figur 8 project sweep3d execut time fix total problem size one time step 20 million grid point b 1 billion grid point figur 9 project sweep3d execut time commun synchron cost one time step number processor time loggp mmi1 mk1 loggp mmi3 mk1 loggp mmi6 mk1 loggp mmi1 mk10 loggp mmi3 mk10 loggp mmi6 mk105001500250035000 5000 10000 15000 20000 25000 number processor number processor total comp comm synch10003000500070000 5000 10000 15000 20000 25000 number processor 6 conclus princip contribut research loggp model analyz project perform import applic complex synchron structur wavefront applic loggp equat captur princip synchron cost also elucid basic pipelin synchron structur illustr abstract capabl domain compar simplic commun paramet l g research provid case studi model valid extrem well measur applic perform illustr potenti loggp model analyz wide varieti interest applic includ import class wavefront applic signific result obtain sweep3d applic studi paper follow first scale beyond one two thousand processor yield greatli diminish return term improv execut time even larg problem size second solv problem size order 10 grid point group 10000 time step appear impract current algorithm final synchron overhead princip factor limit scalabl applic futur work includ gener model present research creat reusabl analyt model wavefront applic execut product parallel architectur develop model sharedmemori version sweep3d develop loggp model applic complex synchron structur r analyz behavior perform parallel program loggp incorpor long messag logp model logp toward realist model parallel comput poem endtoend perform design larg parallel adapt comput system fast parallel sort logp experi cm5 lopc model content parallel algorithm effect latenc occup bandwidth distribut share memori multiprocessor solut firstord form fo 3d discret orgin equat massiv parallel processor effect commun latenc overhead bandwidth cluster architectur logpc model network content messag pass program tr logp toward realist model parallel comput analyz behavior perform parallel program loggp predict applic behavior larg scale sharedmemori multiprocessor fast parallel sort logp effect commun latenc overhead bandwidth cluster architectur poem effect latenc occup bandwidth distribut share memori multiprocessor ctr ewa deelman gurmeet singh meihui su jame blyth yolanda gil carl kesselman gaurang mehta karan vahi g bruce berriman john good anastasia laiti joseph c jacob daniel katz pegasu framework map complex scientif workflow onto distribut system scientif program v13 n3 p219237 juli 2005 fumihiko ino noriyuki fujimoto kenichi hagihara loggp parallel comput model synchron analysi acm sigplan notic v36 n7 p133142 juli 2001 gabriel marin john mellorcrummey crossarchitectur perform predict scientif applic use parameter model acm sigmetr perform evalu review v32 n1 june 2004 daniel nurmi anirban mandal john brevik chuck koelbel rich wolski ken kennedi grid schedul protocolsevalu workflow schedul use integr perform model batch queue wait time predict proceed 2006 acmiee confer supercomput novemb 1117 2006 tampa florida kirk w cameron rong ge predict evalu distribut commun perform proceed 2004 acmiee confer supercomput p43 novemb 0612 2004 ruom jin gagan agraw perform predict random write reduct case studi model share memori program acm sigmetr perform evalu review v30 n1 june 2002 vikram adv rizo sakellari applic represent multiparadigm perform model largescal parallel scientif code intern journal high perform comput applic v14 n4 p304316 novemb 2000 rajiv bagrodia ewa deelman thoma phan parallel simul largescal parallel applic intern journal high perform comput applic v15 n1 p312 februari 2001 david k lowenth accur select block size runtim pipelin parallel program intern journal parallel program v28 n3 p245274 june 2000 det buakle gregori f traci mari k vernon stephen j wright nearoptim adapt control larg grid applic proceed 16th intern confer supercomput june 2226 2002 new york new york usa vikram adv rajiv bagrodia jame c brown ewa deelman aditya dube elia n housti john r rice rizo sakellari david j sundaramstukel patricia j teller mari k vernon poem endtoend perform design larg parallel adapt comput system ieee transact softwar engin v26 n11 p10271048 novemb 2000 ruom jin gagan agraw methodolog detail perform model reduct comput smp machin perform evalu v60 n14 p73105 may 2005 vikram adv mari k vernon parallel program perform predict use determinist task graph analysi acm transact comput system toc v22 n1 p94136 februari 2004