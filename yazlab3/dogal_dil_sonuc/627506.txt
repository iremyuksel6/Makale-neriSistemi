gener neural network author discuss requir learn gener tradit method base gradient descent limit success stochast learn algorithm base simul anneal weight space present author verifi converg properti feasibl algorithm implement algorithm valid experi describ b introduct neural network appli wide varieti applic speech generation1 handwrit recognition2 last decad seen great advanc design neural network class problem call recognit problem design learn algorithms35 57 learn weight neural network mani recognit problem longer difficult task howev design neural network gener problem well understood domain neural network applic classifi two broad categori recognit gener 1 8 class first train neural network set inputoutput pair 2 n n recognit problem train network test previous seen input j 1 j n corrupt nois shown fig1 train network expect reproduc output j correspond j spite nois shape recognit 9 10 handwrit recognition2 exampl recognit prob lem hand gener problem train neural network test input n1 distinct input 1 2 n use train network shown fig1 network expect correctli predict output n1 input n1 model learn train typic exampl gener problem bond rating11 robotics12 neural network gener problem import sinc mani applic enorm import real world1315 would benefit work mani applic difficult success appli either convent mathemat techniqu eg statist regress standard ai approach eg rule base system neural network gener abil use domains11 requir priori specif function domain model rather attempt learn underli domain model train inputoutput exampl r n e 1 2 1 2 n n r n e 1 2 1 2 n n n1o n1 n2o n2 figur 1 class problem learn algorithm gener problem differ learn algorithm recognit problem recognit problem network expect reproduc one previous seen put network may rememb output input fit curv use train ing rememb output one often use larg network mani node weight howev memor learn sampl suit gener problem sinc lead wors perform predict output unseen input furthermor gener problem allow small amount error output predict network henc fit curv need pass pair use train network address gener problem may instead fit simpl curv eg low degre polynomi basic analyt function like logx sinex tangentx etc inputoutput pair rather fit crook curv neural network use gener problem tend simpler small number hidden node layer interconnect edg weight enabl one use comput sophist algorithm earlier work neural network 4 9 relat recognit problem littl research toward develop neural network model gener problem 16 17 present new learn algorithm stochast backpropag gener problem verifi converg algorithm provid theoret argument toward capabl propos algorithm discov optim weight also describ implement algorithm experi algorithm solv gener problem 2 problem formul gener problem neural network formul three differ way analyt construct function learn 2527 c symbol semant network8 analyt formal focus exist network capabl gener also provid worst case time complex discov network solv arbitrari gener problem provid way discov network construct function learn formal approach gener problem complementari fashion studi algorithm discov network solv class gener problem aim discov function f map input domain output domain set learn exampl function discov may defin boolean number real number input output assum number symbol mean function network repres symbol mean beyond numer comput third approach symbol semant network associ symbol mean network gener occur attach new node appropri parent node network inherit properti parent classif formul gener problem includ special case exampl signal detect problem consid special case task signal detect problem learn recogn paramet function f rather learn function recogn frequenc given sinusoid function exampl signal detect problem backpropag neural network appli success problem28 focu construct function learn formul term learn numer function simpl neural network describ direct graph e vertex set v three kind node input node leav b hidden node intern node c output node root edg e e associ weight w j shown fig2 network use comput output set input node comput function weight sum input signal g inset output function g map 11 given input network would comput output finput invers problem discov function f ie set edg weight given set inputoutput pair refer learn problem state follow given set exampl input output pair 1 1 n n find weight edg e j e neural network network map j j j12n close possibl repres possibl infinit domain input repres possibl infinit rang output dimension featur space f 1 f k describ input input j consid ktupl cartesian space f 1 f 2 f k given learn sampl per sampl error function e real 1 2 n 1 2 n gener involv find map function f neural network exampl symbol node edg weight output input i2 input i1 w36 figur 2 feedforward neural network minim error function e entir domain particular f learn sampl reduc error function entir domain look small subset difficult arbitrari learn sampl arbitrari input domain gener problem often simplifi make assumpt domain learn sampl assum repres entir domain adequ valu e estim valu e function f assum smooth continu well behav domain rang function f boolean set set real number usual one function fit given set learn sampl inputoutput pair make gener problem harder exampl learn specif boolean function subset domain difficult 25 sinc sever boolean function domain fit learn sampl littl consensu criteria prefer one candid boolean function rest break tie restrict attent function set real number draw upon notion simplic function real number choos one function among set possibl function fit learn sampl simplic intuit defin term number maxima minima function simplic reduc notion degre polynomi polynomi function 3 stochast backpropag gener idea behind algorithm use simul anneal weight space weight space defin collect configur w connect weight neural network simul anneal procedur search weight space configur w opt minim errortofit function ew search procedur base mont carlo method 29 given current configur w network character valu weight small randomli gener perturb appli small chang randomli chosen weight differ error de current configur w slightli perturb one neg ie perturb result lower error fit process continu new state de 0 probabl accept new configur given exp dek b occasion transit higher error configur help search process get local minima accept rule new configur refer metropoli criteria follow criteria probabl distribut configur approach boltzman distribut given eqa1 zt normal factor known partit function depend temperatur boltzman constant factor exp ek b known boltzman factor denot control paramet call temperatur due histor reason start high valu temperatur decreas slowli execut algorithm temperatur decreas boltzman distribut concentr configur lower error final temperatur approach zero minimum error configur nonzero probabl occur way get global optim weight network minim error fit train exampl provid maximum temperatur suffici high cool carri suffici slowli algorithm descript one defin configur cost function gener mechanismor equival neighborhood structur describ algorithm assum weight take discret valu set sdd0d2dsd restrict weight discret valu limit learn abil neural network gener problem configur defin ntupl weight n number weight network configur space construct allow weight take valu cost function defin error desir output network output learn exampl shown j c refer jth network output input cth train exampl j c refer jth desir output cth train exampl indic c refer differ output network variou train exampl respect gener neighbor configur chang one randomli chosen weight element configur use uniform probabl distribut chose weight chang thu probabl gener neighbor configur current configur uniformli distribut neighbor one alway choos larg valu scale input output data valu small rang achiev better accuraci procedur begin stopcriterion system frozen repeat accept fals w lm chang perturb els exp c accept updateconfigur j equilibriumisapproachedsufficientlyclos c m1 fc fig3 stochast backpropag algorithm psuedopasc psuedopasc descript stochast backpropag shown fig3 initi routin assign default valu variabl particular current configur temperatur outeriterationcount loop correspond simul anneal weight space inner loop repres simul anneal fix valu control paramet execut probabl distribut current configur possibl configur becom stabl help us achiev boltzman distribut outer loop chang control paramet slowli final valu near 0 correspond slow cool achiev configur global minimum error step insid innermost loop combin backpropag transit simul anneal use backprop backpropag algo rithm 4 subroutin comput error deriv e w lm respect variou weight deriv help us estim chang error function particular weight chang perturb produc neighbor configur chang randomli chosen weight chang error function due perturb estim use error deriv obtain backpropag algorithm new configur accept uncondit iff lower error current one otherwis new configur accept probabl final program variabl updat updat state newli chosen configur note accept criterion implement draw random number uniform distribut 01 compar exp eit basic algorithm shown fig3 made effici one chang function backprop procedur notic backpropag produc partial deriv e respect weight wherea use one deriv subsequ comput better modifi backpropag algorithm comput one deriv requir comparison exist algorithm exist learn algorithm eg hopfield net works30 31 base memor learn exampl accur use predict gener problem two flexibl learn method includ backpropag 9 32 boltzman machin learn 33 iter algorithm base gradient descent error surfac backpropag error fit given set weight defin eq b1 j c actual state unit j inputoutput train exampl c j c desir state backpropag algorithm comput gradient error respect weight hidden unit j layer j affect error via effect unit k next layer k deriv error e j given eq b2 index c suppress clariti dy k weight chang direct reduc error output one may chang weight simultan avoid conflict local weight adjust boltzman machin stochast natur aim learn weight achiev probabl distribut inputoutput map boltzman machin learn 5 base seri simul anneal state space network state space network character defin state net work state node describ output state network ntupl vector one compon node learn algorithm aim achiev certain probabl distribut state gradient descent minim error probabl distribut use simul anneal weight space quit differ boltzman machin 33 simul anneal carri state space network learn method work costerror surfac concav sinc algorithm base simpl heurist gradient descent get stuck local minima furthermor get stuck plateau gradient small shown fig4 algorithm guarante optim discov weight learn problem npcomplet gener 34 remain npcomplet sever restrict surpris heurist learn algorithm like backpropag alway work trust find global optim weight gener problem two way approach npcomplet problem approxim method 35 b stochast enumer method simul anneal 36 sinc difficult formul gener approxim method neural network learn use simul anneal method extend backpropag algorithm stochast weight chang learn weight algorithm converg properti achiev global optim weight simpl network gener implement perform studi show algorithm perform well mani gener problem global minima figur 4 two bad case gradient descent 4 model analysi stochast backpropag given neighborhood structur stochast backpropag view algorithm continu attempt transform current configur one neighbor mechan mathemat describ mean markov chain sequenc trial outcom trial depend outcom previou one 37 case stochast backpropag trial correspond transit clear outcom transit depend outcom previou one ie current confi gurat markov chain describ mean set condit probabl pair outcom probabl outcom kth trial j given outcom k 1th trial let k denot probabl outcom kth trial k obtain solv recurs l k1p li k1k k12c1 sum taken possibl outcom hereinaft xk denot outcom kth trial henc condit probabl depend k correspond markov chain call homogen otherwis call inhomogen case stochast backpropag condit probabl denot probabl kth transit transit configur configur j thu xk configur obtain k tran sition view call transit probabl r r matrix pk1k transit matrix r denot size configur space transit probabl depend valu control paramet temperatur thu kept constant correspond markov chain homogen transit matrix defin l 1li g il il forallji ie transit probabl defin product follow two condit probabl gener gener configur j configur accept probabl ij accept configur j gener configur correspond matric gt call gener accept matric respect result definit eqc4 pt stochast matrix ie 1 gt repres uniform distribut neighborhood sinc transit implement choos random neighbor configur j current configur comput metropoli criteria ie min1 expdek b stochast backpropag algorithm attain global minimum possibl larg number transit say k follow relat hold r opt set global optim configur minimum error fit shown eqc5 hold asymptot ie lim prx k r opt 1 certain condit matrix l gt l satisfi 2 3 certain addit condit matrix k rate converg sequenc k faster log k proof carri three step show exist stationari distribut homogen markov chain b show converg innerloop stochast backpropag stationari distribu tion c show stationari distribut final frozen system nonzero probabl optim configur proof last step conting cool rate provid us bound rate 41 exist stationari distribut follow theorem establish exist stationari distribut theorem 1 feller 37 stationari distribut q finit homogen markov chain exist markov chain irreduc aperiod furthermor vector q uniqu determin follow equat note q left eigenvector matrix p eigenvalu 1 markov chain irreduc pair configur ij posit probabl reach j finit number transit ie markov chain aperiod configur r greatest common divisor integ n1 equal 1 case stochast backpropag matrix p defin eq c4 sinc definit guarante foral ijc suffici irreduc check markov chain induc gt irreduc 38 ie g lk lk1 0 k01p1 c10 establish aperiod one use fact irreduc markov chain aperiod follow condit satisfi 38 thu aperiod suffici assum use inequ eq c12 fact foral 1 prove follow l 1lit r l g l l 1lit jt r l g l l 1lit jt r g l l 1lit r g l l 1 r g l thu p l 1lit r l g l 0 c14 thu eq c11 hold ii summar follow result homogen markov chain condit probabl given eq c4 stationari distribut matric gt satisfi eq c10 c12 respec tive note stochast backpropag accept probabl defin henc eq c12 alway satisfi set foral 0 r opt j r opt 42 converg stationari distribut impos condit matric gt ensur converg qt distribut p given eq c5 theorem 2 38 two argument function yei e opt taken i0i arbitrari configur depend stationari distribut qt given i0i provid matric g satisfi follow condit proof theorem discuss elsewher 39 implicitli assum accept probabl depend cost valu configur configur henc i0i depend particular choic 0 sinc ensur eq c5 follow condit suffici 38 thu condit c19c23 guarante converg easili check matric gt stochast backpropag meet condit 43 cool rate certain condit matric gt stochast backpropag algorithm converg global minimum probabl 1 valu l control parameterl 012 correspond markov chain infinit length l eventu converg 0 l ie valid follow equat shown limq r opt howev cool rate constraint sequenc l control parameterl 012 satisfi certain properti assur converg global optim configur particular k form g one guarante converg global optim configur 40 44 converg result list condit simul anneal converg global optim configur formul stochast backpropag use similar accept probabl gener probabl cool schedul simul anneal algorithm 38 use metropoli criteria accept cri teria configur gener mechan uniform distribut neighbor gener mechan across two neighbor configur symmetr one gener arbitrari configur given configur finit number step thu satisfi condit theorem 1 2 guarante converg global minima follow cool schedul n logn g satisfi condit cool rate guarante converg global minima provid g greater depth local minima 5 implement carri complet implement stochast backpropag conduct valid stu die implement algorithm unix platform sequenti machin eg sun 360 sinc gener often take larg amount comput plan reimplement algorithm vector processor eg cray xmp speedup current prototyp base sourc code public domain softwar implement backpropag algorithm41 studi softwar reus pertin modul implement stochast backpropag implement compris 5000 line c code requir approxim seven man month design code debug larg fraction effort direct toward read understand backpropag softwar reus effort reward reduc time design code debug code provid g maxim depth local minima error surfac verifi walkthrough method extens usag prototyp use seminar cours research backpropag packag 41 three modul user interfac learn modul test modul user interfac implement command associ command intern function via tabl command allow user examin modifi state softwar choos option specifi type speed comput display learn modul implement backpropag algorithm comput error deriv weight adjust tune weight iter train repeat weight adjust fix number time till total squar error reach valu set user learn algorithm provid option adjust weight examin pattern examin pattern test modul comput output given input neural network also comput total squar error output produc network desir output specifi input pattern augment user interfac modul ad command examin modifi paramet stochast backpropag routin process command instal tabl associ command process routin command enabl user choos altern learn algorithm also ad implement stochast backpropag c simplifi cool schedul cool schedul base expotenti cool simplic effici cool schedul use mani applic 42 main routin learn modul name trial modifi adjust weight base weight randomli choos neighbor accept metropoli criteria test modul alter implement 6 valid evalu stochast backpropag learn algorithm gener two type function monoton function b non monoton function experiment setup consist four modul data set gener neural network simul data collect data analysi shown fig 5 data set gener modul use four function linear quadrat logarithm trigonometr shown tabl 1 neural network simul implement altern learn algorithm backpropag stochast backpropag take network configur data set modul simul learn algorithm produc output well weight data collect modul compris set routin sampl state neural network simul period say everi 100 epoch learn sampl weight collect termin learn data analysi modul produc graph statist algorithm monitor learn phase well test phase perform algorithm learn phase measur total squar error learn set inputoutput pair perform algorithm test phase measur per pattern error new set inputoutput pair distinct learn set behavior altern algorithm learn shown figur 6 7 figur 6 show chang total squar error along learn step monoton function ie linear logarithm quadrat function stochast backpropag backpropag yield compar total squar error test train network independ set sampl stochast backpropag train network yield 17 error per pattern backpropag train network yield 09 error per pattern network predict output sampl within 5 desir output figur 7 show chang total squar error epoch learn nonmonoton function ie trigonometr function stochast backpropag network configur monoton non monoton experiment setup data set backpropag stochast backprop neural network simul output input finat weight experi per datafil plot statist plot statist etc analysi modul weight input outpput weight sampl period intermedi data collect problem fig 5 experi design perform comparison studi tabl 1 function control studi yield better total squar error learn well test network train backpropag network yield per pattern error 15 predict output 14 sampl 50 within 5 desir put network train stochast backpropag yield per pattern error 11 predict output sampl 50 within 5 desir output initi stochast backpropog stochast backpropog backpropogation2epoch figur learn monoton function initi stochast backpropog backpropogation2epoch figur 7 learn nonmonoton function 7 conclus stochast backpropag provid feasibl learn algorithm gener problem reduc error fit train exampl critic gener problem stochast backpropag perform well monoton function use simpl network fewer hidden node perform better backpropag algorithm nonmonoton function stochast backpropag learn algorithm theoret properti converg also provid stochast guarante find optim weight howev experi confirm one need tune paramet implement stochast backpropag get better result 8 acknowledg acknowledg use help cs 8199 class spring 1991 urop univers minnesota backpropag package41 9 r handwritten numer recognit multilay neural network improv learn algorithm learn learn intern represent back propag error learn algorithm boltzman machin linear function neuron structur train art adapt pattern recognit selforgan neural network brain style comput learn gener learn recogn shape parallel network learn translat invari recognit massiv parallel network design intellig robot feder geometr mchine neural comput use artifici neural net statist discoveri observ use backpropag beyond regress new tool predict analysi behaviori scienc connectionist expert system materi handl conserv domain neural connect propag represent continu function sever variablesbi superposit continu function one variabl addit mean gener ch map abil three layer train 3node neural net npcomplet complex load shallow network neural network design complex learn scale gener learn algorithm gener problem memor gener ch creat artifici equat state calcul fast comput machin neural network physic system emerg collect comput abil neural comput decis optim problem parallel distribut process explor microstructur cognit constraint stisfact machin learn complex connectionist learn variou node function comput interact guid theori npcomplet optim simul anneal introduct probabl theori applic probabilist hill climb algorithm properti applic theori applic cool schedul optim anneal explor parallel distribut process handbook model john wiley tr linear function neuron structur train connectionist expert system art adapt pattern recognit selforgan neural network cool schedul optim anneal complex load shallow neural network simul anneal boltzmann machin stochast approach combinatori optim neural comput neurocomput applic explor parallel distribut process neural network design complex learn train 3node neural network npcomplet parallel distribut process explor microstructur cognit vol 1 creat artifici neural network gener design intellig robot feder geometr machin brain style comput comput intract learn translat invari recognit massiv parallel network complex connectionist learn variou node function ctr v kumar shekhar b amin scalabl parallel formul backpropag algorithm hypercub relat architectur ieee transact parallel distribut system v5 n10 p10731090 octob 1994