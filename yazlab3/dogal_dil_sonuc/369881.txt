cascad gener use multipl classifi increas learn accuraci activ research area paper present two relat method merg classifi first method cascad gener coupl classifi loos belong famili stack algorithm basic idea cascad gener use sequenti set classifi step perform extens origin data insert new attribut new attribut deriv probabl class distribut given base classifi construct step extend represent languag high level classifi relax bia second method exploit tight coupl classifi appli cascad gener local iter divid conquer algorithm reconstruct instanc space occur addit new attribut new attribut repres probabl exampl belong class given base classifi implement three local gener algorithm first merg linear discrimin decis tree second merg naiv bay decis tree third merg linear discrimin naiv bay decis tree algorithm show increas perform compar correspond singl model cascad also outperform method combin classifi like stack gener compet well boost statist signific confid level b introduct abil chosen classif algorithm induc good gener depend appropri represent languag express gener exampl given task represent languag standard decis tree dnf formal split instanc space axisparallel hyperplan represent languag linear discrimin function set linear function split instanc space obliqu hyper plane sinc differ learn algorithm employ differ knowledg represent search heurist differ search space explor divers result obtain statist heneri 1997 refer rescal method use class overpredict lead bia rescal consist appli algorithm sequenc output algorithm use input anoth algorithm aim would use estim probabl deriv learn algorithm input second learn algorithm purpos produc unbias estim qc jw condit probabl class problem find appropri bia given task activ research area consid two main line research one hand method tri select appropri algorithm given task instanc schaf fer select cross valid schaffer 1993 hand method combin predict differ algorithm instanc stack gener wolpert 1992 work present near follow second line research instead look method fit data use singl represent languag present famili algorithm gener name cascad gener whose search space contain model use differ represent languag cascad gener perform iter composit classifi iter classifi gener input space extend addit new attribut form probabl class distribut obtain exampl gener classifi languag final classifi languag use high level gener languag use term express languag low level cla sifier sens cascad gener gener unifi theori base theori gener earlier use form cascad gener perform loos coupl classi fier method appli local iter divideandconqu algorithm gener tight coupl classifi method refer local cascad gener implement gener decis tree interest relat multivari tree brodley utgoff 1995 neural network name cascad correl architectur fahlman gener local cascad gener describ analyz paper experiment studi show methodolog usual improv accuraci decreas theori size statist signific level next section review previou work area multipl model section 3 present framework cascad gener section 4 discuss strength weak propos method comparison approach multipl model section 5 perform empir evalu cascad gener use uci data set section 6 defin new famili multistrategi algorithm appli cascad gener local section 7 empir evalu local cascad gener use uci data set section 8 examin behavior cascad gener provid insight work last section summar main point work discuss futur research direct 2 relat work combin classifi vote common method use combin classifi point ali pazzani 1996 strategi motiv bayesian learn theori stipul order maxim predict accuraci instead use singl learn model one ideal use model hypothesi space vote hypothesi weight posterior probabl hypothesi given train data sever variant vote method found machin learn literatur uniform vote opinion base classifi contribut final classif strength weight vote base classifi weight associ could chang time strengthen classif given classifi anoth approach combin classifi consist gener multipl model sever method appear literatur paper analyz biasvari analysi kohavi wolpert 1996 method mainli reduc varianc bag boost method mainli reduc bia stack gener metalearn 21 varianc reduct method breiman 1996 propos bag produc replic train set sampl replac replic train set size origin data exampl appear other may appear replic train set classifi gener classifi use classifi exampl test set usual use uniform vote scheme boost algorithm freund schapir 1996 maintain weight exampl train set reflect import adjust weight caus learner focu differ exampl lead differ classifi boost iter algorithm iter weight adjust order reflect perform correspond classifi weight misclassifi exampl increas final classifi aggreg learn classifi iter weight vote weight classifi function accuraci 22 bia reduct method wolpert 1992 propos stack gener techniqu use learn two level learn algorithm use determin output base classifi combin origin data set constitut level zero data base classifi run level level one data output base classifi anoth learn process occur use input level one data output final classif sophist techniqu cross valid could reduc error due bia chan stolfo 1995 present two scheme classifi combin arbit combin scheme base meta learn metaclassifi gener meta data built base predict base classifi arbit also classifi use arbitr among predict gener differ base classifi train set arbit select avail data use select rule exampl select rule select exampl whose classif base classifi predict consist arbit togeth arbitr rule decid final classif base base predict exampl arbitr rule use predict arbit base classifi obtain major later chan stolfo 1995a framework extend use arbiterscombin hierarch fashion gener arbitercombin binari tree skalak 1997 present dissert discuss method combin classifi present sever algorithm base stack gener abl improv perform nearest neighbor classifi brodley 1995 present mc hybrid algorithm combin singl tree node univari test multivari test gener linear machin instanc base learner node mc use set ifthen rule perform hillclimb search best hypothesi space search bia given partit dataset set rule incorpor knowledg expert mc use dynam search control strategi perform automat model select mc build tree appli differ model differ region instanc space 23 discuss result boost bag quit impress use 10 iter ie gener classifi quinlan 1996 report reduct error rate 10 19 quinlan argu techniqu mainli applic unstabl classifi techniqu requir learn system stabl obtain differ classifi small chang train set analysi biasvari decomposit error kohavi wolpert1996 reduct error observ boost bag mainli due reduct varianc breiman 1996 reveal boost bag improv predict accuraci learn algorithm unstabl mention kohavi bauer 1998 main problem boost seem robust nois expect noisi exampl tend mi classifi weight increas exampl present sever case perform boost algorithm degrad compar origin algorithm also point bag improv dataset use experiment evalu conclud although boost averag better bag uniformli better bag higher accuraci boost bag mani domain due reduct bia boost also found frequent higher varianc bag boost bag requir consider number member model reli vari data distribut get divers set model singl learn algorithm wolpert 1992 say success implement stack gener classif task black art condit stack work still unknown exampl current hard fast rule say level 0 gener use level 1 gener one use k number use form level 1 input space etc recent ting witten 1997 shown success stack gener requir use output class distribut rather class predict experi mlr algorithm linear discrimin suitabl 3 cascad gener consid learn set multidimension input vector yn output variabl sinc focu paper classif problem yn take valu set predefin valu yn 2 fcl 1 cl c g c number class classifi function appli train set construct model gener model map input space x discret output variabl use predictor repres x assign valu exampl x tradit framework classif task framework requir predictor x output vector repres condit probabl distribut p1 pc p repres probabl exampl x belong class ie p jx class assign exampl x one maxim last express commonli use classifi naiv bay discrimin classifi exampl way classifi eg c45 quinlan 1993 differ strategi classifi exampl requir chang obtain probabl class distribut defin construct oper x repres model train data x repres exampl exampl x oper concaten input vector x output probabl class di tribut oper appli exampl dataset 0 obtain new dataset 00 cardin 00 equal cardin 0 ie number exampl exampl x 2 00 equival exampl 0 augment c new attribut c repres number class new attribut element vector class probabl distribut obtain appli classifi exampl x repres formal follow ad 0 repres applic model data set 0 repres effect dataset dataset contain exampl appear 0 extend probabl class distribut gener model cascad gener sequenti composit classifi gener level appli phi oper given train set l test set two classifi gener proce follow use gener level 1 data level level learn level 1 train data classifi level 1 test data step perform basic sequenc cascad gener classifi classifi 1 repres basic sequenc symbol r previou composit could repres succinctli appli equat 2 3 equival simplest formul cascad gener possibl extens includ composit n classifi parallel composit classifi composit n classifi repres case cascad gener gener n1 level data final model one given n classifi model could contain term form condit base attribut build previou built classifi variant cascad gener includ sever algorithm parallel could repres formal run parallel oper return new data set l 0 contain number exampl l exampl l 0 contain n gamma 1 theta cl new attribut cl number class algorithm set contribut cl new attribut 31 illustr exampl exampl consid uci blake keogh merz 1999 data set monks2 monk data set describ artifici robot domain quit well known machin learn commun robot describ six differ attribut classifi one two class chosen monks2 problem known difficult task system learn decis tree attributevalu formal decis rule problem robot ok exactli two six attribut first valu problem similar pariti problem combin differ attribut way make complic describ dnf cnf use given attribut exampl origin train data present head bodi smile hold color tie class round round ye sword red ye ok round round balloon blue ok use tenfold cross valid error rate c45 329 naiv bay 342 composit model c45 naiv bay c45rnaivebay oper follow level 1 data gener use naiv bay classifi naiv bay build model origin train set model use comput probabl class distribut exampl train test set level 1 obtain extend train test set probabl class distribut given naiv bay exampl shown earlier take form head bodi smile hold color tie pok pnot ok class round round ye sword red ye 0135 0864 ok round round balloon blue 0303 0696 ok new attribut pok pnot ok probabl exampl belong class oknot ok c45 train level 1 train data classifi level 1 test data composit c45rnaivebay obtain error rate 89 substanti lower error rate c45 naiv bay none algorithm isol captur underli structur data case cascad abl achiev notabl increas perform figur 1 present one tree gener c45rnaivebay tree contain mixtur origin attribut smile tie new attribut construct naiv bay pok pnot ok root tree appear attribut pok attribut repres particular class probabl class ok calcul naiv bay decis tree gener c45 use construct attribut given naiv bay redefin differ threshold two class problem bay rule use p ok threshold 05 decis tree set threshold 027 decis node kind function given bay strategi exam ple attribut pok seen function comput pclass okjx use bay theorem branch decis tree perform one test class probabl certain sens decis tree combin two represent languag naiv bay languag decis tree construct step perform cascad insert new attribut incorpor new knowledg provid naiv bay new knowledg allow signific increas perform verifi decis tree despit tie smile smile ok tie smile ok pnot ok pnot ok pnot ok ok pnot ok pnot ok ok ok ok ok ok figur 1 tree gener c45rbay fact naiv bay fit well complex space cascad framework lower level learner delay decis high level learner kind collabor classifi cascad gener explor 4 discuss cascad gener belong famili stack algorithm wolpert 1992 defin stack gener gener framework combin cla sifier involv take predict sever classifi use predict basi next stage classif cascad gener may regard special case stack gener mainli due layer learn structur aspect make cascad gener novel ffl new attribut continu take form probabl class distribut combin classifi mean categor class loos strength classifi predict use probabl class distribut allow us explor inform ffl classifi access origin attribut new attribut built lower layer consid exactli way origin attribut ffl cascad gener use intern cross valid aspect affect comput effici cascad mani idea discuss literatur ting 1997 use probabl class distribut level1 attribut use origin attribut possibl use origin attribut class predict level 1 attribut point wolpert origin paper stack gener aliz skalak 1997 refer schaffer use origin attribut class predict level 1 attribut disappoint result view could explain fact combin three algorithm similar behavior biasvari analysi decis tree rule neuralnetwork see section 82 detail point chan stolfo 1995a use origin attribut class predict scheme denot classattribut combin mix result exploit aspect make cascad gener succeed particular combin impli conceptu differ ffl stack parallel natur cascad sequenti effect intermedi classifi access origin attribut plu predict low level classifi interest possibl explor paper provid classifi n origin attribut plu predict provid classifi ffl ultim goal stack gener combin predict goal cascad gener obtain model use term represent languag lower level classifi ffl cascad gener provid rule choos low level classifi high level classifi aspect develop follow section 5 empir evalu 51 algorithm ali pazzani 1996 tumer gosh 1995 present empir analyt result show combin error rate depend error rate individu classifi correl among suggest use radic differ type classifi reduc correl error criterion select algorithm experiment work use three classifi differ behavior naiv bay linear discrimin decis tree 511 naiv bay bay theorem optim predict class unseen exampl given train set chosen class one maxim pc px attribut independ pxjci decompos product px 1 show procedur surprisingli good perform wide varieti domain includ mani clear depend attribut implement algorithm requir probabl estim train set case nomin attribut use count continu attribut discret equal size interv found produc better result assum gaussian distribut domingo pazzani 1997 j dougherti r kohavi sahami 1995 number bin use function number differ valu observ train set lognr differ valu heurist use dougherti et al 1995 good overal result miss valu treat anoth possibl valu attribut order classifi queri point naiv bay classifi use avail attribut langley 1996 state naiv bay reli import assumpt variabl dataset summar singl probabilist descript suffici distinguish class analysi biasvari impli naiv bay use reduc set model fit data result low varianc data adequ repres set model obtain larg bia 512 linear discrimin linear discrimin function linear composit attribut maxim ratio betweengroup varianc withingroup varianc assum attribut vector exampl class c independ follow certain probabl distribut probabl densiti function f new point attribut vector x assign class probabl densiti function f x maxim mean point class distribut cluster center boundari separ two class hyperplan michi spiegelhalt taylor 1994 two class uniqu hyperplan need separ class gener case q class need separ appli linear discrimin procedur describ get hyperplan equat hyperplan given use singular valu decomposit svd comput gamma1 svd numer stabl tool detect sourc collinear last aspect use method reduc featur linear combin linear discrimin use almost avail attribut classifi queri point breiman 1996 state analysi biasvari linear discrimin stabl classifi achiev stabil limit set model fit data result low varianc data adequ repres set model obtain larg bia 513 decis tree dtree version univari decis tree use standard algorithm build decis tree split criterion gain ratio stop criterion similar c45 prune mechan similar pessimist error c45 dtree use kind smooth process usual improv perform tree base classifi classifi new exampl exampl travers tree root leaf dtree exampl classifi take account class distribut leaf also class distribut node path node path contribut final classif instead comput class distribut path tree classif time done buntin 1990 dtree comput class distribut node grow tree done recurs take account class distribut current node predecessor current node use recurs bayesian updat formula pearl 1988 p e n probabl one exampl fall node n seen shorthand p e 2 en e repres given exampl en set exampl node n similarli p e n1 je n probabl one exampl fall node n goe node n1 p e n1 je probabl one exampl class c goe node n node n1 recurs formul allow dtree comput effici requir class distribut smooth class distribut influenc prune mechan treatment miss valu relev differ c45 decis tree use subset avail attribut classifi queri point breiman 1996 among research note decis tree unstabl classifi small variat train set caus larg chang result predictor high varianc fit kind data bia decis tree low 52 experiment methodolog chosen 26 data set uci repositori previous use compar studi estim error rate algorithm given dataset use 10 fold stratifi cross valid minim influenc variabl train set repeat process ten time time use differ permut dataset 1 final estim mean error rate obtain run cross valid iter cv algorithm train train partit data classifi also evalu test partit data algorithm use default set comparison algorithm perform use pair ttest signific level set 999 dataset use wilcoxon matchedpair signedrank test compar result algorithm across dataset goal empir evalu show cascad gener plausibl algorithm compet quit well well establish techniqu stronger statement done extens empir evalu tabl 1 data characterist result base classifi dataset class exampl dtree bay discrim c45 c50 australian 2 690 1413sigma06 1448sigma04 1406sigma01 1471sigma06 1417sigma07 balanc 3 625 2235sigma07 band 2 238 2135sigma13 2324sigma12 2320sigma14 2398sigma18 2416sigma14 diabet 2 768 2646sigma07 german 2 1000 2793sigma07 glass 6 213 3014sigma24 ionospher iri 3 150 467sigma09 427sigma06 letter 26 20000 satimag 6 6435 1347sigma02 segment 7 vehicl 4 846 vote tabl 1 present error rate standard deviat base classifi rel algorithm gamma sign first column mean error rate algorithm significantli better wors dtree error rate c50 present refer result provid evid singl algorithm better overal 53 evalu cascad gener tabl 2 3 present result pairwis combin three base classifi promis combin three model column correspond cascad gener combin combin conduct pair ttest composit model compar compon use pair ttest signific level set 999 gamma sign indic combin eg c4rbay significantli better compon algorithm ie c45 bay result summar tabl 4 5 first line show arithmet mean across dataset show promis combin c45rdiscrim c45rnaiv bay c45rdiscrimrna bay c45rnaiv tabl 2 result cascad gener composit model compar compon bayrbay bayrdi bayrc45 disrdi disrbay disrc45 australian 1469sigma05 1361sigma02 balanc 706sigma11 band 2236sigma09 2199sigma08 1876sigma12 2328sigma14 2201sigma16 breast credit 1491sigma04 1335sigma03 1397sigma06 1422sigma01 1359sigma04 1434sigma03 diabet german glass heart ionospher 976sigma07 914sigma03 857sigma08 1338sigma08 iri letter segment sonar 2559sigma14 2372sigma11 2184sigma20 2481sigma12 vehicl vote 1000sigma03 bayesrdiscrim confirm second line show geometr mean third line show averag rank base cascad algorithm comput dataset assign rank 1 accur algorithm rank 2 second best remain line compar cascad algorithm toplevel algorithm fourth line show number dataset toplevel algorithm accur correspond cascad algorithm versu number less fifth line consid dataset error rate differ signific 1 level use pair ttest last line show pvalu obtain appli wilcoxon matchedpair signedrank test statist show promis combin use decis tree highlevel classifi naiv bay discrim lowlevel classifi new attribut built discrim naiv bay express relat attribut outsid scope dnf algorithm like c45 new attribut systemat appear root composit model one main problem combin classifi algorithm combin empir evalu suggest ffl combin classifi differ behavior biasvari analysi tabl 3 result cascad gener composit model compar compon dataset c45rc45 c45rdi c45rbay c45rdiscrbay c45rbayrdisc stack gen australian 1474sigma05 1399sigma09 1541sigma08 1424sigma05 1534sigma09 1399sigma04 balanc band 2377sigma17 2173sigma25 2275sigma18 2148sigma20 2218sigma15 2145sigma12 breast credit 1421sigma06 1385sigma04 1507sigma07 1484sigma04 1375sigma06 diabet german glass 3202sigma24 3609sigma18 3360sigma16 3468sigma18 3511sigma25 3128sigma19 hepat ionospher 1021sigma13 iri letter segment 321sigma02 sonar 2802sigma32 2475sigma29 2436sigma19 2445sigma18 2383sigma21 2481sigma10 vote tabl 4 summari result cascad gener measur bay bayrbay bayrdi bayrc4 disc discrdisc discrbay discrc4 arithmet mean 1762 1729 1642 1529 1780 1772 1639 1794 geometr mean 1331 1272 1261 1071 1397 1377 1214 1482 averag rank 967 946 663 752 906 877 723 1029 nr win gamma 1412 test ffl low level use algorithm low varianc ffl high level use algorithm low bia cascad framework lower level learner delay final decis high level learner select learner low bia high level abl fit complex decis surfac take account stabl surfac drawn low level learner tabl 5 summari result cascad gener measur c45 c45rc45 c45rbay c45rdi c45rdisrbay c45rbayrdi arithmet mean 1598 1598 1344 1419 1309 1327 geometr mean 1140 1120 825 993 795 781 averag rank 983 904 785 617 646 669 nr win gamma 715 419 1115 818 818 test tabl 6 summari comparison stack gener c45rdiscrbay vs stackg c45rbayrdisc vs stackg number win 11 signific win 6 5 6 4 test given equal perform would prefer fewer compon classifi sinc train ing applic time lower smaller number compon larger number compon also advers affect comprehens studi version three compon seem perform better version two compon research need establish limit extend scenario 54 comparison stack gener compar variou version cascad gener stack gener aliz defin ting 1997 reimplement stack gener level 0 classifi c45 bay level 1 classifi discrim attribut level 1 data probabl class distribut obtain level 0 classifi use 5fold stratifi crossvalid 2 tabl 3 show last column result stack gener stack gener compar use pair ttest c45rdiscrimrna bay c45rnaiv bayesrdiscrim order gamma sign indic dataset cascad model perform significantli better wors tabl 6 present summari result provid evid gener abil cascad gener model competit stack gener comput level 1 attribut use intern crossvalid use intern crossvalid affect cours learn time cascad model least three time faster stack gener cascad gener exhibit good gener abil comput effici aspect lead hypothesi improv cascad gener 6bi appli iter divideandconqu algorithm hypothesi examin next section 6 local cascad gener mani classif algorithm use divid conquer strategi resolv given complex problem divid simpler problem appli recurs strategi subproblem solut subproblem combin yield solut origin complex problem basic idea behind well known decis tree base algorithm id3 quinlan 1984 cart breiman et al 1984 assist kononenko et al 1987 c45 quin lan 1993 power approach deriv abil split hyper space subspac fit subspac differ function section explor cascad gener problem subproblem divid conquer algorithm gener intuit behind propos method behind divid conquer strategi relat captur global level discov simpler subproblem follow section present detail appli cascad gener local develop strategi decis tree although possibl use conjunct divid conquer method like decis list rivest 1987 61 local cascad gener algorithm gener composit classif algorithm elabor build classifi given task iter divid conquer algorithm local cascad gener extend dataset insert new attribut new attribut propag subtask paper restrict use local cascad gener decis tree base algorithm howev possibl use divideandconqu algorithm figur 2 present gener algorithm local cascad gener restrict decis tree method refer cgtree grow tree new attribut comput decis node appli phi oper new attribut propag tree number new attribut equal number class appear exampl node number vari differ level tree gener deeper node may contain larger number attribut parent node could disadvantag howev number new attribut gener decreas rapidli tree grow class discrimin deeper node also contain exampl decreas number class mean tree grow number new attribut decreas order appli predictor cgtree must store node model gener base classifi use exampl node classifi new exampl exampl travers tree usual way input data set classifi output decis tree function cgtreed stop return leaf class probabl distribut els choos attribut maxim split criterion 0 partit exampl base valu attribut gener subtre ree return tree contain decis node base attribut ai store descend subtre ree endif end figur 2 local cascad algorithm base decis tree decis node extend insert probabl class distribut provid base classifi predictor node framework local cascad gener develop cgltree use phid adiscrimd oper construct step intern node cgltree construct discrimin function discrimin function use build new attribut exampl valu new attribut comput use linear discrimin function decis node number new attribut built cgltree alway equal number class taken exampl node order restrict attent well popul class use follow heurist consid class number exampl node belong class greater n time number attribut 3 default n 3 impli differ node differ number class consid lead addit differ number new attribut anoth restrict use construct oper ad error rate result classifi less 05 train data empir studi use two algorithm appli cascad gener local first one cgbtree use construct oper second one cgbltree use construct oper aspect algorithm similar cgltree one restrict applic phid induc classifi must return correspond probabl class distribut x 2 0 classifi satisfi requisit could appli possibl imagin cgtree whose intern node tree exampl small modif c45 4 enabl construct cgtree whose intern node tree gener c45 62 illustr exampl bayes7 bayes11 3 bayes7 3 ok ok ok ok figur 3 tree gener cgtree use discrimrbay construct oper figur 3 repres tree gener cgtree monks2 problem construct oper use phid discrimrbayesx root tree naiv bay algorithm provid two new attribut bay 7 bay 8 linear discrimin use continu attribut two continu attribut built naiv bay case coeffici linear discrimin shrink zero process variabl elimin use discrimin algorithm gain ratio criterion choos bay 7 attribut test dataset split two partit one contain exampl class ok leaf gener partit two new bay attribut built bay 11 bay 12 linear discrimin gener base two bay attribut built root tree attribut base linear discrimin chosen test attribut node dataset segment process tree construct proce exampl illustr two point ffl interact classifi linear discrimin contain term built naiv bay whenev new attribut built consid regular attribut attribut combin built deeper node contain term base attribut built upper node ffl reus attribut differ threshold attribut bay 7 built root use twice tree differ threshold 63 relat work multivari tree respect final model clear similar cgltree multivari tree brodley utgoff langley refer multivari tree topolog equival threelay infer network construct abil system similar cascad correl learn architectur fahlman lebier 1991 also final model cgbtree relat recurs naiv bay present langley interest featur unifi singl framework sever system differ research area previou work gama brazdil 1999 compar system ltree similar cgltree oc1 murthi et al lmdt brodley et al cart breiman et al focu paper methodolog combin classifi compar algorithm method gener combin multipl model 7 evalu local cascad gener section evalu three instanc local cascad algorithm cgbtree cgltree cgbltree compar local version correspond global model two standard method combin classifi boost stack gener implement local cascad gener algorithm base dtree use exactli split criteria stop criteria prune mechan moreov share mani minor heurist individu small mention collect make differ decis node cgltree appli linear discrimin describ cg btree appli naiv bay algorithm cgbltree appli linear discrimin order attribut naiv bay categor attribut order prevent overfit construct new attribut constrain depth 5 addit level prune greater level prune dtree tabl 7a present result local cascad gener column correspond local cascad gener algorithm algorithm compar similar cascad model use pair ttest exampl cgltree compar c45rdiscrim gamma sign mean error rate composit model statist signific level lower higher correspond model tabl 8 present compar summari result local cascad gener correspond global model illustr benefit appli cascad gener local tabl 7 result aloc cascad gener bboost stack c boost cascad algorithm second row indic model use comparison dataset cgbtree cgltree cgbltree c50boost stack c5brbay vs correspond cascad model vs cgbltree vs c50boost adult 1346sigma04 1356sigma03 1352sigma04 gamma 1433sigma04 1396sigma06 1441sigma05 australian balanc 532sigma11 band 2098sigma12 2360sigma12 2069sigma12 credit 1535sigma05 1441sigma08 1452sigma08 1341sigma08 1343sigma06 1357sigma09 diabet german glass ionospher 962sigma09 1106sigma06 1100sigma07 iri 473sigma13 280sigma04 letter mushroom sonar 2623sigma17 vehicl vote 329sigma04 430sigma05 system cgbltree compar c50boost varianc reduct method 5 stack gener bia reduct method tabl 7b present result c50boost default paramet 10 aggreg tree stack gener defin ting 1997 describ earlier section boost stack compar cgbltree use pair ttest signific level set 999 gamma sign mean boost stack perform significantli better wors cgbltree studi cgbltree perform significantli better stack 6 dataset wors 2 dataset 71 step ahead compar c50boost cgbltree significantli improv 10 dataset lose 9 dataset interest note 26 dataset 19 signific differ evid boost cascad differ behavior improv observ boost appli decis tabl 8 summari result local cascad gener cgbtree cgltree cgbltree c50boost stack g c5brbay arithmet mean 1343 1398 1292 1325 1387 1163 geometr mean 870 946 820 881 1013 608 averag rank 390 392 329 327 350 312 c45rbay c45rdi c4rbayrdi cgbltree cgbltree vs vs vs vs vs cgbtree cgltree cgbltree c50boost stack g number win 1016 1214 719 1313 1511 signific win 33 35 38 109 62 test tree mainli due reduct varianc compon error rate cascad algorithm improv mainli due reduct bia compon tabl 7c present result boost cascad algo rithm case use global combin c50 boostrnaiv bay improv c50boost 4 dataset lose 3 summari result present tabl 8 evid promis result intend near futur boost cgbltree 72 number leav anoth dimens comparison involv measur number leav correspond number differ region instanc space partit algorithm consequ seen indic model complex almost dataset 6 cascad tree split instanc space half region need dtree c50 clear indic cascad model captur better underli structur data 73 learn time learn time dimens compar classifi comparison less clear result may strongli depend implement detail well underli hardwar howev least order magnitud time complex use indic c50 c50boost run sparc 10 machin 7 algorithm run pentium 166mhz 32 mb machin linux tabl 9 present averag time need algorithm run dataset take time naiv bay refer result demonstr cgtree faster c50boost c50boost slower gener 10 tree increas complex also cgtree faster stack gener due intern cross valid use stack gener tabl 9 rel learn time base composit model bay discrim c45 bayrdi disrdi c50 dtree bayrbay disrbay bayrc4 disrc4 c4rdi c4rc4 c4rbay cgbtree c4rdisrbay cgltree cgbltree c50boost stack 41 455 481 670 685 772 1108 1516 1529 8 cascad gener improv perform cascad gener local cascad gener transform instanc space new highdimension space principl could turn given learn problem difficult one phenomenon known curs dimension section analyz behavior cascad gener three dimens error correl biasvari analysi mahalanobi distanc 81 correl ali pazzani 1996 shown desir properti ensembl classifi divers use concept error correl metric measur degre divers ensembl definit error correl two classifi defin probabl make error definit satisfi properti correl object 1 prefer defin error correl two classifi condit probabl two classifi make error given one make error definit error correl lie interv 0 1 correl one classifi 1 formula use provid higher valu one use ali pazzani expect lowest degre correl decis tree bay decis tree discrim use differ represent languag error correl bay discrim littl higher despit similar two algorithm use differ search strategi result provid evid decis tree discrimin function make uncorrel error classifi make error differ region instanc space desir properti combin classifi tabl 10 error correl base classifi c4 vs bay c4 vsdiscrim bay vs discrim averag 032 032 040 82 biasvari decomposit biasvari decomposit error tool statist theori analyz error supervis learn algorithm basic idea consist decompos expect error three compon x comput term bia varianc zeroon loss function use decomposit propos kohavi wolpert 1996 bia measur close averag guess learn algorithm match target comput x 2 varianc measur much learn algorithm guess bounc around differ set given size comput varianc x 2 estim bia varianc first split data train test set train set obtain ten bootstrap replic use build ten classifi ran learn algorithm train set estim term varianc equat 7 bia 8 equat 6 use gener classifi point x evalu set e term estim use frequenc count base algorithm use experiment evalu differ behavior biasvari analysi decis tree known low bia high varianc naiv bay linear discrimin known low varianc high bia experiment evalu shown promis combin use decis tree high level classifi naiv bay linear discrimin low level classifi illustr result measur bia varianc c45 naiv bay c45rnaiv bay dataset studi result shown figur 4 summari result present tabl figur 4 biasvari decomposit error rate c45 bay c45rbay differ dataset tabl 11 bia varianc decomposit error rate bay c45rbay averag varianc 48 159 472 averag bia 1153 1519 864 11 benefit cascad composit well illustr dataset like balancescal hepat monks2 waveform satimag comparison bay c45rbay show latter combin obtain strong reduct bia compon cost increas varianc compon c45rbay reduc bia varianc compar c45 reduct error mainli due reduct bia 83 mahalanobi distanc consid class defin singl cluster 9 euclidean space class centroid correspond cluster defin vector attribut mean x comput exampl class shape cluster given covari matrix use mahalanobi metric defin two distanc 1 withinclass distanc defin mahalanobi distanc exampl centroid cluster comput figur 5 averag increas betweenclass distanc x repres exampl attribut vector denot centroid cluster correspond class covari matrix class 2 betweenclass distanc defin mahalanobi distanc two cluster comput pool denot centroid cluster correspond class pool pool covari matrix use j intuit behind withinclass distanc smaller valu lead compact cluster intuit behind betweenclass distanc larger valu would lead us believ group suffici spread term separ mean measur betweenclass distanc withinclass distanc dataset numer attribut distanc measur origin dataset dataset extend use cascad algorithm observ withinclass distanc remain almost constant class distanc increas exampl use construct oper discrimrbay betweenclass distanc almost doubl figur 5 show averag increas betweenclass distanc respect origin dataset extend use discrim bay discrimrbay respect 9 conclus futur work paper provid new gener method combin learn model mean construct induct basic idea method use learn algorithm sequenc iter two step process occur first model built use base classifi second instanc space extend insert new attribut gener built model given exampl construct step gener term represent languag base classifi high level classifi choos one term represent power extend bia restrict high level classifi relax incorpor term represent languag base classifi basic idea behind cascad gener architectur examin two differ scheme combin classifi first one provid loos coupl classifi second one coupl classifi tightli 1 loos coupl base classifi preprocess data anoth stage framework use combin exist classifi without chang rather small chang method requir origin data extend insert probabl class distribut must gener base classifi 2 tight coupl local construct induct framework two classifi coupl local although work use local cascad gener conjunct decis tree method could easili extend divideandconqu system decis list exist method bag boost combin learn model use vote strategi determin final outcom although lead improv accuraci strong limit loss interpret model easier interpret particularli classifi loos coupl final model use represent languag high level classifi possibl enrich express represent languag low level classi fier cascad gener appli local model gener difficult interpret gener loos coupl classifi new attribut built deeper node contain term base previous built attribut allow us built complex decis surfac affect somewhat interpret final model use power represent necessarili lead better result introduc flexibl lead increas instabl varianc need control local cascad gener achiev limit depth applic construct oper requir error rate classifi use construct oper less 05 one interest featur local cascad gener provid singl framework collect differ method method relat sever paradigm machin learn exampl similar multivari tree brodley utgoff 1995 neural network fahlman lebier 1990 recurs bay langley 1993 multipl model name stack gener wolpert 1992 previou work gama brazdil 1999 present system ltree combin decis tree discrimin function mean construct induct local cascad combin extend work ltree construct oper singl discrimin function local cascad composit restrict lax use classifi construct oper moreov composit sever classifi like cgbltree could use unifi framework use overcom superfici distinct enabl us studi fundament one practic perspect user task simplifi aim achiev better accuraci achiev singl algorithm instead sever one done effici lead reduc learn time shown methodolog improv accuraci base classifi compet well method combin classifi preserv abil provid singl albeit structur model data 91 limit futur work open issu could explor futur involv ffl perspect biasvari analysi main effect propos methodolog reduct bia compon possibl combin cascad architectur varianc reduct method like bag boost ffl cascad gener work classifi could use neural network nearest neighbor think methodolog present work type classifi intend verifi empir futur problem involv basic research includ ffl cascad gener improv perform experiment studi suggest combin algorithm complementari behavior point view biasvari analysi form complementar consid exampl search bia one interest issu explor given dataset predict algorithm complementari ffl cascad gener improv perform dataset cascad abl improv perform base classifi character dataset predict circumst cascad gener lead improv perform ffl mani base classifi use gener prefer smaller number base classifi circumst reduc number base classifi without affect perform ffl cascad gener architectur provid method design algorithm use multipl represent multipl search strategi within induct algorithm interest line futur research explor flexibl induct strategi use sever divers represent possibl extend local cascad gener provid dynam control make step direct acknowledg gratitud express financi support given feder praxi xxi project eco plurianu support attribut liacc esprit ltr project thank also pedro domingo anonym review colleagu liacc valuabl comment note 1 except case adult letter dataset singl 10fold crossvalid use 2 also evalu stack gener use c45 top level version use somewhat better use c45 top level averag mean error rate 1514 3 heurist suggest breiman et al 1984 4 two differ method present ting 1997 gama 1998 5 prefer c50boost instead bag avail us allow crosscheck result differ result previou publish quinlan think may due differ method use estim error rate 6 except monks2 dataset dtree c50 produc tree one leaf 7 run time c50 c50boost reduc factor 2 suggest wwwspecorg 8 intrins nois train dataset includ bia term 9 analysi assum singl domin class cluster although may alway satisfi give insight behavior cascad composit r reduct learn multipl descript empir comparison vote classif algorithm bag uci repositori machin learn databas arc classifi classif regress tree wadsworth intern group recurs automat bia select classifi construct multivari decis tree multivari analysi optim simpl bayesian classifi zeroon loss supervis unsupervis discret continu featur recurr cascadecorrel architectur experi new boost algorithm combin classifi construct induct linear tree combin classif procedur induct recurs bayesian classifi element machin learn machin learn machin learn system induct obliqu decis tree journal artifici intellig research probabilist reason intellig system network plausibl infer learn decis list select classif method crossvalid prototyp select composit nearest neighbor classifi stack gener work correl error reduct ensembl classifi connect scienc stack gener tr probabilist reason intellig system network plausibl infer recurr cascadecorrel architectur stack gener c45 program machin learn bitechn noteib multivari decis tree element machin learn recurs automat bia select classifi construct reduct learn multipl descript prototyp select composit nearest neighbor classifi optim simpl bayesian classifi zeroon loss machin learn empir comparison vote classif algorithm learn decis list induct decis tree induct recurs bayesian classifi combin classifi construct induct ctr robert munro daren ler jon patrick metalearn orthograph contextu model languag independ name entiti recognit proceed seventh confer natur languag learn hltnaacl 2003 p192195 may 31 2003 edmonton canada csar ferri peter flach jo hernndezorallo deleg classifi proceed twentyfirst intern confer machin learn p37 juli 0408 2004 banff alberta canada saddi segrera mara n moreno experiment compar studi web mine method recommend system proceed 6th wsea intern confer distanc learn web engin p5661 septemb 2224 2006 lisbon portug ljupo todorovski sao deroski combin classifi meta decis tree machin learn v50 n3 p223249 march joo gama function tree machin learn v55 n3 p219250 june 2004 huimin zhao sudha ram entiti identif heterogen databas integr multipl classifi system approach empir evalu inform system v30 n2 p119132 april 2005 zeng dan pan jianbin predict mhc iibind peptid use rough setbas rule set ensembl appli intellig v27 n2 p153166 octob 2007 b kotsianti zaharaki p e pintela machin learn review classif combin techniqu artifici intellig review v26 n3 p159190 novemb 2006