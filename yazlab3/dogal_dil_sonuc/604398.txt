high perform comput larg scale simul subsurfac multiphas fluid heat flow tough2 wide use reservoir simul solv subsurfac flow relat problem nuclear wast geolog isol environment remedi soil groundwat contamin geotherm reservoir engin solv set coupl mass energi balanc equat use finit volum method contribut present design analysi parallel version tough2 parallel implement first partit unstructur comput domain time step set coupl nonlinear equat solv newton iter newton step jacobian matrix calcul illcondit nonsymmetr linear system solv use precondit iter solver commun requir converg test data exchang across partit border parallel perform result cray t3e900 present two real applic problem aris yucca mountain nuclear wast site studi execut time reduc 7504 second two processor 126 second 128 processor 2d problem involv 52752 equat larger 3d problem 293928 equat time decreas 10055 second 16 processor 329 second 512 processor b introduct subsurfac flow relat problem touch mani import area day societi natur resourc develop nuclear wast underground storag environment remedi groundwat contami nation geotherm reservoir engin complex model domain physic process involv numer simul play vital role solut problem contribut present design analysi parallel implement wide use tough2 softwar packag 9 10 numer simul flow transport porou fractur media contribut includ descript algorithm method use parallel implement perform evalu present address depart comput scienc high perform comput center north umea univers se901 87 umea sweden c 2000 kluwer academ publish print netherland high perform comput subsurfac simul 3 parallel simul 512 processor cray t3e900 two real applic problem although implement analysi made cray t3e use standard fortran 77 program languag mpi messag pass interfac make softwar portabl platform fortran 77 mpi avail serial version tough2 transport unsatur groundwat heat version 2 use 150 organ 20 countri see 11 exampl major applic area includ geotherm reservoir simul environment remedi nuclear wast isol tough2 one offici code use us depart energi civilian nuclear wast manag evalu yucca mountain site repositori nuclear wast context aris largest demand applic tough2 far scientist lawrenc berkeley nation laboratori current develop 3d flow model yucca mountain site involv comput grid 10 5 block relat coupl equat water ga flow heat transfer radionuclid migrat subsurfac 3 consider larger difficult applic anticip near futur analysi solut transport ever increas demand spatial resolut comprehens descript complex geolog ical physic chemic process high perform capabl tough2 code essenti applic earli result project present 5 2 tough2 simul tough2 simul packag solv mass energi balanc equat describ fluid heat flow gener multiphas multicompon system fundament balanc equat follow dt z z z integr arbitrari volum v bound surfac k denot mass kth compon water ga heat etc f k flux fluid heat surfac q k sourc sink insid v gener form flow mass paramet arbitrari nonlinear function primari thermodynam variabl densiti pressur satur etc given comput geometri space discret mani small volum block integr block becom variabl lead natur finit volum method result follow ordinari differenti equat dt volum block n anm interfac area border block n fnm flow note flow term usual contain spatial deriv replac simpl differ variabl defin block n divid distanc block center see figur 1 illustr lefthand side 3dimension grid block illustr arrow illustr flow throw interfac area neighbor grid block lefthand side two neighbor block n illustr 2dimension pictur block center mark cross includ also variabl vm v n volum dm n distanc grid block center interfac area nm f nm figur 1 space discret geometri data time implicitli discret first order differ equat deltat nm vector x consist prime variabl time flow sourcesink term right hand side evalu deltat high perform comput subsurfac simul 5 initi setup time step advanc newton iter calcul jacobian matrix solv linear system output figur 2 sketch main loop tough2 simul numer stabil multiphas problem lead coupl nonlinear algebra equat solv use newton method 3 comput procedur main solut procedur schemat outlin figur 2 read data set problem time consum part main loop time step newton iter iter linear solver time step nonlinear discret coupl algebra equat solv newton method within newton iter jacobian matrix first calcul numer differenti implicit system linear equat solv use spars linear solver precondit sever newton iter converg check control paramet measur maximum compon residu newton iter newton iter converg time advanc one time step process repeat predefin total time reach newton procedur converg preset max newtoniter current time step reduc usual half newton procedur tri reduc time step converg time advanc otherwis time step reduc anoth round newton iter follow procedur repeat converg newton iter reach system linear equat usual illcondit requir robust solver dynam adjust time step size key overcom combin possibl converg problem newton iter linear solver highli dynam 6 elmroth ding wu system trajectori sensit variat converg paramet comput major part 65 execut time spent solv linear system second major part 30 assembl jacobian matrix 4 design parallel implement aim work develop parallel prototyp tough2 demonstr abil effici solv problem significantli larger problem previous solv use serial version softwar problem larger number block number equat per block target comput system prototyp version parallel tough2 696 processor cray t3e900 nersc lawrenc berkeley nation laboratori follow section give overview design main step includ grid partit grid block reorder assembl jacobian matrix solv linear system well detail parallel implement 41 grid partit grid block reorder given finit domain describ section 2 follow consid dual mesh grid obtain repres block volum element centroid repres interfac block connect word block connect use consist origin tough2 document 10 physic properti block interfac repres data associ block connect respect tough2 comput domain defin set connect given input data inform adjac matrix construct ie matrix nonzero entri element j connect block j current implement valu 1 alway use nonzero element differ weight may use adjac matrix store compress row format call cr format slight modif harwellbo format see eg 2 descript cr harwellbo format actual partit grid p almost equals part perform use three differ partit algorithm implement meti softwar packag version 40 8 three algorithm denot kway vkway recurs partit algorithm consist meti document kway multilevel version tradit graph partit algorithm minim number edg straddl partit vkway modif kway instead minim actual total commun volum recurs recurs bisect algorithm object minim number edg cut partit grid processor block specif vector element matrix row associ block reorder processor local order block processor comput result denot updat set processor updat set partit intern set border set border set consist block edg block assign anoth processor intern set consist block updat set block includ updat set need read comput defin extern set figur 3 illustr block distribut pro cessor vertic graph repres block edg repres connect ie interfac area pair block tabl show block classifi updat extern set updat set divid intern border set tabl element place local order global number illustr reordering11314135 processor 0 processor 2processor 11 figur 3 grid partit 3 processor tabl exampl block distribut local order intern border extern set intern processor 0 7 processor 1 2 3 j processor 2 6 14 j 5 10 13 k12 4 order facilit commun element correspond borderextern block local renumb node made particular way block updat set preced block extern set updat set intern block preced border block final extern block order intern block assign specif processor place consecut one possibl order given exampl tabl processor 0 exampl grid block number 7 11 intern block ie block updat processor 0 depend block block assign processor grid block 8 12 border block processor 0 ie block updat processor 0 depend block assign processor final block 1 13 extern block processor 0 ie block updat processor 0 data associ block need readonli comput amount data processor send receiv comput approxim proport number border extern block respect consecut order extern block resid processor make possibl receiv data correspond block appropri vector without use buffer need reorder provid send processor access order inform howev possibl gener order border block transform avoid send basic block border set may sent one processor 42 jacobian matrix calcul new jacobian matrix calcul newton step ie sever time time step algorithm parallel al gorithm processor respons comput row high perform comput subsurfac simul 9 jacobian matrix correspond block processor updat set deriv comput numer jacobian matrix store distribut variabl block row format dvbr 7 matrix block store row wise diagon block store first block row scalar element matrix block store column major order use dens matrix block enabl use dens linear algebra softwar eg optim level 2 level subproblem dvbr format also allow variabl number equat per block comput element jacobian matrix basic perform two phase first phase consist comput relat individu block begin phase processor alreadi hold inform necessari perform calcul second phase includ comput relat interfac quan titi ie calcul use variabl correspond pair block perform comput exchang relev variabl requir number variabl processor send element correspond border block appropri processor receiv element correspond extern block 43 linear system nonsymmetr linear system solv gener illcondit difficult solv therefor parallel implement tough2 made differ iter solver precondition easili test result present obtain use stabil biconjug gradient method bicgstab 14 aztec softwar packag 7 3 theta 3 block jacobi scale domain decomposit base precondition possibl overlap subdomain ie addit schwarz see eg 13 use ilut 12 incomplet lu factor domain decomposit base procedur perform differ level overlap case procedur turn anoth variant block jacobi precondition order distinguish 3 theta 3 block jacobi scale full subdomain block jacobi scale obtain chose domain decomposit precondit procedur refer former block jacobi scale latter domain decomposit base precondition though cours precondition illustr difficulti aris linear system would like mention small problem yucca mountain simul mention introduct nonsymmetr problem includ 45 block 3 equat per block 64 connect solv linear system jacobian matrix size 135 theta 135 1557 nonzero element first jacobian gener first newton step first time step ie matrix involv first linear system solv largest smallest singular valu 248theta10 32 227theta10 gamma12 respect give condit number 11 theta 10 44 appli block jacobi scale block row multipli invers 3 theta 3 diagon block condit number significantli reduc scale reduc largest singular valu 769 theta 10 3 smallest increas 983 theta 10 gamma5 altogeth reduc condit number 78 theta 10 7 howev still illcondit problem therefor domain decomposit base precondition incomplet lu factor mention appli block jacobi scale procedur shown absolut vital converg problem significantli larger 44 parallel implement section outlin parallel implement describ major step import routin parallel tough2 includ 20000 line fortran code exclud meti aztec packag numer subroutin use mpi messag pass 6 howev order understand main issu parallel implement suffici focu coupl routin cours sever routin also modifi compar serial version softwar detail would distract cycit initi processor 0 read data describ problem solv essenti way serial version softwar processor call routin cycit contain main loop time step newton iter routin also initi grid partit data distribut partit describ section 41 defin input data distribut processor distribut perform sever routin call cycit five categori data distribut possibl reorder vector element correspond grid block distribut accord grid partit reorder local order intern border extern element describ section 41 vector element correspond connect distribut adjust local grid block number high perform comput subsurfac simul 11 processor determin connect involv local partit vector element correspond sink sourc replic full processor extract reorder part need addit number scalar small vector matric fulli replic ie data structur size depend number grid block connect final processor 0 construct data structur store jacobian matrix distribut appropri part processor includ integ vector defin matrix structur larg array hold float point number matrix element problem distribut time step procedur begin brief descript routin cycit given figur 4 descript lot detail omit clariti call includ coupl routin requir descript exchangeextern routin exchangeextern particular interest parallel implement main loop routin outlin figur 5 call processor vector scalar noel argu ment exchang vector element correspond extern grid block perform neighbor processor paramet noel number vector element exchang per extern grid block addit paramet defin current partit eg inform neighbor etc need also pass routin clariti chosen includ figur though detail omit chosen includ full mpi syntax use fortran interfac commun primit routin pack call exchangeextern copi appropri element vector consecut work array extern element given processor specifi sendindex remark element store directli appropri vector receiv sinc extern block order consecut neighbor wherea border element sent need pack consecut work space sent note use nonblock mpi routin send receiv data use block routin would assur messag sent receiv appropri order avoid deadlock use nonblock primit send receiv made arbitrari order minor inconveni use nonblock routin work space use store element sent need larg enough store element processor send neighbor cycit initi grid partit data distribut etc set first time step first newton step number secondari variabl par per grid block time endtim newton converg call multi newton converg result converg test newton converg updat primari variabl increment time defin new time step set els iter maxit solv linear system call eo call exchangeexternalpar num sec var iter maxit physic properti rang time step decreas mani time stop execut print messag failur solv problem els reduc time step call eo call exchangeexternalpar num sec var figur 4 outlin routin cycit execut processor exchangeexternalvector noel neighbor call mpi irecvvectorrecvstart rlen mpi doubl precis proc tagmyid mpi comm world req2i1 ierr call packi vector sendindex slen workiw noel call mpi isendworkiw slen mpi doubl precis proc tagproc mpi comm world req2i ierr call mpi waitall2num neighbor req stat ierr figur 5 outlin routin exchangeextern simultan call processor perform exchang noel element per extern grid block data vector routin multi call set linear system ie main part comput multi comput element jacobian matrix comput multi perform three major step first perform comput depend individu grid block follow comput term aris sink sourc far comput made independ processor last comput step multi interfac quantiti ie comput involv pair grid block perform last step exchang extern variabl requir vector x primari variabl dx last increment newton process delx small increment x valu use calcul increment paramet need numer calcul deriv 14 elmroth ding wu r residu number element sent per extern grid block equal number equat per grid block four vector oper perform call exchangeextern perform comput involv interfac quantiti eos3 eo routin thermophys properti fluid mixtur need assembl govern mass energi balanc equat provid routin call eo equat state main task eo routin provid valu secondari thermophys variabl function primari variabl though also perform addit import task see 10 page 1726 detail sever eo routin avail tough2 new eo routin becom avail howev eos3 one use parallel implement order provid maximum flexibl strive minim number chang need done eo routin move serial parallel implement done organ data assign appropri valu certain variabl call eo routin current parallel implement eos3 routin serial code use unmodifi except statement though still need verifi practic believ current parallel version tough2 handl also eo routin except write statement need adjust 45 cray t3eth target parallel system parallel implement tough2 made portabl use standard fortran 77 program languag mpi messag pass interfac interprocessor commun develop analysi howev perform 696 processor cray t3e900 system t3e distribut memori comput processor local memori togeth network interfac hardwar processor known digit ev5 alpha local memori form process element pe sometim call node 696 pe connect network arrang 3dimension toru see eg 1 detail perform cray t3e system 5 perform analysi parallel perform evalu perform 2d 3d real applic problem aris yucca mountain nuclear wast site studi result obtain 512 processor cray t3e900 nersc lawrenc berkeley nation laboratori linear system solv use bicgstab 3 theta 3 block jacobi scale domain decomposit base precondition ilut incomplet lu factor differ level overlap tri procedur though result present nonoverlap test gener shown give good perform stop criteria use linear solver denot residu right hand side respect test problem requir simul time 10 4 10 5 year would requir signific execut time also good parallel perform larg number processor order investig parallel perform therefor limit simul time 10 year 2d problem 01 year 3d problem still requir enough time step perform analysi parallel perform shorter simul time cours give initi phase unproport larg impact perform figur initi phase therefor exclud time test perform use kway vkway recurs partit algorithm meti see later differ order grid block lead variat time discret follow unstructur natur problem turn lead variat number time step requir therebi total amount work perform tri three partit algorithm chose one lead best perform problem number processor reduc somewhat artifici perform variat result differ number time step requir result present indic partit algorithm use 51 result 2d 3d real applic problem 2d problem consist 17584 block 3 compon per block 43815 connect block give total 52752 equat jacobian matrix linear system solv newton step size 52752 theta 52752 946926 nonzero element topmost graph figur 6 illustr reduct execut time increas number processor execut time number processor execut time 2h 5m 4s 2m 6s number processor figur 6 execut time parallel speedup 2d problem 2 4 8 16 32 64 128 processor cray t3e900 high perform comput subsurfac simul 17 reduc 7504 second ie 2 hour 5 minut 4 second two processor 126 second ie 2 minut 6 second 128 processor parallel speedup 2d problem present second graph figur 6 sinc problem solv one processor parallel code speedup normal 2 two pro cessor ie speedup p processor calcul 2t 2 p denot wall clock execut time 2 p processor respect complet also report execut time origin serial code 8245 second 2d problem 3d problem consist 97976 block 3 compon per block 396770 connect block give total 293928 equa tion jacobian matrix linear system solv newton step size 293928 theta 293928 8023644 nonzero element topmost graph figur 7 illustr reduct execut time 3d problem increas number processor memori batch system time limit prohibit test less 16 processor result therefor present 16 32 64 128 256 512 pro cessor execut time significantli reduc number processor increas way 512 processor reduc 10055 second ie 2 hour 47 minut 35 second processor 329 second ie 5 minut 29 second 512 processor abil effici use larger number processor even better illustr speedup shown second graph figur 7 speedup defin 16t 16 p sinc perform result avail smaller number processor result clearli demonstr good parallel perform larg number processor problem observ speedup 1191 128 processor 2d problem 4893 512 processor 3d problem repeatedli doubl number processor 2 4 4 8 etc 128 processor 2d problem obtain speedup factor 158 285 219 191 196 162 3d problem correspond speedup factor repeatedli doubl number processor 16 512 processor 270 228 195 150 169 200 would ideal speedup time number processor doubl speedup eg 270 228 3d problem often call superlinear speedup present explan later section number processor execut time 2h 47m 35 5m 29 number processor figur 7 execut time parallel speedup 3d problem 16 32 64 128 256 512 processor cray t3e900 high perform comput subsurfac simul 19 overal parallel perform satisfactori complet analysi provid insight explain superlinear speedup 52 unstructur problem ideal case problem evenli divid among processor approxim number intern grid block per processor also roughli number extern block per processor problem howev unstructur mean partit made even aspect lead exampl imbal number extern element per processor intern block evenli di tribut 3d problem 512 processor averag number extern grid block 234 maximum number extern block processor 374 follow least one processor 60 higher commun volum averag processor assum commun volum proport number extern block note averag number intern grid block 191 case mean averag processor actual extern block intern block final averag number neighbor processor 1259 maximum number neighbor processor 25 altogeth indic commun pattern irregular amount commun becom signific term number messag total commun volum time amount comput perform without extern element becom fairli small despit difficulti parallel implement show abil effici use larg number processor everi half second wall clock time 512 processor new linear system size 293928 theta 293928 8023644 nonzero element gener solv includ time numer differenti element jacobian matrix 3 theta 3 block jacobi scale block row ilut factor domain decomposit base precondition number bicgstab iter 53 analysi work load variat sever issu need consid analyz perform number processor increas first size individu task perform differ processor decreas give increas commun comput ratio rel load imbal also like increas addit may find variat time discret perform number time step number iter newton process linear solver order conduct detail studi present summari iter count time two test problem tabl ii tabl show averag number newton iter per time step averag number iter linear solver per time step per newton step well total number time step newton iter iter linear solver recal linear system solv time consum oper comput jacobian matrix second largest time consum oper perform newton step problem note variat occur time discret problem solv differ number pro cessor similar behavior observ exampl use differ linear solver serial version tough2 variat time discret lead variat number time step need number newton iter requir notabl 4 processor execut 2d problem requir 15 time step 35 newton step 91 iter linear solver compar execut 2 processor increas work fulli explain low speedup 4 processor similar variat amount work also contribut good speedup case howev figur tabl ii alon fulli explain superlinear speedup observ case therefor continu studi look perform linear solver howev show exampl motiv continu studi ie case speedup actual higher would expect look iter count exampl 2d problem speedup 8 processor 124 larger maximum expect ie 899 vs 800 compar execut two processor 8 processor execut actual requir slightli newton iter iter linear solver number time step test doubl number processor 8 16 see anoth factor 219 speedup even though reduct number newton iter iter linear solver 24 96 respect speedup 3d problem 16 processor 270 32 64 processor 228 also higher would expect look tabl ii alon far summar follow observ two problem unstructur natur problem natur lead variat work load differ test alon explain tabl ii iter count execut time 2d 3d test problem 2d problem partit algorithm vk vk k rec rec rec k rec time step 104 120 104 104 104 94 94 103 total newton iter 645 869 669 653 663 697 620 637 newton itertim step 620 724 643 628 638 741 660 618 total lin solv iter 8640 16528 10934 9888 11011 11282 11894 19585 lin solv iternewton step 1340 1902 1634 1514 1661 1846 1918 3075 lin solv itertim step 831 1371 1051 951 1059 1200 1265 1901 time spent lin solv time spent total time 3d problem partit algorithm k rec k k rec rec time step 154 149 143 137 185 166 total newton iter 632 606 585 561 708 646 newton itertim step 410 407 409 409 383 389 total lin solv iter 8720 10275 9357 10362 14244 14487 lin solv iternewton step 1380 1696 1599 1847 2012 2243 lin solv itertim step 566 690 654 756 770 873 time spent lin solv time spent total execut time 22 elmroth ding wu speedup anomali observ coupl case evid issu investig therefor continu studi focus perform linear solver precondition 54 perform precondition linear solver breakup speedup one part linear solver includ precondition one comput mainli assembl jacobian matrix present problem figur 8 figur illustr superlinear speedup whole problem follow superlinear speedup linear solver note result present total time spent part ie differ number linear system solv differ number iter requir solv linear system affect number speedup part close p test problem also indic part comput may show good perform also larger number processor slight decreas 256 512 processor 3d problem due increas number time step conclud perform part satisfi need explan continu studi superlinear speedup linear solver 541 effect precondition precondition crucial number iter per linear system solv domain decomposit base process expect becom less effici number processor increas best effect precondition expect whole matrix use factor order achiev good parallel perform size precondit oper processor restrict local subdomain averag matrix use precondit processor n size whole global matrix p number processor reduc effect follow natur smaller subdomain ie decreas size matric use precondition sinc diagon block use calcul approxim solut number iter requir per linear system two test problem confirm theori see tabl ii test problem number iter requir per linear system increas number processor except go 4 8 8 processor 2d problem 32 64 3d problem number processor linear solver part number processor linear solver part figur 8 breakup speedup 2d 3d problem one part linear solver mark r one part comput mark 4 ideal speedup defin straight line also includ result 256 processor 2d problem tabl ii note increas number iter per linear system 50 compar 128 processor clear precondition perform good job number processor increas 256 introduc one level overlap addit schwarz domain decomposit base precondition 256 processor number iter linear solver reduc order smaller number proce sor howev done addit cost perform overlap overal time roughli unchang main observ despit overal increas number iter linear solver increas number processor speedup linear solver higher would normal expect certain number processor increas number iter per linear system larger number processor obvious follow reduc effect precondition follow section conclud perform analysi investig parallel perform actual comput perform precondit linear iter process 542 perform precondition anoth effect decreas size subdomain domain decomposit base precondition total amount work perform incomplet lu factor becom significantli smaller number processor increas exampl number processor doubl size processor local matrix ilut factor decreas factor 4 averag n p n 2p theta n 2p henc amount work per processor reduc factor 2 8 depend sparsiti structur henc amount work per processor reduc faster normal expect assum ideal speedup 2 figur 9 give breakup speedup speedup linear solver separ one part precondition ie ilut factor one part linear solver precondition show dramat improv perform number processor increas follow natur decreas work ilut factor continu turn singl import explan sometim superlinear speedup number processor part precondition linear solver exclud precondition number processor part precondition linear solver exclud precondition figur 9 breakup speedup 2d 3d problem one part linear solver exclud ilut factor precondition mark r one part ilut factor mark one part comput mark 4 ideal speedup defin straight line 26 elmroth ding wu 543 perform linear iter explain superlinear speedup precondit part linear solver present figur 9 also observ modest speedup part comput low speedup part partli explain increas number iter seen previou section increas commun comput ratio slightli increas rel load imbal factor alreadi figur present section 52 show larg number extern element per processor imbal number extern element per processor present 3d problem 512 processor indic commun comput ratio would eventu becom larg perform obtain iter process linear solver support observ 544 impact overal perform order fulli understand total combin effect superlinear behavior precondition moder speedup part linear solver need investig larg proport spent precondit total time requir solv linear system illustr tabl iii tabl iii time spent precondit percentag total time spent linear solver 2d problem processor percentag 835 771 739 692 616 499 364 3d problem processor percentag 884 789 730 660 586 394 number processor becom larg amount time spent precondit becom small compar time spent iter wherea relat opposit 2 processor 2d problem 16 processor 3d problem exampl 16 processor 3d problem 884 time linear solver spent factor precondition wherea correspond number 394 512 processor long precondition consum larg portion time superlinear speedup signific effect overal perform implement evid certain number processor superlinear speedup incomplet lu factor domain decomposit base precondition suffici give superlinear speedup whole applic number processor becom larg factor consum smaller proport execut time henc superlinear behavior less impact overal per formanc instead issu becom critic larg number processor increas number iter linear solver 6 conclus contribut present design analysi parallel prototyp implement tough2 softwar packag parallel implement show effici use least 512 processor crayt3 system implement construct flexibl use differ linear solver precondition grid partit algorithm well altern eo modul solv differ problem comput experi real applic problem show high speedup 128 processor 2d problem 512 processor 3d problem result accompani analysi explain good parallel perform observ also explain minor variat perform follow unstructur natur problem superlinear speedup follow decreas work precondit process result also illustr tradeoff time spent precondit effect result object minim wall clock execut time note particular problem smaller subdomain could use least small number processor seen variat perform test use three differ partit algorithm variat clearli follow variat amount work requir eg due differ time discret analysi requir order determin whether variat follow particular pattern result unpredict circumst 28 elmroth ding wu problem target near futur larger term number block number equat per block moreov simul time significantli longer increas problem size expect abl effici use even larger number processor avail longer simul directli affect parallel perform futur investig includ studi altern nonlinear solver studi interplay time step proce dure nonlinear system linear system evalu differ linear solver precondition paramet set would gener interest may help improv perform particular implement relat studi partit algorithm recent complet 4 acknowledg thank karsten pruess author origin tough2 softwar valuabl discuss work anonym refere construct comment suggest work support director offic scienc offic laboratori polici infrastructur us depart energi contract number deac0376sf00098 research use resourc nation energi research scientif comput center support offic scienc us depart energi r aztec user guid tough user guid domain decomposit tr bicgstab fast smoothli converg variant bicg solut nonsymmetr linear system domain decomposit parallel implement tough2 softwar packag larg scale multiphas fluid heat flow simul perform cray t3e multiprocessor