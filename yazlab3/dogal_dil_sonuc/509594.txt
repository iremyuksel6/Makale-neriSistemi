effect commun paramet end perform share virtual memori cluster recent lot effort provid costeffect share memori system employ softwar solut cluster highend workstat coupl highbandwidth lowlat commod network much work far focus improv protocol work restructur applic perform better svm system result progress promis good perform rang applic least 1632 processor rang new system area network network interfac provid significantli lower overhead lower latenc higher bandwidth commun cluster inexpens smp becom common node cluster svm protocol quit matur progress use examin import system bottleneck stand way effect parallel perform particular paramet commun architectur import improv rel processor speed one alreadi adequ modern system applic chang technolog futur inform assist system design determin focu energi improv perform user determin system characterist appropri applicationsw find import system cost improv overhead gener deliv interrupt improv network interfac io bu bandwidth rel processor speed help bandwidthbound applic current avail ratio bandwidth processor speed alreadi adequ mani other surprisingli neither processor overhead handl messag occup commun interfac prepar push packet network appear requir much improv b introduct success hardwar cachecoher distribut share memori dsm lot effort made support program model coher share address space use commod orient commun architectur addit commod node techniqu commun architectur rang use less custom integr control 17 2 support share virtual memori svm page level oper system 12 8 techniqu reduc cost unfortun usual lower perform well great deal research effort made improv system larg class applic focu paper svm system last year much improv svm protocol system sever applic restructur improv perform 12 8 25 14 progress interest examin import system bottleneck stand way effect parallel perform particular paramet commun architectur import improv rel processor speed alreadi adequ modern system applic chang technolog futur studi hope assist system design determin focu energi improv perform user determin system characterist appropri applic paper examin question detail architectur simul use applic wide differ behavior simul cluster architectur smp node fast system area interconnect programm network interfac ie myrinet use homebas svm protocol demonstr compar better perform famili svm protocol base case protocol call homebas lazi releas consist hlrc requir addit hardwar support later examin variant call automat updat releas consist aurc use automat hardwar propag write remot node perform updat share data also extend analysi use uniprocessor node use major perform paramet consid host processor overhead send messag network interfac occup prepar transfer packet nodetonetwork bandwidth often limit io bu bandwidth interrupt cost consid network link latenc sinc small usual constant part endtoend latenc system area network san deal perform paramet also briefli examin impact key granular paramet commun architectur page size granular coher number processor per node assum realist system quit easili implement today rang applic well optim svm system 10 see figur 1 applic protocol commun overhead substanti speedup obtain realist implement much lower ideal case commun cost zero motiv current research whose goal twofold first want understand perform chang fft lu ocean contigu water nsquar water spatial radix volrend raytrac barn rebuild barn space26101418speedup figur 1 ideal realist speedup applic ideal speedup comput ratio uniprocessor execut time divid sum comput local cach stall time parallel execut ie ignor commun synchron cost realist speedup correspond realist set valu see section commun architectur paramet today configur four processor per node paramet commun architectur vari rel processor speed see invest system energi understand like evolut system perform technolog evolv use wide rang valu paramet second focu three specif point paramet space first point system gener achiev best perform within rang paramet valu examin perform applic point call best perform second point aggress set valu commun paramet current nearfutur system especi certain oper system featur well optim perform point space call achiev perform third point ideal point repres hypothet system incur commun synchron overhead take consider comput time stall time local data access goal understand gap achiev best ideal perform identifi paramet contribut perform differ lead us primari commun paramet need improv close gap find somewhat surprisingli host overhead send messag perpacket network interfac occup critic applic perform case interrupt cost far domin perform bottleneck even though protocol design aggress reduc occurr interrupt nodetonetwork bandwidth typic limit io bu also signific applic interrupt cost import applic studi result suggest system design focu reduc interrupt cost support svm well svm protocol design tri avoid interrupt possibl perhap use poll use programm commun assist run part protocol avoid need interrupt main processor result show rel effect paramet ie rel processor speed f f f processor e r processor io first level cach buffer write second level cach network interfac e r snoop devic figur 2 simul node architectur one anoth absolut paramet valu use achiev set match consid achiev aggress current nearfutur system view paramet rel processor speed allow us understand behavior technolog trend evolv instanc ratio bandwidth processor speed chang use result reason system perform section 2 present architectur simul use work section 3 discuss paramet use methodolog studi section 4 present applic suit section 5 6 present result commun perform paramet section 5 examin effect paramet system perform section 6 discuss applic paramet limit perform section 7 present effect page size degre cluster system perform discuss relat work section 8 final discuss futur work direct conclus section 9 10 respect simul environ simul environ use built top augmint 18 execut driven simul use x86 instruct set run x86 system section present architectur paramet vari simul architectur figur 2 assum cluster cprocessor smp connect commod interconnect like myrinet 3 content model level except network link switch processor p6like instruct set assum 1 ipc pro cessor data cach hierarchi consist 8 kbyte firstlevel direct map writethrough cach 512 kbyte secondlevel twoway set associ cach line size 32 byte buffer 19 26 entri 1 cach line wide retireat4 polici write buffer stall simul read hit cost one cycl satisfi write buffer first level cach 10 cycl satisfi secondlevel cach memori subsystem fulli pipelin network interfac ni two 1 mbyte memori queue hold incom outgo packet size queue constitut bottleneck commun subsystem network queue fill ni interrupt main processor delay allow queue drain network link oper processor speed 16 bit wide assum fast messag system 5 16 4 basic commun librari memori bu splittransact 64 bit wide clock cycl four time slower processor clock arbitr take one bu cycl prioriti decreas order outgo network path ni second level cach write buffer memori incom path ni io bu bit wide rel bu bandwidth processor speed match modern system assum processor 200 mhz clock memori bu 400 mbytess protocol handler cost variabl number cycl code protocol handler simul sinc simul multithread use handler estim cost code sequenc cost access tlb handler run kernel 50 processor cycl cost creat appli diff 10 cycl everi word need compar 10 addit cycl word actual includ diff protocol use two version homebas protocol hlrc aurc 8 25 protocol either use hardwar support automat write propag aurc tradit softwar diff hlrc propag updat home node page releas point necessari page invalid acquir point accord lazi releas consist lrc subsequ page fault whole page fetch home guarante date accord lazi releas consist 8 protocol smp node attempt util hardwar share synchron within smp much possibl reduc softwar involv 1 optim use includ use hierarch barrier avoid interrupt much possibl interrupt use remot request page lock arriv node request synchron rpc like avoid interrupt repli arriv request node barrier implement synchron messag interrupt interrupt deliv processor 0 node complic scheme ie round robin random assign result better load balanc interrupt handl use oper system provid necessari support scheme howev may increas cost deliv interrupt paper also examin round robin scheme mention earlier focu follow perform paramet commun archi tectur host overhead io bu bandwidth network interfac occup interrupt cost examin network link latenc sinc small usual constant part endtoend latenc system area network san paramet describ basic featur commun sub system rest paramet system exampl cach memori configur total number processor etc remain constant messag exchang two host put post queue network interfac asynchron send oper assum sender free continu use work network interfac process request prepar packet queue outgo network queue incur occup per packet transmiss packet enter incom network queue receiv process network interfac deposit directli host memori without caus interrupt 2 4 thu interrupt cost overhead relat much data transfer process request examin rang valu paramet vari paramet usual keep other fix set achiev valu recal valu might consid achiev current system provid optim oper system support interrupt choos rel aggress fix valu effect paramet vari observ detail host overhead time host processor busi send messag rang paramet cycl post send system support asynchron send time need transfer messag data host memori network interfac synchron send use asynchron send avail achiev valu host overhead hundr processor cycl recal processor overhead data transfer destin end rang valu consid 0 almost cycl 10000 processor cycl 50 5n processor clock system support asynchron send probabl closer smaller valu system synchron send closer higher valu depend messag size achiev valu use overhead 600 processor cycl per messag ffl io bu bandwidth determin host network bandwidth rel processor speed contemporari system limit hardwar compon avail nodetonetwork network link memori buse tend much faster rang valu io bu bandwidth 025 mbyte per processor clock mhz 2 mbyte per processor clock mhz 50 mbytess 400 mbytess assum 200 mhz processor clock achiev valu 05 mbytesmhz 100 mbytess assum 200 mhz processor clock ffl network interfac occup time spent network interfac prepar packet network interfac employ either custom state machin network processor gener purpos custom design perform process thu process cost network interfac vari wide vari occup network interfac almost 0 10000 processor cycl 50 5n processor clock per packet achiev valu use 1000 main processor cycl 5s assum 200 mhz processor clock valu realist current avail programm ni given programm commun assist ni usual much slower main processor ffl interrupt cost cost issu interrupt two processor smp node cost interrupt processor network interfac includ cost context switch oper system process although interrupt cost paramet commun subsystem import aspect svm system interrupt cost depend oper system use vari greatli system system affect perform portabl svm across differ platform therefor vari interrupt cost free interrupt 0 processor cycl 50000 processor cycl issu deliv interrupt total 100000 processor cycl 500 5n processor clock achiev valu use 500 processor cycl result cost 1000 cycl null interrupt choic significantli aggress current oper system provid howev achiev fast interrupt technolog 21 use achiev valu vari paramet ensur interrupt cost swamp effect vari paramet captur effect paramet separ keep paramet fix achiev valu necessari also perform addit guid simul clarifi result addit result obtain vari paramet result obtain achiev paramet valu interest result speedup obtain use best valu rang paramet limit perform obtain improv commun architectur within rang paramet paramet valu best configur host overhead 0 processor cycl io bu bandwidth equal memori bu bandwidth network interfac occup per packet 200 processor cycl total interrupt cost 0 processor cycl best con figur content still model sinc valu system paramet still nonzero tabl 1 summar valu paramet 200 mhz processor achiev set valu discuss assum paramet valu host overhead 600 processor cycl memori bu bandwidth 400 mbytess io bu bandwidth 100 mbytess network interfac occup per packet 1000 processor cycl total interrupt cost 1000 processor cycl paramet rang achiev best host overhead cycl 010000 600 0 io bu bandwidth mbytesmhz 0252 05 2 ni occup cycl 010000 1000 200 tabl 1 rang achiev best valu commun paramet consider applic use splash2 22 applic suit section briefli describ basic characterist applic relev studi detail classif descript applic behavior svm system uniprocessor node provid context aurc lrc 9 applic divid two group regular irregular 41 regular applic applic categori fft lu ocean common characterist optim singlewrit applic given word data written processor assign given appropri data structur singlewrit page granular well page alloc among node write share data almost local hlrc need comput diff aurc need use write cach polici protocol action requir fetch page applic differ inher induc commun pattern 22 9 affect perform impact smp node applic page fault page fetch local lock acquir remot lock acquir barrier waternsquar 512 6919 2206 804 6826 1901 729 waterspati 512 9786 2142 923 9381 1773 604 001 183 260 394 216 139 419 volrend head 10509 4406 3449 10478 2935 653 000 2934 4380 4434 1764 397 161 raytrac car 8980 2564 683 8979 2557 676 003 221 396 489 326 134 010 barnesrebuild barnesspac tabl 2 number page fault page fetch local remot lock acquir barrier per processor per 10 7 cycl applic 14 8 processor per node fft alltoal readbas commun fft essenti transposit matrix complex number use two problem size 256k512x512 1m1024x1024 element fft high inher commun comput ratio lu use contigu version lu alloc page data assign one processor lu exhibit small commun comput ratio inher imbalanc use 512x512 matrix ocean commun pattern ocean applic larg nearestneighbor iter regular grid run contigu 4d array version ocean 514x514 grid error toler 0001 42 irregular applic irregular applic suit barn radix raytrac volrend water barn ran experi differ data set size present result 8k particl access pattern barn irregular finegrain use two version barn differ manner build share tree time step first version barnesrebuild one splash2 processor load particl assign forc calcul directli share tree lock frequent necessari second version barnesspac 10 optim svm avoid lock much possibl use differ treebuild algorithm disjoint subspac match tree cell assign differ processor subspac includ particl particl assign processor forc calcul processor build partial tree partial tree merg global tree without lock radix radix sort seri integ key irregular applic highli scatter write remot alloc data high inher commun comput ratio use unmodifi splash2 version fft lu ocean contigu water nsquar water spatial radix volrend raytrac barn rebuild barn space200600100014001800norm number messag sent figur 3 number messag sent per processor per 10 7 comput cycl applic 14 8 processor per node raytrac raytrac render complex scene comput graphic version use modifi splash2 version run effici svm system global lock necessari remov task queue implement better svm smp 10 inher commun small volrend version use slightli modifi splash2 version provid better initi assign task process steal 10 improv svm perform greatli inher commun volum small water use version water splash2 waternsquar waterspati water nsquar categor regular applic put eas comparison waterspati version updat water molecul posit veloc first accumul local processor perform share data end iter inher commun comput ratio small use data set size 512 molecul tabl 2 figur 3 4 use character applic tabl 2 present count protocol event applic 1 4 8 processor per node 16 processor total case figur 3 4 show number messag mbyte data applic protocol sent processor system characterist measur per 10 7 cycl applic comput time per processor averag processor system use categor applic term commun exhibit number messag mbyte data exchang import perform use geometr mean properti captur multipl effect metric divid applic fft lu ocean contigu water nsquar water spatial radix volrend raytrac barn rebuild barn normal mbyte sent figur 4 number mbyte sent per processor per 10 7 comput cycl applic 14 8 processor per node three group first group barnesrebuild fft radix exhibit lot commun second group belong waternsquar volrend exhibit less commun third group rest applic lu ocean waterspati raytrac barnesspac exhibit littl commun import note categor hold 4processor per node configur chang number processor node dramat chang behavior applic pictur differ instanc ocean exhibit high commun 1 processor per node 5 effect commun paramet section present effect paramet perform allsoftwar hlrc protocol rang valu tabl 3 present maximum slowdown applic paramet consider maximum slowdown comput speedup smallest biggest valu consid paramet keep paramet achiev val ue neg number indic speedup rest section discuss paramet one one paramet also identifi applic characterist close predict effect paramet next section take differ cut look bottleneck perappl rather perparamet basi end section also present result aurc host overhead figur 5 show slowdown due host overhead gener low especi realist valu asynchron messag overhead howev vari among applic less 10 barnesspac oceancontigu raytrac 35 volrend radix barnesrebuild across entir rang valu gener applic send messag exhibit higher depend host overhead seen figur 6 show two applic host overhead ni occup io bu bandwidth interrupt cost page size procsnod fft 226 119 408 866 726 138 lucontigu 179 75 159 708 344 353 oceancontigu 45 28 65 352 196 632 waternsquar 324 166 108 832 622 871 waterspati 237 85 89 679 510 875 radix 358 318 776 587 3682 6994 volrend 347 128 157 913 639 681 raytrac 82 29 89 523 91 161 barnesrebuild 407 218 448 803 715 3834 barnesspac 44 06 275 590 1096 494 tabl 3 maximum slowdown respect variou commun paramet rang valu experi neg number indic speedup curv one slowdown applic smallest highest host overhead simul normal biggest slowdown second curv number messag sent processor per 10 6 comput cycl normal biggest number messag note asynchron messag host overhead low side rang conclud host overhead send messag major perform factor coars grain svm system unlik becom near futur network interfac occup figur 7 show network interfac occup even smaller effect host overhead perform realist occup applic insensit except coupl applic send larg number messag applic slowdown 22 observ highest occup valu speedup observ radix realiti caus time issu content bottleneck radix io bu bandwidth figur 8 show effect io bandwidth applic perform reduc bandwidth result slowdown 82 4 11 applic exhibit slowdown 40 howev mani applic depend bandwidth fft radix barnesrebuild benefit much increas io bu bandwidth beyond achiev relationship processor speed today cours mean import worri improv bandwidth processor speed increas bandwidth trend keep quickli find relationship reflect lower bandwidth case examin even wors mean bandwidth keep processor speed like major limit svm system applic figur 9 show depend bandwidth number byte sent per processor applic unit normal maximum number present curv applic exchang lot data necessarili lot messag need higher bandwidth show interrupt cost import paramet system unlik bandwidth affect perform applic dramat mani case rel fft lucontigu oceancontigu waternsquar waterspati radix volrend raytrac barnesspac barnesrebuild figur 5 effect host overhead applic perform data point applic correspond host overhead 0 600 1000 5000 10000 processor cycl small increas interrupt cost lead big perform degrad applic interrupt cost 2000 processor cycl initi deliveri seem hurt much howev commerci system typic much higher interrupt cost increas interrupt cost beyond point begin hurt sharpli applic slowdown 50 interrupt cost vari 0 50000 processor cycl except oceancontigu exhibit anomali sinc way page distribut among processor chang interrupt cost suggest architectur oper system work harder improv interrupt cost support svm well svm protocol tri avoid interrupt much possibl figur 11 show slowdown due interrupt cost close relat number protocol event caus interruptspag fetch remot lock acquir smp node mani option interrupt may handl within node protocol use one particular method system uniprocessor node less option experi configur well found interrupt cost import case well differ system seem littl less sensit interrupt cost 2500 5000 cycl rang perform degrad quickli smp configur also experi round robin interrupt deliveri result look similar case interrupt deliv fix processor smp overal perform seem increas slightli compar static interrupt scheme static scheme degrad quickli cost increas moreov implement scheme real system may complic may incur addit cost lucontigu oceancontigu radix raytrac volrend waternsquar waterspatial0103050709norm unit slowdown due host overhead normal largest slowdown number messag sentprocessor1m comput cycl normal largest figur 6 relat slowdown due host overhead number messag sent aurc mention introduct besid hlrc also use aurc studi effect commun paramet use hardwar support automat write propag instead softwar diff result look similar hlrc except network interfac occup much import aurc automat updat mechan may gener traffic network interfac new valu data may sent multipl time home node releas importantli number packet may increas significantli sinc updat sent much finer granular apart space time may coalesc well packet figur 12 show perform chang ni overhead increas regular irregular applic 6 limit applic perform section examin differ perform best configur ideal system speedup comput comput local stall time ignor commun synchron cost differ perform achiev best configur per applic basi recal best stand configur commun paramet assum best valu achiev stand configur commun paramet assum achiev valu goal identifi applic properti architectur paramet respons differ best fft lucontigu oceancontigu waternsquar waterspati radix volrend raytrac barnesspac barnesrebuild figur 7 effect network interfac occup applic perform data point applic correspond network occup 50 250 500 1000 2000 10000 processor cycl ideal perform paramet respons differ achiev best perform speedup configur call ideal best achiev respect tabl 4 show speedup applic mani case achiev speedup close best speedup howev case fft radix barn remain gap perform best configur often quit far ideal speedup understand effect let us examin applic separ fft best speedup fft 135 differ ideal speedup 162 come data wait time page fault cost even best configur despit high bandwidth zerocost interrupt achiev speedup 77 two major paramet respons drop perform cost interrupt bandwidth io bu make interrupt cost 0 result speedup 11 increas io bu bandwidth memori bu bandwidth give speedup 10 modifi paramet time give speedup almost best speedup lu best speedup 137 differ ideal speedup due load imbal commun due barrier cost achiev speedup lu best speedup sinc applic low commun comput ratio commun problem ocean best speedup ocean 1055 reason interrupt cost 0 anomali observ first touch page alloc speedup low due larg number mbytesmhz13579111315speedup fft lucontigu oceancontigu waternsquar waterspati radix volrend raytrac barnesspac barnesrebuild figur 8 effect io bandwidth applic perform data point applic correspond io bandwidth 2 1 05 025 mbyte per processor clock mhz 400 200 100 50 mbytess assum 200 mhz processor page fault achiev speedup 130 main cost barrier synchron worth note speedup ocean artifici high local cach effect processor work set fit cach uniprocessor fit cach processor thu sequenti version perform poorli due high cach stall time barnesrebuild best speedup barnesrebuild 590 differ ideal page fault larg number critic section lock achiev speedup 39 differ best achiev speedup presenc page fault synchron wait time even higher due increas protocol cost increas cost mostli host overhead loss 1 speedup ni occup 08 verifi disabl remot page fetch simul page fault appear local speedup becom 1464 best 1062 achiev case respect gap best achiev speedup due host ni overhead barnesspac second version barn run improv version minim lock 10 best speedup 145 close ideal achiev speedup 125 differ two mainli lower avail io bandwidth achiev case increas data wait time imbalanc way waternsquar best speedup waternsquar 99 achiev speedup 9 reason high best speedup page fault occur contend critic section lucontigu oceancontigu radix raytrac volrend waternsquar waterspatial0103050709norm unit slowdown due io bu bandwidth normal largest slowdown number byte sentprocessor1m comput cycl normal largest figur 9 relat slowdown due io bu bandwidth number byte transfer greatli increas serial lock artifici disabl remot page fault best speedup increas 99 141 cost lock artifici case small nonid speedup due imbal comput waterspati best speedup 1375 differ ideal mainli due small imbal comput lock wait time data wait time small achiev speedup 133 radix best speedup radix 7 differ ideal speedup 161 due data wait time exagger content even best paramet valu result imbal among processor lead high synchron time imbal observ due content network interfac achiev speedup 3 differ best speedup due factor data wait time much higher much imbalanc due much greater content effect main paramet respons io bu bandwidth instanc quadrupl io bu bandwidth achiev speedup radix becom 7 like best speedup raytrac raytrac perform well best speedup 1564 achiev speedup 1480 fft lucontigu oceancontigu waternsquar waterspati radix volrend raytrac barnesspac barnesrebuild figur 10 effect interrupt cost applic perform six bar applic correspond interrupt cost 0 500 1000 2500 5000 10000 50000 processor cycl volrend best speedup 1095 reason low number imbal comput due cost task steal larg lock wait time due page fault critic section artifici elimin remot page fault comput perfectli balanc synchron cost neglig speedup 149 fiction case achiev speedup 940 close best speedup see differ ideal best perform due page fault occur critic section io bandwidth limit imbal commun comput differ best achiev perform primarili due interrupt cost io bandwidth limit less due host overhead overal applic perform svm system today appear limit primarili interrupt cost next io bu bandwidth host overhead ni occup per packet substanti less signific order 7 page size degre cluster addit perform paramet commun architectur discuss granular coher data transferi page sizeand number processor per node two import paramet affect behavior system play import role determin amount commun take place system cost determin perform paramet page size page size system import mani reason defin size transfer sinc softwar protocol data fetch perform page size also affect lucontigu oceancontigu radix raytrac volrend waternsquar waterspatial0103050709norm unit slowdown due interrupt cost normal largest slowdown number page fetch remot lock acquir normal largest figur 11 relat slowdown due interrupt cost number page fetch remot lock acquir amount fals share system import svm two aspect page size conflict bigger page reduc number messag system spatial local well exploit commun increas amount fals share vice versa moreov differ page size lead differ amount fragment memori may result wast resourc figur 13 show effect page size applic vari lot applic seem favor smaller page size except radix benefit lot bigger page vari page size 2 kbyte 32 kbyte page system today support either 4 kbyte 8 kbyte page note two caveat studi respect page size first tune applic specif differ page size second effect page size often relat problem size use applic amount fals share fragment ie granular access interleav memori differ processor chang problem size larger problem run real system may benefit larger page ie fft size degre cluster number processor per node figur 14 show applic greater cluster help even memori configur bandwidth kept 1 use cluster size 1 4 8 16 processor alway keep total number processor assumpt keep memori subsystem increas number processor per node realist sinc system higher degre cluster usual aggress memori subsystem well fft lucontigu oceancontigu waterspati volrend raytrac barnesrebuild figur 12 effect network interfac occup applic perform aurc data point applic correspond network occup 50 250 500 1000 2000 10000 processor cycl system 16 configur cover rang uniprocessor node configur cachecoher busbas multiprocessor typic svm system today use either uniprocessor 4way node coupl interest point emerg first unlik applic oceancontigu optim cluster four processor per node reason oceancontigu gener lot local traffic memori bu due capac conflict miss processor bu exacerb problem hand oceancontigu benefit lot cluster commun pattern thu four processor per node use perform improv one processor per node come share system four processor per node memori bu satur although system benefit share perform degrad memori bu content radix fft also put greatli increas pressur share bu crossnod svm commun howev high reduct via increas spatial local page grain due cluster outweigh problem second import point applic perform poorli svm well share bu system scale reason applic either exhibit lot synchron make fine grain access much cheaper hardwarecoher share bu architectur exampl applic problem svm page fault within critic section ie barn rebuild perform much better architectur result show bu bandwidth signific problem applic scale use hardwar coher synchron outweigh problem share bu like provid greater nodetonetwork bandwidth applic best achiev ideal fft 135 77 162 ocean 105 130 160 waternsquar 99 90 158 waterspati 137 133 158 radix 70 30 161 volrend 109 940 154 raytrac 156 148 164 barnesrebuild 59 39 154 barnesspac 145 125 156 tabl 4 best achiev speedup applic 8 relat work work similar spirit earlier studi conduct 15 7 differ context 15 author examin impact commun paramet end perform network workstat applic written splitc top gener activ messag find applic perform demonstr linear depend host overhead gap transmiss fine grain messag svm find paramet import sinc cost usual amort page granular applic found quit toler latenc bulk transfer bandwidth splitc studi well 7 holt et al find occup commun control critic good perform dsm machin provid commun coher cach line granular overhead signific unlik 15 sinc small 11 karlsson et al find latenc bandwidth atm switch accept cluster svm architectur 13 lazi releas consist protocol hardwar cachecoher present differ context find applic sensit bandwidth latenc compon commun sever studi also examin perform differ svm system across multiprocessor node compar perform configur uniprocessor node erlichson et al 6 find cluster help share memori applic yeung et al 23 find true svm system node hardwar coher dsm machin 1 find true gener softwar svm system svm system support automat write propag 9 discuss futur work work show room improv svm cluster perform variou direct ffl interrupt sinc reduc cost interrupt system improv perform signifi cantli import direct futur work design svm system reduc frequenc fft lucontigu oceancontigu waternsquar waterspati radix volrend raytrac barnesspac barnesrebuild figur 13 effect page size applic perform data point applic correspond page size 2 kbyte 4 kbyte 8 kbyte 16 kbyte kbyte andor cost interrupt poll better oper system support support remot fetch involv remot processor mechan help direct oper system architectur support inexpens interrupt would improv system perform unfortun alway achiev especi commerci system case protocol modif use noninterrupt remot fetch oper implement optim use poll instead interrupt improv system perform lead predict portabl perform across differ architectur oper system poll done either instrument applic smp system reserv one processor protocol process recent result interrupt versu poll svm system vari one studi find poll may add signific overhead lead inferior perform interrupt page grain svm system 24 hand stet et al find poll give gener better result interrupt 20 believ research need modern system understand role poll anoth interest direct explor move protocol process network processor found programm network interfac like myrinet thu reduc need interrupt main processor ffl system bandwidth provid high bandwidth also import keep increas processor speed although fast system interconnect avail softwar perform practic rare close hardwar provid low level commun librari fail deliv close raw hardwar perform mani case work low level commun interfac may also help provid lowcost highperform svm system multipl network interfac per node anoth approach increas avail bandwidth case protocol chang may necessari ensur proper event order fft lucontigu oceancontigu waternsquar waterspati radix volrend raytrac barnesspac barnesrebuild figur 14 effect cluster size applic perform data point applic correspond cluster size 1 4 8 16 processor per node ffl cluster scale examin ad processor per node help almost case applic perform increas quickli cluster size scale system paramet memori bu io bandwidth desir effect ffl applic work found restructur applic area make big differ understand applic behav restructur properli dramat improv perform far beyond improv system paramet protocol 10 howev alway easi unfortun mani tool avail parallel system help easili discov caus bottleneck obtain insight applic restructur need especi content major problem often commoditybas commun architectur architectur simul one tool current use understand applic behav detail point work limit certain famili homebas svm protocol systemsfor instanc fine grain svm systemsmay exhibit differ behavior depend commun paramet similar studi protocol architectur help us understand better differ similar among svm system work base 16 processor system address question happen bigger system run experi processor configur compar number protocol event two configur tabl 5 show ratio protocol event commun traffic 32 16 processor configur case event count scale proport size system lead us believ result present far hold bigger configur well least 32 processor moreov larger problem size problem relat commun architectur usual allevi howev sophist applic page fault page fetch remot lock acquir local lock acquir barrier mbyte sent messag sent lu 194 253 186 200 190 1290 2 366 ocean 075 053 277 157 199 250 195 waternsquar 289 263 140 250 199 280 237 waterspati 185 205 168 226 198 200 208 radix 183 243 270 410 199 219 238 volrend raytrac 208 208 133 240 200 208 183 tabl 5 ratio protocol event 32 16 processor configur 4 processor per node scale model take account problem size may necessari detail accur predict anoth import question commun paramet go scale time seem paramet close follow hardwar perform host overhead network interfac occup bandwidth potenti get better rel processor speed interrupt cost depend oper system special architectur support conclus examin effect commun paramet famili svm protocol detail architectur simul cluster smp varieti applic find applic sensit interrupt cost would benefit improv bandwidth rel processor speed well unbalanc system rel high interrupt cost low io bandwidth result substanti loss applic perform case observ slowdown 90 factor 10 longer execut time howev applic sensit host overhead network interfac occup regular applic achiev good svm perform best configur paramet irregular applic though even best perform low mainli due serial effect critic section ie due page fault incur insid critic section dilat critic section increas serial exampl reduc amount lock use differ algorithm parallel tree build perform barn improv factor 23 overal achiev applic perform today limit primarili interrupt cost node network bandwidth host overhead ni occup appear less import improv rel processor speed interrupt free bandwidth high rel processor speed achiev perform approach best perform case acknowledg thank hongzhang make avail us improv version barn anonym review comment feedback r comparison share virtual memori across uniprocessor smp node virtual memori map network interfac shrimp multicomput gigabitpersecond local area network design implement virtual memorymap commun myrinet activ messag mechan integr commun comput benefit cluster share address space multiprocessor applicationsdriven investig effect latenc improv releaseconsist share virtual memori use automat updat understand applic perform share virtual memori applic restructur perform portabl share virtual memori hardwarecoher multiprocessor perform evalu clusterbas multiprocessor built atm switch busbas multiprocessor server distribut share memori standard workstat oper system lazi releas consist hardwarecoher multi processor effect commun latenc fast messag fm 20 stream interfac tempest typhoon userlevel share memori augmint multiprocessor simul environ intel x86 architectur design issu tradeoff write buffer fast interrupt prioriti manag oper system kernel methodolog consider character splash2 parallel applic suit mg multigrain share memori system relax consist coher granular dsm system perform evalu perform evalu two homebas lazi releas consist protocol share virtual memori system tr activ messag virtual memori map network interfac shrimp multicomput tempest typhoon benefit cluster share address space multiprocessor lazi releas consist hardwarecoher multiprocessor mg understand applic perform share virtual memori system perform evalu two homebas lazi releas consist protocol share virtual memori system applic restructur perform portabl share virtual memori hardwarecoher multiprocessor vmbase share memori lowlat remotememoryaccess network myrinet design implement virtual memorymap commun myrinet fast interrupt prioriti manag oper system kernel improv releaseconsist share virtual memori use automat updat perform evalu clusterbas multiprocessor built atm switch busbas multiprocessor server design issu tradeoff write buffer effect latenc occup bandwidth distribut share memori multiprocessor effect commun latenc overhead bandwidth cluster ctr mainak chaudhuri mark heinrich chri holt jaswind pal singh edward rothberg john hennessi latenc occup bandwidth dsm multiprocessor perform evalu ieee transact comput v52 n7 p862880 juli cheng liao dongm jiang liviu iftod margaret martonosi dougla w clark monitor share virtual memori perform myrinetbas pc cluster proceed 12th intern confer supercomput p251258 juli 1998 melbourn australia soichiro araki angelo bila cezari dubnicki jan edler koichi konishi jame philbin userspac commun quantit studi proceed 1998 acmiee confer supercomput cdrom p116 novemb 0713 1998 san jose ca angelo bila courtney r gibson reza azimi rosalia christodoulopoul peter jamieson use system emul model nextgener share virtual memori cluster cluster comput v6 n4 p325338 octob angelo bila liviu iftod jaswind pal singh evalu hardwar write propag support nextgener share virtual memori cluster proceed 12th intern confer supercomput p274281 juli 1998 melbourn australia angelo bila dongm jiang jaswind pal singh acceler share virtual memori via generalpurpos network interfac support acm transact comput system toc v19 n1 p135 feb 2001 sanjeev kumar yitzhak mandelbaum xiang yu kai li esp languag programm devic acm sigplan notic v36 n5 p309320 may 2001 zoran radovi erik hagersten remov overhead softwarebas share memori proceed 2001 acmiee confer supercomput cdrom p5656 novemb 1016 2001 denver colorado angelo bila cheng liao jaswind pal singh use network interfac support avoid asynchron protocol process share virtual memori system acm sigarch comput architectur news v27 n2 p282293 may 1999 salvador petit julio sahuquillo ana pont david kaeli address workload character studi design consist protocol journal supercomput v38 n1 p4972 octob 2006