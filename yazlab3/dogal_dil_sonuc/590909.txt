knowledg extract transduc neural network previous neural network shown interest perform result task classif still suffer insuffici focu structur knowledg repres therein paper analyz variou knowledg extract techniqu detail develop new transduc extract techniqu interpret recurr neural network learn first provid overview differ possibl express structur knowledg use neural network analyz type recurr network rigor appli broad rang differ techniqu argu analysi techniqu weight analysi use hinton diagram hierarch cluster analysi princip compon analysi may use provid certain view underli knowledg howev demonstr techniqu static lowlevel interpret recurr network classif contribut paper particularli broad analysi knowledg extract techniqu furthermor propos dynam learn analysi transduc extract two new dynam interpret techniqu dynam learn analysi provid better understand network learn transduc extract provid better understand network repres b introduct lot interest late knowledg structur represent artici neural network holldobl 1990 kurf 1991 sperduti et al 1995 wermter 1995 hallam 1995 medsker 1995 sun 1995 wermter et al 1996 elman et al 1996 craven 1996 wermter 1999 artici neural network connectionist network alreadi demonstr interest learn result variou classic task howev continu dicult understand underli represent within connectionist network lead perfor manc better understand connectionist represent learn import improv credibl comput techniqu also improv network perform integr possibl symbol represent sever attempt made interpret connectionist network focus feedforward network particular andrew diederich 1996 abe et al 1993 shavlik 1994 stanc visual intern activ weight strength use get impress intern knowledg hinton 1986 gorman sejnowski 1988 eort also made reduc network size order simplifi knowledg express therein elimi 28 wermter nate small weight furthermor group similar weight replac averag strength shavlik 1994 addit techniqu hierarch cluster analysi use interpret connectionist network never theless often interpret dynam learn process underli knowledg neglect especi case dynam recurr neural network interpret recurr network dicult nonrecurr feedforward network sinc previou context recurr network import dynam uenc within network intern state recurr network depend input also intern state local memori base previou input elman 1995 gile omlin 1993 omlin gile 1996 reason date focu primarili smaller recurr network artici gener data instanc interest current approach interpret train srn network two input two output two intern element learn sequenc n b n wile elman 1996 discov network behav like spiral move x point wherea seem plausibl interpret behavior recurr network train learn sequenc n b n dierent interpret requir move dierent task data set closer realworld scenario past develop larg real world system spoken languag analysi make extens use srn network wermter weber 1997 wermter meurer 1997 spoken input recogn speech recogn analyz syntact semant dialog level base increment anali si parallel syntact semant interpret robust process error date howev yet possibl focu interpret learn process interpret connectionist knowledg paper primarili concern detail interpret learn behavior well symbol interpret learn knowledg train order carri detail analysi concentr syntact transform task repres task largescal speechlanguag system task recurr network process sentenc associ syntact class phrasal level eg noun phrase preposit phrase etc use task analyz recurr neural network use mani dierent techniqu structur paper follow first introduc repres syntact transform task dene illustr dynam learn analysi b weight analysi c hierarch activ analysi compon activ analysi e transduc extract rigor compar techniqu network data set argu dierent techniqu provid mutual complementari interpret contribut paper particularli broad concret analysi knowledg extract process done furthermor propos dynam learn analysi transduc extract two new interpret techniqu dynam learn analysi provid better understand network learn transduc extract provid better understand network repres 2 extract structur knowledg use syntact analysi task order examin number dierent techniqu extract structur knowledg connectionist network rigor manner focu particular task spoken languag environ wermter lochel 1996 wermter weber 1997 wermter meurer 1997 train mani variat srn network elman 1991 mani sentenc use variou corpora sever thousand word base corpu sentenc domain schedul appoint 2355 word tabl 1 summar accuraci label assign unknown test set relat experi result report elsewher detail wermter lochel 1996 wermter weber 1997 wermter meurer 1997 wermter 1998 want illustr realworld network perform tabl 1 focu knowledg extract transduc neural network 29 ever analysi process extract explicit knowledg implicitli learn knowl edg paper concentr syntact phrasal assign mark tabl 1 tabl 1 perform network test set appoint schedul corpu task accuraci test set basic syntact disambigu 89 basic semant disambigu 86 syntact phrasal assign 84 semant phrasal assign 83 dialog act assign 79 word repair detect 94 phrase repair detect 98 demonstr process knowledg ex traction use 15 sentenc contain 76 word domain appoint schedul illustr purpos concentr learn syntact phrasal assign task sequenc basic categori word associ sequenc abstract syntact categori actual occur syntact basic categori noun n verb v adverb adject j preposit r determin pronoun u abstract phrasal categori noun group ng verb group vg preposit group pg task recurr network learn assign phrasal categori basi basic syntact categori order support robust understand spontan spoken languag show exampl utter corpu togeth syntact categori basic phrasal level 1 u ng thought v vg r pg 2 u ng v vg ng thursday n ng r pg easter n base seven basic syntact three phrasal syntact categori use srn network seven input unit three intern unit three output unit network actual system contain categori train sever thousand word illustr purpos restrict smaller network learn rate 005 momentum 09 weight updat perform increment train pattern train pattern consist basic syntact categori input layer abstract phrasal categori output layer figur 1 show simpli exampl recurr network task syntact phrase assign output input context layer aaaa aaaa aaaa noun group preposit group verb group pronoun noun adject verb adverb preposit determin fig 1 recurr network knowledg extract syntact phrase assign activ output element j time srn network comput basi weight activ h incom connect limit logist function f activ element intern layer h l comput similar manner activ input layer k time use activ intern layer previou time step 1 wermter 3 dynam learn analysi knowledg structur lazi learn past work knowledg structur connectionist network focus static connectionist network represent howev import insight gain examin certain knowledg structur emerg develop time certain task learn complet frequent interpret learn behavior demonstr mean learn curv overal error reduct time howev learn curv rst step detail analysi provid preliminari hint perform network train time figur 2 show learn curv overal sum squar error time pattern 50000 100000 150000 2000000206error fig 2 learn curv syntact phrasal assign learn curv show speed learn dier substanti time see dierent stage learn process begin learn proce fast later learn slower take longer make signic improv instanc 70000 140000 seem learn nish nal signic improv examin network reach perform start analysi directli random initi weight state learn start want give overview overal perform input pattern dierent time step ef fect show error 76 pattern demonstr set dierent time step figur 3 show individu error 76 pattern trainingindividu pattern fig 3 perform individu pattern learn ing base random initi pattern show rel high error point expect valu output element dier desir valu 0 1 05 therefor expect error individu pattern three output element expect error valu conrm gure shown gure 2 error decreas quickli start train state 100 pattern train set shown gure 4 first observ 100 train pat tern error 76 pattern shown could reduc signicantli pattern still show high error obvious network start learn pattern select knowledg extract transduc neural network 31 individu pattern pattern pattern fig 4 perform individu pattern 100 train pattern detail analysi reveal pattern lower error exactli pattern belong noun group ng 100 pattern network recogn global error minim signicantli focus ng pattern sinc pattern occur frequent instanc preposit group verb group therefor rst network learn constant map pattern noun group sinc reduc overal error stage explain certain pattern gure 4 still exhibit high error other low error pattern low error exactli pattern classi correctli noun group figur 5 show detail perform pattern network learn constant map ng observ perform ng pattern improv even howev also observ v g pattern learn detail analysi output prefer reveal stage addit ng v g pattern also learn correctli also demonstr gure 5 remain error pattern stage pattern belong preposit group pg still categor noun group ng ng pattern v g pattern classi correctli network learn frequent ng pattern second frequent v g pattern learn thu one could state network pursu conserv lazi learn strategi learn frequent occur simpl regular rst individu pattern pattern pattern pattern fig 5 perform individu pattern 600 train pattern afterward network attempt improv pattern especi remain pattern preposit group pg occur noun pronoun determin adject either part pg pattern ng pattern order resolv potenti ambigu previou context must use learn correct class assign exampl conserv lazi learn strategi network individu pattern 3 except pg pattern pattern ng pattern vg pattern fig 6 perform individu pattern 3000 train pattern wermter sinc rst network learn pattern need previou context knowledg categori assign simpl noncontextdepend categori assign learn pattern learn requir context previou pattern assign state network 3000 pattern shown gure 6 pattern classi correctli except three compar g ure 5 6 remain error individu pattern could reduc signicantli learn pg pattern necessari network integr local preced con text 150000 pattern regular learn shown gure 7 comparison gure 6 point smaller scale vertic axi stage pattern learn even though dierenc error rate individu pattern order reach 100 correct train set may necessari give reason good state certain stage order reach even better stage later also ect global learn curv gure 2 individu pattern ng patternsvg pattern pg pattern correct fig 7 perform individu pattern 150000 train pattern gener network pursu conserv lazi learn strategi first simpl frequent occur gener one categori learn network minim error signicantli frequent occur categori integr fur thermor pattern learn requir previou local context pattern learn requir context correct categori assign otherwis ambigu input final remain except learn conserv learn process may possibl overal error increas brie order reach better overal state later 4 weight analysi knowledg extrac tion visual intern weight strength use get impress intern knowl edg experi train set learn correctli 150000 pattern start analysi start weight analysi sinc weight provid lowest level interpret connectionist represen tation figur 8 show weight network three dierent time step illustr weight chang time learn gure identi sourc connectionist element shown horizont identi goal connectionist element shown vertic start horizont axi left right see weight threshold element input connectionist element syntact basic categori n j v r u three intern element three context element c3 vertic axi top bottom see weight three intern element output element repres abstract syntact categori vg ng pg knowledg extract transduc neural network 33 100 pattern 600 pattern 150000 pattern fig 8 weight analysi begin train 100 pattern train 600 pattern train 150000 pattern white box repres posit weight black box neg weight size box correspond size weight copi connect intern layer context layer chang therefor shown sinc alway equal 1 start analysi rst third gure 8 random initi rst third show weight network 100 pattern point ng pattern classi correctli pattern learn yet network learn constant output order reduc overal error much possibl see gure 8 network produc constant ng class 34 wermter see weight input element syntact basic categori n j v u intern element rel small similar hold weight context element c1 c2 c3 intern element due random initi begin train ing weight intern element output element abstract syntact categori neg v g pg intern element ng close 0 reason network produc constantli ng categori stage focu state network present 600 pattern also shown gure 8 point ng v g pattern assign correctli also ect weight observ posit weight n intern element posit weight intern element ng howev see neg weight v intern element intern element v g pg pattern categor correctli point one reason pg pattern depend signicantli previou con text howev point network learn obviou prefer start chang weight context layer network state 150000 pattern shown bottom gure intern layer distribut represent develop therefor direct interpret easili po sibl howev observ rst intern element primarili import pg detec tion second intern element play import role v g assign third intern element import ng nevertheless distribut rather local represent addit uenc ele ment furthermor weight context layer c h chang necessari order learn pg group assign gener speak explain certain phenomena use type weight analysi lowest interpret level network howev dicult extract explicit knowledg deeper understand behavior network directli weight reason diculti includ 1 static represent weight show dynam recurr network 2 distribut weight activ 3 number weight especi case larger network fore eort could made reduc size network elimin small weight furthermor group similar weight could replac averag strength noneth less weight analysi still detail larger network 5 compon activ analysi knowledg extract weight analysi focus weight provid lowlevel analysi one way address problem move toward activ analysi activ intern element analyz sinc intern element receiv activ number weight connect activ intern element integr sever weight connect provid higher abstract level analysi order demonstr analysi perform use srn network introduc previou section store vector represent intern layer pattern vector represent constitut input cluster algorithm provid hierarch represent form dendrogram vector similar vector represent end cluster figur 9 show initi part pattern cluster accord intern activ clearli observ intern represent ect classic accord three class pg base weight intern layer learn represent particularli support classic singl word appear dierent context lead differ intern represent instanc word shown two dierent represen tation one represent use part ng class part pg class therefor nd represent dierent posit within dendrogram knowledg extract transduc neural network 35 allung iung iung usung weung wednesdaynng morningnng inrpg weeknpg isvvg comevvg mustvvg isvvg fig 9 hierarch cluster analysi intern classic represent visibl purpos portion shown 6 princip compon analysi knowledg extract anoth kind analysi use interpret intern represent cla sicat princip compon analysi figur show result analysi current task vector intern layer correspond identi provid input princip compon analysi vector dier substanti depict gure larg distanc also observ intern represent ect prefer map learn three categori class ng v g pg pattern distribut across dierent area thu classic intern represent clearli seen show network actual learn classic task well learn complet intern represent character prefer map ping accord cluster analysi princip compon analysi similar intern vector represent respons represent similar prefer assign equal cate gori howev interpret weight mean hinton diagram activ via cluster analysi princip compon analysi provid limit form structur extract knowledg 36 wermter iung inrpg thursdaynng morningnng weung unsung onapg thedng isvvg wednesdaynng iung comevvg mustvvg iung thatung isvvg allung tuesday couldvvg usung letvvg usung dasung isvvg makevvg tillrpg weung inrpg marchnpg otherjng thatung isvvg fig 10 princip compon analysi intern classic represent 7 transduc extract word sequenc word repres syntact semant pragmat categori prefer input instanc srn network input repres sequenc categori prefer associ sequenc correspond output prefer simpl descript sequenc analysi similar function synchron sequenti machin booth 1967 kohavi 1970 shield 1987 although prefer learn yet consid machin therefor shall focu extens synchron sequenti machin repres sequenti knowl edg especi synchron moor machin start basic denit synchron sequenti machin also call transduc denit synchron sequenti chine transduc synchron sequenti machin tupl 1 nite nonempti set input output 2 nonempti set state 3 function f state transit function 4 function f output function output depend state input machin socal meali machin output function f output depend state machin later socal moor machin output function f synchron sequenti machin sometim call transduc sequenti machin assign output new state input old state done whole sequenc input state discret time set necessarili nite booth 1967 although assum case nite machin wherea automata knowledg extract transduc neural network 37 acceptor languag decid whether certain input belong correspond grammar sequenti machin transduc chang intern state dynam depend input previou state also provid output input meali moor machin slightli dier ent moor machin determin state rst afterward state use provid output contrast output meali machin depend also directli current input howev shown moor machin equival meali machin vice versa booth 1967 hopcroft ullman 1979 case concentr moor machin sinc output certain neural network base intern state hold stanc feedforward network srn network wherea sometim sun 1995 sequenti machin use model singl element neural network want use sequenti machin descript whole network also motiv fact real neuron system seen physic entiti perform state transit churchland sejnowski 1992 specifi languag knowledg describ moor machin state transit function f output function f also integr f f function f os f correspond instanc transform within srn network specic moor machin could perform use state tabl potenti entri task assign syntact phrasal categori syntact basic categori could verb current state prep group new state verbal group output verbal group may possibl assign direct interpret state reason simpl identi may use verb current state 4 new state 5 output verbal group possibl dene state transit tabl assign combin input current state output new state way symbol synchron sequenti machin speci clear regular known beforehand number limit tabl compos manual howev number input state combin quickli get larg automat procedur becom necessari abovement state transit tabl discret symbol therefor support gradual represent instanc input state could ambigu dierent gradual prefer could exist dierent inter pretat instanc meet could stronger prefer syntact interpret noun smaller prefer verb form consequ want use prefer input output state machin prefer type abl take valu 0 1 multipl prefer repres integr extend singl categori verb ndimension prefer input mdimension prefer output obtain new synchron machin call prefer moor machin want describ synchron sequenti prefer moor machin transform sequenti input prefer sequenti output prefer see simpl recurr network feedforward network interpret neural prefer moor machin furthermor show symbol neural knowledg integr quit natur use prefer moor machin 38 wermter denit prefer moor machin prefer moor machin pm synchron sequenti machin character 4tupl nonempti set input output state sequenti prefer map contain state transit function f output function f n ldimension prefer valu 0 1 n 0 1 0 1 l respect gener version prefer moor machin shown gure 11 left prefer moor machin realiz sequenti prefer map use current state prefer input prefer assign output prefer new state prefer prefer map state aaaa aaaa output input fig 11 neural prefer moor machin relationship srn network describ new techniqu extract knowledg within recurr network form transduc symbol transduc extract recurr network assign input vector basic syntact categori new output vector phrasal categori depend previou context network intern state context repres threedimension vector simplic strict symbol interpret threedimension vector take 2 3 8 state order acquir symbol interpret network present pattern train set store intern state vector hidden layer network output vector state vector next corner prefer determin use euclidean distanc metric thu euclidean distanc metric assign one three symbol abstract syntact phrase categori output vector one eight state number identi state vector knowledg extract transduc neural network 39000 100010 110011 nng rpg vvg dng dng nng rpg ung dng nng nng dpg jpg npg vvg dng ang apg jng nng vvg vvg rpg vvg ung rpg nng nng dng ung nng rpg apg vvg vvg ung fig 12 transduc extract recurr network exampl sentenc ung vvg dng thursday nng rpg easter nng figur 12 show knowledg learn network extract symbol transduc corner node repres eight strict state center node repres start state tran ducer edg nd symbol singl transduct input output categori separ colon eg ng mean start sourc state edg determin prefer assign noun group prefer ng transduct made end state edg extract transduc see clear regular certain state instanc transduct state 100 primarili respons assign preposit group pg exampl transduct state 010 state 000 primarili respons verbal group vg assign furthermor gure 12 show exampl transduct sentenc thursday easter begin start state center see transduct ng word assign noun group ng pronoun u assign verb group vg verb transduct ng ng assign noun group ng thursday final transduct assign preposit group pg sequenc easter dierent abstract syntact categori ng pg assign categori n depend learn previou context nng rpg vvg dng dng nng rpg ung dng nng nng dpg jpg npg vvg dng ang apg jng nng vvg vvg rpg vvg ung rpg nng nng dng ung nng rpg apg vvg vvg ung fig 13 transduc extract recurr network exampl sentenc ung thought vvg rpg dpg next jpg week npg detail less detail transduc obtain state output vector map fewer node thu gener abstract level symbol transduc quit variabl symbol transduc repres abstract detail network knowledg abstract also hide numer complex allow direct symbol interpret provid summari network behavior give exampl gure 13 show transduct exampl sentenc thought next week begin start state center see transduct ng word assign noun group ng pronoun u assign verb group vg verb thought final transduct r pg assign preposit group pg word sequenc next week one advantag transduc extract higher abstract level use represent recurr network lead better understand function origin network contain detail knowledg numer weight activ possibl see declar sequenti symbol knowledg network repres extract symbol transduc allow better understand learn sequenti knowledg repres explicit manner knowledg extract transduc neural network 41 8 discuss analysi 81 comparison knowledg extract techniqu previou work use individu techniqu isol interpret neural network extract structur knowledg paper analyz dierent techniqu use train network order interpret network knowl edg extens comparison detail network knowledg need order gain better understand knowledg extract repres neural network also introduc two new techniqu dynam learn analysi transduc extract dynam learn analysi examin format develop categori time learn thu provid much deeper understand neural network arriv learn represent transduc extract develop repres sequenti process recurr network higher level abstract gener found dierent interpret techniqu provid dierent view knowledg contain neural network thu singl best techniqu dier ent aspect knowledg extract use particular techniqu depend rather requir interpret tabl 2 illustr summar gener properti dierent techniqu dynam learn analysi dla base output represent provid high level understand base known output represent techniqu easi interpret use network type hand particularli support recurr network symbol integr exibl knowledg structur furthermor structur relationship extract transduc extract te new techniqu use output represent well intern activ main advantag techniqu high level understand form extract symbol transduc specic support sequenti recurr network possibl extract structur relationship extract transduc integr symbol knowledg eg code symbol transduc dierent transduc gener exibl base number state use intern activ layer lead rel straightforward interpret network involv compar techniqu also requir addit eort extract symbol transduc intern activ output represent compar dla caa see dla techniqu specic provid high level interpret dynam learn process argu wa haa caa techniqu tendenc toward gener detail lowlevel interpret dla te howev techniqu special highlevel dynam interpret focus output interpret dynam recurr network provid new level understand wherea lot previou work focus lowlevel terpret believ futur higher level interpret knowledg extract requir 82 relat work transduc extract relat work finit state automata transduc wide use variou form within tradit eg hopcroft ullman 1979 basic automata transduc alway certain context state analyz certain word symbol move new state potenti gener new word sym bol use chang state possibl encod sequenti context although nite automata regular languag sucient describ possibl construct natur languag complet see eg winograd 1983 automata still constitut central minim requir represent natur languag thu occupi lowest level chomski hierarchi languag hopcroft ullman 1979 furthermor possibl design ecient realiz nite automata dierent domain kaplan 1995 eg 42 wermter tabl 2 comparison dierent knowledg extract techniqu dynam learn analysi dla weight analysi wa hierarch activ analysi haa compon activ analysi caa transduc extract te abbrevi activationsweightsoutput lowmediumhigh network represent use w ao gener level understand h l h specic support recurr network l l l l h degre structur relationship l l h integr symbol knowledg l l h flexibl level knowledg structur l l h comput eort l l easi interpret h l h gener portabl network h h h h morpholog lexicon access inform extract sentenc syntact tag etc recurr network potenti learn sequenti prefer map f automat base input output exampl see gure 11 wherea tradit moor machin fuzzysequentialfunct santo 1973 involv manual encod recent illustr srn network emul symbol moor machin nite automaton kremer 1995 kremer 1996 also shown howev goudreau gile 1995 goudreau et al 1994 recurr network singl input layer one context layer one output layer socal singl layerrstordernetwork sucient realiz arbitrari nite automata natur languag process represent least power nite au tomata consequ singlelayerrstord network appropri use srn network recurr network contain nite transduc special case also support much power properti base gradual mdimension prefer represent instanc could shown srn network emul certain restrict properti pushdown automaton particular recurs represent structur limit depth elman 1991 wile elman 1996 apart tradit symbol regular rep resent gradual learn represent also repres furthermor number input state output prefer necessarili nite therefor neural prefer moor machin power nite transduc er recurr neural network seen learn augment simpl nite symbol transduc respect learn within gradual prefer space perspect symbol knowledg special abstract region neural prefer space import line research automata recurr network report gile et al 1992 goudreau gile 1995 tino et al 1995 gile colleagu studi nite state automata neural network substanti dierenc research start often known nite state automaton use gener sequenc sequenc use train secondord neural network use partit algorithm nite state automaton extract network activ minim compar origin known nite state automaton way gile colleagu could studi comput properti extract particularli well nite state automata also frequent reli rel simpl 10 sequenc knowledg extract transduc neural network 43 motiv methodolog dierent sever respect assum initi nite state automaton transduc known especi realworld problem interest case one automaton known advanc wherea interest comparison sequenc gen erat gener sequenc nite state automaton alreadi introduc certain regular train set thu sequenc gener import uenc learn behav ior someth want rule fact interest situat know machin ex tract especi noisi realworld learn data underli regular may quit dispar regularli gener sequenc furthermor task network quit dierent secondord network employ gile colleagu train recognit output layer repres state represent fed back input layer next step recurr network perform assign task sequenc input associ sequenc output determin whether certain sequenc belong certain automaton simpl structur sequenc interest transduc extract rather recogn ex traction gener design nal state network sinc network extract symbol transduc produc output long input provid transduc behavior therefor quit dierent recognit perform report gile omlin 1993 base acceptor artici languag 9 conclus main contribut paper particularli broad analysi knowledg extract recurr network addit propos dynam learn analysi transduc extract two new dynam interpret tech niqu dynam learn analysi provid better understand network learn transduc extract provid better understand network repres learn conserv lazi learn strategi lead connectionist represent describ symbol transduc transduc allow much better interpret sequenti network knowledg compar standard analysi use hierarch cluster hinton diagram weight analysi cluster analysi princip compon analysi detail static contrast new method extract symbol transduc describ learn classic perform much bet ter sinc transduc extract consid sequenti charact learn represent recurr network allow better symbol inspect possibl direct integr symbol classier explor futur work conclud dynam learn analysi transduc extract lot potenti improv knowledg structur base recurr network r extract algorithm pattern classi rule network sequenti machin automata theori comput brain extract comprehens model train neural network distribut repr sentat languag dynam system learn extract recurr neural network repres hybrid prob lem learn distribut represent concept introduct automata theori finit state technolog switch finit automata theori comput power elmanstyl recurr network theori grammat induct connectionist paradigm hybrid intellig system extract rule discretetim recurr neural network fuzzi sequenti func tion framework combin symbol neural learn introduct automata theori learn distribut represent classi finit state machin recurr neural network hybrid connectionist natur languag process hybrid approach arti prefer moor machin neural fuzzi integr build lexic represent dynam use arti screen learn syntact semant spoken languag analysi use arti learn count without counter case studi dynam activ landscap recurr net work languag cognit process tr