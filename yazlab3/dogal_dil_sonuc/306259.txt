elev group control use multipl reinforc learn agent recent algorithm theoret advanc reinforc learn rl attract widespread interest rl algorithm appear approxim dynam program increment basi train basi real simul experi focus comput area state space actual visit control make comput tractabl larg problem member team agent employ one algorithm new collect learn algorithm emerg team whole paper demonstr collect rl algorithm power heurist method address largescal control problemselev group control serv testb difficult domain pose combin challeng seen multiag learn research date use team rl agent respons control one elev car team receiv global reward signal appear noisi agent due effect action agent random natur arriv incomplet observ state spite complic show result simul surpass best heurist elev control algorithm awar result demonstr power multiag rl larg scale stochast dynam optim problem practic util b introduct interest develop capabl learn system increas within multiag ai research commun eg weiss sen 1996 learn enabl system flexibl robust make better abl handl uncertainti chang circumst especi import multiag system design system often face extrem difficult task tri anticip possibl conting interact among agent ahead time much could said concern field decentr control polici control station develop global vantag point learn play role even though execut polici depend inform avail control station polici design central way access complet descript problem research focus constitut optim polici given inform pattern polici might learn constraint reinforc learn rl barto sutton forthcom bertseka tsitsik li 1996 appli natur case autonom agent receiv sensat input take action affect environ order achiev goal rl base idea tendenc produc action strengthen reinforc produc favor result weaken produc unfavor result framework appeal biolog point view sinc anim certain builtin prefer pleasur alway teacher tell exactli action take everi situat member group agent employ rl algorithm result collect algorithm allow control polici learn decentr way even situat central inform avail may advantag develop control polici decentr way order simplifi search polici space although may possibl synthes system whose goal achiev agent conflict object paper focus team agent share ident object correspond directli goal system whole demonstr power multiag rl focu difficult problem elev group supervisori control elev system oper highdimension continu state space continu time discret event dynam system state fulli observ nonstationari due chang passeng arriv rate use team rl agent respons control one elev car agent use artifici neural network store action valu estim compar parallel architectur agent share network decentr architectur agent independ network either case team receiv global reinforc signal noisi perspect agent due part effect action agent despit difficulti system outperform heurist elev control algorithm known us also analyz polici learn agent show learn rel robust even face increasingli incomplet state inform result suggest approach decentr control use multiag rl consider promis follow section give addit background rl introduc elev domain describ detail multiag rl algorithm network architectur use present discuss result final draw conclus detail topic see crite 1996 2 reinforc learn symbol connectionist learn research focus primarili supervis learn teacher provid learn system set train exampl form inputoutput pair supervis learn techniqu use wide varieti problem involv pattern classif function approxim howev mani situat train exampl costli even imposs obtain rl applic difficult situa tion help avail critic provid scalar evalu output select rather specifi best output direct chang output rl one face difficulti supervis learn combin addit difficulti explor determin best output given input rl task divid natur two type nonsequenti task agent must learn map situat action maxim expect immedi payoff sequenti task agent must learn map situat action maxim expect longterm payoff sequenti task difficult action select agent may influenc futur situat thu futur payoff case agent interact environ extend period time need evalu action basi longterm consequ perspect control theori rl techniqu way find approxim solut stochast optim control problem agent control environ system control object maxim perform measur time given model state transit probabl reward structur environ problem solv principl use dynam program dp algorithm howev even though dp requir time polynomi number state mani problem interest mani state amount time requir solut infeas recent rl algorithm design perform dp increment manner unlik tradit dp algorithm requir priori knowledg state transit probabl reward structur environ use improv perform onlin interact environ onlin learn focus comput area state space actual visit control thu algorithm comput tractabl way approxim dp larg problem focus phenomenon also achiev simul onlin train ing one often construct simul model without ever explicitli determin state transit probabl environ barto sutton forthcom crite barto 1996 exampl simul model see section 33 sever advantag use simul model suffici ac curat possibl gener huge amount simul experi quickli potenti speed train process mani order magnitud would possibl use actual experi addit one need concern perform level simul system train success exampl simul onlin train found tesauro tdgammon system 1992 1994 1995 use rl techniqu learn play strong masterlevel backgammon 21 multiag reinforc learn varieti disciplin contribut studi multiag system mani research focus topdown approach build distribut system creat global vantag point one drawback topdown approach extraordinari complex design agent sinc extrem difficult anticip possibl interact conting ahead time complex system research recent taken opposit approach combin larg number rel unsophist agent bottomup manner see emerg put togeth group amount sort iter procedur design set agent observ group behavior repeatedli adjust design note effect group behavior although group simpl agent often exhibit interest complex dynam littl understand yet creat bottomup design achiev complex predefin goal multiag rl attempt combin advantag approach achiev simplic bottomup approach allow use rel unsophist agent learn basi experi time rl agent adapt topdown global reinforc signal guid behavior toward achiev complex predefin goal result robust system complex problem creat minimum human effort crite barto 1996 research multiag rl date back least work russian mathematician tsetlin 1973 other field learn automata see narendra thathachar 1989 number theoret result obtain context nonsequenti rl certain type learn automata converg equilibrium point zerosum nonzerosum repeat game see narendra thathachar 1989 detail team equilibrium point local maximum element game matrix maximum row column howev gener nonzerosum game equilibrium point often provid poor payoff player good exampl prison dilemma equilibrium point produc lowest total payoff axelrod 1984 start approxim 1993 number research began investig appli sequenti rl algorithm multiag context although much work simplist domain grid world sever interest applic appear point promis sequenti multiag rl markey 1994 appli parallel qlearn problem control vocal tract model 10 degre freedom discuss two architectur equival distribut parallel architectur describ section 44 agent control one degre freedom action space distinguish qvalu base action select bradtk 1993 describ initi experi use rl decentr control flexibl beam task effici damp disturb beam appli forc discret locat time use 10 independ adapt control distribut along beam control attempt minim local cost observ local portion state inform dayan hinton 1993 propos manageri hierarchi call feudal rl scheme higherlevel manag set task lower level manag reward see fit sinc reward may differ differ level hierarchi team furthermor singl action select lowest level actual affect environ sens hierarch architectur singl agent tan 1993 report simpl hunterprey experi multiag rl focu share sensori inform polici experi among agent shoham tennenholtz 1993 investig social behavior emerg agent simpl learn rule focu two simpl nkg iter game n agent meet k time randomli play game g littman boyan 1993 describ distribut reinforc learn algorithm packet rout base asynchron bellmanford algorithm scheme use singl qfunction state entri qfunction assign node network respons store updat valu entri differ work distribut rl entir qfunction singl entri must store node addit multiag rl research concern team problem signific amount work focus zerosum game singl agent learn play oppon one earliest exampl samuel checkerplay program recent exampl tesauro tdgammon program 1992 1994 1995 learn play strong master level backgam mon type program often train use selfplay gener view singl agent littman 1994 1996 provid detail discuss rl appli zerosum game case agent altern action take simultan littl work done multiag rl gener nonzero sum game sandholm crite 1996 studi behavior multiag rl context iter prison dilemma show qlearn agent abl learn optim strategi fix oppon titfortat addit investig behavior result two qlearn agent face 3 elev group control section introduc problem elev group control serv testb multiag reinforc learn familiar problem anyon ever use elev system spite conceptu simplic pose signific difficulti elev system oper highdimension continu state space continu time discret event dynam system state fulli observ nonstationari due chang passeng arriv rate optim polici elev group control known use exist control algorithm standard comparison elev domain provid opportun compar parallel distribut control architectur agent control one elev car monitor amount degrad occur agent face increas level incomplet state inform button dn figur 1 elev system schemat diagram schemat diagram elev system lewi 1991 present figur 1 elev car repres fill box diagram repres hall call someon want enter car gamma repres car call someon want leav car left side shaft repres upward move car call right side shaft repres downward move car call car therefor move clockwis direct around shaft section 31 consid natur differ passeng arriv pattern implic section 32 review varieti elev control strategi literatur section 33 describ particular simul elev system focu remaind paper 31 passeng arriv pattern elev system driven passeng arriv arriv pattern vari cours day typic offic build morn rush hour bring peak level traffic peak traffic occur afternoon part day characterist pattern differ arriv pattern differ effect pattern requir analysi peak downpeak elev traffic simpli equival pattern opposit direct one might initi guess downpeak traffic mani arriv floor singl destin uppeak traffic singl arriv floor mani destin distinct signific implic exampl light traffic averag passeng wait time kept low keep idl car lobbi immedi avail arriv passeng light traffic wait time longer sinc possibl keep idl car everi upper floor build therefor addit wait time incur car move servic hall call situat revers heavi traffic heavi traffic car may fill lobbi passeng desir stop mani differ upper floor larg number stop caus significantli longer roundtrip time heavi traffic car may fill stop upper floor reason downpeak handl capac much greater uppeak capac siikonen 1993 illustr differ excel graph obtain extens simul sinc uppeak handl capac limit factor elev system design predict heaviest like uppeak demand build determin configur accomod demand uppeak capac suffici downpeak gener also uppeak traffic easiest type analyz sinc passeng enter car lobbi destin floor servic ascend order empti car return lobbi standard capac calcul strakosch 1983 siikonen 1993 assum car leav lobbi passeng 80 100 percent capac averag passeng likelihood select destin floor known probabl theori use determin averag number stop need round trip one estim averag round trip time repres averag amount time car arriv lobbi l number car assum car evenli space averag wait time one half interv realiti averag wait somewhat longer control decis pure traffic determin open close elev door lobbi decis affect mani passeng board elev lobbi door close realli choic next action car call regist passeng must servic ascend order empti car must return lobbi pepyn cassandra 1996 show optim polici handl pure traffic thresholdbas polici close door optim number passeng enter car optim threshold depend upon traffic intens may also affect number car call alreadi regist state car cours traffic seldom complet pure method must use assign hall call gener two way traffic come two varieti two way lobbi traffic upmov passeng arriv lobbi downmov passeng depart lobbi compar pure traffic round trip time longer passeng serv two way interfloor traffic passeng travel floor lobbi interfloor traffic complex lobbi traffic requir almost twice mani stop per passeng lengthen round trip time two way downpeak traffic pattern requir mani decis pure traffic leav lobbi car must decid high travel build turn floor make addit pickup decis requir wider varieti context control strategi also possibl two way downpeak traffic situat reason peak traffic pattern chosen testb research describ testb detail review variou elev control strategi literatur 32 elev control strategi oldest relaybas automat control use principl collect control strakosch 1983 siikonen 1993 car alway stop nearest call run direct one drawback scheme mean avoid phenomenon call bunch sever car arriv floor time make interv thu averag wait time much longer advanc electron includ advent microprocessor made possibl sophist control polici approach elev control discuss literatur gener fit follow categori often one categori unfortun descript proprietari algorithm often rather vagu sinc written market purpos specif intend benefit competitor reason difficult ascertain rel perform level mani algorithm accept definit current state art ovaska 1992 321 zone approach oti elev compani use zone start point deal variou traffic pattern strakosch 1983 car assign zone build ing answer hall call within zone park idl goal zone approach keep car reason well separ thu keep interv approach quit robust heavi traffic give signific amount flexibl sakai kurosawa 1984 hitachi describ concept call area control relat zone possibl assign hall call car alreadi must stop floor due car call otherwis car within area ff hall call assign possibl area ff control paramet affect averag wait time power consumpt 322 searchbas approach anoth control strategi search space possibl car assign select one optim criterion averag wait time greedi search strategi perform immedi call assign assign hall call car first regist never reconsid assign ment nongreedi algorithm postpon assign reconsid light updat inform may receiv addit hall call passeng destin greedi algorithm give measur perform due lack flexibl also requir less comput time western countri arriv car gener signal wait passeng begin deceler si ikonen 1993 allow use nongreedi algorithm custom japan signal car assign immedi upon call registr type signal requir use greedi algorithm tobita et al 1991 hitachi describ system car assign occur hall button press assign car minim weight sum predict wait time travel time number rider fuzzi rulebas system use pick coeffici estim function simul use verifi effect reced horizon control exampl nongreedi searchbas approach everi event perform expens search best assign hall call assum new passeng arriv closedloop control achiev recalcul new openloop plan everi event weak approach comput demand lack consider futur arriv exampl reced horizon control finit intervisit minim empti system algorithm esa bao et al 1994 fim attempt minim squar wait time esa attempt minim length current busi period 323 rulebas approach sens control polici could consid rulebas situat action howev narrowli consid type product system commonli use artifici intellig ujihara tsuji 1988 mitsubishi describ ai2100 system use expertsystem fuzzylog technolog claim expert groupsupervisori control experi knowledg necessari shorten wait time variou traffic condit admit expert knowledg fragmentari hard organ difficult incorpor creat rule base compar decis made convent algorithm decis determin simul anneal discrep analyz expert whose knowledg solv problem use creat fuzzi control rule fuzzi lie part rule ujihara amano 1994 describ latest chang system previou version use fix evalu formula base current car posit call locat recent version consid futur car posit probabl futur hall call exampl one rule hall call regist upper floor larg number car ascend toward upper floor assign one ascend car basi estim time arriv note immedi call alloc algorithm consequ particular rule assign car basi estim time arriv bear similar greedi searchbas algorithm describ 324 heurist approach longest queue first lqf algorithm assign upward move car longest wait queue highest unansw floor first huff algorithm assign upward move car highest queue peopl wait bao et al 1994 algorithm design specif downpeak traffic assign downward move car unassign hall call encount dynam load balanc dlb algorithm attempt keep car evenli space assign contigu nonoverlap sector car way balanc load lewi 1991 dlb nongreedi algorithm reassign sector everi event 325 adapt learn approach imasaki et al 1991 toshiba use fuzzi neural network predict passeng wait time distribut variou set control paramet system adjust paramet evalu altern candid paramet neural network explain control algorithm actual use paramet network train hitachi research fujino et al 1992 tobita et al 1991 use greedi control algorithm combin multipl object wait time travel time crowd ing power consumpt weight object accomplish use paramet tune onlin modul call learn function unit collect traffic statist attempt classifi current traffic pattern tune function unit gener paramet set current traffic pattern test use builtin simul best paramet use control system search entir paramet space would prohibit expens heurist use paramet set test levi et al 1977 use dynam program dp offlin minim expect time need complet current busi period discount factor use sinc assum valu finit major differ qlearn must perform offlin sinc use model transit probabl system perform sweep state space troubl use dp calcul optim polici state space larg requir drastic simplif levi et al use sever method keep size state space manag consid build car 8 floor number button simultan restrict state button restrict binari valu ie elaps time discard car unlimit capac construct transit probabl matrix principl part procedur assum intens poisson arriv floor known valu iter polici iter perform obtain solut markon et al 1994 devis system train neural network perform immedi call alloc three phase train phase one system control exist control flex8820 fuzzyai group control system fujitec supervis learn use train network predict hall call servic time first phase train use learn appropri intern represent ie weight input layer hidden layer network end first phase train weight fix phase two output layer network retrain emul exist control phase three singl weight output layer network perturb result perform measur traffic sampl weight modifi direct improv perform view form nonsequenti reinforc learn singlestag reward determin measur system perform traffic sampl input represent use 25 unit car output represent use one unit car hall call alloc car correspond output unit highest activ also describ clever way incorpor permut symmetri problem architectur network say state two car interchang output also interchang done mani set hidden unit car explicitli link togeth appropri weight system test simul 6 car 15 floor typic build train 900 passeng per hour small improv around 1 second averag wait time exist control untyp build uniformli distribut origin destin floor 1500 passeng per hour improv averag wait time almost 4 second one advantag system maintain adequ servic level begin sinc start preexist control hand clear whether also may trap control suboptim region polici space would interest use central immedi call alloc network architectur part sequenti reinforc learn algorithm 33 elev testb particular elev system studi paper simul 10stori build 4 elev car simul written lewi 1991 passeng arriv floor assum poisson arriv rate vari cours day simul use traffic profil bao et al 1994 dictat arriv rate everi 5minut interv typic afternoon peak rush hour tabl 1 show mean number passeng arriv floor 5minut interv head lobbi addit interfloor traffic vari 0 10 traffic lobbi tabl 1 downpeak traffic profil time 331 system dynam system dynam approxim follow paramet ffl floor time time move one floor maximum speed 145 sec ffl stop time time need deceler open close door acceler sec ffl turn time time need stop car chang direct 1 sec ffl load time time one passeng enter exit car random variabl 20th order truncat erlang distribut rang 06 60 sec mean 1 sec ffl car capac 20 passeng simul quit detail certainli realist enough purpos howev minor deviat realiti note simul car acceler full speed deceler full speed distanc one half floor distanc would somewhat longer real system thu simul acceler deceler time alway real system vari depend speed elev exampl express car descend tenth floor top speed take longer deceler first floor car descend second floor simul also allow car commit stop floor one half floor away though realist car move top speed concept make decis regard next floor car could commit stop valid although elev car system homogen learn techniqu describ paper also use gener situat eg sever express car car servic subset floor 332 state space state space continu includ elaps time sinc hall call regist realvalu even real valu approxim binari valu size state space still immens compon includ 2 possibl combin button button land except top bottom 2 40 possibl combin 40 car button possibl combin posit direct car round nearest floor part state fulli ob servabl exampl exact number passeng wait floor exact arriv time desir destin ignor everyth except configur hall car call button approxim posit direct car obtain extrem conserv estim size discret approxim continu state space state 333 control action car small set primit action stop floor must either move move motion floor must either stop next floor continu past next floor due passeng expect two constraint action car pass floor passeng want get turn servic car button present direct also ad three addit heurist constraint attempt build primit prior knowledg car stop floor unless someon want get stop pick passeng floor anoth car alreadi stop given choic move prefer move sinc downpeak traffic tend push car toward bottom build last constraint real choic left car stop continu action action elev car execut asynchron sinc may take differ amount time complet 334 perform criteria perform object elev system defin mani way one possibl object minim averag wait time time arriv passeng entri car anoth possibl object minim averag system time sum wait time travel time third possibl object minim percentag passeng wait longer dissatisfact threshold usual 60 second anoth common object minim averag squar wait time chose latter perform object sinc tend keep wait time low also encourag fair servic exampl wait time 2 8 second averag 5 second wait time 4 6 second averag squar wait time differ 34 2 8 versu 26 4 6 4 algorithm network architectur section describ multiag reinforc learn algorithm appli elev group control scheme agent respons control one elev car agent use modif qlearn discreteev system togeth employ collect form reinforc learn begin describ modif need extend qlearn discreteev framework deriv method determin appropri reinforc signal face uncertainti exact passeng arriv time describ algorithm feedforward network use store qvalu distinct parallel distribut version algorithm 41 discreteev reinforc learn elev system model discret event system cassandra 1993 signific event passeng arriv occur discret time amount time event realvalu variabl system constant discount factor fl use discretetim reinforc learn algorithm inadequ problem approach use variabl discount factor depend amount time event bradtk duff 1995 case costtogo defin integr rather infinit sum c immedi cost discret time c instantan cost continu time sum squar wait time current wait pa senger fi control rate exponenti decay experi describ paper sinc wait time measur second scale instantan cost c factor 10 6 keep costtogo valu becom exceedingli larg elev system event occur randomli continu time branch factor effect infinit complic use algorithm requir explicit lookahead therefor employ discret event version qlearn algorithm sinc consid event encount actual system trajectori requir model state transit probabl qlearn updat rule watkin 1989 take follow discret event form e gammafi gammat x action taken state x time x next decis requir state time ff learn rate paramet c fi defin e gammafit gammat x act variabl discount factor depend amount time event consid case c constant event extend formul case c quadrat sinc goal minim squar wait time integr qlearn updat rule take form e gammafi gammat x w p amount time passeng p wait time alreadi wait time x special care need handl passeng begin wait x see section 421 integr solv part yield difficulti aris use formula sinc requir knowledg wait time wait passeng howev wait time passeng press hall call button known real elev system number subsequ passeng arriv exact wait time avail examin two way deal problem call omnisci onlin reinforc scheme simul access wait time passeng could use inform produc necessari reinforc signal call omnisci reinforc sinc requir inform avail real system note control receiv extra inform howev rather critic evalu control reason even omnisci reinforc use design phase elev control simul system result train control instal real system without requir extra knowledg possibl train use inform would avail real system onlin onlin reinforc assum wait time first passeng queue known elaps button time poisson arriv rate queue known estim gamma distribut use estim arriv time subsequ passeng time n th subsequ arriv follow gamma distribut gamman 1 queue subsequ arriv gener follow expect cost first b second hall button pressedx z bprob n th arriv occur time delta cost given arriv time z bgamma z bz bgamma integr also solv part yield expect cost gener solut provid section 422 describ section 54 use onlin reinforc produc result almost good obtain omnisci reinforc 42 collect discreteev qlearn elev system event divid two type event first type import calcul wait time therefor also reinforc includ passeng arriv transfer car omnisci case hall button event onlin case second type car arriv event potenti decis point rl agent control car car motion floor gener car arriv event reach point must decid whether stop next floor continu past next floor case car constrain take particular action exampl stop next floor passeng want get agent face decis point unconstrain choic action 421 calcul omnisci reinforc omnisci reinforc updat increment everi passeng arriv event passeng arriv queue passeng transfer event passeng get car car arriv event control decis made increment updat natur way deal discontinu reinforc aris passeng begin end wait car decis eg anoth car pick wait passeng amount reinforc event car sinc share object function amount reinforc car receiv decis differ sinc car make decis asynchron fore car associ storag locat ri total discount reinforc receiv sinc last decis time di accumul time event follow comput perform let 0 time last event 1 time current event passeng p wait 0 1 let w 0 p w 1 p total time passeng p wait 0 1 respect car 422 calcul onlin reinforc onlin reinforc updat increment everi hall button event sig nale arriv first wait passeng queue arriv car pick wait passeng queue car arriv event control decis made assum onlin reinforc caus passeng wait queue end immedi car arriv servic queue sinc possibl know exactli passeng board car poisson arriv rate queue estim reciproc last interbutton time queue ie amount time last servic button push howev ceil 004 passeng per second place estim arriv rate prevent small interbutton time creat huge penalti might destabil costtogo estim time event follow comput perform let 0 time last event 1 time current event hall call button b activ 0 1 let w 0 b w 1 b elaps time button b 0 1 respect car f g 423 make decis updat qvalu car motion floor gener car arriv event reach point must decid whether stop next floor continu past next floor case car constrain take particular action exampl stop next floor passeng want get agent face decis point unconstrain choic action algorithm use agent make decis updat qvalu estim follow 1 time x observ state x car arriv decis point select action use boltzmann distribut qvalu estim posit temperatur paramet anneal decreas train valu control amount random select action begin train qvalu estim inaccur high valu use give nearli equal probabl action later train qvalu estim accur lower valu use give higher probabl action thought superior still allow explor gather inform action discuss section 53 choos slow enough anneal schedul particularli import multiag set 2 let next decis point car time state car includ car updat rdelta valu describ last two section car adjust estim qx toward follow target valu fstopcontg car reset reinforc accumul ri zero 3 let x x go step 1 43 network use store qvalu use lookup tabl store qvalu rule larg system stead use feedforward neural network train error backpropag algorithm rumelhart et al 1986 network receiv state inform input produc qvalu estim output qvalu estim written qx oe oe vector paramet weight network exact weight updat equat fstopcontg ff posit learn rate stepsiz paramet gradient 5 oe vector partial deriv qx oe respect compon oe start train weight network initi uniform random number gamma1 1 experi paper use separ singleoutput network actionvalu estim other use one network multipl output unit one action basic network architectur pure traffic use 47 input unit 20 hidden sigmoid unit 1 2 linear output unit input unit follow ffl unit two unit encod inform nine hall ton realvalu unit encod elaps time button push binari unit button push unit unit repres possibl locat direct car whose decis requir exactli one unit given time note car differ egocentr view state system unit unit repres one 10 floor car may locat car footprint depend direct speed exampl stop car caus activ unit correspond current floor move car caus activ sever unit correspond floor approach highest activ closest floor inform provid one car particular locat unit car whose decis requir highest floor wait passeng unit car whose decis requir floor passeng wait longest amount time unit bia unit alway section 4 introduc represent includ restrict state inform 44 parallel distribut implement elev car control separ qlearn agent experi parallel decentr implement parallel implement agent use central set share network allow learn other experi forc learn ident polici total decentr implement agent network allow special control polici either case none agent given explicit access action agent cooper learn indirectli via global reinforc signal agent face ad stochast nonstationar environ contain learn agent 5 result discuss 51 basic result versu algorithm sinc optim polici elev group control problem unknown measur perform algorithm heurist algorithm includ best awar algorithm sector sectorbas algorithm similar use mani actual elev system dlb dynam load balanc attempt equal load car huff highest unansw floor first give prioriti highest floor peopl wait lqf longest queue first give prioriti queue person wait longest amount time fim finit intervisit minim reced horizon control search space admiss car assign minim load function esa empti system algorithm reced horizon control search fastest way empti system assum new passeng arriv fim comput intens would difficult implement real time present form esa use queue length inform would avail real elev system esanq version esa use arriv rate inform estim queue length detail see bao et al 1994 rlp rld denot rl control parallel decentr rl control train 60000 hour simul elev time took four day 100 mip workstat result algorithm averag hour simul elev time ensur statist signific averag wait time list train rl algorithm correct within sigma013 95 confid level averag squar wait time correct within sigma53 averag system time correct within sigma027 tabl 2 show result traffic profil traffic tabl 3 show result downpeak traffic profil traffic includ averag 2 passeng per minut lobbi algorithm train downonli traffic yet gener well traffic ad upward move car forc stop upward hall call tabl 4 show result downpeak traffic profil traffic includ averag 4 passeng per minut lobbi time twice much traffic rl agent gener extrem well new situat tabl 2 result downpeak profil traffic algorithm avgwait squaredwait systemtim percent60 sec dlb 194 658 532 274 esa 151 338 471 025 rld 147 313 417 007 tabl 3 result downpeak profil traffic algorithm avgwait squaredwait systemtim percent60 sec huff 196 608 505 199 rld 169 468 427 140 tabl 4 result downpeak profil twice much traffic algorithm avgwait squaredwait systemtim percent60 sec basic huff 232 875 547 494 fim 208 685 534 310 esa 201 667 523 312 rld 188 593 454 240 one see rl system achiev good perform notabl measur system time sum wait travel time measur directli minim surprisingli decentr rl system abl achiev good level perform parallel rl system 52 analysi decentr result view outstand success decentr rl algorithm sever question suggest similar polici agent learn one anoth polici learn parallel algorithm result improv use vote scheme happen one agent polici use control car section address question first simul modifi poll four decentr qnetwork agent well parallel qnetwork everi decis everi car compar action select one hour simul elev time total 573 decis requir four agent unanim 505 decis 1 47 decis 8 percent split evenli 21 decis 4 percent parallel network agre 493 505 unanim decis 98 percent reason parallel network tend favor stop action decentr network though appar littl impact overal perform complet result list tabl 5 tabl 5 amount agreement decentr agent agent say agent say number parallel parallel stop continu instanc say stop say cont result show consider agreement minor situat agent disagre next experi agent vote action select car case agent evenli split examin three way resolv tie favor stop rl favor continu rlc randomli rlr follow tabl show result vote scheme compar origin decentr algorithm rld result averag hour simul elev time pure traffic result show signific improv vote situat agent evenli split break tie randomli produc result almost ident origin decentr algorithm seem impli agent gener agre import decis tabl 6 comparison sever vote scheme algorithm avgwait squaredwait systemtim percent60 sec rlc 150 325 417 009 rl 149 322 417 010 rlr 148 314 417 012 rld 147 313 417 007 disagre decis littl consequ action valu similar next experi agent singl car select action car rl1 use agent car 1 control car rl2 use agent car 2 follow tabl compar control origin decentr algorithm rld result averag hour simul elev time pure traffic tabl 7 let singl agent control four car algorithm avgwait squaredwait systemtim percent60 sec rld 147 313 417 007 agent 1 outperform agent agent perform well rel nonrl control discuss summari appear decentr parallel agent learn similar polici similar learn polici may caus part symmetri elev system input represent select distinguish among car futur work would interest see whether agent input represent distinguish among car would still arriv similar polici 53 anneal schedul one import factor perform algorithm anneal schedul use control amount explor perform agent slower anneal process better final result illustr tabl 8 figur show result one train run number anneal rate temperatur anneal accord schedul repres hour train complet result measur hour simul elev time even though somewhat noisi due averag multipl train run trend still quit clear schedul test share start end temper atur although anneal process end time current qvalu estim use determin control polici amount time avail train known advanc one select anneal schedul cover full rang temperatur tabl 8 effect vari anneal rate factor hour avgwait squaredwait systemtim pct60 sec gradual anneal import singleag rl even import multiag rl tradeoff explor exploit agent must also balanc need agent learn stationari environ agent best begin learn process agent extrem inept gradual anneal abl rais perform level parallel tesauro 1992 1994 1995 note slightli differ relat phenomenon context zerosum game train selfplay allow agent learn wellmatch oppon stage develop 54 omnisci versu onlin reinforc section examin rel perform omnisci onlin reinforc describ section 41 given network structur temperatur learn rate schedul shown tabl 9 omnisci reinforc led slightli better perform onlin reinforc littl concern regard applic rl real elev system sinc one would want perform initi train simul case huge amount experi need also perform would poor earli stage train real elev system initi train could perform use simul network could finetun real system final averag squar wait hour train freez figur 2 effect vari anneal rate tabl 9 omnisci versu onlin reinforc avgwait squaredwait systemtim pct60 sec omnisci 152 332 421 007 onlin 153 342 416 016 55 level incomplet state inform parallel decentr rl implement real elev system would problem provid whatev state inform avail agent howev truli decentr control situat might possibl section look perform degrad agent receiv less state inform experi amount inform avail agent vari along two dimens inform hall call button inform locat direct statu car input represent hall call button real consist input unit two unit encod inform nine hall button realvalu unit encod elaps time button push binari unit button push binari consist 9 binari input unit correspond nine hall button quantiti consist two input unit measur number hall call current decisionmak car none input unit convey inform hall button input represent configur car foot print consist 10 input unit unit repres one 10 floor car may locat car footprint depend direct speed exampl stop car caus activ unit correspond current floor move car caus activ sever unit correspond floor approach highest activ closest floor activ caus variou car addi quantiti consist 4 input unit repres number upward downward move car decisionmak car none consist input unit convey inform hall button network also possess bia unit alway activ 20 hidden unit 2 output unit stop continu action use decentr rl algorithm train 12000 hour simul elev time use downpeak profil omnisci reinforc temperatur anneal accord schedul hour train learn rate paramet decreas accord schedul result shown tabl 10 measur term averag squar passeng wait time hour simul elev time consid fairli noisi averag multipl train run nevertheless show interest trend tabl 10 averag squar wait time variou level incomplet state inform hall locat car button footprint quantiti none real 370 428 474 binari 471 409 553 quantiti 449 390 530 none 1161 778 827 clearli inform hall call import inform configur car fact perform still remark good even without inform car technic speak inform alway avail car constraint prevent car stop pick passeng floor anoth car alreadi stop doubt constraint help perform consider hall call inform complet miss network weight increas tendenc becom unstabl grow without bound learn rate paramet lower case discuss network instabl see section 57 way inform present import exampl suppli number hall call decisionmak car use network potenti inform binari button inform also appear inform along one dimens help util inform along dimens exampl footprint represent made perform much wors car inform absenc hall call inform time footprint outperform represent maximum amount hall call inform overal perform quit good except complet absenc hall call inform signific handicap inde could improv slower anneal seem reason say algorithm degrad grace presenc incomplet state inform problem final experi two binari featur ad realfootprint input represent activ decisionmak car highest floor wait passeng floor longest wait pa senger respect addit featur averag squar wait time decreas 370 359 appear valu 56 practic issu one biggest difficulti appli rl elev control problem find correct temperatur learn rate paramet help start scale version consist 1 car 4 floor lookup tabl qvalu made easier determin rough valu temperatur learn rate schedul import focus experi learner appropri area state space overstress train trajectori system import start ad reason constraint describ section 333 also help evid support import focus given choic train heavier lighter traffic one expect face test better train heavier traffic type train give system experi state queue length long thu make correct decis crucial 57 instabl weight neural network becom unstabl magnitud increas without bound two particular situat seem lead instabl first occur learn algorithm make updat larg happen learn rate larg network input larg happen heavi traffic situat second occur network weight initi random valu produc excess inconsist qvalu exampl learn rate 10 gamma2 suitabl train random initi network moder traffic 700 passengershour consist bring instabl heavi traffic 1900 passengershour howev learn rate 10 gamma3 keep network stabl even heavi traffic train network way sever hundr hour elev time lead weight repres consist set qvalu learn rate safe rais back 10 gamma2 without caus instabl 58 linear network one may ask whether nonlinear function approxim feedforward sigmoid network necessari good perform elev control prob lem test run use linear network train delta rule linear network much greater tendenc unstabl order keep weight blow learn rate lower sever order magnitud 10 gamma3 10 gamma6 initi improv linear network unabl reduc averag td error result extrem poor perform failur linear network lend support content elev control difficult problem 6 discuss parallel distribut multiag rl architectur abl outperform elev algorithm test two architectur learn similar polici gradual anneal appear crucial factor success train accomplish effect use omnisci onlin reinforc algorithm robust easili gener new situat ad traffic final degrad grace face increas level incomplet state inform although network becam unstabl certain circumst techniqu discuss prevent instabl practic taken togeth result demonstr multiag rl algorithm power techniqu address larg scale stochast dynam optim problem crucial ingredi success multiag rl care control amount explor perform agent explor context mean tri action believ suboptim order gather addit inform potenti valu begin learn process rl agent choos action randomli without knowledg rel valu thu agent extrem inept howev spite nois reinforc signal caus action agent action begin appear better other gradual anneal lower amount explor perform agent better action taken greater frequenc gradual chang environ agent continu explor rais perform level parallel even though rl agent team face ad stochast nonstationar due chang stochast polici agent team display except abil cooper one anoth learn maxim reward mani area research elev group control gener multiag rl deserv investig implement rl control real elev system would requir train sever traffic profil includ uppeak interfloor traffic pattern addit action would need order handl traffic pattern exampl uppeak traffic would use action specif open close door control dwell time lobbi interfloor traffic unconstrain action would need sake flexibl car also abil park variou floor period light traffic would interest tri someth uniform anneal schedul agent exampl coordin explor strategi roundrobin type anneal might way reduc nois gener agent howev coordin explor strategi may greater tendenc becom stuck suboptim polici theoret result sequenti multiag rl need supplement result nonsequenti multiag rl describ section 21 anoth area need studi rl architectur reinforc tailor individu agent possibl use hierarchi advanc organiz structur local reinforc architectur potenti greatli increas speed learn requir much knowledg part whatev produc reinforc signal barto 1989 fi nalli import find effect method allow possibl explicit commun among agent 7 conclus multiag control system often requir spatial geograph distribut situat central inform avail practic even distribut approach requir multipl agent may still provid excel way scale approxim solut larg problem streamlin search space possibl polici multiag rl combin advantag bottomup topdown approach design multiag system achiev simplic bottomup approach allow use rel unsophist agent learn basi experi time rl agent adapt topdown global reinforc signal guid behavior toward achiev complex specif goal result robust system complex problem creat minimum human effort rl algorithm train use actual simul experi allow focu comput area state space actual visit control make comput tractabl larg problem member team agent employ rl algorithm new collect algorithm emerg group whole type collect algorithm allow control polici learn decentr way even though rl agent team face ad stochast nonstationar due chang stochast polici agent team display except abil cooper one anoth maxim reward order demonstr power multiag rl focus difficult problem elev group supervisori control use team rl agent respons control one elev car result obtain simul surpass best heurist elev control algorithm awar perform also robust face increas level incomplet state inform acknowledg thank john mcnulti christo cassandra asif gandhi dave pepyn kevin markey victor lesser rod grupen rich sutton steve bradtk anw group assist simul help discuss research support air forc offic scientif research grant f49620 9310269 r evolut cooper elev dispatch peak traffic chemotaxi cooper abstract exercis neuron learn strategi learn interact introduct modern reinforc learn distribut adapt optim control flexibl structur reinforc learn method continuoustim markov decis problem discret event system model perform analysi phd thesi form control polici simul model use reinforc learn improv elev perform use reinforc learn feudal reinforc learn fuzzi neural network applic elev group control optim control elev dynam load balanc approach control multiserv poll system applic elev system dispatch distribut reinforc learn scheme network rout technic report cmucs93165 markov game framework multiag reinforc learn algorithm sequenti decis make effici learn multipl degreeoffreedom control problem quasiindepend qagent adapt optim elev group control use neural network learn automata introduct electron inform technolog highrang elev system optim dispatch control elev system uppeak traffic pdp research group develop elev supervisori group control system artifici intellig studi machin learn use game checker multiag reinforc learn iter prison dilemma elev traffic simul vertic transport elev escal neural comput tempor differ learn tdgammon elev character group supervisori control system automaton theori model biolog system latest elev groupcontrol system revolutionari ai2100 elevatorgroup control system new intellig option seri learn delay reward adapt learn multiag system receiv date accept date final manuscript date tr ctr shingo mabu kotaro hirasawa jinglu hu graphbas evolutionari algorithm genet network program gnp extens use reinforc learn evolutionari comput v15 n3 p369398 fall 2007 rajbala makar sridhar mahadevan mohammad ghavamzadeh hierarch multiag reinforc learn proceed fifth intern confer autonom agent p246253 may 2001 montreal quebec canada shin ishii hajim fujita masaoki mitsutak tatsuya yamazaki jun matsuda yoichiro matsuno reinforc learn scheme partiallyobserv multiag game machin learn v59 n12 p3154 may 2005 mohammad ghavamzadeh sridhar mahadevan learn commun act use hierarch reinforc learn proceed third intern joint confer autonom agent multiag system p11141121 juli 1923 2004 new york new york theodor j perkin andrew g barto lyapunov design safe reinforc learn journal machin learn research 3 312003 shimon whiteson matthew e taylor peter stone empir studi action select reinforc learn adapt behavior anim animat softwar agent robot adapt system v15 n1 p3350 march 2007 tadhg omeara ahm patel topicspecif web robot model base restless bandit ieee internet comput v5 n2 p2735 march 2001 hajim fujita shin ishii modelbas reinforc learn partial observ game samplingbas state estim neural comput v19 n11 p30513087 novemb 2007 andrew g barto sridhar mahadevan recent advanc hierarch reinforc learn discret event dynam system v13 n12 p4177 januaryapril andrew g barto sridhar mahadevan recent advanc hierarch reinforc learn discret event dynam system v13 n4 p341379 octob philipp fries jrg rambau onlineoptim multielev transport system reoptim algorithm base setpartit model discret appli mathemat v154 n13 p19081931 15 august 2006 shimon whiteson peter stone evolutionari function approxim reinforc learn journal machin learn research 7 p877917 1212006 gang chen zhonghua yang hao kiah mok goh coordin multipl agent via reinforc learn autonom agent multiag system v10 n3 p273328 may 2005 pasqual fiengo giovanni giamben edmondo trentin neuralbas downlink schedul algorithm broadband wireless network comput commun v30 n2 p207218 januari 2007 dars bill lourd pea jonathan schaeffer duan szafron learn play strong poker machin learn play game nova scienc publish inc commack ny 2001