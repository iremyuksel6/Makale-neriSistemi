rel loss bound onlin densiti estim exponenti famili distribut consid onlin densiti estim parameter densiti exponenti famili onlin algorithm receiv one exampl time maintain paramet essenti averag past exampl receiv exampl algorithm incur loss neg loglikelihood exampl respect current paramet algorithm offlin algorithm choos best paramet base exampl prove bound addit total loss onlin algorithm total loss best offlin paramet rel loss bound hold arbitrari sequenc exampl goal design algorithm best possibl rel loss bound use bregman diverg deriv analyz algorithm diverg rel entropi two exponenti distribut also use method prove rel loss bound linear regress b introduct main focu statist decis theori consist follow receiv statist inform form sampl data goal make decis minim loss expect loss respect underli distribut assum model data distribut often dene term certain paramet statist decis depend specic valu chosen paramet thu statist decis theori three import element paramet valu take decis loss function evalu decis extend abstract appear uai 99 aw99 z support nsf grant ccr 9700201 ccr9821087 c 2000 kluwer academ publish print netherland k azouri k warmuth bayesian statist decis theori prior distribut paramet data distribut addit import element inform need assess decis perform simpl case suppos given sampl data point fx 1 also refer exampl assum exampl independ gener gaussian unknown mean known varianc one want nd paramet set mean minim expect loss new exampl drawn data distribut bayesian framework prior distribut mean would also enter decis make process context learn theori setup bayesian would describ batch olin learn model sinc exampl given learner ahead time decis made base inform entir data set paper focu onlin learn model olin learn fundament element paramet deci sion loss function dierenc howev exampl given learner one time thu onlin learn natur partit trial trial one exampl process trial proce follow begin current paramet set hypothesi next exampl present learner onlin algorithm incur loss loss function recent exampl current paramet set final algorithm updat paramet set new trial begin context onlin learn decis paramet updat learner goal design onlin learn algorithm good bound total loss clearli meaning bound total loss onlin algorithm stand alon also hold arbitrari sequenc exampl howev onlin learn literatur certain type rel loss bound desir use success term rel mean comparison best paramet chosen olin see whole batch exampl paramet space call comparison class rel loss bound quantifi addit total loss onlin algorithm total loss best line paramet compar sinc onlin learner see sequenc exampl advanc addit loss sometim call regret price hide futur exampl learner paper design motiv onlin algorithm paramet updat util avail inform best possibl manner lead good rel loss bound focu rel loss bound 3 two type learn problem onlin densiti estim onlin regress densiti estim onlin algorithm receiv sequenc unlabel exampl data vector fx 1 g start learner current paramet set use predict next exampl x make predic tion algorithm receiv exampl x incur loss l algorithm updat paramet set t1 con trast onlin regress problem receiv label sequenc exampl call instanc label trial learner start current paramet receiv instanc x learner make predict label predict depend x loss l incur paramet updat t1 type problem densiti estim regress use abbrevi notat l denot loss incur trial subscript indic depend exampl present trial total loss onlin algorithm total loss best olin batch paramet b charg size b rel loss bound give upper bound dierenc two total loss prove rel loss bound densiti estim underli model member exponenti famili distribut also onlin linear regress consid two algorithm rst algorithm call increment olin algorithm predict ie choos paramet best olin algorithm would predict base exampl seen far second algorithm call forward algorithm algorithm call forward use guess futur exampl correspond futur loss form predict motiv vovk work linear regress rel loss bound algorithm grow logarithm number trial motiv paramet updat bayesian probabilist interpret howev rel loss bound prove hold arbitrari worstcas sequenc ex ampl key element design analysi onlin learn algorithm gener notion distanc call bregman diver genc diverg interpret rel entropi two exponenti distribut 4 k azouri k warmuth outlin rest paper organ follow section 2 present brief overview previou work section 3 dene bregman diverg give relev background exponenti famili distribut show rel entropi two exponenti distribut ama85 special bregman diverg conclud section list basic properti bregman diverg section 4 introduc increment olin algorithm gener set appli algorithm problem densiti estim exponenti famili linear regress give number rel loss bound specic exampl section 5 dene motiv forward algorithm seen gener increment olin algorithm appli algorithm densiti estim exponenti famili case linear regress reprov rel loss bound obtain vovk vov97 proof concis altern simpl proof forward linear regress algorithm given for99 section 6 brie discuss altern method develop vovk prove rel loss bound use integr gener posterior discuss advantag method final conclud section 7 discuss number open problem 2 overview previou work method prove bound addit total loss onlin algorithm total loss best paramet comparison class essenti goe back work blackwel bla56 hannnan han57 investig bound context game theori comparison class consist mixtur strategi later cover cov91 prove bound context mathemat nanc use comparison class constant rebalanc portfolio research paper root studi rel loss bound onlin learn algorithm comput learn theori commun even though bound may underestim perform natur data use power yardstick analyz compar onlin algorithm comput learn theori commun line research initi littleston discoveri winnow algorithm lit88 littleston also pioneer style amort analysi rel loss bound 5 prove rel loss bound use certain diverg function potenti function winnow design disjunct comparison class total number mistak use loss next wave onlin algorithm design nite set expert comparison class wide rang loss function algorithm develop onlin linear least squar regress ie comparison class consist linear neuron linear combin ex pert llw95 cblw96 kw97 work gener case comparison class set sigmoid linear neuron hkw95 kw98 also start littleston work rel loss bound comparison class linear threshold function investig lit88 gls97 onlin algorithm cite previou paragraph use xed learn rate simpl set rel loss bound grow number trial howev alreadi linear regress squar loss rel loss bound algorithm xed learn rate grow squar root loss best linear predictor cblw96 kw97 best loss often linear number trial contrast algorithm use variabl learn rate rel loss bound grow logarithm bound proven gener bay algorithm xb97 yam98 maintain posterior paramet comparison class outlin method prove rel loss bound section 6 bound grow logarithm also proven previous onlin linear regress import insight gain research olog rel loss bound seem requir use variabl learn rate paper learn rate appli trial o1t use o1t learn rate exponenti famili also suggest gordon possibl strategi lead better bound howev specic exampl work case linear regress o1t learn rate becom invers covari matrix past exampl gener framework onlin learn algorithm develop gls97 kw97 kw98 gor99 follow philosophi kivinen warmuth kw97 start diverg function diverg function deriv onlin updat use diverg potenti amort analysi similar method develop gls97 case comparison class consist linear threshold function start updat construct appropri diverg use analysi 6 k azouri k warmuth recent learn diverg use onlin learn employ extens convex optim call bregmandist bre67 cl81 csi91 jb90 bregman method pick set allow model one minim distanc current model word current hypothesi project onto convex set allow model mild addit assumpt assur uniqu project assumpt gener pythagorean theorem proven bregman diverg hw98 latter theorem often contradict triangular inequ reason use term diverg instead distanc project respect bregman diverg recent appli hw98 case olin compar allow shift time project use keep paramet algorithm reason region aid recoveri process underli model shift 3 bregman diverg exponenti famili paper use notion diverg due bregman bre67 deriv analyz onlin learn algorithm diverg interpret rel entropi distribut exponenti famili begin section dene bregman diverg review import featur exponenti famili distribut relev paper conclud section properti diverg arbitrari realvalu convex dierenti function g paramet space r bregman diverg two paramet e dene r denot gradient respect throughout paper vector column vector use denot dot product vector note bregman diverg g e minu rst two term taylor expans g e around word g e tail taylor expans g e beyond linear term sinc g convex g e properti list section 34 exampl let paramet space 2 case bregman diverg becom squar rel loss bound 7 euclidean distanc ie also e e e e 31 exponenti famili featur exponenti famili use throughout paper includ measur diverg two member famili intrins dualiti relationship see bn78 ama85 comprehens treatment exponenti famili multivari parametr famili fg distribut said exponenti famili member densiti function x vector r p 0 x repres factor densiti depend ddimension paramet usual call natur canon paramet mani common parametr distribut member famili includ gaussian function g normal factor dene z space r integr nite call natur paramet space exponenti famili call regular open subset r well known bn78 ama85 convex set g strictli convex function function g call cumul function play fundament role character member famili distribut use g denot gradient r g r 2 g denot hessian g let repres loglikelihood view function standard regular condit loglikelihood function satisfi wellknown moment ident mn89 appli ident exponenti famili reveal special role play cumul function g 8 k azouri k warmuth expect respect distribut pg xj rst moment ident loglikelihood function gradient loglikelihood 32 linear x r g appli 33 get show mean x equal gradient g let call expect paramet sinc cumul function g strictli convex map invers denot imag map g invers map g 1 set call expect space may necessarili convex set second moment ident loglikelihood function 0 denot transpos exponenti famili r 2 thu second moment ident variancecovari matrix x hessian cumul function g also call fisher inform matrix sinc g strictli convex hessian symmetr posit denit 32 dualiti natur paramet expect paramet sometim conveni parameter distribut exponenti famili use expect paramet instead natur paramet pair parameter dual relationship provid aspect dualiti relev paper first dene second function rang g follow let f rf denot gradient f rel loss bound 9 note take gradient f 36 respect treat function get r jacobian respect thu f invers map g 1 two parameter relat follow transform sinc g posit denit jacobian 2 g 1 posit denit jacobian 2 thu second function f strictli convex well function call dual g ama85 furthermor f neg entropi pg xj respect refer measur p 0 x ie follow 38 hessian f invers fisher inform matrix ie r 2 consid function v gf fisher inform matrix express term expect paramet function dene expect space take valu space symmetr matric call varianc function varianc function play import role character member exponenti famili mor82 gps95 matrix v posit denit 2 v thu context exponenti famili function f g arbitrari convex function must posit denit hessian 33 diverg two exponenti distribut consid two distribut pg xj e old paramet set e pg xj new paramet set follow amari ama85 one may see exponenti famili fg manifold paramet e repres two point manifold sever measur distanc diverg two point propos literatur amari introduc diverg ama85 relat distanc introduc csiszar csi91 known f diverg use letter h also cherno distanc renyi inform relat ama85 diverg k azouri k warmuth follow gener form h continu convex function main choic h give rel entropi anoth interest choic give opposit two entropi respect call 1 1 diverg amari ama85 34 properti diverg section give simpl properti diverg properti need g posit denit hessian henc properti hold gener denit bregman diverg 31 allow g arbitrari realvalu dierenti convex function g paramet space throughout paper use repres r g 1 g e convex rst argument sinc g e convex 2 g strictli convex equal hold e 3 gradient diverg respect rst argument follow simpl form r e 4 diverg usual symmetr ie g e 5 diverg linear oper ie ag 0 rel loss bound 11 6 diverg aect ad linear term g g g 7 1 2 3 dot product usual sign neg contradict triangular inequ case dot product zero exploit proof gener pythagorean theorem bregman diverg see exampl 8 g strictli convex denit dual convex function f paramet transform still hold 36 38 bregman diverg follow dualiti properti rst six properti immedi properti 7 proven appendix properti rst use wj98 prove rel loss bound last properti follow denit dual function f also call convex conjug roc70 note order argument g e switch paper need properti 8 case g strictli con vex howev realvalu dierenti convex function g one dene dual function f paramet r roc70 denit properti 8 still hold note gordon gor99 give eleg gener bregman diverg case convex function g necessarili dierenti sake simplic restrict dierenti case paper final g dierenti bregman diverg also written path integr integr version diverg use dene notion convex loss match increas transfer function g artici neuron ahw95 hkw95 kw98 12 k azouri k warmuth 4 increment olin algorithm section give basic algorithm show prove rel loss bound gener set learn proce trial exampl process densiti estim exampl data vector x domain x regress set tth exampl consist instanc x instanc domain x label label domain setup learn problem dene three part paramet space r realvalu loss function diverg function measur distanc initi paramet set paramet space repres model algorithm compar loss paramet vector th exampl denot l l 1t shorthand usual loss nonneg third compon setup initi parmet 0 bregman diverg u0 0 initi pa ramet initi paramet 0 may interpret summari prior learn diverg u0 0 repres measur distanc initi paramet olin batch algorithm see exampl set paramet u t1 assumpt loss l 1 u 0 dier entiabl convex function paramet space real furthermor assum argmin u t1 alway solut note olin algorithm trade total loss exampl close origin paramet altern diverg u 0 may interpret size paramet interpret olin algorithm nd paramet minim sum size total loss onlin algorithm see one exampl time accord follow protocol onlin protocol increment olin algorithm initi hypothesi 0 predict get tth exampl incur loss l updat hypothesi t1 rel loss bound 13 goal onlin algorithm incur loss never much larger loss olin algorithm see exampl end trial onlin algorithm know rst exampl expect see next exampl one reason desir setup paramet updat point make onlin algorithm exactli olin algorithm would done see exampl use name increment olin onlin algorithm properti increment olin algorithm u t1 addit assumpt assum argmin u t1 1 alway solut one solut interpret t1 2 argmin u t1 learn problem use exampl paper u t1 typic strictli convex one solut note nal paramet t1 increment olin algorithm coincid paramet b chosen batch algorithm consist protocol given necessari begin index parallel index second onlin algorithm given next section second algorithm call forward algorithm use guess next loss updat paramet setup updat 41 seem truli onlin sinc need previou exampl truli onlin yet equival setup given follow lemma lemma 41 increment olin algorithm 1 proof note sinc ru thu sinc u constant argmin t1 use denit 41 argmin lemma qed readi show key lemma increment line algorithm lemma compar total loss onlin algorithm total loss compar total loss compar includ diverg term u 0 0 14 k azouri k warmuth lemma 42 increment olin algorithm sequenc exampl 2 u t1 proof 0 expand diverg u t1 t1 use ru t1 t1 give us u t1 t1 sinc u t1 special case subtract 43 44 appli u u version 42 give sum trial obtain u lemma follow equal u 1 equal follow properti 6 linear qed obtain rel loss bound choos best olin paramet b compar bound righthandsid equat lemma note case increment olin algorithm thu last diverg righthandsid zero diverg u t1 repres cost updat t1 incur onlin algorithm rel loss bound bound total cost onlin updat rel loss bound 15 41 increment offlin algorithm exponenti famili appli increment olin algorithm problem densiti estim exponenti famili distribut rst give gener treatment prove rel loss bound specic member famili subsect follow make obviou choic loss function name neg loglikelihood use gener form loglikelihood 32 loss paramet exampl x purpos rel loss bound see lemma 42 chang loss constant depend inconsequenti thu form refer measur p 0 x immateri allow algorithm initi paramet valu 0 choos u 0 multipl cumul function ie thu context densiti estim exponenti famili increment olin algorithm becom u t1 throughout paper use notat 1 denot tradeo paramet two reason first invers tradeo paramet becom learn rate algorithm learn rate commonli denot also use 1 instead 1 linear regress paramet gener matric setup 45 interpret nding maximum aposteriori map paramet diverg term correspond conjug prior 1 hyper paramet 1 diverg term disappear maximum likelihood esti mation altern one think 0 initi paramet estim base hypothet exampl seen rst real exampl 0 number exampl also one interpret paramet 1 0 tradeo paramet stay close initi paramet 0 minim loss exampl seen end trial yet anoth interpret 45 follow rewrit u t1 k azouri k warmuth thu u t1 correspond neg loglikelihood exponenti densiti cumul function 1 exampl 1 develop altern onlin motiv given lemma 41 let 1 linear follow properti diverg u thu onlin motiv lemma 41 becom diverg measur distanc last paramet tradeo paramet 1 updat paramet t1 obtain minim u t1 dene 45 strictli convex function gradient function term expect paramet set zero 0 give updat expect paramet increment olin algorithm 1 also express t1 convex combin last instanc x note 1 altern recurs form updat use later 1 thu onlin updat may seen gradient descent differ learn rate updat 49 use gradient loss t1 410 use gradient loss evalu special case 1 valid consist updat 48 410 rel loss bound 17 rel loss bound proven use lemma 42 thu densiti estim equal simpli follow lemma give concis express minimum u t1 see 45 term dual cumul function lemma follow discuss interest right although essenti main develop paper use bernoulli exampl discuss later combin 411 lemma one also get express total loss onlin algorithm lemma 31 aw99 lemma 43 min proof rewrit righthandsid equal lemma use denit dual function f 36 express rewrit use 1 denit loss diverg equal u t1 t1 qed 1 case maximum likelihood rewritten k azouri k warmuth thu essenti inmum averag loss data equal expect loss paramet minim averag loss ie maximum likelihood paramet relationship use gru98 remain subsect discuss specic exampl give rel loss bound 411 densiti estim gaussian deriv rel loss bound gaussian densiti estim problem consid gaussian densiti r known xed without loss gener develop bound special case ident matrix similar bound immedi follow gener case xed arbitrari variancecovari matrix linear transform argument gaussian densiti ident matrix variancecovari matrix x 2 shorthand x x densiti member exponenti famili natur paramet cumul paramet transform function dual convex function f loss l const note constant loss immateri bound therefor set zero sake simplic set 2 recal increment olin updat dierenc total loss increment olin algorithm olin algorithm rel loss bound 19 use onlin updat eg 49 410 rewrit diverg righthandsid want allow 1 case updat 49 appli 1 howev updat 48 1 1 1 412 rewritten follow easi nd two exampl x 1 x 2 dierenc 414 depend order two exampl present develop upper bound permut invari ie depend order exampl present drop neg term 414 use sinc obtain follow rel loss bound k azouri k warmuth theorem 44 gaussian densiti estim increment olin algorithm note special case 1 olin algorithm choos maximum likelihood paramet bound simpli 1 412 densiti estim gamma give rel loss bound gamma distribut densiti shape paramet invers scale paramet member exponenti famili natur paramet densiti term written assum known xed paramet scale loss diverg invers call dispers paramet sake simplic drop ignor xed varianc case gaussian densiti estim cumul paramet transform dual convex function f loss l bound diverg t1 lead rel loss bound increment olin algorithm see 411 4 rel loss bound 21 use updat 410 notat r convex combin element fx g 1 sum diverg righthandsid 411 bound summari follow rel loss bound theorem 45 densiti estim gamma distribut use increment olin algorithm 1 better rel loss bound includ case 1 possibl bound 415 care 413 densiti estim gener exponenti famili give brief discuss form bound take member exponenti famili rewrit diverg second order taylor expans f t1 last e e 22 k azouri k warmuth e convex combin t1 r 2 f constant essenti gaussian gener case may seen local gaussian timevari curvatur reason method proceed casebycas basi mor82 gps95 base form r 2 f invers varianc function recal sum last term alway give logt style bound sometim rang x need restrict done previou subsect densiti estim gaussian gamma distribut 414 linear regress subsect bound linear regress develop instanc domain x r label domain r paramet domain also r compon paramet vector 2 linear weight given exampl paramet vector linear model predict x squar loss use measur discrep predict label exampl note l strictli convex thu make u 0 strictli convex initi diverg u 0 strictli convex updat alway uniqu solut use u posit denit matrix diverg initi paramet becom u 0 0 thu linear regress updat 41 increment olin updat becom u t1 note use transpos notat x 0 q instead dot product subsequ deriv use matrix algebra setup linear regress usual interpret condit densiti estim problem gaussian label given x cumul function trial 1 diverg correspond gaussian prior develop altern onlin version 416 done gener lemma 41 sinc u equal 1 linear term onlin version becom rel loss bound 23 dierenti 416 0 obtain increment olin updat linear regress standard linear least squar updat easi deriv follow recurs version 1 note correspond updat updat 47 410 densiti estim also lemma 42 becom follow quadrat equat see 411 correspond equat densiti reprov bound obtain vovk vov97 increment olin algorithm sake simplic choos 1 0 multipl ident matrix similar bound proven foster algorithm howev assum compar probabl vector fos91 theorem 46 linear regress increment olin algorithm 1 tg note theorem assum predict x 0 label trial lie assumpt satis might use clip ie algorithm predict number yy closest x 0 clip requir algorithm know k azouri k warmuth littl incent work detail increment line algorithm algorithm next section prove better rel loss bound predict dont need lie proof appli updat 419 twice diverg sum righthandsid 420 give rst two equal third equal follow lemma a1 appendix2 t1 last inequ use assumpt x 0 fact z 1 ln z note last inequ may hold without assumpt x 0 theorem follow appli crude approxim equal lemma 42 last inequ follow assumpt x qi x 2 second last inequ follow fact 1 ai determin symmetr matrix product diagon element see bb65 chapter 2 theorem 7 qed ideal dont want use crude bound method goal rewrit sum diverg telescop occur increment onlin updat abl partial attempt follow gaussian densiti rel loss bound 25 estim 4132 t1 last equal use fact 1 symmetr note last two term nal express telescop gaussian case 413 surprisingli forward algorithm introduc next section correspond two term telescop thu forward algorithm one prove bound one given theorem 46 except last term bound 1 1 quarter theorem 46 relat problem densiti estim gaussian correspond improv bound factor 1 hold increment olin algorithm 5 estim futur loss forward algorithm section present second algorithm call forward algorithm give lemma use prove rel loss bound trial forward algorithm expect see next exampl allow incorpor estim loss next exampl choos paramet regress part exampl name instanc x avail trial algorithm must commit paramet algorithm use instanc x form estim loss trial shall see linear regress incorpor estim motiv use includ current instanc learn rate algorithm lead better rel loss bound densiti estim howev instanc yet algorithm still use estim futur loss 26 k azouri k warmuth onlin protocol forward algorithm regress densiti estim initi hypothesi 0 initi hypothesi 0 get instanc x guess loss tth exampl guess loss tth exampl updat hypothesi 1 updat hypothesi 1 predict predict get label tth exampl get exampl x incur loss l incur loss l dene updat analog previou section minim sum diverg plu loss past trial estim loss next trial forward algorithm u t1 assumpt loss l 1 estim loss dierenti convex function paramet space real furthermor assum argmin u t1 1 alway solut note increment olin algorithm special case forward algorithm estim loss zero altern onlin motiv updat use diverg last paramet vector lemma 51 forward algorithm 1 u rewrit argument argmin rel loss bound 27 thu sinc u constant argmin t1 use denit 51 argmin lemma qed follow key lemma gener lemma 42 increment olin algorithm lemma 52 forward algorithm sequenc exampl 2 u t1 proof 0 expand diverg u t1 t1 use ru t1 t1 proof lemma 42 give us u t1 t1 sinc u t1 obtain special case subtract 53 54 appli u u version 52 obtain u t1 equat lemma follow sum trial subtract u 0 side qed rel loss bound forward algorithm must base bound righthandsid lemma 51 densiti estim exponenti famili appli forward algorithm problem densiti estim exponenti famili distribut choos u 0 28 k azouri k warmuth 0 g done increment olin algorithm thu initi diverg becom u 0 estim futur loss use const may seen averag loss number exampl lie instanc domain 0 seen guess futur instanc correspond loss estim loss rewritten g const thu choic forward algorithm 51 becom follow u t1 increment olin algorithm 45 except tradeo paramet 1 0 1 onlin motiv becom onlin motiv increment line algorithm 46 except 1 1 increas one 1 updat 47410 lemma 43 remain learn rate shift updat hold 1 rst one hold well show choic estim sinc estim loss independ trial estim loss last equal lemma 52 cancel get rel loss bound 29 52 densiti estim gaussian section give bound forward algorithm better correspond bound increment olin al gorithm follow step 413 simplifi follow diverg use 59 becom set 0 thu 1 zero choos t1 thu last three term equat rewritten as2 2 0 511 equat 510 bound by2 x 2 theorem 53 gaussian densiti estim forward algorithm k azouri k warmuth note bound forward algorithm better bound increment olin algorithm see theorem 44 improv essenti 2x 2 53 densiti estim bernoulli subsect give rel loss bound bernoulli distribut exampl x coin ip f0 1g distribut typic express p probabl 1 let 1 distribut term member exponenti famili natur paramet cumul paramet transform 1e dual function f loss l consid forward algorithm 1 case maximum likelihood forward algorithm use t1 2 t1 rst develop concis express total loss algorithm lemma 54 bernoulli densiti estim forward algorithm proof rst rewrit loss trial variou way let abbrevi develop formula note trial x increas one loss l contain rel loss bound 31 term ln t2 trial term contribut similarli trial x one loss l contain term lnt t2 trial term contribut fact lemma follow qed note righthandsid express lemma independ order exampl seen thu bernoulli distribut total loss forward algorithm permut invari lemma 43 l 1t thu equival express use gamma function rst deriv freund fre96 base laplac method integr use standard approxim gamma function one bound righthandsid 1 theorem 55 fre96 bernoulli densiti estim forward algorithm 54 linear regress subsect deriv rel loss bound forward algorithm appli linear regress increment olin algorithm let u 0 posit denit diverg initi 0 u use estim futur loss ie next label t1 guess x 0 forward updat 51 linear regress becom k azouri k warmuth u t1 denit u 1 0 minim 0 thu dierenti u t1 obtain forward updat linear regress 0 note t1 depend x t1 thu forward algorithm differ increment olin algorithm 417 use current instanc form predict sake simplic assum rest section recurs version updat follow rewrit equal lemma 52 linear regress follow step use increment olin algorithm 422 rel loss bound 33 use argument sum righthandsid equat 515 simpli note last two term telescop correspond deriv increment olin updat last two term telescop 422 515 gaussian densiti estim 510 show last three term equat zero first rewrit b ident matrix last term becomes2 second last term simpli to2 x t1 x 0 sum last three term pull factor 1 34 k azouri k warmuth thu last three term zero gaussian densiti estim forward algorithm 511 final use upper bound see theorem 462 2 sum righthandsid 516 bound 1 summar rel loss bound prove forward algorithm theorem 56 linear regress forward algorithm note bound forward algorithm better correspond bound increment olin algorithm see theorem 46 improv factor four bound rst proven vovk vov97 use integr altern proof exact express 517 given forster for99 6 relationship bay algorithm altern method pioneer vovk vov97 freund yamanishi yam98 prove rel loss bound section sketch method compar distribut maintain set paramet paramet name algorithm call expert onlin learn literatur 97 initi work consid case nite howev method also appli continu yam98 begin trial rel loss bound 35 distribut form r e p 1 prior 0 learn rate follow type inequ main part method z z z la denot loss algorithm trial import special case occur loss l q neg loglikelihood respect parameter densiti ie call bay algorithm case p given 61 posterior distribut see rst exampl algorithm trial predict predict distribut 62 equal dmw88 gener set 6 1 loss necessarili neg loglikelihood predict algorithm chosen inequ 62 hold matter tth exampl also larger learn rate better result rel loss bound learn rate chosen larg possibl predict alway guarante exist inequ 62 hold learn rate use trial inequ often tight exampl lie boundari set possibl exampl sum inequ 62 trial one get bound z integr comput exactli bound use laplac method integr around best olin paramet would like point follow distinct method algorithm present paper predict bay algorithm ie predict distribut usual repres paramet expert instead algorithm analyz paper choos map paramet trial case bernoulli distribut bay algorithm base jerey prior coincid forward 36 k azouri k warmuth algorithm 1 section 53 similarli case linear regress gaussian prior vovk algorithm vov97 coincid forward algorithm linear regress section 54 howev clear densiti estim problem exponenti famili predict bay algorithm algorithm produc gener case 6 1 repres paramet paramet space contrast method prove rel loss bound avoid use involv integr method need 63 co96 paramet maintain base simpl averag past exampl sucient statist exponenti famili 7 conclus paper present techniqu prove rel loss bound densiti estim exponenti famili gave number exampl appli method includ case linear regress exponenti densiti cumul function g use bregman diverg g e measur distanc paramet e thu loss l diverg base function g howev lemma 42 52 methodolog prove rel loss bound gener initi diverg loss need relat lemma might extend nondierenti convex function use gener notion bregman diverg introduc gordon gor99 paramet maintain algorithm invari permut past exampl howev total onlin loss algorithm permut invari therefor adversari could use fact present exampl order disadvantag learn algorithm suggest algorithm better rel loss bound increment olin algorithm forward algorithm incompar sens either one may larger total loss howev believ sens forward algorithm better need formal belief inspir phenomenon stein estim statist ste56 sinc like stein estim forward updat use shrinkag factor compar increment olin updat still need explor bound obtain paper relat larg bodi research minimum descript length rel loss bound 37 literatur ris89 ris96 literatur lower upper bound rel loss form explor extens number paramet howev contrast setup use paper work minimum descript length literatur requir algorithm predict paramet paramet space methodolog develop paper prove rel loss bound still need work learn problem instanc alway logt style rel loss bound member exponenti famili see section 413 anoth open problem prove logt style bound logist regress final lower bound rel loss need explor case algorithm restrict predict paramet paramet space bound shown linear regress vov97 particular proven constant lnt rel loss bound forward algorithm theorem 56 tight howev gener logt style lower bound known arbitrari member exponenti famili acknowledg thank leonid gurvitz introduc us bregman diverg claudio gentil peter grunwald georey gordon eiji taki moto two anonym refere mani valuabl comment r exponenti mani local minima singl neuron rel loss bound onlin densiti estim exponenti famili distribut analog minimax theorem vector payo relax method iter rowact method interv convex pro gram univers portfolio side inform univers portfolio least squar maximum entropi learn probabilist predict function predict worst case predict binari sequenc almost well optim bias coin gener converg result linear discrimin updat technic report cmucs99143 minimum discript length principl reason uncertainti approxim bay risk repeat play learn algorithm track chang concept investig error surfac singl arti sequenti predict individu sequenc gener loss function track best regressor conf comput addit versu exponenti gradient updat linear predict rel loss bound multidimension regress problem learn irrelev attribut abound new linearthreshold algorithm journal comput complex weight major algorithm gener linear model natur exponenti famili quadrat varianc function stochast complex statist inquiri fisher inform stochast complex convex analysi inadmiss usual estim mean multivari normal distribut asymptot minimax regret exponenti famili asymptot minimax regret bay mixtur aggreg strategi competit onlin linear regress continu discret time nonlinear gradient descent rel loss bound converg minimax redund class memoryless sourc ieee transact inform theori decisiontheoret extens stochast complex applic learn tr ctr arindam banerje inderjit dhillon joydeep ghosh srujana merugu inform theoret analysi maximum likelihood mixtur estim exponenti famili proceed twentyfirst intern confer machin learn p8 juli 0408 2004 banff alberta canada shai shalevshwartz yoram singer primaldu perspect onlin learn algorithm machin learn v69 n23 p115142 decemb 2007 jrgen forster manfr k warmuth rel loss bound temporaldiffer learn machin learn v51 n1 p2350 april srujana merugu joydeep ghosh privacysensit approach distribut cluster pattern recognit letter v26 n4 p399410 march 2005 vn vishwanathan nicol n schraudolph alex j smola step size adapt reproduc kernel hilbert space journal machin learn research 7 p11071133 1212006 nicol cesabianchi claudio gentil luca zaniboni worstcas analysi select sampl linear classif journal machin learn research 7 p12051230 1212006 claudio gentil robust pnorm algorithm machin learn v53 n3 p265299 decemb arindam banerje srujana merugu inderjit dhillon joydeep ghosh cluster bregman diverg journal machin learn research 6 p17051749 1212005 mark herbster manfr k warmuth track best linear predictor journal machin learn research 1 p281309 912001