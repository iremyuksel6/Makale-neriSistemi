parallel hierarch solver precondition boundari element method method moment import tool solv boundari integr equat aris varieti applic transform physic problem dens linear system due larg number variabl associ comput requir system solv iter use method gmre cg variant core oper iter solver applic system matrix vector requir thetan2 oper memori use accur dens method comput complex reduc log n memori requir thetan use hierarch approxim techniqu algorithm speedup approxim combin parallel yield fast dens solver paper present effici parallel formul dens iter solver base hierarch approxim solv potenti integr equat first kind studi impact variou paramet accuraci perform parallel solver demonstr parallel formul incur minim parallel process overhead scale larg number processor present two precondit techniqu acceler converg iter solver techniqu base innerout scheme blockdiagon scheme base truncat green function present detail experiment result 256 processor cray t3d code achiev raw comput speed 5 gflop compar accur solver correspond speed approxim 776 gflop b introduct method moment 12 popular method solv integr equat extens applic comput electromagnet wave propag heat transfer 22 21 3 11 transform physic problem defin integr equat dens linear system integr equat term volum boundari integr equat depend whether variabl defin volum surfac model object paper address solut boundari integr equat complex 3d object model arbitrarili complex 3d object may requir larg number boundari el ement object boundari element method result dens linear system hundr thousand unknown memori comput requir solv system formid iter solut techniqu gener minim residu gmre 18 method choic memori comput requir solver grow thetan 2 per iter solv system 10k variabl manner challeng current supercomput memori requir method reduc form coeffici matrix ex plicitli addit hierarch algorithm fast multipol method fmm relat particl dynam method allow us reduc comput complex iter approxim hierarch techniqu receiv lot attent context particl simul given system n particl particl influenc everi particl system total n 2 interact must comput howev physic system influenc particl anoth diminish distanc system possibl aggreg singl express impact sever particl anoth distant particl use approach total number interact system reduc significantli form basi hierarch method method provid systemat way aggreg entiti comput interact control overal error model algorithm base hierarch techniqu includ barneshut 2 fast multipol 10 appel 1 algorithm approxim long rang interact manner reduc sequenti complex typic simul involv n particl 2 log n clearli reduc comput complex hierarch method repres signific reduct time solv system howev model hundr thousand boundari element still take inordin larg amount time convent serial comput parallel process offer tool effect speed comput enabl us solv problem larg number element increas accuraci simul incorpor higher precis approxim hierarch matvec parallel formul hierarch method involv partit domain among variou processor combin object optim commun balanc load particl densiti uniform across domain object easili met 4 25 13 19 9 irregular distribut object hard achiev highli unstructur natur comput commu nicat singh et al 20 warren salmon 24 23 present scheme irregular distribut tri meet object 6 8 5 present altern scheme irregular distribut improv perform earlier scheme 7 5 use parallel hierarch techniqu comput dens matrixvector product studi impact variou paramet accuraci perform import aspect use iter solver solv larg system use effect precondit techniqu acceler converg use hierarch method comput matrixvector product parallel process signific implic choic precondition sinc system matrix never explicitli construct precondition must deriv hierarch domain represent furthermor precondit strategi must highli paralleliz sinc earli work rokhlin16 rel littl work done dens hierarch solver even serial context 14 17 22 3 paper investig accuraci converg gmre solver built around parallel hierarch matrixvector product investig impact variou paramet accuraci perform propos two precondit strategi acceler converg solver precondition base innerout scheme truncat green function demonstr excel parallel effici perform solver 256 processor cray t3d rest paper organ follow section 2 present brief overview hierarch method use solv integr equat section 3 describ parallel formul hierarch method section 4 describ precondit techniqu section 5 present experiment result cray t3d section 6 draw conclus outlin ongo research hierarch method solv integr equat boundari element method bem solv integr equat use potenti theori method discret boundari domain panel use associ green function potenti panel repres sum contribut everi panel appli dirichlet boundari condit yield larg scale linear system equat n basi boundari discret n theta n linear system aris approach dens iter solut system requir applic system matrix vector iter process facilit fact coupl coeffici two boundari element green function integr equat diminish function distanc r element instanc laplac equat green function 1r three dimens logr two dimens function decreas function distanc r allow us aggreg impact sever boundari element singl express appli constant time similar principl singl iter nbodi algorithm5 integr boundari element perform use gaussian quadratur nearbi element higher number gauss point use desir accuraci comput coupl coeffici distant basi function fewer gauss point may use simplest scenario far field evalu use singl gauss point assum triangular surfac element process involv comput mean basi function triangl scale area triangl comput matrixvector product manner involv follow step 1 construct hierarch represent domain particl simul method particl inject empti domain everi time number particl subdomain exce preset constant partit eight oct manner oct tree structur comput boundari element method element center correspond particl coordin octtre therefor construct base element center node tree store extrem along x z dimens subdomain correspond node 2 number particl tree correspond boundari element method equal product number boundari element number gauss point far field case singl gauss point far field multipol expans comput center triangl particl coordin mean basi function scale triangl area charg addit singl gauss point code also support three gauss point far field 3 comput matrixvector product need comput potenti n basi function done use variant barneshut method hierarch tree travers boundari element boundari element fall within near field observ element integr perform use direct gaussian quadratur code provid support integr use 3 13 gauss point near field invok base distanc sourc observ element contribut basi function observ element accru farfield contribut comput use multipol expans ff criterion barneshut method slightli modifi size subdomain defin extrem boundari element correspond node tree unlik origin barneshut method use size oct comput ff criterion 3 parallel gmre use hierarch matrixvector product implement parallel formul restart gmre 18 algorithm critic compon algorithm product system matrix vector x n dot product vector distribut across processor first np element vector go processor p 0 next np processor p 1 matrixvector product comput use parallel hierarch treecod parallel treecod compris two major step tree construct hierarch represent domain tree travers start distribut panel processor processor construct local tree set node highest level tree describ exclus subdomain assign processor refer branch node processor commun branch node tree form global consist imag tree processor proce comput potenti panel assign travers tree encount node local avail two possibl scenario panel coordin commun remot processor evalu interact node commun request processor refer former function ship latter data ship parallel formul base function ship paradigm discuss advantag function ship 5 7 loadbalanc techniqu effici implement costzon scheme messagepass comput node tree contain variabl store number boundari element interact comput previou matvec comput first matvec variabl sum along tree valu load node store number interact node root subtre load balanc inord travers tree assign equal load processor figur 1 illustr parallel formul barneshut method sinc discret assum static load need balanc parallel formul assign boundari element associ basi func tion processor two implic multipl processor may contribut element matrixvector product map basi function processor may match partit assum gmre algorithm problem solv hash vector element processor design gmre partit destin processor job accru vector element ad necessari commun perform use singl alltoal person commun variabl messag sizes15 alltoal broadcast insert branch node recomput top part travers local tree need insert remot processor buffer send buffer correspond broadcast branch node processor full messag process forc comput tree construct aggreg load local tree period check pend branch node broadcast load node local tree aggreg toplevel load root node processor total load w within processor domain locat node correspond load wp 2wp left determin destin point commun point use alltoal person commun insert load branch assum initi particl distribut construct local tree branch node balanc load move particl b balanc load commun particl particl schemat parallel algorithm figur 1 schemat parallel treecod formul load balanc techniqu precondit techniqu iter solver section present precondit techniqu iter solver sinc coeffici matrix never explicitli comput precondition must construct hierarch represent domain limit explicit represent coeffici matrix form basi two precondition 41 innerout scheme hierarch represent domain provid us conveni approxim coeffici matrix increas accuraci matrixvector product increas number direct interact thu runtim convers reduc accuraci reduc runtim therefor possibl visual two level scheme outer solv desir accuraci precondit inner solv base lower resolut matrixvector product accuraci inner solv control ff criterion matrixvector product multipol degre sinc top node tree avail processor matrixvector product requir rel littl commun degre diagon domin determin method control accuraci coeffici matrix highli diagon domin case mani applic high valu ff desir ensur minimum commun overhead howev matrix diagon domin desir use lower valu ff correspondingli lower valu multipol degre fact possibl improv accuraci inner solv increas multipol degre reduc valu ff inner solv solut converg use flexibl precondit gmre solver howev paper present precondit result constant resolut inner solv 42 truncat green function primari drawback two level scheme inner iter still poorli condi tion diagon domin mani problem allow us approxim system truncat green function leaf node hierarch tree coeffici matrix explicitli construct assum truncat green function done use criteria similar ff criterion barneshut method follow let constant fi defin truncat spread green function boundari element travers barneshut tree appli multipol accept criteria constant fi node tree use determin near field boundari element correspond constant fi construct coeffici matrix 0 correspond near field precondition comput direct invers matrix 0 approxim solv basi function comput dotproduct specif row correspond basi function near field element number element near field control preset constant k closest k element near field use comput invers number element near field less k correspond matrix assum smaller easi see precondit strategi variant block diagon precondition simplif scheme deriv follow assum leaf node barneshut tree hold element coeffici matrix correspond element explicitli comput invers matrix use precondit solv perform precondition howev expect wors gener scheme describ hand comput precondition requir commun sinc data correspond node local avail paper report perform gener precondit techniqu base truncat green function simplif 5 experiment result object experiment studi follow ffl studi error parallel perform iter solver base hierarch matrixvector product ffl studi impact ff criterion multipol degre accuraci perform solver ffl studi impact number gauss point far field perform ffl studi precondit effect iter count solut time precondition impact parallel perform section report perform gmre solver precondit techniqu cray t3d 256 processor varieti test case highli irregular geometri use evalu perform solver precondition test sphere 24k unknown bent plate 105k unknown experiment result organ three categori perform raw parallel effici solver accuraci stabil solver precondit techniqu problem runtim eff mflop runtim eff mflop pscan 374 093 1352 100 087 5056 28060 053 089 1293 016 075 4357 tabl 1 runtim second effici comput rate t3d differ problem 51 perform matrixvector product comput intens part gmre method applic coeffici matrix vector remain dot product comput take neglig amount time therefor raw comput speed matvec good approxim overal speed solver two import aspect perform raw comput speed term flop count parallel effici addit sinc hierarch method result signific save comput larger problem use determin comput speed dens solver use hierarch metvec requir solv problem time present parallel runtim raw comput speed effici four differ problem instanc imposs run instanc singl processor memori requir therefor use forc evalu rate serial parallel version comput effici comput mflop rate code count number float point oper insid forc comput routin appli mac intern node use number mac forc comput determin total number float point oper execut code divid total time obtain mflop rate code tabl 1 present runtim effici comput rate four problem valu ff paramet case 07 degre multipol expans 9 effici comput determin sequenti time mac forc comput sequenti time larger problem instanc project use valu effici comput code achiev peak perform 5 gflop although may appear high must note code littl structur data access result poor cach perform furthermor divid squareroot instruct take significantli larger number processor cycl hand perform achiev hierarch code correspond 770 gflop dens matrixvector product clearli loss accuraci accept applic use hierarch method result two order magnitud improv perform combin speedup 200 256 processor parallel treecod provid power tool solv larg dens system loss parallel effici result commun overhead residu load imbal also exist minor variat raw comput rate across differ problem instanc ident runtim differ percentag mac comput near field interact farfield interact comput instanc farfield interact comput use particleseri interac tion involv evalu complex polynomi length 2 degre multipol seri comput good local properti yield good flop count convent risc processor alpha contrast nearfield interact mac comput exhibit good data local involv divid squar root instruct result vari raw comput speed across problem instanc detail studi impact variou paramet accuraci matrixvector product present author 5 7 52 parallel perform unprecondit gmre solver one import metric perform code time solut investig solut time differ number processor differ accuraci paramet object follow show speedup fast matvec translat scalabl solut time larg number processor studi impact ff criterion multipol degre solut time case assum desir solut reach residu norm reduc factor 10 gamma5 choic reduct residu norm lower accuraci matvec may becom unstabl beyond point first set experi studi impact ff criterion solut time degre multipol expans fix 7 parallel runtim reduc residu norm factor 10 gamma5 note time present tabl 2 cap 3600 second therefor one miss entri tabl number use infer drawn tabl ffl case rel speedup 8 64 processor around 6 correspond rel effici 74 demonstr parallel solver highli scalabl ffl given number processor multipol degre increas accuraci matvec reduc ff result higher solut time lower effici former increas number interact comput near field result higher comput load loss effici processor 8 64 8 64 tabl 2 time reduc rel residu norm 10 gamma5 degre multipol expans fix 7 time second processor 8 64 8 64 degre 5 2692 471 20103 3296 6 3823 652 27296 4412 tabl 3 time reduc rel residu norm 10 gamma5 valu ff fix 0667 time second increas commun overhead increas number interact need perform lower tree sinc local avail element coordin need commun processor farther away consist observ comput matvec studi impact increas multipol degre solut time parallel perform valu ff fix 0667 multipol degre vari 5 7 tabl 3 record solut time reduc residu norm factor 10 gamma5 8 64 processor expect increas multipol degre result increas solut time modulo parallel process overhead serial comput increas squar multipol degre sinc commun overhead high trend visibl parallel runtim also increas multipol degre also result better parallel effici raw comput speed commun overhead remain constant comput increas furthermor longer polynomi evalu conduc cach perform tabl lead us believ desir accuraci point identifi better use higher degre multipol oppos tighter ff criterion achiev accuraci approx accur figur 2 rel residu norm accur approxim iter scheme 53 accuraci gmre solver use approxim hierarch matvec sever implic iter solver import cours error solut often possibl comput accur solut due excess memori comput requir therefor difficult comput error solut howev norm axgammab good measur close current solut desir solut unfortun possibl comput sinc never explicitli assembl comput correspond approxim matvec valu match ax close say measur confid approxim solut math real solut examin norm vector iter studi stabil unprecondit gmre iter 531 converg accuraci iter solver section demonstr possibl get nearaccur converg signific save comput time use hierarch method fix valu ff multipol degre compar reduct error norm iter tabl 4 present log rel residu norm gmre variou degre approxim execut 64 processor t3d follow infer drawn experiment data ffl iter method base hierarch matvec stabl beyond residu norm reduct 10 gamma5 also illustr figur 2 plot reduct residu norm iter accur worst case inaccur matvec seen even worst case accuraci residu norm near agreement rel residu norm 10 gamma5 mani problem accuraci adequ ffl increas accuraci matvec result closer agreement accur hierarch solver also accompani increas solut time therefor desir oper desir accuraci rang ffl parallel runtim indic hierarch method capabl yield signific save time expens slight loss accuraci iter accur time 12446 15619 9216 11202 tabl 4 converg log 10 rel error norm runtim second gmre solver 64 processor cray t3d problem consist 24192 unknown 532 impact number gauss point far field comput farfield interact code allow flexibl use either 3point gaussian quadratur singl point gaussian quadratur investig impact comput farfield potenti use overal runtim error tabl 5 present converg solver two case case valu ff fix 0667 multipol degre 7 near point interact comput ident manner either case depend distanc boundari element code allow 3 13 point gaussian quadratur nearfield follow infer drawn experiment result ffl use larger number gauss point yield higher accuraci also requir comput consist understand fix ff criterion comput complex increas number gauss point far field ffl singl gauss point integr farfield extrem fast adequ approxim solut iter gauss time 11202 689 tabl 5 converg log 10 rel error norm runtim second gmre solver 64 processor cray t3d problem consist 24192 unknown valu ff 0667 multipol degre 7 5 unprecondit block diag innerout 5 block diag innerout unprecondit figur 3 rel residu norm accur approxim iter scheme 54 perform precondit gmre section examin effect blockdiagon innerout precondit scheme fix valu ff 05 multipol degre 7 effect precondition judg number iter comput time reduc residu norm fix factor although certain precondition may yield excel iter count may difficult comput vice versa third perhap equal import aspect parallel process overhead incur precondition tabl 6 present reduct error norm iter unprecondit innerout blockdiagon precondit scheme figur 3 illustr converg two problem graphic easi see innerout scheme converg small number outer iter howev runtim fact block diagon scheme number inner iter innerout scheme rel high drawback innerout scheme sinc attempt improv condit inner solv current investig techniqu solv hand sinc block diagon matrix factor commun overhead high block diagon precondition provid effect lightweight precondit techniqu reflect slightli higher iter count lower solut time iter unprecon innerout block diag unprecon innerout block diag time 15619 12540 10361 70978 58477 51106 tabl rel error norm runtim second precondit gmre solver 64 processor cray t3d 6 conclud remark paper present dens iter solver base approxim hierarch matrixvector product use solver demonstr possibl solv larg problem hundr thousand unknown extrem fast problem even gener let alon solv use tradit method memori comput requir show possibl achiev scalabl high perform solver term raw comput speed parallel effici 256 processor cray t3d combin improv use hierarch techniqu parallel repres speedup four order magnitud solut time reason size problem also examin effect variou accuraci paramet solut time parallel effici overal error present two precondit techniqu innerout scheme blockdiagon scheme evalu perform precondition term iter count solut time although innerout scheme requir fewer iter iter inner solv may expens hand due diagon domin mani system blockdiagon scheme provid us effect lightweight precondition treecod develop highli modular natur provid gener framework solv varieti dens linear system even serial context rel littl work done sinc initi work rokhlin16 promin piec work area includ 14 17 22 3 best knowledg treecod present paper among first parallel multilevel solverprecondition toolkit current extend hierarch solver scatter problem electromagnet 17 16 22 21 3 freespac green function field integr equat depend wave number incid radiat high wave number boundari discret must fine correspond larg number unknown applic hierarch method particularli suitabl desir level accuraci high r effici program manybodi simul hierarch log n forc calcul algorithm guidelin use fast multipol method calcul rc larg object acceler molecular dynam fast multipol algorithm effici parallel formul hierarch method applic scalabl parallel formul barneshut method nbodi simul parallel matrixvector product use hierarch method parallel version fast multipol method fast algorithm particl simul field comput method moment matrix method field problem map adapt fast multipol algorithm mimd system multipol acceler precondit iter method threedimension potenti integr equat first kind rapid solut integr equat classic potenti theori rapid solut integr equat scatter theori two dimens gmre gener minim residu algorithm solv nonsymmetr linear system implement fast multipol method three dimen sion load balanc data local hierarch nbodi method fast multipol method solut use parametr geometri multilevel fast multipol algorithm solv combin field integr equat electromagnet scatter astrophys nbodi simul use hierarch tree data structur parallel hash oct tree nbodi algorithm parallel multipol method connect machin tr ctr vivek sarin ananth grama ahm sameh analyz error bound multipolebas treecod proceed 1998 acmiee confer supercomput cdrom p112 novemb 0713 1998 san jose ca sreekanth r sambavaram vivek sarin ahm sameh ananth grama multipolebas precondition larg spars linear system parallel comput v29 n9 p12611273 septemb ananth grama vivek sarin impact farfield interact perform multipolebas precondition spars linear system proceed 18th annual intern confer supercomput june 26juli 01 2004 malo franc hariharan sriniva aluru balasubramaniam shanker scalabl parallel fast multipol method analysi scatter perfect electr conduct surfac proceed 2002 acmiee confer supercomput p117 novemb 16 2002 baltimor maryland qian xi wang variabl order revis binari treecod journal comput physic v200 n1 p192210 10 octob 2004