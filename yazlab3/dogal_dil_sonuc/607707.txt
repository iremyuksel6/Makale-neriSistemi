algebra geometr method hierarch learn machin hierarch learn machin layer perceptron radial basi function gaussian mixtur nonidentifi learn machin whose fisher inform matric posit definit fact show convent statist asymptot theori appli neural network learn theori exampl either bayesian posteriori probabl distribut converg gaussian distribut gener error proport number paramet purpos paper overcom problem clarifi relat learn curv hierarch learn machin algebra geometr structur paramet space establish algorithm calcul bayesian stochast complex base blowingup technolog algebra geometri prove bayesian gener error hierarch learn machin smaller regular statist model even true distribut contain parametr model b introduct learn artifici neural network understood statist estim unknown probabl distribut base empir sampl white 1989 watanab fukumizu 1995 let pyx w condit probabl densiti function repres probabilist infer artifici neural network x input output paramet w consist lot weight bias optim infer pyx w approxim true condit probabl densiti train sampl taken let us reconsid basic properti homogen hierarch learn machin map paramet w condit probabl densiti pyx w onetoon model call identifi otherwis call nonidentifi word model identifi paramet uniqu determin behavior standard asymptot theori mathemat statist requir given model identifi exampl identifiabl necessari condit ensur distribut maximum likelihood estim bayesian posteriori probabl densiti function converg normal distribut number train sampl tend infin cramer 1949 approxim likelihood function quadrat form paramet select optim model use inform criteria aic bic mdl implicitli assum model identifi howev mani kind artifici neural network layer perceptron radial basi function boltzmann machin gaussian mixtur nonidentifi henc either statist properti yet clarifi convent statist design method appli fact failur likelihood asymptot normal mixtur shown viewpoint test hypothesi statist hartigan 1985 research artifici neural network point aic correspond gener error maximum likelihood method hagiwara 1993 sinc fisher inform matrix degener paramet repres smaller model fukumizu 1996 asymptot distribut maximum likelihood estim nonidentifi model analyz base theorem empir likelihood function converg gaussian process satisfi donsker condit dacunhacastel gassiat 1997 proven gener error bayesian estim far smaller number paramet divid number train sampl watanab 1997 watanab 1998 paramet space conic sym metric gener error maximum likelihood method dierent regular statist model amari ozeki 2000 log likelihood function analyt paramet set paramet compact gener error maximum likelihood method bound constant divid number train sampl watanab 2001b let us illustr problem caus nonidentifi layer learn machin pyx w threelay perceptron k hidden unit w 0 paramet pyx w 0 equal machin k 0 hidden unit set true paramet consist sever submanifold paramet space moreov fisher inform matrix log pyx w log pyx wpyx wqxdxdi qx probabl densiti function input space posit semidefinit posit definit rank rank iw depend paramet fact indic artifici neural network mani singular point paramet space figur 1 typic exampl shown example2 section 3 reason almost homogen hierarch learn machin boltzmann machin gaussian mixtur competit neural network singular paramet space result mathemat foundat analyz learn previou paper watanab 1999b watanab 2000 watanab 2001a order overcom problem prove basic mathemat relat algebra geometr structur singular paramet space asymptot behavior learn curv construct gener formula calcul asymptot form bayesian gener error use resolut singular base assumpt true distribut contain parametr model paper consid threelay perceptron case true probabl densiti contain parametr model clarifi singular paramet space aect learn bayesian estim employ algebra geometr method show follow fact 1 learn curv strongli aect singular sinc statist estim error depend estim paramet 2 learn ecienc evalu use blowingup technolog algebra geometri 3 gener error made smaller singular bayesian estim appli result clarifi reason bayesian estim use practic applic neural network demonstr possibl algebra geometri play import role learn theori hierarch learn machin dierenti geometri regular statist model amari 1985 paper consist 7 section section 2 gener framework bayesian estim formul section 3 analyz parametr case true probabl densiti function contain learn model deriv asymptot expans stochast complex use resolut singular section 4 also studi nonparametr case true probabl densiti contain clarifi eect singular paramet space section 5 problem asymptot expans gener error consid final section 6 7 devot discuss conclus bayesian framework section formul standard framework bayesian estim bayesian stochast complex schwarz 1974 akaik 1980 levin tishbi solla 1990 mackay 1992 amari fujita shinomoto 1992 amari murata 1993 let pyx w probabl densiti function learn machin input x output paramet w n dimension vector respect let qyxqx true probabl densiti function input space train sampl x independ taken paper mainli consid bayesian framework henc estim probabl densiti n w paramet space defin expnh n ww log z n normal constant w arbitrari fix probabl densiti function paramet space call priori distribut h n w empir kullback distanc note posteriori distribut n w depend qy x constant function w henc written form infer p n yx train machin new input x defin averag condit probabl densiti function gener error gn defin kullback distanc p n yx qyx qyx log qxdxdi 1 repres expect valu overal set train sampl one import purpos learn theori clarifi behavior gener error number train sampl sucient larg well known levin tishbi solla 1990 amari 1993 amari murata 1993 gener error gn equal increas stochast complex f n arbitrari natur number n f n defin stochast complex f n gener concept sometim call free energi bayesian factor logarithm evid seen statist inform theori learn theori mathemat physic schwarz 1974 akaik 1980 rissanen 1986 mackay 1992 opper haussler 1995 meir merhav 1995 haussler opper 1997 yamanishi 1998 exampl bayesian model select hyperparatemet optim often carri minim stochast complex averag call bic abic import practic applic stochast complex satisfi two basic inequ firstli defin hw f n respect qxdxdi note hw call kullback inform appli jensen inequ hold arbitrari natur number n opper haussler 1995 watanab 2001a secondli use notat f n f n f n f n explicitli show priori probabl densiti w f n f n understood gener stochast complex case w nonneg function w w satisfi immedi follow therefor restrict integr region paramet space make stochast complex smaller exampl defin expnhwwdw 7 sucient small 0 two inequ eq4 eq8 give upper bound stochast com plexiti hand support w compact lower bound proven moreov learn machin contain true distribut hold watanab 1999b watanab 2001a paper base algebra geometr method prove rigor upper bound f n constant olog n function n satisfi olog n log n mathemat speak although gener error gn equal f n natur number n deriv asymptot expans gn howev section 5 show gn asymptot expans satisfi inequ sucient larg n eq11 main result paper upper bound stochast complex howev also discuss behavior gener error base eq12 3 parametr case section consid parametr case true probabl distribut qyxqx contain learn machin pyx wqx show relat algebra geometr structur machin asymptot form stochast complex 31 algebra geometri neural network subsect briefli summar essenti result previou paper mathemat proof subsect see watanab 1999b watanab 2001a strictli speak need assumpt log pyx w analyt function w analyt continu holomorph function w whose associ converg radii posit uniformli arbitrari x satisfi qyxqx 0 watanab 2000 watanab 2001a paper appli result previou paper threelay perceptron threelay perceptron redund approxim true distribut set true paramet w union sever submanifold paramet space gener set zero point analyt function call analyt set analyt function hw polynomi set call algebra varieti well known analyt set algebra varieti complic singular gener introduc state densiti function vt dirac delta function 0 sucient small constant definit 0 use vt f n rewritten expnhwwdw dt henc vt asymptot expans 0 f n n asymptot expans n order examin vt introduc kind zeta function jz sato kullback inform hw priori probabl densiti w function one complex variabl z hw z wdw 14 jz analyt function z region rez 0 well known theori distribut hyperfunct hw analyt function w jz analyt continu meromorph function entir complex plane pole neg part real axi atiyah 1970 bernstein 1972 sato shintani 1974 bjork 1979 moreov pole jz ration number kashiwara 1976 let 1 largest pole order jz respect note eq15 show jz z c mellin transform vt use invers mellin transform show vt satisfi c 0 0 posit constant eq13 f n asymptot expans o1 bound function n henc eq8 moreov support w compact set eq9 obtain asymptot expans f n first theorem theorem 1 watanab 1999b watanab 2001a assum support w compact set stochast complex f n asymptot expans respect largest pole order function analyt continu hw z wdw hw kullback inform w priori probabl densiti function remark support w compact theorem 1 give upper bound f n import constant 1 1 calcul algebra geometr method defin set paramet w proven hironaka resolut theorem hironaka 1964 atiyah 1970 exist manifold u resolut map arbitrari neighborhood arbitrari u u satisfi au 0 strictli posit function k nonneg even integ figur 2 let decomposit w finit union suitabl neighborhood w appli resolut theorem function jz hw z wdw hw z wdw given recurs blowingup jacobian g u direct product local variabl u 1 cu posit analyt function h j nonneg integ neighborhood u au gu set constant function calcul pole jz take u small enough henc set loss gener k j depend neighborhood u find jz pole h j ration number neg part real axi sinc resolut map gu found use finit recurs procedur blowingup 1 1 found algorithm also proven 1 d2 w w 0 1 theorem 2 watanab 1999b watanab 2001a largest pole 1 function jz algorithm calcul hironaka resolut theorem moreov 1 ration number 1 natur number w w 0 dimens paramet note learn machin regular statist model alway also note jerey prior employ neural network learn equal zero singular assumpt w w 0 satisfi even fisher metric degener watanab 2001c example1 regular model let us consid regular statist model exp2 set paramet assum true distribut exp2 priori distribut uniform distribut w subset w defin introduc map 2 z pole z 1 show jw 2 z pole way henc result f n log n coincid well known result bayesian asymptot theori regular statist model map eq17 typic exampl blowingup example2 nonidentifi model let us consid learn machin pyx b c 2 exp2 assum true distribut eq16 priori probabl distribut uniform one set kullback inform let us defin two set paramet use blowingup recurs find map defin use transform obtain therefor hw z dw 2 z largest pole jw 1 z 34 order one also shown jww 1 z largest pole 34 order one henc result log n o1 32 applic layer perceptron appli theori forego subsect threelay perceptron threelay perceptron paramet defin k b k x fx w h n dimension vector x b h dimension vector c h real number k number input unit output unit hidden unit paper consid machin estim standard deviat 0 constant assum true distribut say true regress function special case analysi case import follow section true regress function contain model theorem 3 assum learn machin given eq18 eq19 train use sampl independ taken distribut eq20 priori distribut satisfi w 0 neighborhood origin proof theorem use notat kullback inform b c hp kp purpos find pole function let us appli blowingup techniqu kullback inform ha b c firstli introduc map defin let u variabl u except u 11 word jacobian g u map g defin set paramat 0 assumpt exist 0 order obtain upper bound stochast complex restrict integr region paramet space use eq5 6 assumpt w 0 gu calcul pole jz assum constant gu du db dc pole function respect largest pole jz sinc h 1 zero point interv 1 larger 1 z nk2 pole jz otherwis jz larger pole nk2 henc 1 nk2 secondli consid anoth blowingup g defin method first half exist analyt function impli therefor comb two result largest pole 1 jz satisfi inequ complet proof theorem 3 end proof theorem 1 moreov gn asymptot expans see section 5 obtain inequ gener error hand well known largest pole regular statist model equal d2 number paramet threelay perceptorn 100 input unit 10 hidden unit 1 output unit employ regular statist model number paramet emphas gener error hierarch learn machin far smaller regular statist model use bayesian estim adopt normal distribut priori probabl densiti shown result theorem 3 direct calcul watanab 1999a howev theorem 3 show systemat result hold arbitrari priori distribut moreov easi gener result case learn machin input unit k 1 first hidden unit k 2 second hidden unit k p pth hidden unit n output unit assum hidden unit output unit bia paramet use blowingup gener proof theorem 3 cours result hold true regress function special case howev follow section show result necessari obtain bound gener regress function 4 nonparametr case previou section studi case true probabl distribut contain parametr model section consid nonparametr case true distribut contain parametr model illustr figur 3 let w 0 paramet minim hw point c figur 3 main purpos clarifi eect singular point b figur 3 contain neighborhood w 0 let us consid case threelay perceptron given eq18 eq19 train use sampl independ taken true probabl distribut gx true regress function qx true probabl distribut input space let ek minimum function approxim error use threelay perceptron k hidden unit assum 1 k k exist paramet w attain minimum valu theorem 4 assum learn machin given eq18 eq19 train use sampl independ taken distribut eq21 priori distribut satisfi w 0 arbitrari w proof theorem 4 jensen inequ eq4 hw kullback distanc natur number satisfi 0 k 1 k divid paramet also let 1 2 real number satisfi 1 1 arbitrari u v r n therefor arbitrari x w henc inequ use definit f n increas function hw function satisfi choos 1 w 1 2 w 2 compact support function firstli evalu f 1 n let w 1 paramet minim h 1 w 1 eq22 theorem 2 number paramet threelay perceptron k 1 hidden unit secondli appli theorem 3 f 2 n combin eq23 eq24 take 1 sucient close 1 obtain arbitrari given theorem 4 end proof base theorem 4 gn asymptot expans see section 5 gn satisfi inequ n n 0 sucient larg n 0 henc ek n n 0 sucient larg n 0 figur 4 illustr sever learn curv correspond k 0 k k gener error gn smaller everi curv well known barron 1994 murata 1996 gx belong kind function space sucient larg k cg posit constant determin true regress function gx asymptot properti gener error 22 n k sucient larg choos inequ 27 hold n sucient larg n sucient larg extens larg gn bound gener error middl size model n becom larger bound larger model n extens larg bound largest model complex hierarch learn machin contain lot smaller model paramet space analyt set singular choos appropri model adapt number train sampl bayesian estim appli properti caus fact model nonidentifi quantit eect evalu use algebra geometri 5 asymptot properti gener er ror section let us consid asymptot expans gener error eq2 f n equal accumul gener error g0 defin f 1 henc gn asymptot expans f n also asymptot expans howev even f n asymptot expans gn may asymptot expans forego section prove f n satisfi inequ constant determin singular true distribu tion order mathemat deriv inequ gn eq30 need assumpt asymptot properti gener error 23 assumpt assum gener error gn asymptot expans q q n q real constant q n 0 posit nonincreas function n satisfi base assumpt follow lemma lemma 1 gn satisfi assumpt eq30 hold gn satisfi inequ proof assumpt show 1 1 eq35 hold ks 2 k eq32eq33 eq34 tk tk c c 0 tk arbitrari 0 exist k 0 henc contradict eq36 henc tn c 2 c end proof lemma 1 paper proven inequ eq30 theorem 1 2 3 4 without assumpt obtain correspond inequ adopt assumpt word gn asymptot expans eq30 hold gn satisfi eq35 conjectur natur learn machin satisfi assumpt sucient condit assumpt f n asymptot expans r 1 exampl learner pyx 2 exp2 priori distribut standard normal distribut true distribut shown direct calcul stochast complex asymptot expans henc gn asymptot expans c 22n expect gener case gn asymptot expans assumpt howev mathemat speak necessari sucient condit yet establish import problem statist learn theori futur 6 discuss section univers phenomena observ hierarch learn machin 61 bia varianc singular consid cover neighborhood paramet space w w j sucient small neighborhood paramet w j number j eq38 finit compact upperbound stochast complex rewritten exphwwdw function approxim error paramet w j hw v w j statist estim error neighborhood w j log n mw j 1 c 0 0 constant valu w j mw j respect largest pole multipl meromorph function note bw j v w j call bia varianc respect bayesian estim neighborhood paramet w j minim select largest probabl regular statist model varianc depend paramet word w j arbitrari paramet w j henc paramet minim function approxim error select hand hierarch learn machin varianc v w j strongli depend paramet w j paramet minim sum bia varianc select number train sampl larg extens larg paramet among singular point figur 3 repres middl size model automat select result smaller gener error n increas larger largest model b select last n becom extens larg paramet c minim bia select univers phenomenon hierarch learn machin indic essenti dierenc regular statist model artifici neural network 62 neural network overcomplet basi singular hierarch learn machin origin homogen structur learn model set function use artifici neural network exampl set overcomplet basi word coecient ab c wavelet type decomposit given function gx uniqu determin gx chui 1989 murata 1996 practic applic true probabl distribut seldom contain parametr model howev adopt model almost approxim true distribut compar fluctuat caus random sampl k b k x appropri number sampl choos appropri learn model expect model almost redund state output function hidden unit almost linearli depend expect paper mathemat foundat studi learn machin state 7 conclus consid case true distribut contain parametr model made hierarch learn machin show paramet among singular point select bayesian distribut result small gener error quantit eect singular clarifi base resolut singular algebra geometri even true distribut contain parametr model singular strongli aect improv learn curv univers phenomenon hierarch learn machin observ almost artifici neural network r likelihood bay procedur univers theorem learn curv four type learn curv neural comput statist theori learn curv entrop loss resolut singular divis distribut commun pure appli mathemat approxim estim bound artifici neural network analyt continu gener function respect paramet mathemat method statist introduct wavelet test local conic model gener function problem appli aic determin structur layer feedforward neural network failur likelihood asymptot normal mixtur mutual inform resolut singular algebra varieti field characterist zero statist approach learn gener layer neural network bayesian interpol stochast complex learn realiz unrealiz rule integr represent ridg function approxim bound threelay network bound predict error statist mechan supervis learn stochast complex model zeta function associ prehomogen vector space optim method layer neural network base modifi inform criterion essenti di gener error layer statist model bayesian estim algebra analysi nonregular learn machin neural comput probabilist design layer neural network base unifi framework learn artifici neural network statist prespect neural comput decisiontheoret extens stochast complex applic learn tr bayesian interpol four type learn curv univers theorem learn curv introduct wavelet statist theori learn curv entrop loss criterion approxim estim bound artifici neural network stochast complex learn realiz unrealiz rule regular condit inform matrix multilay perceptron network integr represent function use threelay network approxim bound algebra analysi singular statist estim ctr miki aoyagi sumio watanab stochast complex reduc rank regress bayesian estim neural network v18 n7 p924933 septemb 2005 keisuk yamazaki sumio watanab singular mixtur model upper bound stochast complex neural network v16 n7 p10291038 septemb sumio watanab shunichi amari learn coeffici layer model true distribut mismatch singular neural comput v15 n5 p10131033 may shunichi amari hiroyuki nakahara difficulti singular popul code neural comput v17 n4 p839858 april 2005 haikun wei jun zhang florent cousseau tomoko ozeki shunichi amari dynam learn near singular layer network neural comput v20 n3 p813843 march 2008 shunichi amari hyeyoung park tomoko ozeki singular affect dynam learn neuromanifold neural comput v18 n5 p10071065 may 2006