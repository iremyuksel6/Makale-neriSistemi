increment featur select featur select problem find relev featur number featur dataset larg number pattern huge effect method featur select help dimension reduct increment probabilist algorithm design implement altern exhaust heurist approach theoret analysi given support idea probabilist algorithm find optim nearoptim subset featur experiment result suggest 1 probabilist algorithm effect obtain optimalsuboptim featur subset 2 increment version expedit featur select number pattern larg scale without sacrif qualiti select featur b introduct featur select find use relev featur describ applic domain problem featur select formal defin select minimum set relev featur n origin featur n probabl distribut differ class given valu featur close possibl origin distribut given valu n featur mathemat fn origin featur set fm chosen featur subset condit probabl pc j close possibl pc j possibl class c f f n valu vector respect featur vector fm fn 11 dimension domain expand number featur increas gener role featur select threefold 1 simplifi data descrip tion 2 reduc task data collect 3 improv qualiti problem solv problem represent three featur gener simpler one six featur benefit simpl represent abund easier understand problem better faster problem solv context data collect featur mean data collect mani applic could time consum costli qualiti improv problem solv result featur select illustr via classic supervis learn task pattern classif problem given train set label pattern induc classif model predict class label set previous unseen pattern socal test set although featur enhanc discrimin power represent excess featur would introduc mani difficulti induct algorithm 11 first time requir induct algorithm often grow dramat number featur render algorithm impract problem larg number featur second mani learn algorithm view perform estim probabl class label given set fea ture mani featur distribut high dimens becom complex unless exponenti amount data avail difficult obtain good estim train dataset third irrelev redund featur may confus learn algorithm obscur distribut small set truli relev featur addit irrelev redund featur requir exponenti increas data storag requir 1 featur much data requir effect induct instanc binari domain extra irrelevantrelev featur would time pattern describ whole data induct algorithm reduc featur also result simpler induct model shorter fewer classif rule n featur relev exhaust approach find optim featur would requir examin subset number possibl subset grow exponenti research design differ strategi search optim subset featur branch bound 20 variat 26 mani heurist stochast method 5 7 view featur select algorithm perspect use induct algorithm point 8 work featur select divid filter wrapper model filter model featur selector independ induct algorithm serv filter siev irrelev andor redund featur wrapper model featur selector wrap around induct algorithm reli relev featur determin one problem wrapper model restrict time complex learn algorithm 12 time complex depend number featur often wrapper method prohibit expens run intract larg number featur recal mani case featur select perform excess number featur favorit induct algorithm difficulti handl mani featur differ model howev suit differ applic classifi chosen run applic hand may wise choos wrapper model sinc featur select classifi induct use bia work consid case learn classifi becom cumbersom ineffect due larg size dataset larg defin number featur n number pattern p latter make induct algorithm falter henc larg dataset term p main concern natur filter model adopt aim provid simpl practic method select featur larg dataset follow first review relat work featur select relat work problem featur select long activ research topic within statist pattern recognit 30 6 7 work area dealt linear regress 12 assumpt appli machin learn algorithm 8 research 12 8 point common assumpt monoton increas number featur improv perform learn algorithm 1 recent featur select receiv consider attent research machin learn knowledg discoveri interest improv perform algorithm clean data handl larg databas featur select even import sinc mani learn algorithm may falter take long time run data reduc featur select method 9 12 8 group two categori exhaust heurist search optim set featur exampl almuallim dietterich focu algorithm 2 start empti featur set carri exhaust search find minim set featur suffici construct hypothesi consist given set exampl work binari noisefre data time complex ominn 1 monoton assumpt valid mani induct algorithm use machin learn exampl dataset 1 corral section 5 reproduc 8 propos three heurist algorithm speed search 2 select minim subset known intract problem practic often trade optim solut less time spent search mani heurist featur select algorithm relief algorithm 9 assign relevanceweight featur meant denot relev featur target concept relief sampl pattern randomli train set updat relev valu base differ select pattern two nearest pattern opposit class accord 9 relief assum twoclass classif problem help redund featur given featur relev concept includ redund featur would select even though fraction necessari concept descript preset algorithm 19 anoth heurist featur selector use theori rough set rank featur heurist assum noisefre binari domain order consid higher order relat among featur liu wen 16 suggest use high order inform gain select featur sinc last two algorithm tri explor combin featur certain fail problem whose featur highli interdepend pariti problem combin small number featur help find relev one anoth common understand learn algorithm builtin featur select exampl id3 23 fring 21 c45 24 result 2 suggest one reli id3 fring filter irrelev featur detail survey found 5 latest develop featur select pattern recognit found 7 sum exhaust search approach infeas practic heurist search approach reduc search time significantli fail hard problem eg pariti problem remov redund featur probabilist approach propos altern 15 select op timalsuboptim subset featur context larg size databas howev would still take consider long time check subset valid 2 firsthand experi problem probabilist system dispatch local institut onsit usag evid show reduc data size significantli speed select featur see case studi section 33 henc increment probabilist method design implement follow describ probabilist method first increment one follow empir studi effect algorithm verifi end paper provid relev discuss 2 check done op p number pattern use hash method 3 probabilist featur select propos probabilist approach la vega algorithm 4 la vega algorithm make probabilist choic help guid quickli correct solut one kind la vega algorithm use random guid search way correct solut guarante even unfortun choic made mention earlier heurist search method vulner dataset high interdepend among featur la vega algorithm free us worri situat even time requir differ situat anoth similar type algorithm mont carlo algorithm often possibl reduc error probabl arbitrarili cost slight increas comput time refer page 341 4 work lvf la vega filter 3 suitabl sinc probabl gener distinct subset time perform la vega algorithm may better heurist algorithm lvf algorithm input maxtri fl allow inconsist rate dataset n featur output set featur satisfi inconsist criterion best i1 maxtri best inconcheck fl els best printcurrentbest end lvf algorithm gener random subset n featur everi round number featur c less current best ie best data featur prescrib check inconsist criterion inconsist rate defin later prespecifi one fl c best best replac c respect new current best print best inconsist criterion satisfi equal good current best found print max tri algorithm use control number loop valu max tri defin accord applic base experi exper iment small big max tri affect perform lvf compromis made good fast solut longer lvf run better result refer analysi section 32 max tri set 77thetan experiment studi 4 follow ruleofthumb counterpart lvw wrapper featur selector appli la vega algorithm 4 tri first constant c alon instead c theta n link n 77 chosen c featur dataset word larger n harder problem featur select parity5 difficult parity2 eg henc tri need lvf loop max tri time stop altern stop criterion let lvf run forev take full advantag anytim algorithm natur section 6 function randomsetse return set featur randomli seed chang dynam differ set gener function numoffeaturess return cardin set inconcheck return inconsist rate data select featur specifi printcurrentbest print subset sophist version lvf like sinc know cardin better subset smaller c best cardin current best subset need randomli gener subset whose cardin smaller c best new round select sampl featur without replac 31 measur featur good inconsist criterion inconcheck fl key lvf criterion specifi extent dimension reduc data accept inconsist rate data describ select featur check prespecifi rate fl smaller fl mean dimension reduc data accept default valu fl 0 unless specifi inconsist rate dataset calcul follow 1 two pattern consid inconsist match class label 2 inconsist count number match pattern minu largest number pattern differ class label exampl n match pattern among c 1 pattern belong label 1 c 2 label 2 c 3 label 3 3 largest among three inconsist count 3 inconsist rate sum inconsist count divid total number pattern p easili shown inconsist rate 0 dataset n featur fn origin featur set fm chosen featur subset condit probabl pc exactli equal pc j differ possibl class c f f n repres vector valu respect featur vector fm fn inconsist criterion conserv way achiev class separ commonli use pattern recognit basic select criterion 6 limit version first propos 2 minfeatur bia binari domain instead aim maxim class separ measur tri maintain origin class separ data inconsist criterion also line informationtheoret experi paper tri use larg c small larg dataset could use one fix max tri reader may done anoth version lvf link max tri percentag total search space accord desir qualiti select featur consider 28 suggest use featur good discrimin provid compact descript two class descript maxim distinct geometr constraint interpret 17 mean featur take nearli ident valu exampl class ii take differ valu exampl class inconsist criterion aim retain discrimin power data multipl class featur select 32 theoret analysi analysi show lvf give good solut optim solut max tri suffici larg good pseudo random number gener 22 select optim subset featur consid sampl without replac probabl find optim subset k1th experi 1 probabl conduct k1 experi find optim subset theta theta 1 n number origin featur n larg max tri 2 n assum one optimum exist l optima mani applic k 1th toss probabl find one optimum l roughli number optima doubl number run time halv refer lvf algorithm notic inconsist criterion check c c best thu c best reduc due random search number inconsist check also reduc shown section 51 realworld dataset c best one fifth origin number featur mushroom addit time complex check op henc lvf expect run fast equival good subset requir last two line insid forloop lvf algorithm remov lvf made even faster 33 appli lvf huge dataset practic case featur select particularli use dataset huge sinc mani learn algorithm may encount difficulti mention earlier featur select help reduc dimension dataset learn algorithm use induc rule henc huge dataset also ultim test featur select algorithm lvf opportun undergo real test huge dataset section 5 empir studi result lvf benchmark dataset report dataset involv relat servic industri lvf given local institut 5 need method reduc number featur appli machin learn algorithm dataset due huge size dataset dataset confidenti access user institut ran lvf independ 5 japansingapor ai centr singapor without modif provid follow account one dataset let us call hd1 65000 pattern 59 featur hd2 5909 pattern 81 featur dataset discret featur valu rang 2 13 lvf found 10 35 featur relev describ hd1 hd2 respect without sacrif discrimin power hour run lvf sun sparc workstat due long wait time anoth experi 10000 pattern hd1 use took lvf 5 minut complet run obtain result result summar tabl stark differ hour minut inspir us extend work lvf short find manifest two point 1 lvf significantli reduc number featur 2 reduc number pattern significantli reduc run time second find lead us increment featur select data featur pattern select time hour hour larg dataset differenti two type 1 horizont larg number featur 2 vertic larg number pattern implement lvf consid overcom horizont larg appli la vega algorithm order avoid exhaust search attack vertic larg use hash mechan order speed howev practic case show done overcom vertic larg henc follow mention larg mean vertic one p 4 increment probabilist featur select although lvf gener optimalsuboptim solut see experiment result dataset huge shown section 32 check whether dataset consist still take time due op complex natur think increment version lvf significantli reduc number inconsist check studi lvf algorithm notic reduc data decreas number check howev featur select reduc data may suitabl whole data follow algorithm design achiev featur select reduc data gener inconsist whole data furthermor done without sacrif qualiti featur subset measur number featur relev lvi algorithm percentag data use featur select dataset n featur fl allow inconsist rate output set featur satisfi inconsist criterion randomli chosen rest data loop checkinconsubset 1 incondata return subset els removeincondata loop lvi checkincon similar inconcheck lvf addit save inconsist pattern 1 incondata experi design demonstr claim made lvi increment algorithm lvi start portion data p accept inconsist rate fl usual set 0 prior knowledg minimum valu fl obtain appli inconcheckf lvf f set n featur lvi split data 0 1 0 p 1 remain lvi use subset featur subset 0 found lvf check subset 1 actual inconsist found 0 fi fl inconsist rate 1 exceed stop otherwis append pattern incondata 1 caus addit incon sistenc 0 delet incondata 1 select process repeat solut found subset found whole set return solut 5 empir studi error probabl play import role featur select algorithm ultim alway use metaselect criterion 25 regardless differ featur select algorithm subset lowest estim error alway select classif task error caus wrongli classifi pattern number error divid number total pattern set give us error rate dataset split two set train vs test error rate obtain set error rate test set estim perform classif algorithm order check error rate featur select artifici realworld dataset use studi effect lvf lvi dataset either commonli use comparison known relev featur two corral parity55 dataset obtain uci repositori 18 artifici ffl corral data design 8 six binari featur irrelev featur c correl class label 75 time boolean target concept chose featur c root exampl dataset featur like c remov accur tree result ffl monk1 monk2 monk3 dataset taken 27 six featur train dataset provid use featur select monk1 monk3 need three featur describ target concept monk2 requir six train data monk3 contain nois dataset use show relev featur alway select ffl led17 data gener artifici program uci data mine repositori gener 24 featur among first 7 use display valu 0 9 seven segment display system remain 17 featur gener randomli valu binari except class take valu 0 9 represent seven segment number pattern gener determin user 20000 pattern gener experi ffl parity55 target concept pariti five bit dataset contain uniformli random irrelev train set contain 100 pattern randomli select 1024 pat tern anoth independ 100 pattern drawn form test set heurist featur selector fail sort problem sinc individu featur mean anyth realworld ffl lungcan lung cancer data describ 3 type patholog lung cancer found uci repositori data contain pattern 56 featur take valu 03 ffl soybeanl uci machin learn repositori found train test dataset two separ file contain 307 376 pattern respect contain 35 featur describ symptom 19 differ diseas soybean plant ffl vote dataset includ vote us hous repres congressperson 16 key vote identifi congression quarterli almanac volum xl dataset consist 16 featur 300 train pattern 135 test pattern tabl 1 notat c number distinct class n number featur size dataset size train data size test data train test dataset split randomli specifi dataset c n lungcan 3 56 mushroom 2 22 8125 7125 1000 ffl mushroom dataset total 8124 pattern 1000 pattern randomli select test rest use train data 22 discret featur featur 2 10 valu ffl krvskp data chess endgam kingrook versu kingpawn a7 pawn a7 mean one squar away queen kingrook side white move data contain 3196 pattern 36 featur class valu 1 indic white win mean white check black pawn advanc vice versa pattern board descript chess endgam first 36 featur describ board last one classif major measur dataset summar tabl 1 sinc dataset first group larg number pattern choos vote mushroom plu paritymix led17 krvskp form second group dataset show effect lvi relat size dataset small medium larg paritymix compos two parity55 side side 20 featur total 51 effect lvf artifici dataset evalu lvf simpl sinc relev featur known howev realworld dataset clear relev featur therefor whether select featur relev determin indirectli one way see effect fea tabl 2 result 100 run lvf dataset one exampl minimum set featur dataset n number origin featur number select featur f frequenc vote mushroom 22 4 57 5 43 a4 a5 a12 a22 6 allow 5 inconsist four featur select chosen 3 plu a1 ture select learn algorithm among mani choic chose c45 24 nbc 29 experi 1 c45 decis tree induct algorithm work well dataset report mani searcher 2 employ heurist find simplest tree structur naiv bayesian classifi employ bay rule assum featur independ approxim bayesian classifi optim classifi nbc chosen work differ way c45 lvf run 100 time train dataset number select featur frequenc report tabl 2 condit inconsist criterion satisfi also report sampl select featur dataset directli use reader experi artifici dataset relev featur alway select albeit irrelev one also chosen sometim problem like parity55 lvf correctli identifi correct featur time plu one irrelev featur sometim realworld dataset number featur reduc least half less one fifth origin tabl 2 show featur last column necessari order satisfi inconsist criterion inconsist rate 0 except monk3 featur use c45 nbc test perform improv compar use featur tenfold cross valid usual recommend ttest use instead ztest calcul pvalu sinc need take account small sampl effect 10 data sampl 10fold cross valid default set c45 use experi experi featur select featur shown last column tabl 2 use given tabl 3 4 averag accuraci rate c45 appli featur select dataset appli nbc instead report tree size report tabl size tabl 3 10fold cross valid result tree size error rate nbc appli lvf dataset pval stand pvalu ttest mean pool varianc zero tabl size dataset pval pval monk2 360 360 374 374 1 monk3 360 220 363 363 1 lungcan 7228 954 00 5666 6333 6685 vote 980 620 99 99 1 mushroom 2360 740 033 116 0004 case indic comparison obviou tabl 3 show result consist known fact bad featur standpoint bayesian decis rule 26 dataset test use nbc tabl size reduc except monk2 due featur select error rate significantli chang seven nine dataset two dataset soybeanl mushroom latter error rate increas littl absolut percentag soybeanl error rate much wors featur select train dataset fewer pattern test dataset recal divis done data contributor featur select base train data dataset put togeth run 10fold cross valid verifi conjectur anoth experi featur select use data set train test fifteen instead fourteen featur chosen result 10fold cross valid nbc c45 144 70 97 73 respect thu error rate lower data use featur select improv nbc perform parity55 statist signific result tabl 4 suggest perform c45 improv gener tree size get smaller error rate lower artifici dataset experi show relev featur c45 better full set featur realworld dataset c45 also better select featur indic lvf select relev featur dataset particular c45 poorli parity55 tabl 4 10fold cross valid result tree size error rate c45 appli lvf dataset pval stand pvalu ttest mean pool varianc zero tree size dataset pval pval monk1 419 410 0782 13 00 1937 monk2 143 143 1 354 354 1 monk3 190 190 11 11 1 lungcan 183 166 03 567 575 2627 73 152 0001 vote 145 61 0001 53 55 8357 featur select nevertheless c45 well mushroom 22 featur 4 featur demonstr c45 select relev featur dataset though seriou deterior c45 perform seen result soybeanl reason given explain nbc poor perform dataset gain featur select differ nbc c45 differ due way featur use induc classifi c45 select induct algorithm select best featur test tree branch nbc use featur condit probabl determin pattern class sinc nbc assum featur condit independ given class condit probabl irrelev featur given class approxim good discrimin 52 effect lvi set experi want verifi four claim 1 lvi may suitabl small dataset 2 lvi run faster lvf larg dataset 3 lvi sacrific qualiti select featur 4 solut found lvf earlier run neither later run earlier run start less data five dataset second group chosen experi 1 vote 2 mushroom 3 paritymix 4 krvskp 5 led17 experi conduct follow dataset start 10 data 0 featur select run lvi 10 time record number featur featur select time run subsequ experi 20 30 90 100 data averag time number featur comput experi use 100 data refer calcul pvalu size 0 low pvalu eg 5 suggest null hypothesi two averag reject refer figur 1 2 vari pvalu shade differ summar find experi follow ffl effect lvi becom obviou data size larger lvi perform well three dataset data size small around hundr vote even time save best 0 much howev save signific case paritymix clear trend observ due overhead requir increment featur select sinc inconsist check fast op p suffici larg time save appar may even neg p small lvi suitabl larg size dataset ffl anoth issu number pattern lvi start dataset either mani pattern affect lvi per formanc pattern use 0 small lvf could select featur pass inconsist check remain data 1 incondata larg worst case first loop 0 becom whole dataset one case small 0 observ figur 1 vote 10 data use took longer time use 20 50 mani pattern use 0 larg overhead ie time spent step insid loop lvi algorithm lvf call plu time lvf may exceed time simpli run lvf case vote mushroom differ time statist signific 70 use ffl increment algorithm sacrific qualiti featur select time save mainli due 1 small 0 usual portion say 10 2 rememb inconsist pattern lvi avoid check wrong guess twice qualiti measur two dimens one number featur relev featur shown figur 2 statist signific differ number featur select variou size 0 number featur lower use 100 data otherwis statist signific differ accord ttest paritymix relev 5 featur alway select plu 1 2 irrelevantredund one vote mushroom relev test done learn algorithm c45 nbc perform deterior even improv conclud featur relev experiment result shown tabl 3 4 verifi set featur two dataset via 10fold cross valid qualiti select featur warrant reduct simplifi data analysi rule induct well data collect futur ffl lvi scale time complex featur select algorithm describ along two dimens number featur n number pattern p approxim max tri lvf 77thetan reduc 2 n time complex lvf mainli determin p sinc n rel small increment version lvi make possibl start fix small number pattern eg thousand 0 matter larg origin dataset experiment result show time save statist signific p larg ffl data krvskp featur remov 10 data 100 data use indic side increment featur select lvf lvf reduc featur base smaller portion data data help reduc featur either thing equal help extend run time exampl link max tri percentag total search space 6 discuss conclus time perform lvf report first set data 1 lvf complet run fast second elaps time 2 much compar among small dataset 3 time measur lvi larg dataset indic time perform lvf lvi algorithm simpl implement fast obtain result predefin fl accord prior knowledg lvi lvf handl noisi data shown case monk3 deal multipl class valu anoth featur lvf relat socal anytim algorithm 3 algorithm whose qualiti result improv gradual comput time increas lvf print possibl solut whenev found afterward lvf report either better subset equal good one realli nice featur work hard find optim solut provid near optim solut need user wait result end search optimalsuboptim solut type search longer lvf run better solut produc one salient featur lvi scale capabl without lose qualiti select featur suggest modif sampl subset featur without replac select featur constrain subset gener newli found minimum number featur allow lvf work faster order verifi filter featur selector easili turn wrapper one lvw built prove case 14 favorit induct algorithm avail lvf easili transform lvw experiment result show lvw much slower lvf find consist result report 10 may problem use inconsist featur select criterion one featur alon social secur number guarante inconsist data obvious featur irrelev rule induct problem solv leav featur featur select process prior knowledg take one run lvf locat kind featur 7 anoth run lvf featur identifi right set featur lvf work discret featur sinc reli inconsist cal culat one way appli discret algorithm eg chi2 13 discret continu featur first one run lvf possibl 1 simpli treat continu featur discret one case 2 appli lvf discret featur number featur larg work need search new criteria addit inconsist continu lvi lvf find use well mention earlier la vega algorithm may fast domain specif heurist method lvi still play role refer design domain specif heurist method easi task verifi heurist method especi dataset involv huge lvi help case valid featur subset found heurist method anoth featur lvf may produc number equal good solut one dataset base inconsist criterion one solut chosen accord predict accuraci learn algorithm choos solut gener best accuraci suggest straightforward extens work ie combin filter wrapper model increment probabilist algorithm signific advantag lvi move one step handl larg size dataset necessari addit present repertoir 8 far algorithm autom featur select mention anoth import practic issu use domain knowledg featur select domain knowledg expert understand data help tremend featur select instanc domain knowledg use verifi find autom featur select domain knowledg use remov obvious irrelev redund featur domain knowledg also help design heurist expertis avail one alway start featur select known first appli autom select algorithm next acknowledg author would like thank hy lee suggest earlier version paper hl ong pang provid result appli lvf huge dataset japan singapor ai center thank also go manoranjan dash farhad hussain conduct experi use 7 recal one run lvf max tri loop 8 lvi lvf avail research purpos upon request lvf lvi jian shu implement nbc use experi suggest anonym refere also significantli help improv paper r toler noisi learn boolean concept presenc mani irrelev featur deliber schedul problem solv timeconstrain environ fundament algorithm featur select method classif pattern recognit statist approach featur select evalu irrelev featur subset select problem featur select problem tradit method new algorithm wrapper perform enhanc oblivi decis graph toward optim featur select select relev featur machin learn chi2 featur select discret numer attribut featur select classif probabilist wrapper approach probabilist approach featur select filter solut concept learn featur select principl construct induct uci repositori machin learn databas featur select use rough set theori branch bound algorithm featur subset select boolean featur discoveri empir learn numer recip c induct decis tree induct pattern classif method featur sen sor automat featur select monk problem perform comarison differ learn algorithm pattern recognit human mechan comput system learn critic evalu intrins dimension algorithm tr ctr stergio papadimitri seferina mavroudi liviu vladutu g pavlid anastasio bezeriano supervis network selforgan map classif larg data set appli intellig v16 n3 p185203 mayjun 2002 myungkuk park ki k lee keymok shon wan c yoon autom diagnosi rectif deflect yoke product use hybrid knowledg acquisit casebas reason appli intellig v15 n1 p2540 julyaugust 2001 weichou chen mingchun yang shianshyong tseng bitmapbas featur select method proceed acm symposium appli comput march 0912 2003 melbourn florida huilin ye bruce w n lo featur competit algorithm dimens reduct selforgan map input space appli intellig v13 n3 p215230 novemberdecemb 2000 weichou chen mingchun yang shianshyong tseng novel featur select method largescal data set intellig data analysi v9 n3 p237251 may 2005 samuel h huang dimension reduct automat knowledg acquisit simpl greedi search approach ieee transact knowledg data engin v15 n6 p13641373 novemb ki k lee wan c yoon adapt classif ellipsoid region multidimension pattern classif problem pattern recognit letter v26 n9 p12321243 1 juli 2005 xindong wu shichao zhang synthes highfrequ rule differ data sourc ieee transact knowledg data engin v15 n2 p353367 februari bill b wang r bob mckay hussein abbass michael barlow compar studi domain ontolog guid featur extract proceed twentysixth australasian confer comput scienc research practic inform technolog p6978 februari 01 2003 adelaid australia