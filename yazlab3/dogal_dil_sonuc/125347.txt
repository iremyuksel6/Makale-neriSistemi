distribut represent simpl recurr network grammat structur paper three problem connectionist account languag considered1 natur linguist representations2 complex structur relationship constitu structur represented3 appar openend natur languag accommod fixedresourc systemus predict task simpl recurr network srn train multiclaus sentenc contain multiplyembed rel claus princip compon analysi hidden unit activ pattern reveal network solv task develop complex distribut represent encod relev grammat relat hierarch constitu structur differ srn state represent tradit pushdown store discuss final section b introduct recent year consider progress develop connectionist model languag work demonstr abil network model account varieti phenomena phonolog eg gasser lee 1990 hare 1990 touretzki 1989 touretzki wheeler 1989 morpholog eg hare corina cottrel 1989 macwhinney et al 1989 plunkett marchman 1989 rumelhart mcclelland 1986b ryder 1989 spoken word recognit mcclelland elman 1986 written word recognit rumelhart mcclelland 1986 seidenberg mcclelland 1989 speech product dell 1986 stemberg 1985 role assign kawamoto mcclelland 1986 miikkulainen dyer 1989a st john mcclelland 1989 clear connectionist network mani properti make attract languag process time remain signific shortcom current work hardli surpris natur languag difficult domain pose difficult challeng paradigm challeng seen posit light test power framework also motiv develop new connectionist approach paper would like focu see three princip challeng success connectionist account languag hat natur linguist represent ow complex structur relationship constitu ow appar openend natur languag accommod fixedresourc system interestingli problem close intertwin represent press machin learn one approach address first two problem use localist represen tation localist network node assign discret interpret model eg kawamoto mcclelland 1986 st john mcclelland 1988 node may repres grammat role eg agent theme modifi relat eg subject daughterof may bound node repres wordtoken instanti either spatial assign kawamoto mcclelland 1986 miikkulainen dyer 1989b concurr activ st john mcclelland 1989 variou techniqu eg smolenski press although localist approach mani attract number import drawback well first localist dictum one nodeon concept taken togeth fact network typic fix resourc seem varianc openend natur languag node prealloc defin role subject agent order process sentenc multipl subject agent case complex sentenc must appropri number type node one know type need mani provid situat becom even troublesom one interest discours phenomena gener theori languag chomski 1965 made much unbound gener natur languag point rumelhart mcclelland 1986a realiti languag product practic fact finit length number still even one accept practic limit noteworthi soft contextsensit rather hard absolut way localist approach would predictfor instanc consid difficulti understand cat dog mous saw chase ran away compar planet astronom univers hire saw explod clearli semant pragmat consider facilit pars structur otherwis hard process see also labov 1973 reich dell 1977 schlesing 1968 stolz 1967 experiment demonstr point thu although one might anticip commonli occur structur relat one would like limit process soft rather hard way localist approach would second shortcom use localist represent often underestim actual rich linguist structur even basic notion word one might assum straightforward linguist primit turn difficult defin one might thought dramat differ term count word across languag even within english morpholog syntact process yield entiti wordlik respect eg appl pie maninthestreet man season fact much linguist theori today concern natur role represent less focu natur oper thu localist approach certain posit aspect definit shortcom well provid good solut problem account openend natur languag commit discret welldefin represent may make difficult captur rich high dimension requir languag represent anoth major approach involv use distribut represent hinton 1988 hinton mcclelland rumelhart 1986 van gelder press togeth learn algorithm order infer linguist represent model use localist approach typic made priori commit linguist represent agent patient etc network explicitli train identifi represent input activ node correspond presuppos target represent theoret valid also beg question real world correspond teach inform might come altern approach task must devis abstract linguist represent play explicit role model input output target limit variabl directli observ environ naturalist approach sens model learn use surfac linguist form commun purpos rather linguist analysi whatev linguist analysi done whatev represent develop intern network servic task valu approach need depend preexist preconcept abstract linguist represent instead connectionist model seen mechan gain new theoret insight thu approach offer potenti satisfi answer first question natur linguist represent second advantag approach abstract represent form hidden layer also tend distribut across highdimension continu space describ analog hidden unit activ vec tor mean larger much finergrain represent space work usual possibl localist represent space infinit practic purpos may larg approach may also provid better respons third question appar openend natur languag accommod fixedresourc system rosi still left second question repres complex structur relationship constitu distribut represent far complex difficult understand localist represent tendenc feel murki intract distribut entail unanalyz although fact exist variou techniqu analyz distribut represent includ cluster analysi elman 1990 hinton 1988 sejnowski rosenberg 1987 servanschreib cleereman mcclelland press direct inspec tion pollack 1988 princip compon phase state analysi elman 1989 contribut 4analysissang 1989 result studi limit analys demonstr distribut represent may poss intern structur encod relationship kinship hinton 1987 lexic categori structur elman 1990 relationship static thu instanc elman 1990 network train predict order word sentenc network learn repres word categor noun verb subcategor noun animateinanim humannonhuman etc represent develop network explicitli taught lexic categori sure import languag process easi think sort categor seem differ natur consid follow sentenc 1 boy broke window b rock broke window c window broke underlin word sentenc noun represent reflect nounhood categori properti belong inalien word true regardless appear noun deriv process may result noun use verb viceversa differ level de scription underlin word also similar categoriz subject sentenc properti howev contextdepend word win dow subject sentenc 1c two sentenc object still anoth level descript three underlin word differ 1a subject also agent event 1b subject instrument 1c subject patient theme sentenc contextdepend properti exampl simpl demonstr effect grammat structur structur manifest level utter addit contextfre categor word inherit properti virtu linguist environ although distribut represent seem potenti abl respond first last problem pose outset clear address question h ow complex structur relationship constitu repres fodor pylyshyn 1988 phrase need two degre freedom specifi thought intent system entertain time one paramet activ vs inact pick node express concept system mind construct vs determin concept system mind distribut propost entertain pp 2526 point worth remind way complex structur relationship dealt symbol system contextfre properti typic repres abstract symbol np v etc contextsensit properti dealt variou way theori eg gener phrase structur grammar design context explicit manner socal slashcatego rie approach use addit categori label eg cognit grammar relat grammar govern bind design element subject theme argu ment trajectori path etc addit theori may make use tree bracket coin dex spatial organ tier arc circl diacrit order convey complex relationship map process implement version exist theori nearli requir work buffer stack order account appar recurs natur utter rather formid armamentarium requir return three question pose outset although distribut represent characterist plausibl may address need represent rich flexibl may provid soft rather hard limit process must ask whether approach captur structur relationship sort requir languag question motiv work report preliminari evid encourag regard hinton 1988 describ scheme involv reduc descript complex structur repres partwhol hierarchi pollack 1988 press develop train regimen call recurs autoassoci memori raam appear composit properti support structuresensit oper see also chalmer 1989 discuss earlier elman 1990 use simpl recurr network srn servanschreib cleereman mcclelland press provid yet anoth approach encod structur relationship distribut form work describ extend latter approach srn taught task involv stimuli underli hierarch recurs relat ship structur abstract sens implicit stimuli goal see network could infer abstract structur b repres composit relationship manner support structuresensit oper remaind paper organ follow first network architectur briefli introduc second stimulu set task present properti task make particularli relev question hand describ next result simul present final discuss differ similar approach tradit symbol approach languag process discuss network architectur time import element languag question repres serial order input crucial variou propos advanc review see elman 1990 mozer 1988 approach taken involv treat network simpl dynam system previou state made avail addit input jordan 1986 jordan work network state anyh point time function input current time step plu state output unit previou time step work network state depend current input plu intern state repres hidden unit previou cycl hidden unit taught assum specif valu mean develop represen tation cours learn task encod tempor structur task word hidden unit learn becom kind memori taskspecif insert figur 1 type network use current work shown figur 1 network typic connect input unit hidden unit hidden unit output unit addit hidden layer input main hidden main hidden output may use serv transduc compress input output vector addit set unit call context unit provid limit recurr may call simpl recurr network context unit activ oneforon basi hidden unit fix weight 10 linear activ function result time cycl hidden unit activ copi context unit next time cycl context combin new input activ hidden unit hidden unit therefor take job map new input prior state output constitut prior state must develop represent facilit inputoutput map simpl recurr network studi number task elman 1990 gasser 1989 hare press task stimuli predict task elman 1990 network similar figur 1 train predict order word simpl 2 3word sentenc point time word present network network target output simpli next word sequenc lexic item input output repres localist form use basi vector ie word randomli assign vector singl bit turn lexic item thu orthogon one anoth form item encod inform item categori membership predict made basi current input word togeth prior hidden unit state save context unit task chosen sever reason first task meet desideratum input target output limit observ environ net work input output immedi avail requir minim priori theoret 7analysi lexic item orthogon arbitrarili assign role extern teacher minim sinc target output suppli environ next moment time task involv might call selfsupervis learn ing second although languag process obvious involv great deal predict predict seem play role process listen inde predict grosjean 1980 sequenc word violat expectationsi un predictableresult distinct electr activ brain kuta 1988 kuta hill yard 1980 tanenhau et al press third accept predict anticip play role languag learn provid partial solut call baker paradox baker 1979 pinker 1989 paradox children appar receiv ignor neg evid process languag learn given frequent tendenc initi overgener posit data clear children abl retract faulti overgener gold 1967 howev suppos children make covert predict speech hear other fail predict constitut indirect sourc neg evid could use refin retract scope gener fourth task requir network discov regular underli tempor order word sentenc simul report elman regular result network construct intern represent input mark word form class nounverb well lexicosemant characterist animateinanim humananim largesmal etc result simul howev bore represent lexic categori structur relev grammat structur unclear mono clausal sentenc use share basic structur thu question remain open whether intern represent learn architectur abl encod hierarch relationship necessari mark constitu structur stimuli stimuli simul sequenc word form sentenc addit monoclaus sentenc larg number complex multiclaus sentenc sentenc form lexicon 23 item includ 8 noun 12 verb rel pronoun endofsent indic period item repres randomli assign 26bit vector singl bit set 1 3 bit reserv anoth purpos phrase structur grammar shown tabl 1 use gener sentenc result sentenc possess certain import properti includ follow insert tabl 1 agreement subject noun agre verb thu exampl 2a grammat 2b train corpu consist posit exampl star exampl actual occur 2a john feed dog 2b boy see mari word mark number singularplur form class verbnoun etc grammat role subjectobject etc network must learn first item function would call noun verb etc must learn item exampl singular plural must learn noun subject object sinc agreement hold subject noun verb b verb argument structur verb fall three class requir direct object permit option direct object preclud direct object result sentenc 3ad grammat wherea sentenc 3e 3f ungrammat 3a girl feed dog requir 3b girl see boy option 3c girl see option 3d girl live preclud 3e girl feed 3f girl live dog word repres orthogon vector type verb overtli mark input class membership need infer time cooccurr fact learn c interact rel claus agreement verb argument fact becom complic rel claus although direct object normal follow verb simpl sentenc rel claus subordin claus direct object head claus case network must recogn gap follow subordin claus verb direct object role alreadi fill thu normal pattern simpl sentenc 3ad appear also 4a contrast 4b 4a dog chase cat see girl 4b dog cat chase see girl hand sentenc 4c seem conform pattern establish 3 4a ungrammat 4c dog cat chase dog see girl similar complic aris agreement fact simpl declar sentenc agreement involv n1 v 1 complex sentenc 5a regular vio late straightforward attempt gener sentenc multipl claus would lead ungrammat 5b 5a dog boy feed see girl 5b dog boy feed see girl recurs grammar permit recurs presenc rel claus expand noun phrase may introduc yet rel claus etc lead sentenc 6 grammat phenomena note ac may extend consider distanc dog chase see hear viabl sentenc one liter insert grammar occur end sen tenc endofsent marker potenti occur anywher string grammat sentenc might termin thu sentenc 7 caret indic posit might legal occur data 47 exampl sort phenomena linguist argu account without abstract represent precis claim abstract represent offer perspicaci account grammtic phenomena one exampl simpli list surfac string chomski 1957 train data gener grammar summar tabl 1 given point train train set consist 10000 sentenc present network 5 time sentenc concaten input stream proceed smoothli without break sentenc howev composit sentenc vari time follow train regimen use order provid increment train network train 5 pass follow 4 corpora phase 1 first train set consist exclus simpl sentenc accomplish elimin rel claus result corpu 34605 word form 10000 sentenc sentenc includ termin phase 2 network expos second corpu 10000 sentenc consist 25 complex sentenc 75 simpl sentenc complex sentenc obtain permit rel claus mean sentenc length 392 minimum 3 word maximum 13 word phase 3 third corpu increas percentag complex sentenc 50 mean sentenc length 438 minimum 3 word maximum 13 word phase 4 fourth consist 10000 sentenc 75 complex 25 simpl mean sentenc length 602 minimum 3 word maximum stage learn strategi develop respons result earlier pilot work work found network unabl learn task given full rang complex data begin train howev network permit focu simpler data first abl learn task quickli move success complex pattern import aspect earlier train constrain later learn use way earli train forc network focu canon version problem appar creat good basi solv difficult form problem result conclus fourth phase train weight frozen final valu network perform test novel set data gener way last train corpu task nondeterminist network unless memor sequenc alway produc error optim strategi case activ output unit ie predict potenti next word extent proport statist likelihood occurr therefor rather assess network global perform look root mean squar error ask close network approxim probabl techniqu describ elman press use accomplish contextdepend likelihood vector gener word everi sentenc vector repres empir deriv probabl occurr possibl predict given sentenc context point network actual output compar likelihood vector error use measur perform error quit low 0177 initi error 1245 minim error equal activ unit would 192 error also normal comput mean cosin angl vector 0852 sd 0259 measur indic network achiev high level perform predict gross measur perform howev tell us well network done specif problem area pose task let us look area turn agreement simpl sentenc agreement simpl sentenc shown figur 2a 2b insert figur 2 network predict follow word boy either singular verb follow word three singular verb categori activ sinc basi predict type verb els next word may rel pronoun convers input word boy expect verb plural follow els rel pronoun similar expect hold noun lexicon result follow perform sentenc shown repres sentenc similar structur b verb argument structur simpl sentenc figur 3 show network predict follow initi noun verb three differ verb type insert figur 3 verb live network expect follow item fact successor permit grammar context verb see hand may either follow option direct object may singular plural noun proper noun final verb chase requir direct object network learn expect noun follow verb class c interact rel claus exampl far involv simpl sentenc agreement verb argument fact complic complex sentenc figur 4 show network predict word sentenc boy mari chase feed cat network gener pattern agreement found simpl sentenc might expect network predict singular verb follow mari chase insofar predict verb posit convers might confus pattern n1 n2 v1 fact predict 4d correctli next verb singular order agre first noun found mechan repres longdist depend main claus noun main claus verb despit presenc interven noun verb agreement relat rel claus insert figur 4 note sentenc also illustr sensit interact verb argument structur rel claus structur verb chase take obligatori direct object simpl sentenc direct object follow verb immedi also true mani complex sentenc eg boy chase mari feed cat sentenc display howev direct object boy head rel claus appear verb requir network learn item function noun verb etc b item fall class c subclass verb differ cooccurr relat noun correspond verbdirect object restrict verb fall class e expect direct object follow verb know alreadi appear network appear learn panel see expect chase follow verb main claus verb case rather noun even subtler point demonstr 4c appear boy follow rel claus contain differ subject mari prime network expect verb follow must class requir direct object precis direct object filler alreadi appear word network correctli respond presenc filler boy know expect gap follow chase also learn filler correspond object posit rel claus verb requir appropri argument structur network analysi natur question ask point network learn accomplish task success task seem constitut prima faci evid exist 3of intern represent possess abstract structur seem reason believ order handl agreement argument structur fact presenc rel claus network would requir develop represent reflect constitu structur argument structur grammat categori grammat relat number least sort infer made case human languag user base behavior data one advantag work artifici system take addit step directli inspect intern mechan gener behavior cours mechan find necessarili use human li tener may nonetheless surpris find solut problem might guess hierarch cluster use analyt tool help understand internatl represent learn network contribut solv problem cluster diagram hidden unit activ pattern good repres similar structur represent space howev certain lim itat one weak provid indirect pictur represent space anoth shortcom tend deemphas dynam involv process state may signific simpli term similar state regard way constrain movement subsequ state space recal exampl 1 import part network learn lie dynam involv process word sequenc inde one might think network dynam encod grammat knowledg certain sequenc word move network welldefin permiss intern state sequenc move network permiss state sequenc permit ungrammat might therefor wish abl directli inspect intern state repres hidden unit activ vector network process word sequenc order see state trajectori encod network grammat knowledg unfortun high dimension hidden unit activ vector simul 70 dimens make impract view state space directli furthermor guarante dimens interest us sens pick region import network solut task correl dimens code hidden unit inde mean represent distribut dimens variat cut across degre dimens pick hidden unit howev reason assum dimens variat exist tri identifi use princip compon analysi pca pca allow us find anoth set dimens rotat axe along maximum variat occur may addit reduc number variabl effect remov linearli depend set axe new axe permit us visual state space way hope allow us see network solv task shortcom pca linear howev combin pca factor next level may nonlinear represent inform may give incomplet pictur actual comput dimens eigenvector associ eigen valu magnitud indic amount varianc account di mension allow one focu dimens may particular signific also allow post hoc estim number hidden unit might actual requir task figur 5 show graph eigenvalu 70 eigenvector extract insert figur 5 agreement sentenc 8 present network hidden unit pattern captur word process sequenc boy hear boy 8c boy boy chase chase boy 8d boy boy chase chase boy sentenc chosen minim differ due lexic content make possibl focu differ grammat structur 8a 8b contain train data 8c 8d novel never present network learn examin trajectori state space along variou dimens appar second princip compon play import role mark number main claus subject figur 6 show trajectori 8a 8b trajectori overlaid differ readili seen path similar practic term anaylsi involv pass train set train network weight frozen save hidden unit pattern produc respons input covari matrix result set hidden unit vector calcul eigenvector covari matrix found eigenvector order magnitud eigenvalu use basi describ origin hidden unit vector new set dimens effect give somewhat local descript hidden unit pattern new dimens correspond locat meaning activ defin term varianc hyperspac sinc dimens order term varianc account may wish look select dimens start largest eigenvalu see fluri 1988 detail explan pca gonzalez wintz 1977 detail descript algorithm diverg first word indic differ number initi noun differ slight elimin main ie second chase verb input appar two sentenc grammar number inform relev task main verb receiv insert figur 6 difficult imagin sentenc number inform may retain interven constitu sentenc 8c 8d exampl sentenc ident rel claus follow initi noun differ regard number two sentenc materi boy chase irrelev far agreement requir main claus verb trajectori state space two sentenc overlaid shown figur 7 seen differ two trajectori maintain main claus verb reach point state converg insert figur 7 verb argument structur represent verb argument structur examin probe sentenc contain instanc three differ class verb sampl sentenc shown 9 boy walk boy see boy 9c boy chase boy first contain verb may take direct object second take option direct object third requir direct object movement state space three sentenc process shown figur 8 insert figur 8 figur illustr network encod sever aspect grammat structur noun distinguish role subject noun three sentenc appear upper right portion space object noun appear princip compon 4 shown encod distinct verb noun collaps across case verb differenti regard argument structur chase requir direct object see take option direct object walk preclud object differ reflect systemat displac plane princip compon 1 3 rel claus presenc rel claus introduc complic grammar represent number verb argument structur must clausespecif would use network way repres constitu structur sentenc train network given follow sentenc 10a boy chase boy 10b boy chase boy chase boy 10c boy chase boy chase boy 10d boy chase boy chase boy chase boy first sentenc simpl three instanc embed sentenc contain train data sentenc 10c 10d 10e novel present network learn phase trajectori state space four sentenc princip compon 1 11 shown figur 9 panel 9a show basic pattern associ fact matrix sentenc four sentenc comparison figur panel 9b 9c show trajectori matrix sentenc appear follow matrix subject noun lower left region state space matrix verb appear left matrix object noun near upper middl region recal look 2 70 dimens along dimens nounverb distinct preserv categor rel claus appear involv replic basic pattern displac toward left move slightli ward rel matrix constitu moreov exact posit rel claus element indic matrix noun modifi thu rel claus modifi subject noun closer rel claus modifi object noun closer trajectori pattern found sentenc grammat form pattern thu systemat insert figur 9 figur 9d show happen multipl level embed success embed repres manner similar way first embed claus distinguish main claus basic patter claus replic region state space displac matrix materi displac provid systemat way network encod depth embed current state howev reliabl encod limit precis state repres turn depend factor number hidden unit precis numer valu current simula tion represent degrad three level embed consequ degrad perform predict task differ differ type sentenc sentenc involv center embed eg 9c 9d level embed crucial maintain correct agreement advers affect sentenc involv socal tailrecurs eg 10d latter sentenc 7the syntact structur principl involv recurs practic level embed relev task ie affect agreement verb argument structur way figur 9d interest anoth respect given natur predict task actual necessari network carri forward inform prior claus would suffici network repres success rel claus iter previou pattern yet two rel claus differenti similarli servanschreib cleereman mcclelland press found simpl recurr network taught predict input gener finit state automaton network develop intern represent correspond fsa state howev also redundantli made finergrain distinct encod path state achiev even though inform use task thu seem properti network abl encod state way minim context far behavior concern nonlinear natur allow remain sensit context level intern represent discuss basic question address paper whether connectionist model capabl complex represent possess intern structur product estens question particularli interest regard gener issu use connectionist paradigm framework cognit model context natur represent interact number close relat issu order understand signific present result may use first consid briefli two issu first statu rule whether exist whether explicit implicit second notion comput power whether suffici whether appropri sometim suggest connectionist model differ classic model latter reli rule wherea connectionist model typic rule sy tem although first glanc appear reason distinct actual clear distinct get us far basic problem obviou meant rule gener sens rule map take input yield output clearli sinc mani although neural network function inputoutput system bulk machineri implement transform difficult see could thought rulesystem perhap meant form rule differ classic model connectionist network one suggest rule state explicitli former wherea implicit network slipperi issu unfortun ambigu meant implicit explicit one sens explicit rule physic present system form rule furthermor physic presenc import correct function system howev kirsh 1989 point intuit count physic presenc highli unreli sometim contradictori seem realli stake speed inform made avail true kirsh argu point persuas qualiti explicit belong data structur alon one must also take account natur process system involv sinc inform form may easili access one process system inaccess anoth unfortun understand inform process capac neural network quit preliminari strong tendenc analyz network view tradit lens suppos inform contain form familiar comput system inform somehow bur i inaccess implicit consid instanc network success learn complic map say text pronunci sejnowski rosenberg 1987 inspect result network immedi obviou explain map work even character map precis way case tempt say network learn implicit set rule realli mean map complic difficult formu late even unknown rather descript failur understand mechan rather descript mechan need new techniqu network analysi princip compon analysi use present work contribut analysi sanger 1989 weight matrix decomposit mc millan smolenski 1988 skeleton mozer smolenski 1989 success analys connectionist network may provid us new vocabulari understand inform process wemay learn new way inform explicit implicit may learn new notat express rule underli cognit notat new connectionist rule may look differ use exampl product rule may expect notat lend describ type regular equal facil thu potenti import differ connectionist model classic model whether one system contain rule whether one system encod inform explicitli encod implicitli differ lie natur rule kind inform count explicitli present potenti differ bring us second issu comput power issu divid two consider connectionist model provid suffici comput power account cognit phenomena provid appropri sort comput power first question answer affirm import qualif shown multilay feedforward network one hidden layer squash output arbitrari nonlinear activ function hidden layer capabl arbitrarili accur approxim arbitrari map thu belong class univers approxim hornik stinchcomb white press stinchcomb white 1989 pollack 1988 also proven ture equival neural network principl network capabl implement function classic system implement import qualif result suffici mani hidden unit provid case pollack proof weight infinit precis current known effect limit resourc comput power sinc human cognit carri system rel fix limit resourc question paramount interest limit provid critic constraint natur function map import empir question whether constraint explain specif form human cognit context question appropri comput power becom interest given limit resourc relev ask whether kind oper represent natur made avail like figur human cognit one theori cognit requir sort randomli order inform eg word frequenc list forster 1979 model lexic access becom extrem import comput framework provid effici support sort oper hand one believ inform store associ abil system fast sort irrelev instead import model provid associ storag retriev 1 cours thing work direct avail certain type oper may encourag one build model type impract framework need work inappropri comput mechan may blind us see thing realli let us return current work would like discuss first way work preliminari limit discuss see posit contribut work final would like relat work connectionist research gener question rais outset discuss viabl connectionist model understand cognit result preliminari number way first one imagin number exampl suggest norman addit test could perform test represent capac simpl recurr network memori capac remain unprob see servan schreiber cleereman mcclelland press gener test limit way mani test involv novel sentenc one would like know whether network inferenti extend know type noun phrase encount second simul simpl noun rel claus noun phrase differ structur second true agreement verb argument structur fact contain present grammar import challeng bare scratch surfac term rich linguist phenomena character natur languag third natur languag contain far complex regard syntact structur also semant aspect inde langack 1987 other argu persuas fruit consid syntax semant autonom aspect languag rather form mean languag close entwin although may thing learn studi artifici languag present one pure syntact natur languag process crucial attempt retriev mean linguist form present work address issu pdp model made progress problem eg st john mcclelland press current work contribut notion represent capac connectionist model variou writer eg fodor pylyshyn 1988 express concern regard abil connectionist represent encod composit structur provid openend gener capac network use simul report two import properti relev concern first network make possibl develop intern represent distribut hinton 1988 hinton mcclelland rumelhart 1986 un bound distribut represent less rigidli coupl resourc localist represent strict map concept individu node also greater flexibl determin dimens import model second network studi build sensit context import result current work suggest sensit context characterist mani connectionist model builtin architectur network use preclud abil captur gener high level abstract paradox sensit context precis mechan underli abil abstract gener fact network exhibit behavior highli regular learn context 1insensit rather learn respond context abstractli de fine recal even network behavior seem ignor context eg figur 9d servanschreib cleereman mcclelland press intern represent reveal contextu inform still retain behavior strike contrast tradit symbol model represent system natur contextinsensit insensit make possibl express gener fulli regular highest possibl level represent eg pure syntact requir addit apparatu account regular reflect interact mean form contextu defin connectionist model hand begin task abstract end continuum emphas import context interact form mean current work demonstr characterist lead quit natur gener high level abstract appro priat behavior remain everroot represent contextu ground simul report capit subtl distinct con text ampl demonstr model eg kawamoto 1988 mcclelland kawamoto 1986 miikkulainen dyer 1989 st john mcclelland press final wish point current approach suggest novel way think mental represent construct languag input convent wisdom hold word heard listen retriev lexic rep resent although represent may indic context word accept occur represent contextfre exist canon form constant across occurr lexic form use assist construct complex represent form sert one imagin complet result elabor structur word visibl also depict abstract grammat structur bind word account process build mental structur unlik process build physic structur bridg hous word whatev represent element involv play role build block true bridg hous build block unaffect process construct differ imag suggest approach taken word process separ stage lexic retriev represent word isol represent word intern state follow input word alway reflect input taken togeth prior state scenario word build block much cue guid network differ grammat state word distinct virtu differ causal properti ametaphor captur characterist approach combin lock metaphor role word analog role play number combin number causal properti advanc lock differ state effect number depend context enter correct sequenc number move lock open state open state may said function composit van gelder press sens reflect particular sequenc event number present insofar respons final state still physic present limit combin lock cours one correct combin network studi complex causal properti word highli structuredepend network allow mani open ie gram matic state view languag comprehens emphas function import represent similar spirit approach describ bate macwhinney 1982 mcclelland st john taraban 1989 mani other stress function natur languag represent languag construct order accomplish behavior obvious behavior may rang daydream ing verbal duel ask direct compos poetri represent proposit inform content chang constantli time accord demand current task word serv guidepost help establish mental state support behavior represent snapshot mental state acknowledg grate mani use discuss topic jay mcclelland dave rumelhart elizabeth bate steve stich member ucsd pdpnlp research group thank mcclelland mike jordan mari hare ken baldwin two anonym review critic comment earlier version paper research support contract n0001485k0076 offic naval research contract daab0787ch027 armi avion ft monmouth request reprint sent center research languag 0126 univers california san diego 920930126 author reach via electron mail elmancrlucsdedu r sytntact theori project problem functionalist approach grammar mean structur languag syntact transform distribut represent center research concept cognit syntact structur spread activ theori retriev sentenc product symbol schemata connectionist memori role bind evolut structur univers california implement connectionist product system use tensor product represent structur connectionist model find structur time mental space connectionist model properti cognit scienc frame semant common princip compon relat multivari model languag thought connection cognit architectur critic analysi level process structur languag processor network learn phonolog syntax functionaltypolog introduct spoken word recognit process gate paradigm knowledg represent connectionist network role similar hungarian vowel harmoni connectionist account connectionist perspect prosod structur repres partwhol hierarchi connectionist network technic report crgtr882 distribut represent transit grammar discours serial order parallel distribut process approach institut cognit scienc report 8604 distribut represent ambigu word resolut connectionist network function syntax anaphora read senseless sentenc brain potenti reflect semant incongru foundat cognit grammar theoret perspect usagebas model languag learn cue rule tempor structur spoken languag understand analyz connectionist model system soft rule ca morgan kaufmann publish focus backpropag algorithm tempor pattern recognit skeleton techniqu trim fat network via relev assess semant constraint judg prefer interpret ambigu sentenc learnabl cognitiion acquisit argument structur recurs autoassoci memori decis composit distribut represent philosoph implic connection learn intern represent error propag interact knowledg sourc spoken word identif contribut analysi techniqu assign respons hidden unit connectionist network linguist compet parallel network learn pronounc english text connectionist system rule base reason multiplac predic variabl proper treatment connection lexicon model languag product univers approxim use feedforward network nonsigmoid hidden layer activ function studi abil decod grammat novel sentenc journal verbal learn verbal behavior reconcil connection recurs natur stack tree rule map connectionist symbol process technic report cmucs89158 mani map symbol among neuron detail connectionist infer architectur connectionist implement cognit phonolog graph network predict follow sequenc boy live graph network predict word sentenc boy mari chase feed dog graph eigenvalu 70 order eigenvector extract simul 2 trajectori state space sentenc 8a 8b trajectori state space process 8c 8d trajectori state space sentenc 9a trajectori state space sentenc 10ad boy mari chase feed cat tr ctr jame henderson peter lane connectionist architectur learn pars proceed 17th intern confer comput linguist august 1014 1998 montreal quebec canada jame henderson segment state entiti implic learn emerg neural comput architectur base neurosci toward neuroscienceinspir comput springerverlag new york inc new york ny 2001 imran maqsood muhammad riaz khan ajith abraham intellig weather monitor system use connectionist model neural parallel scientif comput v10 n2 p157178 june 2002 matthew h tong adam bickett eric christiansen garrison w cottrel 2007 special issu learn grammat structur echo state network neural network v20 n3 p424432 april 2007 k rahman wang pi yang tommi w chow sitao wu flexibl multilay selforgan map gener process treestructur data pattern recognit v40 n5 p14061424 may 2007 jame henderson neural network parser handl spars data new develop pars technolog kluwer academ publish norwel 2004 kathrin hammervold sentenc gener neural network proceed first intern confer natur languag gener june 1216 2000 mitzp ramon israel michael gasser acquir recept morpholog connectionist model proceed 32nd annual meet associ comput linguist p279286 june 2730 1994 la cruce new mexico junghua wang yiwei yu jiahorng tsai intern represent product unit neural process letter v12 n3 p247254 dec 1 2000 marcin chadi model higher cognit function hebbian cell assembl emerg neural comput architectur base neurosci toward neuroscienceinspir comput springerverlag new york inc new york ny 2001 heejin lim yoonsuck choe facilit neural dynam delay compens predict evolutionari neural network proceed 8th annual confer genet evolutionari comput juli 0812 2006 seattl washington usa peter c r lane jame b henderson increment syntact pars natur languag corpora simpl synchroni network ieee transact knowledg data engin v13 n2 p219231 march 2001 hinrich schtze partofspeech induct scratch proceed 31st annual meet associ comput linguist p251258 june 2226 1993 columbu ohio symbolicconnectionist systemfor word sens disambigu appli intellig v7 n1 p526 januari 1997 xindi cai nian zhang ganesh k venayagamoorthi donald c wunsch ii time seri predict recurr neural network train hybrid psoea algorithm neurocomput v70 n1315 p23422353 august 2007 stephen jo hanson michiro negishi emerg rule neural network neural comput v14 n9 p22452268 septemb 2002 paul rodriguez simpl recurr network learn contextfre contextsensit languag count neural comput v13 n9 p20932118 septemb 2001 imran maqsood ajith abraham weather analysi use ensembl connectionist learn paradigm appli soft comput v7 n3 p9951004 june 2007 samuel w k chan integr linguist primit learn contextdepend represent ieee transact knowledg data engin v13 n2 p157175 march 2001 steve lawrenc c lee gile sandiway fong natur languag grammat infer recurr neural network ieee transact knowledg data engin v12 n1 p126140 januari 2000 ahmad emami frederick jelinek neural syntact languag model machin learn v60 n13 p195227 septemb 2005 bechtel compat complex system reduct case analysi memori research mind machin v11 n4 p483502 novemb 2001 w f g haselag j f h van rappard connection systemat frame problem mind machin v8 n2 p161179 novemb 1998 sheila garfield stefan wermter call classif use recurr neural network support vector machin finit state automata knowledg inform system v9 n2 p131156 februari 2006 sheila garfield stefan wermter siobhan devlin spoken languag classif use hybrid classifi combin intern journal hybrid intellig system v2 n1 p1333 januari 2005 stephan k chalup alan blair increment train first order recurr neural network predict contextsensit languag neural network v16 n7 p955972 septemb stefan c kremer spatiotempor connectionist network taxonomi review neural comput v13 n2 p249306 februari 2001 stefan wermter knowledg extract transduc neural network appli intellig v12 n12 p2742 januaryapril 2000