reinforc learn call admiss control rout qualiti servic constraint multimedia network paper solv call admiss control rout problem multimedia network via reinforc learn rl problem requir network revenu maxim simultan meet qualiti servic constraint forbid entri certain state use certain action problem formul constrain semimarkov decis process show rl provid solut problem abl earn significantli higher revenu altern heurist b introduct number research recent explor applic reinforc learn rl resourc alloc admiss control problem telecommu nicat eg channel alloc wireless system network rout admiss control telecommun network nie haykin 1998 singh 1997 boyan littman 1994 marbach et al 1998 paper focus applic rl method call admiss control cac rout broadband multimedia commun network atm network broadband network carri heterogen traffic type simultan channel channel packetbas custom send vari rate time call arriv depart time network choos accept reject connect request new call accept network choos appropri rout deliv call sourc node destin node network provid qualiti servic qo guarante packet level eg maximum probabl congest call level eg limit call block probabil iti return network collect revenu payoff custom call accept network network want find cac rout polici maxim long term revenueutil meet qo constraint maxim revenu meet qo constraint suggest constrain semimarkov decis process smdp mitra et al 1998 rapid growth number state problem complex led rl approach prob lem marbach tsitsikli 1997 marbach et al 1998 howev rl applic ignor qo criteria work draw close relat fundament problem constrain optim semimarkov decis process studi research control theori oper search artifici intellig commun see eg altman shwartz 1991 feinberg 1994 gabor et al 1998 unlik modelbas algorithm eg linear program mitra et al 1998 rl algorithm use paper stochast iter algorithm requir priori knowledg state transit probabl associ underli markov chain thu use solv real network problem larg state space handl modelbas algorithm automat adapt real traffic condit work build earlier work author brown et al 1999 provid gener framework studi cac rout problem qo constraint also provid detail inform proof rl algorithm use studi contain result combin cac rout multimedia network report tong brown 1999 section 2 describ problem model use studi section 3 formul cac problem smdp give rl algorithm solv smdp section 4 consid qo constraint detail simul cac singl link system present section 5 combin cac network rout studi section 6 simul result 4node 12link network section 7 conclud paper 2 problem descript section describ cac problem singlelink commun system substanti literatur cac one link multiservic network eg marbach tsitsikli 1997 mitra et al 1998 refer dziong mason 1994 singl link case signific sinc basic build block larger network shown section 6 paper combin cac rout multilink network system decompos singl link process thu first focu singlelink system user attempt access link time network immedi choos accept reject call accept call gener traffic term bandwidth function time later time call termin depart network call accept network receiv immedi revenu payment network measur qo metric transmiss delay packet loss ratio call reject probabl servic class compar guarante given call problem describ call arriv traffic departur process revenu payment qo metric qo constraint network model concret describ choic use later exampl call divid discret class index call gener via independ poisson arriv process arriv rate exponenti hold time mean hold time 1 within call bandwidth onoff process traffic either gener packet rate r rate zero mean hold time 1 1 class call admit system collect reinforc learn call admiss control 3 fix amount revenu interpret averag reward carri igammath class call dziong mason 1994 network element connect network fix bandwidth b total bandwidth use accept call vari time one import packetlevel qo metric fraction time total bandwidth exce network bandwidth caus packet loss ie congest probabl choos packetlevel qo guarante upper limit congest probabl p denot capac constraint previou work eg carlstrom nordstrom 1997 marbach et al 1998 call constant bandwidth time effect qo predict variabl rate traffic safe approxim assum alway transmit maximum peak rate peak rate alloc underutil network case order magnitud less possibl network effici improv statist multiplex statist bursti sourc unlik simultan commun peak rate thu possibl carri bursti variabl rate traffic would possibl alloc capac accord peak rate requir maintain servic qualiti stochast traffic rate real traffic desir high network utilizationrevenu result potenti qo violat character problem studi anoth import qo metric calllevel block probabl offer traffic class must cut back meet capac constraint import fairli denot fair constraint fair defin number differ way one intuit notion call everi class entitl admiss probabl equival reject probabl dziong mason 1994 precis defin section 4 ultim goal find polici everi system state choos correct control action maxim revenu subject qo constraint formal consid follow problem find cac polici maxim j 0 1 subject j j l fset policiesg 3 k number qo constraint l real number character qo constraint j 0 character averag network revenu polici j j character qo polici consid object form k action chosen state n accord polici reward function associ revenu assum bound n averag sojourn time state n action n index ngammath decis epoch decis made point time refer decis epoch 3 semimarkov decis process reinforc learn follow section develop compon problem finish justifi particular method suitabl cac problem 31 state action section develop state action model reduc state space represent suitabl cac problem cac problem formul semimarkov decis process smdp state transit control select take place discret time time one transit next continu random variabl given point time system particular configur x defin number type ongo call number call state type random time event e occur one event occur time instant e gammavector indic either class call arriv call termin call turn call turn event configur event togeth determin state system e iclass system 3i dimension vector sinc number possibl choic e gener small compar x size state space domin configur part state shown use nearli complet decompos approxim reduc state descriptor form stand call arriv departur event class let configur e denot gammavector whose element equal zero except ith element whose valu uniti state associ class call arriv state associ class call departur reduct ignor number call state event call turn give us enough accuraci cac problem shown experiment mitra et al 1998 give two reason simplif first moment call turn decis point admiss control therefor action need taken theorem 2 appendix show ignor event call turn valid section 34 also provid discuss similar simplif second intuit clear simplif good approxim process describ number call state reach equilibrium chang number call progress due call arrivaldepartur henc make call admiss decis number call class progress import number call class state quantiti oscil rapidli rel call arriv departur view ignor state aggreg assum fix x qvalu chang much differ reinforc learn call admiss control 5 discuss section 36 justifi reduct drop note affect congest probabl assum process reach equilibrium correspond fix x assum sourc independ probabl configur x given binomi distribut b fraction time class call spend state averag congest probabl class fix x thu x 0 1fdeltag indic function averag congest probabl depend x capac constraint associ 6 conserv set set x long run averag packetlevel qo constraint alway satisfi never go state period time capac constraint violat stay forev set c c uniqu determin state space x e mitra et al 1998 consid aggress approach packetlevel qo constraint averag across allow configur x let x2ca x total system time x portion system spend x c set allow configur x x2ca less equal target p obvious c uniqu c c possibl c although gener conserv occas emphasi depend c c c p also write c c p c p summari choos state descriptor number class call progress e stand new class call arriv gamma class call departur 1 event occur learner choos action feasibl event action set asf0reject 1acceptg upon new call arriv call termin decis point action need taken symbol 6 h tong tx brown state asfgamma1no action due call departuresg note action avail state gener depend exampl ad new call state violat capac constraint action set state constrain f0g subsequ random time anoth event occur cycl repeat revenu structur cac task learner determin polici accept call given maxim longrun averag revenu infinit horizon meet qo requir cac system constitut finit state space eg due capac constraint finit action space afgamma101g semimarkov decis process 32 transit probabl section consid probabl model conclud larg state space classic approach base transit probabl model fea sibl theoret state transit probabl ps 0 probabl go state action next state 0 deriv mitra et al 1998 depend configur x call arriv rate exact system model often infeas sever import reason first call arriv rate may depend call class also configur x dziong mason 1994 therefor call arriv rate class may constant gener second network reason size state space extrem larg exampl 4node 12link network 3 servic type 10 state marbach et al 1998 even possibl explicitli list state final fix model comput optim polici mean robust actual traffic condit depart assum model reason clear practic system larg state space difficult imposs determin exact transit model markov chain perform modelbas algorithm comput optim polici main motiv studi appli modelfre rl algorithm solv cac problem although explicitli comput transit probabl make follow assumpt studi let ss 0 continu random inter transit time state state 0 action probabl distribut f ss 0 ja assumpt a1 assumpt a2 expect ss 0 2 reinforc learn call admiss control 7 z 1df ss 0 ja 10 particular exist assumpt a3 unichain condit everi stationari polici transit matrix ps 0 determin markov chain one ergod class possibl empti set transient state assumpt a1 guarante transit probabl well defin assumpt a2 guarante number transit finit time interv almost sure finit a3 guarante except initi transient state state reach state nonzero probabl 33 qlearn section develop rl methodolog use paper unconstrain maxim revenu qo constraint consid section 4 learn optim polici use watkin qlearn algorithm watkin dayan 1992 given optim qvalu q polici defin optim particular 12 impli follow procedur call arriv qvalu accept call qvalu reject call determin reject higher valu drop call els accept higher valu accept call one action qvalu exist call departur learn q updat valu function follow transit state 0 action time ss 0 stepsiz learn rate k integ variabl index success updat ff 0 chosen suffici close 0 discount problem equival averag reward problem tauberian approxim gabor et al 1998 well known qlearn 13 robbinsmonro stochast approxim method solv socal bellman optim equat associ decis process let z 1e gammaff df ss 0 ja 2 assumpt a2 guarante h contract map contract factor z 1e gammaff df ss 0 ja respect maximum norm theorem 1 suppos thatx 2 stateact pair updat infinit number time q k converg probabl 1 q everi proof see bertseka tsitsikli 1996 34 simplifi learn process practic issu concern implement qlearn 13 discuss qlearn need execut everi state transit includ transit caus call departur feasibl action set one action state associ call departur necessari learn optim qvalu state induc optim polici state possibl avoid updat qvalu departur state still get optim polici reduc amount comput storag qvalu significantli sinc state space almost halv drop call departur state note interest state decis need made associ call arriv g decis point jump one arriv next arriv interarriv period may contain zero one departur given e j first arriv e case n 0 departur two adjac arriv chapmankolmogorov equat bertseka gallag 1992 transit probabl actual decis process intermedi state correspond call departur shown appendix optim polici obtain qlearn state associ call arriv result reinforc learn call admiss control 9 intuit sinc call departur random disturb affect state transit even though 18 complic alreadi intract transit model smdp sinc qlearn depend explicit model asymptot converg optim polici follow 35 explor order qlearn perform well potenti import stateact pair must explor specif converg theorem qlearn requir stateact pair tri infinit often section develop explor strategi suitabl cac problem common way tri stateact pair rl small probabl ffl random action rather action recommend rl chosen decis point train socal fflgammarandom explor cac problem consid paper without explor state visit probabl sever order higher state experi shown fflgammarandom explor unlik help situat therefor train state visit mani time state visit time result qvalu function far converg optim polici expect reason time see call arriv process model truncat independ mm1 queue system truncat system untrunc system except configur capac constraint violat elimin stationari distribut system assum greedi polici polici alway accept new call capac constraint violat ad new call given bertseka gallag 1992 g normal constant allow set configur truncat system sinc state action determinist defin next configur x 0 next state 0 event part 0 e 0 arriv action need taken occur independ x 0 probabl determin due memoryless assumpt stationari distribut state 0 depend 19 exampl consid experiment paramet shown tabl 1 section 5 except simplifi calcul allow configur set c truncat system use peakrat alloc 1g use 19 20 visit state 02297 least visit state p ie five order differ stationari distribut stateact pair small system shown szepesvari 1998 converg rate qlearn approxim suitabl constant b 0 k index 13 defin 15 overcom slow converg caus small valu p min pmax stationari distribut control explor scheme deriv base fact qlearn offpolici learn method sutton barto 1998 section 76 smdp state transit thu state distri bution control choos appropri action state train one feasibl action probabl ffl control action chosen lead least visit configur fflgammadirect heurist effect reduc differ number visit state significantli speed converg valu function term qlearn formula 13 action chosen accord explor scheme action b chosen accord current qvalu 36 function approxim vs lookup tabl qlearn deal effect curs model explicit state transit model need simul use instead anoth major difficulti smdp problem curs dimension exponenti state space explos problem dimens treatment assum problem state space kept small enough lookup tabl use clearli number stateact pair becom larg lookup tabl represent infeas compact represent q repres function smaller set paramet use function approxim necessari paper choos approxim architectur correspond state aggreg consid partit state space disjoint subset gammadimension paramet vector oe whose mth compon meant approxim qvalu function state 2 sm action word deal piecewis constant approxim qs valu small lookup tabl use aggreg problem case shown bertseka tsitsikli 1996 qlearn converg optim polici aggreg problem function approxim use may perform well practic howev converg result state aggreg case wish avoid proposit 68 bertseka tsitsikli 1996 tauberian reinforc learn call admiss control 11 approxim easi show perform loss due state aggreg bound j 0 0 optim averag revenu per unit time origin aggreg problem respect defin 15 cac state aggreg interpret featurebas architectur wherebi assign common valu oem state given share common featur vector exampl featur vector may involv call class three valu indic specifi whether load call class high medium low system instead specifi precis number ongo call class x sinc state similar number call would expect similar qvalu expect small therefor state space greatli reduc lookup tabl use 37 summari section formul cac problem smdp justifi qlearn approach solv cac problem show simplifi problem ignor detail within call process comput qvalu state decis standard fflgammarandom explor polici significantli slow learn problem simpl fflgammadirect explor strategi introduc aggreg state shown simplifi heurist follow readili problem structur next section develop method incorpor constraint framework 4 constraint restrict maxim polici never violat qo guarante 1 3 gener smdp problem constrain optim polici random stationari polici random k state problem k gammaconstraint feinberg 1994 howev modelbas linear program algorithm employ deriv polici impract cac number state larg sinc random need k state usual much smaller total number state nonrandom stationari polici learn rl often good approxim constrain optim polici gabor et al 1998 gener smdp due stochast state transit meet constraint may possibl eg state matter action taken possibl enter restrict state admiss control servic qualiti depend number call admit system ad call strictli control admiss control meet qo constraint possibl consid two import class qo constraint cac integr servic network one statedepend constraint past depend constraint conserv capac constraint exampl statedepend constraint statedepend constraint qo intrins state congest probabl function sole number call progress current state cf 6 pastdepend constraint depend statist past histori exampl fair criterion fair depend statist reject ratio past histori address two constraint separ 41 capac constraint simplic consid total packet congest probabl upper bound p conserv approach mean set c c p cf 6 7 x 0 state conserv capac constraint intrins properti state depend current state allow us collect qo statist state treat principl way eg comput confid interv estim current state action n uniqu determin next configur xn1 project congest probabl next state n1 determin xn1 therefor forecast impact need evalu pxn1 expect congest probabl greater less constraint p action caus pxn1 action elimin feasibl action set asn cac ad new call violat capac constraint feasibl action reject new call request consid aggress capac constraint need determin set c c p allow configur defin implicitli uniqu lim x2ca x total time system spend x x2ca x note distribut xt depend control polici gener case differ servic type differ packetlevel qo requir easili made reinforc learn call admiss control 13 state c c p serv possibl c usual conserv construct aggress set c p gradual decreas p c 1 find seri set c c p c correspond chang p c clearli size c c p nonincreas decreas p c howev must alway contain c c p practic valu p c0 learn polici aggress congest probabl suffici close still less constraint p search c p stop choos c p c0 aggress capac constraint essenc tri find correspond valu conserv threshold p c aggress threshold p construct c conserv approach way aggress capac constraint remain statedepend constraint conserv capac constraint implement constraint constrain action set state although c determin way may aggress one term revenu maxim 1 3 loss optim expect small 42 fair constraint measur reject ratio class upon nth call arriv nth decis made arbitrarili constraint r n may abl find feasibl polici fair constraint involv comparison reject ratio type call formul fair constraint 1ii 1ii l maximum allow reject ratio discrep feasibl polici exist alway reject call type aggress fair constraint formul lim l 28 sn sn1 intertransit durat state n n1 action formul constrain smdp problem 1 3 capac constraint implement constrain feasibl action set state describ preced subsect deal fair constraint use lagrang multipli framework studi beutler ross 1986 sinc fair constraint pastdepend constraint vector rsn1 depend reject ratio past tori fit framework need includ histori inform state descriptor new state descriptor form gammavector req resp rej denot total number call request resp reject class current call arriv time interv last current call request origin state 14 h tong tx brown descriptor obtain markov chain expans howev state space enlarg significantli specif due inclus req rej state space infinit must resort form function approxim solv smdp problem paper use state aggreg approxim architectur quantiz reject ratio r term lagrang multipli consid unconstrain optim parametr reward origin reward function associ cost function associ constraint numer 28 exist nonrandom polici solv bellman optim equat associ reward function 30 mean time achiev equal 28 beutler ross 1986 show constrain optim polici case optim polici exist shown constrain optim achiev random one state 0 two nonrandom polici 2 differ slightli undershoot resp overshoot l clearli case nonrandom constrain optim polici exist 1 next best nonrandom polici loss optim minim reason avoid complic random polici concentr nonrandom polici studi 43 summari section show constraint introduc problem either modul action space modifi reward function optim requir random polici sinc polici need random two state mani state greatli simplifi search restrict determinist polici 5 simul result experi use follow model total bandwidth normal 10 unit traffic per unit time target congest probabl p two sourc type consid properti shown tabl 1 fair constraint averag reject ratio discrep two servic type differ l note hold time exponenti first concentr conserv approach capac constraint sinc explor employ ensur potenti import stateact reinforc learn call admiss control 15 tabl 1 experiment paramet sourc type paramet ii rate r 008 02 mean period 1 5 5 mean period 1 15 45 call arriv rate 0067 02 call hold time 1 immedi payoff pair tri natur enabl us collect statist use estim qo stateact pair emphas singl visit state suffici determin long run qo metric due variabl within call process number time stateact pair visit increas estim servic qualiti becom accur confid gradual elimin stateact pair violat qo requir consequ valu function updat gradual correct subset stateact space sens qo requir met action within subspac state section 4 capac constraint elimin stateact pair violat congest probabl upper limit experi use simpl way elimin stateact pair confid sinc target congest probabl total number visit configur x count number time step simul wx number congest x wx x 200 wx 20000 conclud accept threshold provid close approxim confid interv brown 1997 sophist way estim px propos tong brown 1998 artifici neural network nn train base maximum likelihood principl nn estim px extrapol well p simul discount factor ff chosen 10 gamma4 learn rate explor initi qvalu rl artifici set qlearn start greedi polici train complet appli test data set compar polici obtain rl altern heurist polici final qo measur obtain end rl train learn qo use test differ polici test rl polici new call arriv algorithm first determin accept call violat qo call reject els action chosen accord arg max a2a qs 0rejectg qo constraint use three case peak rate alloc statist multiplex function learn onlin denot qo learn statist multiplex function given priori denot qo given examin six differ case 1 rl qo given 2 rl qo learn 3 rl peak rate heurist accept call valuabl class ie type qo given 5 greedi qo given 6 greedi peak rate result shown fig 1 clear simultan qlearn qo learn converg correctli rl polici obtain give qo priori standard qlearn see signific gain 15 due statist multiplex 6 vs 5 3 vs 1 gain due rl 25 6 vs 3 5 vs 2 togeth yield 45 increas revenu conserv peak rate alloc exampl also clear figur rl polici perform better heurist polici fig 2 show reject ratio differ polici consid aggress approach capac constraint simul found valu p correspond aggress capac constraint p accept region ie c c c aggress conserv approach shown fig 3 aggress accept region much larger conserv one figur number type ii user start two due insuffici measur data confid level region compar fig 4 5 fig 1 2 see aggress approach earn significantli revenu conserv approach greedi polici rl polici note peak rate alloc earn total amount reward unnorm approach fig 4 qvalu initi rl polici start greedi polici exampl perform improv due rl signific improv due statist multiplex fair constraint impos case reject ratio two type call differ significantli fair constraint requir two reject ratio differ 5 averag test rl fair constraint set reward paramet type call 1 type ii call 10 keep paramet tabl 1 unchang state use featurebas state aggreg cope difficulti larg state space caus fair constraint specif learn qh instead qs featur quantiz follow experi experienc reject ratio discrep fr quantiz 100 level quantiz 2 level correspond 4 approxim averag interarriv time although aggreg experi case complic also possibl aggreg simpler featur found 800 simul learn rl polici compar greedi polici fair constraint accept call long fair constraint met otherwis fair constraint violat accept call class experienc highest reject ratio result shown fig 6 7 fair strong constraint possibl polici gain due rl reduc expect fig 1 4 6 see qlearn converg quickli fact rl curv figur show oscil connect learn rate reinforc learn call admiss control 17 total reward comparison differ polici exponenti onoff246 1rl qo given 2rl qo learn 3rl peak rate 4greedi type 5greedi qo given 6greedi peak rate figur 1 comparison total reward rl learn qo capac constraint rl given qo measur rl peak rate greedi polici peak rate alloc normal greedi total reward rate 1greedi peak rate 2rl peak rate 3greedi qo given 4rl qo learn exponenti onoff figur 2 comparison reject ratio polici learn fig 1 13 specif order qlearn converg fl k satisfi 16 17 simul use small constant learn rate condit 17 met reason 17 adher typic prior knowledg decreas learn rate becom small algorithm may stop make notic progress train process could becom long 6 combin cac network rout gener issu cac rout close relat commun network combin cac rout also formul smdp howev exact character network state would requir specif number call progress class possibl rout network detail specif state intract comput assum statist independ link network dziong 1997 krishnan 1990 form decomposit network rout process singl link process usual employ dziong mason 1994 marbach et al 1998 base preced result singl link admiss control link state independ approxim propos decomposit rule allow decentr train decis make combin cac rout network also tri maxim network revenu 123579number user class number user class ii comparison accept region aggress conserv figur 3 comparison accept region total reward comparison differ polici exponenti onoff rl qo learn greedi qo given greedi peak rate figur 4 comparison total reward rl learn qo capac constraint greedi polici peak rate alloca tion normal greedi total reward aggress 30103050709reject rate 1greedi peak rate 2greedi qo given 3rl qo learn exponentialonoff figur 5 comparison reject ratio polici learn fig 4 reinforc learn call admiss control 19 total reward comparison differ polici exponenti onoff rl qo learn greedi qo given figur 6 comparison total reward obtain rl polici greedi polici capac constraint fair constraint impos normal greedi total reward reject rate figur 7 comparison reject ratio capac constraint fair constraint polici learn fig 6 let r denot predefin rout network action space system action due call departur rout new call rout r 2rg link j node node j keep separ link state variabl whenev new call type k rout rout r contain link j immedi reward associ link j equal c ij satisfi ij2r exampl number link along rout r qlearn perform link similarli singl link case arriv updat qvalu link j arriv associ link new type k call origin node destin node decis made node follow way od set rout carri call without violat qo constraint ii defin net gain g r accept new call rout decis r ae p ij2r theta figur 8 network model admiss rout decis r r2a od f0g r 34 decis make r reject call otherwis rout call rout r approach although network state simplifi link state link action space link simplifi acceptg dziong mason 1994 marbach et al 1998 import sinc link qfunction distinguish singlelink call multilink call avoid accept mani multilink call block singlelink call may bring amount revenu use less network resourc tabl 2 experiment paramet sourc type paramet ii iii rate r call arriv rate 01 01 0067 call hold time 1 200 180 120 immedi payoff present simul result obtain case network consist 4 node 12 unidirect link two differ class link total bandwidth 15 2 unit respect indic thick thin arrow reinforc learn call admiss control 21 fig 8 assum three differ sourc type whose paramet given tabl 2 call arriv node independ poisson process mean destin node randomli select among three node sourc destin node pair list possibl rout consist three entri direct path two altern 2hop rout emphas effect rl consid capac constraint assum peak rate alloc link simul use featurebas state aggreg approxim qvalu link learn qh r instead qs r ie number ongo call type aggreg eight level polici obtain rl compar commonli use heurist polici give direct path prioriti direct path reach capac heurist tri 2hop rout find one violat capac constraint rout exist call reject result given fig 9 total fig 10 call reject ratio fig 11 rout behavior result show rl polici increas total revenu almost 30 compar commonli use heurist rout polici 7 conclus paper formul cac rout problem constrain smdp provid rl algorithm comput optim control polici incorpor two import class qo constraint statedepend pastdepend con straint rl solut maxim network revenu formul quit gener appli capac fair constraint approach experi singl link well network problem show signific improv even simpl exampl futur work includ studi combin cac rout studi function approxim neural network approxim qvalu function acknowledg work fund nsf career award ncr9624791 appendix proof simplifi learn process follow theorem show avoid learn qvalu state transit correspond call turn section 31 call departur section 34 let j ff cs asg set intermedi state g 22 h tong tx brown total reward comparison differ rout polici exponenti onoff rl heurist figur 9 comparison total reward 4node network normal heurist total reward 20103050709reject ratio 1heurist 2rl exponenti onoff reject rate figur 10 comparison reject ratio polici fig 9 routingheurist portion call rout direct path direct 1st 2hop 2nd 2hop routingrl portion call rout direct path direct 1st 2hop 2nd 2hop figur 11 comparison rout behavior polici fig 9 reinforc learn call admiss control 23 theorem 2 assum 2 2 2 take sa step go state 0 state 0 1 1 optim stationari polici modifi decis process consid state 2 also optim origin decis process proof optim polici origin problem z 1e gammaff df ss 0 ja 2 optim polici ff modifi decis process sinc one feasibl action 2 1 2 2 n 18 sa due assumpt hi z 1e gammaff df ss 0 ja oe 0 first state 2 defin z 1e gammaff df ss 0 ja sinc sa finit without loss gener assum valu sa procedur similar summat term a3 becom hi deltad second summat second term formula due condit ps combin a5 a6 a3 a4 a7 z 1e gammaff df ss 0 ja uniqu optim valu function easi verifi j ff therefor a1 a2 a8 proof eg a4 use memoryless properti transit process cac state serv 0 sinc 0 possibl call departur due capac constraint state action take finit number consecut call departur reach state like 0 r adapt control constrain markov chain data network adapt statist multiplex broadband commun optim admiss control ensur qualiti servic multimedia network via reinforc learn control selfsimilar atm call traffic reinforc learn call admiss rout multiservic loss network ieee tran atm network resourc manag constrain semimarkov decis process averag reward intern confer machin learn markov decis algorithm dynam rout neurodynam approach admiss control atm network singl singl link case reinforc learn call admiss control rout integr servic network robust dynam admiss control unifi cell call qo statist multiplex reinforc learn dynam channel alloc cellular telephon system reinforc learn asymptot convergencer qlearn advanc nip 10 estim loss rate integr servic network neural network adapt call admiss control qualiti servic con straint reinforc learn solut tr