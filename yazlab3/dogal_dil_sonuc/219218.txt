comparison id3 backpropag english texttospeech map perform error backpropag bp id3 learn algorithm compar task map english text phonem stress distribut output code develop sejnowski rosenberg shown bp consist outperform id3 task sever percentag point three hypothes explain differ explor id3 overfit train data b bp abl share hidden unit across sever output unit henc learn output unit better c bp captur statist inform id3 conclud hypothesi c correct augment id3 simpl statist learn procedur perform bp close match complex statist procedur improv perform bp id3 substanti domain b introduct univers learn algorithm take sampl train exampl arbitrari unknown function f produc good approxim f see dietterich 1989 instead everi learn algorithm embodi assumpt bia natur learn problem appli algorithm exampl assum small number featur describ data relev algorithm assum everi featur make small independ contribut determin classifi cation mani algorithm order hypothes accord syntact simplic represent attempt find simplest hypothesi consist train exampl unfortun mani popular learn algorithm assumpt embodi entir knownor known state term difficult check given applic domain exampl quinlan decisiontre algorithm id3 assum unknown function f repres small decis tree howev given new learn problem difficult know whether assumpt hold without first run id3 algorithm result good understand rang problem id3 appropri similarli backpropag algorithm rumelhart hinton william 1986 assum minimum unknown function f repres multilay feedforward network sigmoid unit although mani success applic backpropag touretzki 1989 1990 still lack understand situat appropri furthermor clear statement assumpt made id3 backpropag unavail understand relationship two algorithm investig even suggest algorithm make similar assumpt lorien pratt person commun henc confront two relat question first assumpt embodi id3 backpropag equival situat algorithm appli second id3 backpropag relat one conceiv two differ approach answer question theoret approach could analyz algorithm attempt articul assumpt experiment approach could test two algorithm nontrivi problem compar behavior paper take experiment approach appli id3 backpropag task map english word pronunci task pioneer sejnowski rosenberg 1987 famou nettalk sy tem employ backpropag rosenberg doctor dissert 1988 includ analysi experi domain replic work discov backpropag outperform id3 task demonstr id3 backpropag make ident assumpt go investig differ id3 backpropag formul three hypothes explain differ conduct experi test hypothes experi show id3 combin simpl statist learn procedur nearli match perform bp also present data show perform id3 backpropag highli correl collect binari concept learn problem data also show id3 bp tend agre concept easi difficult given bp substanti awkward timeconsum appli result suggest follow methodolog appli algorithm problem similar nettalk task first id3 combin statist learn procedur appli perform adequ need appli backpropag howev id3 perform inadequ still use estim perform backpropag much expens backpropag procedur employ see yield better classifi id3 backpropag 3 2 task conduct comparison id3 backpropag chosen task map english text speech complet texttospeech system involv mani stage process ideal sentenc pars identifi word sens part speech individu word sens map string phonem stress final phonem stress combin variou techniqu gener sound wave excel review see klatt 1987 phonem equival class basic sound exampl phonem p individu occurr p slightli differ consid p sound exampl two ps lollypop pronounc differ member equival class phonem p use 54 phonem see appendix a1 stress perceiv weight given syllabl word exampl first syllabl lollypop receiv primari stress third syllabl receiv secondari stress middl syllabl unstress stress inform code assign one six possibl stress symbol letter conson gener receiv one symbol indic princip vowel syllabl left right respect conson vowel gener mark code 0 none 1 primari 2 secondari indic degre stress lastli silent stress assign blank let l set 29 symbol compris letter az comma space period data set comma period appear let p set 54 english phonem set 6 stress employ sejnowki rosenberg task learn map f specif f map word length k string phonem length k string stress length k exampl notic letter phonem stress align silent letter map silent phonem defin f complex discret map larg rang assum word contain 28 letter length antidisestablishmen tarian rang would contain 10 70 element exist learn algorithm focu primarili learn boolean conceptsthat function whose rang set f0 1g algorithm appli directli learn f fortun sejnowski rosenberg 1987 develop techniqu convert complex learn problem task learn collect boolean concept begin reformul f map g sevenlett window singl phonem singl stress exampl word lollypop would convert 8 separ sevenlett window function g appli 8 window result concaten obtain phonem stress string map function g rang 324 possibl phonemestress pair substanti improv final sejnowski rosenberg code possibl phonemestress pair 26bit string 21 bit phonem 5 bit stress bit code correspond properti phonem stress convert g 26 separ boolean function h 26 function h map seven letter window set f0 1g assign phonem stress window 26 function evalu produc 26bit string string map nearest 324 bit string repres legal phonemestress pair use ham distanc two string measur distanc sejnowski rosenberg use angl two string measur distanc report euclidean distanc metric gave similar result test euclidean metric obtain result ident report paper reformul possibl appli boolean concept learn method learn h howev individu h must learn extrem well order obtain good perform level entir word error aggreg exampl h learn well 99 correct error among h independ 26bit string correct 77 time averag word 7 letter whole word correct 16 time far discuss represent output map learn input repres straightforward fashion use approach recommend sejnowski rosenberg 1987 sevenlett window repres concaten seven 29bit string 29bit string repres letter one bit letter period comma blank henc one bit set 1 29bit string produc string 203 bit window 203 bit provid input featur learn algorithm id3 backpropag 5 3 algorithm 31 id3 id3 simpl decisiontre learn algorithm develop ross quinlan 1983 1986b construct decis tree recurs start root node select featur test node featur whose mutual inform output classif greatest sometim call inform gain criterion train exampl partit exampl 1 algorithm invok recurs two subset train exampl algorithm halt exampl node fall class point leaf node creat label class question basic oper id3 quit similar cart algorithm develop breiman friedman olshen stone 1984 treegrow method develop lucassen mercer 1984 algorithm extend handl featur 2 valu featur continu valu well implement id3 employ window quinlan 1983 chisquar forward prune quinlan 1986a kind revers prune quin lan 1987 appli one simpl kind forward prune handl inconsist train data remain featur zero inform gain growth tree termin leaf class train exampl chosen label leaf case tie leaf assign class 0 appli id3 task algorithm must execut 26 timesonc map h execut produc separ decis tree 32 backpropag error backpropag method rumelhart hinton william 1986 wide appli train artifici neural network howev standard form algorithm requir substanti assist user specif user must specifi transfer function artifici neuron unit network architectur number layer interconnect number hidden unit layer learn rate momentum term initi weight valu target threshold 1 furthermor user must decid termin train make comparison id3 backpropag fair necessarili transform bp userassist method algorithm involv user assist develop transform call result algorithm bpcv backpropag crossvalid defin bpcv fix userspecifi properti set remain paramet via crossvalid use method introduc lang waibel hinton 1990 explain bpcv one hidden layer fulli connect input layer output layer everi unit hidden output layer implement take dot product vector weight w vector incom activ x ad bia appli logist function continu differenti approxim linear threshold function use perceptron sever paramet given fix valu learn rate alway 025 momentum term 09 target threshold use criterion minim sum squar error sse basic paramet except target threshold use sejnowski rosenberg conduct crossvalid found perform insensit paramet choic remain parametersnumb hidden unit random start weight stop total sum squar error tssear set follow crossvalid procedur given set exampl subdivid three set train set tr crossvalid set cv test set test execut backpropag sever time train set tr vari number hidden unit random start weight pass train data test perform network cv goal search paramet space find paramet give peak perform crossvalid set paramet use train backpropag union tr cv good estim gener perform obtain test test advantag crossvalid train inform test set employ train henc observ error rate test set fair estim true error rate learn network contrast common unsound practic adjust paramet optim perform test set one advantag bpcv nettalk task unlik id3 necessari appli bpcv 26 output bit learn simultan inde 26 output share set hidden unit may allow output learn accur howev id3 batch algorithm process entir train set bp increment algorithm make repeat pass data complet pass call epoch epoch train exampl inspect oneatatim weight network adjust reduc squar error output use implement provid mcclelland rumelhart 1988 output bp float point number 0 1 adapt ham distanc measur map nearest legal phonemestress pair use follow distanc measur dx reduc ham distanc x boolean vector id3 backpropag 7 tabl 1 optim network size via crossvalid number letter number hidden unit correct tsse epoch 28 100 693 1041 19 693 477 28 33 data set sejnowski rosenberg provid us dictionari 20003 word correspond phonem stress string dictionari drew random without replac train set 800 word crossvalid set 200 word test set 1000 word 4 result 41 crossvalid train present result studi first discuss result crossvalid procedur bpcv perform seri run systemat vari number hidden unit 40 60 80 100 120 140 160 180 random start weight four set random weight gener net work perform crossvalid set evalu complet pass train data epoch network train except case train continu 60 epoch ensur peak perform found tabl 1 show peak perform percent letter correctli pronounc network size total sum squar error tr gave peak perform tsse number appropri adjust number train exampl use decid termin train entir train set tr cv base run best network size 160 hidden unit complet crossvalid train proceed merg train set crossvalid set form 1000word train set crossvalid train store snapshot weight valu first complet epoch random network gener henc perform train entir train set use best store 160hidden unit snapshot start point 2 origin train set tr contain 5807 sevenlett window percent letter correct epoch figur 1 train curv best 160hidden unit network vertic bar indic point maximum perform full train set tr cv contain 7229 sevenlett window henc target tsse full train set 554 surpris figur shown tabl 1 sinc expect reason small network eg 80 hidden unit would give good fit data howev tabl clearli show gener steadili improv qualiti fit train data improv furthermor figur 1 show train network continu past point peak perform perform declin appreci previou work sejnowski rosenberg 1986 rosenberg 1988 use network 40 80 120 hidden unit howev knowledg one previous conduct systemat studi relationship network size perform nettalk task similar result show larger network give improv perform publish martin pittman 1990 42 perform comparison tabl show percent correct 1000word test set word letter phonem stress letter consid correct phonem stress correctli predict map nearest legal phonem id3 backpropag 9 tabl 2 percent correct 1000word test set level aggreg correct method word letter phonem stress bit mean id3 96 656 787 772 961 bpcv 136 706 808 813 967 differ cell signific disagre 1385 192 agre 5857 809 backpropag incorrect correct incorrect correct stress word correct letter correct virtual everi differ tabl word letter phonem stress level statist signific use onetail test differ two proport base normal approxim binomi distribut henc conclud substanti differ perform id3 bpcv task note although test set contain 1000 disjoint word sevenlett window test set also appear train set specifi calli 946 131 window test set appear 1000word train set repres 578 distinct window henc perform letter phonem stress level artifici high one concern abil learn method handl unseen case correctli howev one interest probabl letter phonem stress unseen word correctli classifi number provid right measur take closer look perform differ studi exactli 7242 sevenlett window test set handl algorithm tabl 2 categor window accord whether correctli classifi algorithm one algorithm neither one tabl show window correctli learn bpcv form superset learn id3 instead two algorithm share 4239 correct window algorithm correctli classifi sever window algorithm get wrong overal result bpcv classifi 361 window tabl 3 averag percent correct 1000word test set five trial level aggreg correct method word letter phonem stress bit mean id3 102 652 791 765 961 bp 151 713 813 817 967 differ cell signific correctli id3 show two algorithm overlap substanti learn fairli differ texttospeech map inform tabl summar correl coeffici specif let x id3 xbpcv random variabl 1 id3 bpcv respect make correct predict letter level case correl x id3 xbpcv 5648 four cell tabl 2 equal correl coeffici would zero refer independ run bpcv train set differ random start state correl coeffici 6955 weak tabl 2 show perform valu one particular choic train test set replic studi four time total 5 independ trial trial randomli drew without replac two set 1000 word dictionari 20003 word note mean overlap among five train set among five test set tabl 3 show averag perform 5 run differ signific 0001 level use ttest pair differ anoth weak tabl 2 show perform valu 1000 word train set might rel perform differ id3 bpcv might chang size train set chang tabl 4 show case row tabl give result run id3 bpcv sever differ size train set case bpcv train use crossvalid train methodolog outlin four run network 5 10 20 40 80 120 160 hidden unit differ methodolog outlin train tr determin peak gener perform test 200word cv retrain union tr cv sinc would creat train set larg instead simpli test best network 1000word test conclud consist differ id3 bpcv perform algorithm increas size train set differ still observ remaind paper attempt understand natur differ bpcv id3 main approach experi modif two algorithm enhanc elimin differ id3 backpropag 11 tabl 4 percent correct 1000word test set sampl level aggreg correct size method word letter phonem stress bit mean bpcv 16 492 597 731 939 100 id3 20 473 641 658 940 200 id3 44 566 705 722 951 bpcv 71 611 722 782 954 400 id3 62 587 737 721 955 bpcv 113 664 770 797 960 800 id3 96 638 778 756 962 bpcv 153 709 810 812 966 1000 id3 96 656 787 772 964 bpcv 147 709 811 814 966 differ cell signific tabl 5 result appli three overfittingprevent techniqu level aggreg correct method data set word letter phonem stress bit mean id3 test 96 656 787 772 961 b id3 2 cutoff test 91 648 784 771 961 c id3 prune test 93 624 769 751 958 id3 rule test 82 651 785 772 961 unless state otherwis experi perform use 1000word train set 1000word test set tabl 2 5 three hypothes caus differ id3 bpcv three hypothes hypothesi 1 overfit id3 overfit train data seek complet consist caus make error test set hypothesi 2 share abil bpcv share hidden unit among h allow reduc aggreg problem bit level henc perform better hypothesi 3 statist numer paramet network allow captur statist inform captur id3 hypothes neither mutual exclus exhaust follow three subsect present experi perform test hypothes 51 test hypothesi 1 overfit tendenc id3 overfit train data well establish case data contain nois three basic strategi develop address problem criteria earli termin treegrow process b techniqu prune tree remov overfit branch c techniqu convert decis tree collect rule implement test one method strategi tabl 5 summar result first row repeat basic id3 result given comparison purpos second row show effect appli 2 test 90 confid level decid whether growth decis tree statist justifi quin lan 1986a author report mooney et al 1989 hurt perform nettalk domain third row show effect appli quinlan techniqu reduceerror prune quinlan 1987 minger 1989 id3 backpropag 13 provid evid one best prune techniqu row decis tree built use 800word tr set prune use cv crossvalid set final fourth row show effect appli method convert decis tree collect rule quinlan 1987 describ threestep method convert decis tree rule first path root leaf convert conjunct rule second rule evalu remov unnecessari condit third rule combin unnecessari rule elimin experi perform first two step third step expens execut rule set contain rule none techniqu improv perform id3 task suggest hypothesi 1 incorrect id3 overfit data domain make sens sinc sourc nois domain limit size sevenlett window exist small number word like read one correct pronunci sevenlett window suffici correctli classifi 985 word 20003word dictionari may also explain observ overfit excess train crossvalid run backpropag either 52 test hypothesi 2 share second hypothesi claim key bpcv superior perform fact output unit share singl set hidden unit one obviou way test share hypothesi would develop version id3 permit share among 26 separ decis tree learn could see sharedid3 improv perform altern remov share backpropag train 26 independ network one output unit learn 26 h map hypothesi 2 correct share among separ network see drop perform compar singl network share hidden unit furthermor decreas perform decreas differ bpcv id3 measur correl error call singl network hidden unit share output unit bp1 call 26 separ network bp26 delic issu aris train bp26 ideal want train collect 26 network differ bp1 result lack share hidden unit mean total sum squar error train set bp26 bp1 goal train procedur find among bp26 network collect whose perform crossvalid set maxim henc use follow procedur first measur sum squar error train set 26 bit learn bp1 second train bp26 network follow crossvalid procedur tri altern random seed number hidden unit time alway 14 dietterich hild bakiri termin train individu network attain squar error observ larg network crossvalid tri network 1 2 3 4 5 10 20 hidden unit four random seed network size final select network whose sum squar error minimum 200word crossvalid test set cv surprisingli unabl train success separ network target error level 1000word train set explor smaller subset 1000word train set 800 400 200 100 50word found train succeed train set contain 50 word 100word train set exampl individu network often converg local minima even though bp1 network avoid minima specif bit 4 6 13 15 18 21 25 could train criterion even 2000 epoch bit 100word train set conduct detail studi attempt understand train problem perform hundr run vari number hidden unit learn rate momentum term initi random weight attempt find configur could learn singl bit level bp1 none run succeed run bp26 train converg error hand train exampl 00 error remain train exampl 10 contrast error bp1 extrem tabl 6 show collect sevenlett window test set squar error window nine differ train run first train run bp1 120 unit train epoch next four column show run bp26 5hiddenunit network four differ random start seed train learn rate 4 momentum 8 initi random valu rang 0505 last four column show run bp26 10 hiddenunit network four differ random start seed train learn rate 4 momentum 7 initi random valu rang 0404 demonstr even share hidden unit aid classif per formanc certainli aid learn process consequ train problem abl report result 50word train set crossvalid train bp1 see determin best network train set contain 120 hidden unit train sumsquar error 13228 tabl 7 summar train process 26 output bit bp26 row give number hidden unit best bp26 network squar error obtain bp1 network squar error obtain bp26 network number epoch requir train bp26 notic individu bit slightli overtrain compar bp1 program accumul squar error epoch stop fall target error level perform improv epoch final squar error somewhat lower tabl 8 show perform 26 network train test set perform train set virtual ident 120hiddenunit id3 backpropag 15 tabl 6 comparison individu error bit window bp1 bp26 austr 10 sot 10 breadwi bucksaw 10 10 moi 1000 10 cinnamo figurat 0026 10 10 10 10 10 10 lawyer muumuu pettifo 0028 10 10 10 10 10 10 10 10 ilton 1000 10 10 10 10 10 10 10 valu shown 000 tabl 7 train statist 26 independ network bit number squar error squar error number hidden unit bp1 network bp26 network epoch 9 4 19208 1894 22 5 00894 0074 61 26 20 00011 0 tabl 8 perform 26 separ network compar singl network 120 share hidden unit train 50word train set test 1000word test set level aggreg correct method data set word letter phonem stress bit mean id3 test 08 415 605 601 926 b bp 26 separ net train 920 990 990 1000 999 test 17 463 579 722 932 c bp 120 hidden unit train 920 987 990 997 999 test 16 492 597 731 934 differ bc train test 01 gamma29 gamma18 gamma09 gamma02 differ ac test gamma08 gamma77 08 gamma130 gamma13 differ signific id3 backpropag 17 tabl 9 correl coeffici replic x id3 xbp1 x id3 xbp26 5167 4942 c 5347 5062 4934 4653 averag decreas 0208 network show train regim success perform test set howev show loss perform share hidden unit among output unit henc suggest hypothesi 2 least partial correct howev examin correl id3 bpcv indic wrong correl x id3 xbp1 ie bp singl network 5428 wherea correl x id3 xbp26 5045 replic comparison 5 time 5 differ train test set use less rigor effici uncrossvalid train procedur tabl 9 show result correl coeffici pair differ ttest show differ correl coeffici signific 0001 level henc remov share hidden unit actual made id3 bp less similar rather similar hypothesi 2 claim conclus share backpropag import improv train perform explain id3 bpcv perform differ 53 test hypothesi 3 statist perform three experi test third hypothesi continu paramet bpcv network abl captur statist inform id3 fail captur first experi took output backpropag network threshold valu 5 map 1 valu 5 map map nearest legal phonemestress pair threshold valu chang distanc measur output legal phonem stress pattern tabl 10 present result 1000word train set result show threshold significantli drop perform back propag inde phonem level decreas enough push bpcv id3 level aggreg bpcv still outperform id3 tabl 10 perform backpropag threshold output valu train 1000word train set test 1000word test set level aggreg correct method data set word letter phonem stress bit mean id3 legal test 96 656 787 772 961 b bpcv legal test 136 706 808 813 967 c bpcv threshold test 112 677 784 800 963 differ cb test gamma24 gamma29 gamma24 gamma13 gamma04 differ signific id3 backpropag 19 result support hypothesi continu output neural network aid perform bpcv howev threshold output bpcv caus behav substanti like id3 correl x id3 xbpcv thresh 5685 compar 5648 xbpcv small increas close examin data show sevenlett window lost ie incorrectli classifi bpcv threshold includ 120 window correctli classifi id3 112 window incorrectli classifi id3 henc mistak introduc threshold nearli independ mistak made id3 experi demonstr import continu output tell us kind inform captur continu output reveal anyth role continu weight insid network must turn two experi second experi modifi method use map output 26 bit string one 324 legal phonemestress pair instead consid possibl legal phonemestress pair restrict attent phonemestress pair observ train data specif construct list everi phonemestress pair appear train set along frequenc occurr appendix a3 show frequenc inform 1000word train set test 26element vector produc either id3 bpcv map closest phonemestress pair appear list tie broken favor frequent phonemestress pair call observ decod method sensit phonemestress pair frequenc observ train set tabl 11 present result 1000word train set compar previou techniqu legal decod nearest legal phonemestress pair key point notic decod method leav perform bpcv virtual unchang substanti improv perform id3 inde elimin substanti part differ id3 bpcvthe two method statist indistinguish word phonem level mooney et al 1989 compar studi id3 bpcv task employ version decod techniqu random tie break obtain similar result train set 808 word dictionari occur frequent english text examin correl coeffici show observ decod increas slightli similar id3 bpcv correl id3observ xbpobserv 5865 compar 5648 legal decod ing furthermor observ decod almost alway monoton better ie window incorrectli classifi legal decod becom correctli classifi observ decod vice versa tabl 12 show result four replic paireddiffer ttest conclud correl coeffici increas observ decod signific level better 0001 result conclud bpcv alreadi captur inform frequenc occurr phonemestress pair tabl 11 effect observ decod learn perform level aggreg correct method data set word letter phonem stress bit mean id3 legal test 96 656 787 772 961 b bpcv legal test 136 706 808 813 967 c id3 observ test 130 701 815 792 964 bpcv observ test 143 715 820 814 967 id3 improv ca test 34 45 28 20 03 differ cell signific tabl 12 correl id3 bpcv observ decod data set legal observ 5648 5865 c 5593 5796 5722 5706 e 5563 5738 averag increas 0136 id3 backpropag 21 id3 captur nearli much henc experi strongli support hypothesi 3 drawback observ strategi never decod window phonemestress pair seen henc certainli make mistak test set howev phonemestress pair observ train set make small fraction window test set exampl 7 phonemestress pair appear 1000 word test set appear 1000word train set test set account 11 7242 window 015 train 19003 word dictionari appear 1000word test set would one phonemestress pair present test set would appear train set would appear one window final experi concern hypothesi 3 focus extract addit statist inform train set motiv klatt 1987 view ultim lettertophonem rule need identifi exploit morphem ie commonlyoccur letter sequenc appear within word fore analyz train data find letter sequenc length 1 2 k retain b mostfrequentlyoccur sequenc length paramet b determin crossvalid describ retain letter sequenc form list phonemestress string sequenc map train set frequenc exampl five pronunci letter sequenc ation train set hphonem stringi hstress stringi hfrequencyi esxn 10 22 sxn 10 1 decod word scan left right see contain one top b letter sequenc length l vari l k 1 word contain sequenc letter correspond sequenc process follow first l window center letter sequenc evalu ie 26 decis tree feedforward network obtain 26bit string string concaten produc bit string length l delta 26 observ pronunci sequenc convert l delta 26bit string accord code given appendix a1 final unknown string map nearest observ bit string decod block control skip end match lletter sequenc resum scan anoth top b letter sequenc length l scan complet part word yet match rescan look block length l gamma 1 everi letter word eventu process everi individu letter block length 1 call techniqu block decod 22 dietterich hild bakiri tabl 13 effect block decod learn perform level aggreg correct method data set word letter phonem stress bit mean id3 legal test 96 656 787 772 961 b bpcv legal test 136 706 808 813 967 c id3 block test 172 733 839 804 967 bpcv block test 185 737 838 813 967 id3 improv ca test 76 77 52 32 06 differ cell signific agre 6147 849 disagre 1095 151 1905621374 id3 backpropag incorrect correct incorrect correct employ crossvalid determin maximum block length k number block b store evalu differ valu train 800 word test 200word crossvalid test set tri valu 1 2 3 4 5 6 k valu 100 200 300 400 b id3 peak perform attain 100 bpcv peak perform attain case perform much sensit k b tabl 13 show perform result 1000word test set block decod significantli improv id3 bpcv id3 improv much especi word level inde two method distinguish statist level aggreg furthermor correl coeffici x id3block xbpblock 6122 substanti increas compar 5648 legal decod henc block decod also make perform id3 bpcv much similar tabl 13 show 7242 sevenlett window test set handl id3 bpcv tabl 14 show correl coeffici along four replic paireddiffer ttest conclud correl coeffici increas block decod signific level better 0001 id3 backpropag 23 tabl 14 correl id3 bpcv block decod tabl 15 classif test set window id3 bpcv block decod data set legal block 5648 6122 c 5593 6138 5722 5832 e 5563 6028 averag increas 0385 note method suppli addit inform id3 bpcv could expect improv correl algorithm furthermor sourc new inform would probabl benefit poorer perform algorithm id3 better perform algorithm nonetheless fact block decod elimin differ id3 bpcv provid strong evid identifi import caus differ two method hypothesi 3 correct experi also suggest block decod techniqu use adjunct learn algorithm appli domain 6 discuss 61 improv algorithm mani direct explor improv algorithm pursu sever direct order develop highperform texttospeech system effort report detail elsewher bakiri 1991 one approach design better output code phonemestress pair experi shown bch error correct code provid better output code output code use paper randomlygener bitstr produc similar perform improv see dietterich bakiri 1991 anoth approach widen sevenlett window introduc context lucassen mercer 1984 employ 9letter window also includ input phonem stress four letter left letter center window phonem stress obtain execut letter alreadi pronounc scan lefttoright experi 15letter window indic produc substanti perform gain well howev find work better word scan righttoleft instead third techniqu improv perform suppli addit input featur program one featur letter help bit indic whether letter vowel conson featur phonem help whether phonem tens lax fourth techniqu pursu refin block decod method block chosen care consider statist confid decod consid overlap block fifth direct pursu implement buntin 1990 method obtain class probabl estim decis tree algorithm produc fairli accur probabl estim leav decis tree use estim map nearest phonemestress pair curiou know whether approach would captur statist inform provid observ block decod experi show howev observ block decod superior simpli use legal decod even observ decod class probabl tree id3 backpropag 25 tabl correct code sevenlett phonem stress context domainspecif input featur observ decod simplifi stress level aggreg correct train set word letter phonem stress bit mean 1000 word 406 841 870 914 921 19003 word 648 914 937 951 957 combin errorcorrect output code wider window right toleft scan includ phonem stress context domainspecif featur obtain excel perform 1000word train test set tabl show bestperform configur train 1000 word train 19003 word detail configur describ bakiri 1991 unabl test similar configur bpcv huge comput resourc would requir bakiri 1991 describ studi human judg compar output system output dectalk klatt 1987 lettertosound rule base result show system two machin learn approach significantli outperform dectalk 62 appli id3 aid bpcv interest observ studi perform id3 bpcv highli correl suggest methodolog use id3 aid bpcv even domain bpcv outperform id3 mani realworld applic induct learn substanti vocabulari engin requir order obtain high perform vocabulari engin process typic involv iter select test promis featur test featur necessari train bpcv network use themwhich timeconsum perform id3 correl bpcv use instead test featur set good set featur identifi bpcv network train examin idea detail consid tabl 17 show perform id3 bpcv 26 individu bit ie without decod algorithm train 1000word train set test 1000word test set 160hidden unit network employ bpcv correl coeffici 9817 signific well 001 level henc conclud gener perform id3 good predictor gener perform bpcv 26 dietterich hild bakiri tabl 17 perform complex difficulti learn 1000word train set 1000word test set id3 bp 3 7104 981 7110 982 6 7065 976 7057 974 7 7207 995 7191 993 9 7206 995 7203 995 14 7148 987 7120 983 19 7242 1000 7242 1000 22 6658 919 6738 930 26 7242 100 id3 backpropag 27 7 conclus rel perform id3 backpropag texttospeech task depend decod techniqu employ convert 26 bit se jnowskirosenberg code phonemestress pair decod nearest legal phonemestress pair obviou approach reveal substanti differ perform two algorithm experi investig three hypothes concern caus perform differ first hypothesisthat id3 overfit train datawa shown incorrect three techniqu avoid overfit appli none improv id3 perform second hypothesisthat abil backpropag share hidden unit factorwa shown partial correct share hidden unit improv classif perform backpropag andperhap importantlyth converg gradient descent search howev analysi kind error made id3 backpropag without share hidden unit demonstr differ kind error henc elimin share hidden unit produc algorithm behav like id3 suggest develop share id3 algorithm could learn multipl concept simultan unlik produc perform similar bpcv third hypothesisthat backpropag captur statist inform mechan perhap continu output activationswa demonstr primari differ id3 bpcv ad ob serv decod techniqu algorithm level perform two algorithm classifi test case becom statist indistinguish word phonem level ad block decod techniqu differ algorithm statist insignific given block decod two algorithm perform equival given bpcv much awkward appli timeconsum train result suggest task similar texttospeech task id3 block decod clearli algorithm choic applic bpcv id3 play extrem valuabl role exploratori studi determin good set featur predict difficulti learn task paper also introduc new method experiment analysi comput error correl measur effect algorithm modif shown method appli discov way algorithm relat broader applic methodolog improv understand assumpt bias underli mani induct learn algorithm 28 dietterich hild bakiri 8 acknowledg author thank terri sejnowski provid nettalk phonem dictionari without work would imposs correspond jude shavlik ray mooney geoffrey towel help clarifi possibl kind decod strategi discuss lorien pratt aid design crossvalid studi research support nsf grant number ccr87 16748 iri8657316 presidenti young investig award match support sun microsystem 9 r convert english text speech machin learn approach classif regress tree theori learn classif rule school comput scienc limit induct learn compar studi id3 backpropag english texttospeech map review texttospeech convers english inform theoret approach automat determin phonem base form explor parallel distribut process empir comparison prune method decis tree induct experiment comparison symbol connectionist learn algorithm learn effici classif procedur applic chess endgam simplifi decis tree learn intern represent error propag learn connect spell sound network model oral read parallel network learn pronounc english text advanc neural inform process system 1 advanc neural inform process system 2 tr ctr thoma g dietterich approxim statist test compar supervis classif learn algorithm neural comput v10 n7 p18951923 oct 1998 kang keysun choi two approach resolut word mismatch problem caus english word foreign word korean inform retriev proceed fifth intern workshop inform retriev asian languag p133140 septemb 30octob 01 2000 hong kong china john case sanjay jain matthia ott arun sharma frank stephan robust learn aid context proceed eleventh annual confer comput learn theori p4455 juli 2426 1998 madison wisconsin unit state miroslav kubat robert c holt stan matwin machin learn detect oil spill satellit radar imag machin learn v30 n23 p195215 feb march 1998 melodi kiang compar assess classif method decis support system v35 n4 p441454 juli rich caruana virginia r de sa benefit variabl variabl select discard journal machin learn research 3 312003 rich caruana multitask learn machin learn v28 n1 p4175 juli 1997 walter daeleman antal van den bosch jakub zavrel forget except harm languag learn machin learn v34 n13 p1141 feb 1999 sreerama k murthi automat construct decis tree data multidisciplinari survey data mine knowledg discoveri v2 n4 p345389 decemb 1998