use model tree classif model tree type decis tree linear regress function leav form basi recent success techniqu predict continu numer valu appli classif problem employ standard method transform classif problem problem function approxim surprisingli use simpl transform model tree inducerm5 base quinlan m5 gener accur classifi stateoftheart decis tree learner c50 particularli attribut numer b introduct mani applic machin learn practic involv predict class take continu numer valu techniqu model tree induct prove success address problem quinlan 1992 wang witten 1997 structur model tree take form decis tree linear regress function instead termin class valu leav numer valu attribut play natur role regress function discret attribut also handledthough less natur way convers classic decisiontre situat classif discret attribut play natur role prompt symmetri situat wonder whether model tree could use classif discov turn classifi surprisingli accur order appli continuouspredict techniqu model tree discret classif problem consid condit class probabl function seek modeltre approxim classif class whose model tree gener greatest approxim probabl valu chosen predict class result present paper show model tree induc use gener classifi significantli accur decis tree produc c50 1 next section explain method use review featur respons good perform experiment result thirtythre standard dataset report section 3 section 4 briefli review relat work section 5 summar result 2 appli model tree classif model tree binari decis tree linear regress function leaf node thu repres piecewis linear approxim unknown function model tree gener two stage first build ordinari decis tree use split criterion maxim intrasubset variat target valu second prune tree back replac subtre linear regress function wherev seem appropri whenev model use predict smooth process invok compens sharp discontinu inevit occur adjac linear model leav prune tree although origin formul model tree linear model intern node use smooth process incorpor leaf model manner describ section first describ salient aspect model tree algorithm describ procedur new paper model tree use classif justif procedur given next subsect follow give exampl infer class probabl artifici situat true probabl known 21 modeltre algorithm construct use model tree clearli describ quinlan 1992 account m5 scheme implement call m5 0 describ wang witten 1997 along implement detail freeli avail version 2 m5 0 use paper differ describ wang witten 1997 improv handl miss valu describ appendix 3 chang tune paramet necessari elabor briefli two key aspect model tree surfac discuss experiment result section 3 first central idea model tree linear regress step perform leav prune tree variabl involv regress attribut particip decis node subtre prune away step omit target taken averag target valu train exampl reach leaf tree call regress tree instead second aspect smooth procedur origin formul occur whenev model use predict idea first use leaf model comput predict valu filter valu along path back root smooth node combin valu predict linear model node quinlan 1992 calcul use model tree classif 3 predict pass next higher node p predict pass node q valu predict model node n number train instanc reach node k constant quinlan default valu use experi implement achiev exactli effect use slightli differ represent final stage model format creat new linear model leaf combin linear model along path back root leaf model automat creat smooth predict without need adjust predict made exampl suppos model leaf involv two attribut x linear coeffici b model parent node involv two attribut z combin two model singl one use formula z 3 continu way root give us singl smooth linear model instal leaf use predict thereaft smooth substanti enhanc perform model tree turn appli equal applic classif 22 procedur figur 1 show diagrammat form model tree builder use classifi cation data taken wellknown iri dataset upper part depict train process lower part test process train start deriv sever new data set origin dataset one possibl valu class case three deriv dataset setosa virginica versicolor varieti iri deriv dataset contain number instanc origin class valu set 1 0 depend whether instanc appropri class next step model tree induc employ gener model tree new dataset specif instanc output one model tree constitut approxim probabl instanc belong associ class sinc output valu model tree approxim necessarili sum one test process instanc unknown class process model tree result approxim probabl belong class class whose model tree give highest valu chosen predict class deriv dataset origin dataset predict class attribut target 44 30 47 32 67 31 58 27 01 attribut target 57 30 attribut class 44 30 47 32 67 31 58 27 versicolor virginica instanc model tree attribut target attribut target 44 30 47 32 67 31 58 27 00 44 30 47 32 67 31 58 27 01 setosa b virginica c versicolor c versicolor 57 30 093 f 57 30 005 57 30 007 figur 1 m5 0 use classif use model tree classif 5 x x 00408y02061 c x 00408y02061 figur 2 exampl use model tree classif class probabl data gener b train dataset c infer class probabl 23 justif learn procedur m5 0 effect divid instanc space region use decis tree strive minim expect mean squar error model tree output target valu zero one train instanc within particular region train instanc lie particular region view sampl underli probabl distribut assign class valu zero one instanc within region standard procedur statist estim probabl distribut minim mean squar error sampl taken devroy gyoerfi lugosi 1996 breiman friedman olshen stone 1984 24 exampl consid twoclass problem true class probabl linear function two attribut x pclassjx depict figur 2a sum 1 point dataset 600 instanc gener randomli accord probabl uniformli distribut x valu chosen probabl x valu use determin whether instanc assign first second class data gener depict 6 frank et al figur 2b class repres fill hollow circl appar densiti fill circl greatest lower left corner decreas toward upper right corner convers true hollow circl data figur 2b submit m5 0 gener two model tree case structur tree gener trivialthey consist singl node root figur 2c show linear function fclassjx repres tree discuss intim excel approxim origin class probabl data gener class boundari point intersect two plane figur 2c exampl illustr classifi base model tree abl repres obliqu class boundari one reason model tree produc m5 0 outperform univari decis tree produc c50 anoth m5 0 smooth regress function adjac leav model tree 3 experiment result experi design explor applic model tree classif compar result decis tree induct linear regress determin compon essenti good perform specif address follow question 1 classifi base model tree compar stateoftheart decis tree classifi base simpl linear regress 2 import linear regress process leav b smooth process answer first question compar accuraci classifi base smooth model tree gener m5 0 prune decis tree gener c50 see m5 0 often perform better howev perform improv might conceiv due aspect procedur m5 0 convert nomin attribut n attribut valu binari attribut use procedur employ cart breiman et al 1984 gener one model tree class test ran c50 use exactli encod transform nomin attribut binari one use procedur employ m5 0 gener one dataset class build decis tree dataset use class probabl provid c50 arbitr class refer result algorithm c50 0 also report result linear regress lr use inputoutput encod investig second question first compar accuraci classifi base model tree gener m5 0 one base smooth regress tree srt note regress tree model tree constant function leaf node thu repres obliqu class bound ari appli smooth oper m5 0 routin appli model tree compar accuraci classifi base smooth model tree m5 0 base unsmooth model tree umt use model tree classif 7 tabl 1 dataset use experi dataset instanc miss numer binari nomin class valu attribut attribut attribut balancescal 625 breastw glass g2 163 glass 214 heartstatlog 270 hepat ionospher iri 150 letter 20000 pimaindian 768 segment 2310 sonar 208 vehicl 846 vote 435 56 0 waveformnois 5000 anneal audiolog 226 20 0 61 8 24 australian auto 205 11 15 4 6 6 breastcanc 286 03 horsecol hypothyroid german 1000 labor 57 39 8 3 5 2 lymphographi 148 primarytumor sick 3772 55 7 soybean 683 98 0 vowel smooth regress tree special case smooth model tree unsmooth tree special case smooth tree minor modif code m5 0 need gener srt umt model 31 experi thirtythre standard dataset uci collect merz murphi 1996 use experi summar tabl 1 first sixteen involv numer binari attribut last seventeen involv nonbinari nomin attribut well 4 sinc linear regress function design numericallyvalu domain binari attribut special case numer attribut expect classifi base smooth model tree particularli appropri first group tabl 2 summar accuraci method investig result give percentag correct classif averag ten tenfold nonstratifi crossvalid run standard deviat ten also shown fold use scheme result c50 star show signific improv correspond result m5 0 vice versa throughout speak result significantli differ differ statist signific 1 level accord pair twosid ttest pair data point consist estim obtain one tenfold crossvalid run two learn scheme compar tabl 3 show differ method compar entri indic number dataset method associ column significantli accur method associ row 32 discuss result answer first question observ tabl 2 m5 0 outperform c50 fifteen dataset wherea c50 outperform m5 0 five number also appear boldfac tabl 3 sixteen dataset numer binari attribut m5 0 significantli accur nine significantli less accur none remain dataset significantli accur six significantli less accur five result show classifi base smooth model tree gener m5 0 significantli accur prune decis tree gener c50 major dataset particularli numer attribut tabl 3 show c50 0 significantli less accur c50 twelv dataset first column last row significantli accur five first row last column significantli less accur m5 0 seventeen dataset significantli accur three result show superior perform m5 0 due chang inputoutput encod complet discuss first question compar simpl linear regress lr m5 0 c50 tabl 3 show lr perform significantli wors m5 0 seventeen dataset significantli wors c50 eighteen lr outperform m5 0 eleven dataset c50 fourteen result linear regress surprisingli good howev dataset applic linear regress lead disastr result one recommend gener techniqu answer second two question begin compar accuraci classifi base m5 0 one base smooth regress tree srt assess import linear regress process leav former incorpor latter tabl 3 show m5 0 produc significantli accur classifi twentythre dataset significantli less accur one two compar c50 prune decis tree classifi base smooth regress tree significantli less accur fifteen dataset use model tree classif 9 tabl 2 experiment result percentag correct classif standard deviat balancescal 776sigma10 864sigma07 867sigma03 753sigma11 788sigma09 789sigma07 breastw 945sigma03 953sigma03 958sigma01 943sigma05 942sigma04 945sigma03 glass g2 787sigma21 818sigma22 704sigma04 755sigma17 793sigma23 788sigma22 glass 675sigma26 705sigma28 600sigma13 676sigma16 678sigma27 700sigma20 heartstatlog 787sigma14 822sigma10 837sigma04 799sigma18 784sigma15 786sigma14 hepat 793sigma12 819sigma22 856sigma15 796sigma15 788sigma30 797sigma11 ionospher 889sigma16 897sigma12 866sigma05 882sigma07 873sigma10 889sigma16 iri 945sigma07 947sigma07 827sigma09 940sigma10 939sigma08 947sigma07 letter 880sigma02 903sigma01 555sigma01 863sigma02 867sigma01 875sigma01 pimaindian 745sigma12 762sigma08 772sigma05 757sigma10 720sigma07 745sigma12 segment 968sigma02 970sigma02 845sigma01 962sigma02 959sigma03 957sigma02 sonar 747sigma28 785sigma34 756sigma18 780sigma24 758sigma27 747sigma28 vehicl 729sigma12 765sigma13 757sigma05 709sigma12 693sigma12 720sigma10 vote 963sigma06 962sigma03 956sigma00 956sigma00 959sigma05 964sigma05 waveformnois 754sigma05 820sigma02 859sigma02 803sigma03 723sigma04 752sigma05 zoo 918sigma11 921sigma13 942sigma18 893sigma15 905sigma13 891sigma14 anneal 987sigma03 988sigma02 931sigma02 973sigma01 985sigma02 990sigma02 audiolog 765sigma14 767sigma10 686sigma16 679sigma12 768sigma18 739sigma09 australian 853sigma05 858sigma09 511sigma36 857sigma07 828sigma09 838sigma11 auto 800sigma25 744sigma19 590sigma15 700sigma22 717sigma18 756sigma17 breastcanc 733sigma16 696sigma23 700sigma15 729sigma10 675sigma24 688sigma17 heartc 768sigma14 809sigma14 850sigma04 797sigma16 763sigma13 788sigma16 hearth 798sigma09 790sigma08 819sigma10 792sigma11 769sigma15 775sigma13 horsecol 853sigma06 846sigma07 827sigma07 845sigma09 834sigma15 845sigma06 hypothyroid 995sigma00 966sigma01 909sigma31 956sigma01 962sigma02 994sigma01 german 712sigma10 729sigma07 754sigma06 741sigma09 699sigma08 716sigma14 krvskp 995sigma01 994sigma01 940sigma01 985sigma01 993sigma01 994sigma01 labor 781sigma48 797sigma46 874sigma61 714sigma36 779sigma36 768sigma45 lymphographi 754sigma28 798sigma14 836sigma13 761sigma16 775sigma29 759sigma22 primarytumor 418sigma13 451sigma16 472sigma09 451sigma13 414sigma12 403sigma21 sick 988sigma01 983sigma01 923sigma25 982sigma00 986sigma01 989sigma01 soybean 913sigma05 925sigma05 873sigma06 884sigma05 913sigma05 923sigma05 vowel 798sigma13 817sigma11 431sigma10 739sigma15 783sigma08 781sigma10 significantli accur five result show linear regress function leaf node essenti classifi base smooth model tree outperform ordinari decis tree final complet second question compar accuraci classifi base m5 0 classifi base unsmooth model tree umt tabl 3 show m5 0 produc significantli accur classifi twentyf dataset significantli less accur classifi one comparison c50 prune decis tree also lead conclus smooth process necessari ensur high accuraci modeltre base classifi tabl 3 result pair ttest p001 number indic often method column significantli outperform method row lr 4 relat work neural network obviou altern model tree classif task appli neural network classif standard procedur approxim condit class probabl function output node neural network approxim probabl function one class contrast neural network probabl function class approxim singl network model tree necessari build separ tree class model tree offer advantag neural network user make guess structur size obtain accur result built fulli automat much effici neural network moreov offer opportun structur analysi approxim class probabl function wherea neural network complet opaqu idea treat multiclass problem sever twoway classif prob lem one possibl valu class appli standard decis tree dietterich bakiri 1995 use c45 quinlan 1993 predecessor c50 gener twoway classif tree class howev found accuraci obtain significantli inferior direct applic c45 origin multiclass problemalthough abl obtain better result use errorcorrect output code instead simpl oneperclass code smyth gray fayyad 1995 retrofit decis tree classifi kernel densiti estim leav order obtain better estim class probabl function although improv accuraci class probabl estim three artifici dataset classif accuraci significantli better moreov result structur opaqu includ kernel function everi train instanc torgo 1997 also investig fit tree kernel estim leav time regress tree rather classif tree could appli classif problem manner model tree advantag abl repres nonlinear class boundari rather linear obliqu class boundari model tree howev suffer incomprehens model employ kernel estim import differ smyth et al 1995 torgo 1997 m5 model tree algorithm m5 smooth model use model tree classif 11 adjac leav model tree substanti improv perform model tree classif problem saw also close relat method linear regress method find linear discrimin compar experiment result obtain ordinari linear regress find although mani dataset linear regress perform well sever case give disastr result linear model simpli appropri 5 conclus work shown classif problem transform problem function approxim standard way success solv construct model tree produc approxim condit class probabl function individu class classifi deriv outperform stateoftheart decis tree learner problem numer binari attribut often problem multivalu nomin attribut although result classifi less comprehens decis tree opaqu produc statist kernel densiti approxim expect time taken build model tree loglinear number instanc cubic number attribut thu model tree class built effici dataset modest number attribut acknowledg waikato machin learn group support new zealand foundat research scienc technolog provid stimul environ research thank anonym refere help construct comment zwitter soklic donat lymphographi primari tumor dataset appendix treatment miss valu explain instanc miss valu treat version use result paper test whenev decis tree call test attribut whose valu unknown instanc propag path result combin linearli standard way quinlan 1993 problem deal miss valu train tackl problem breiman et al 1984 describ surrog split method whenev split valu v attribut consid particular instanc miss valu differ attribut use surrog split instead appropri chosen valu v test v replac v attribut valu v select maxim probabl latter test effect former work describ paper made two alter pro cedur first simplif breiman origin procedur follow let set train instanc node whose valu split attribut known let l subset split v assign left branch r correspond subset right branch defin l r way surrog split v number instanc correctli assign left subnod surrog split correspond number right subnod probabl v predict v correctli estim chosen surrog split v maxim estim wherea breiman choos attribut valu v maxim estim simplif alway choos surrog attribut class continu select optim valu v describ stratagem report wang witten 1997 second differ blur sharp distinct made breiman pro cedur accord origin procedur train instanc whose valu attribut miss assign left right subnod accord whether produc sharp stepfunct discontinu inappropri case v poor predictor v modif employ version m5 0 use present paper soften decis make stochast accord probabl curv illustr figur a1 steep transit determin likelihood test v assign instanc incorrect subnod assess consid train instanc valu attribut known first estim probabl p r v assign instanc miss valu rightmost subnod probabl assign left node probabl instanc incorrectli assign left subnod v estim p il likewis probabl correctli assign right subnod p cr l mean class valu instanc l r correspond valu r estim p r model form x class valu b chosen make curv pass point l p il cr shown figur a1 desir effect approxim sharp step function v good predictor p il 0 p cr 1 decis unimport l r howev predict unreliablethat p il significantli greater 0 p cr significantli less 1the decis soften particularli importantthat l r differ appreci train instanc stochast assign right subnod test surrog split use class use model tree classif 13 v0 class r cr figur a1 soft step function model fit train data valu cours unavail instead instanc propag left right subnod result outcom combin linearli use weight scheme describ quinlan 1993 left outcom weight proport train instanc assign left subnod right outcom proport assign right subnod note 1 c50 successor c45 quinlan 1993 although commerci product test version avail httpwwwrulequestcom 2 see httpwwwcswaikatoacnzml 3 realist evalu standard dataset imper miss valu accom modat remov instanc miss valu half dataset lower part tabl would instanc usabl 4 follow holt 1993 g2 variant glass dataset class 1 3 combin class 4 7 delet horsecol dataset attribut 3 25 26 27 28 delet attribut 24 use class also delet identifi attribut dataset r classif regress tree probabilist theori pattern recognit solv multiclass learn problem via errorcorrect output code simpl classif rule perform well commonli use dataset uci repositori machin learn databas httpwww learn continu class retrofit decis tree classifi use kernel densiti estim ca morgan kaufmann kernel regress tree induct model tree predict continu class tr c45 program machin learn simpl classif rule perform well commonli use dataset ctr niel landwehr mark hall eib frank logist model tree machin learn v59 n12 p161205 may 2005 donato malerba floriana esposito michelangelo ceci annalisa appic topdown induct model tree regress split node ieee transact pattern analysi machin intellig v26 n5 p612625 may 2004 rudi setiono feedforward neural network construct use cross valid neural comput v13 n12 p28652877 decemb 2001 v zorkadi karra panayot effici inform theoret strategi classifi combin featur extract perform evalu improv fals posit fals neg spam email filter neural network v18 n56 p799807 june 2005 ronni kohavi j ross quinlan data mine task method classif decisiontre discoveri handbook data mine knowledg discoveri oxford univers press inc new york ny 2002 duncan pott increment learn linear model tree proceed twentyfirst intern confer machin learn p84 juli 0408 2004 banff alberta canada duncan pott claud sammut increment learn linear model tree machin learn v61 n13 p548 novemb 2005 saso deroski bernard enko combin classifi stack better select best one machin learn v54 n3 p255273 march 2004 eib frank leonard trigg geoffrey holm ian h witten technic note naiv bay regress machin learn v41 n1 p525 oct 2000 joo gama function tree machin learn v55 n3 p219250 june 2004 p solomatin b siek modular learn model forecast natur phenomena neural network v19 n2 p215224 march 2006 musavi h ressom srirangam p natarajan r w virnstein l j morri w tweedal neural networkbas light attenu model monitor seagrass popul indian river lagoon journal intellig inform system v29 n1 p6377 august 2007