processor map techniqu toward effici data redistribut abstractruntim data redistribut enhanc algorithm perform distributedmemori machin explicit redistribut data perform algorithm phase differ data decomposit expect deliv increas perform subsequ phase comput redistribut howev repres increas program overhead algorithm comput discontinu data exchang among processor memori paper present techniqu minim amount data exchang block cyclicc viceversa redistribut arbitrari number dimens preserv semant target destin distribut pattern techniqu manipul data logic processor map target pattern implement ibm sp map techniqu demonstr redistribut perform improv approxim 40 tradit data processor map rel tradit map techniqu propos method afford greater flexibl specifi precis data element redistribut element remain onprocessor b introduct effort standard dataparallel fortran program distributedmemori machin high perform fortran forum compos forti academ industri government agenc propos hpf high perform fortran 1 mani concept origin propos fortran 2 vienna fortran 3 dataparallel fortran languag incorpor hpf fundament compon hpf specif distribut align data array compil direct due nonuniform memori access time characterist distributedmemori machin determin appropri data decomposit critic perform dataparallel program machin data distribut deal data array distribut among processor memori data align specifi colloc data array goal data decomposit maxim system perform balanc comput load among processor minim remot memori access commun messag data distribut wellsuit one phase algorithm may good term perform subsequ phase therefor hpf support explicit runtim data redistribut redistribut may also occur implicitli subprogram boundari result runtim oper eg data realign use data redistribut repres perform tradeoff expect higher effici new distribut subsequ comput commun cost redistribut data among processor memori consequ minim execut time data redistribut obviou merit reduc amount data exchang among processor memori one possibl optim toward reduc overal redistribut execut time subject paper present techniqu minim amount data exchang among processor memori block cyclicc viceversa redistribut arbitrari number dimens preserv semant target destin distribut pattern techniqu manipul data logic processor map target pattern clearer present map techniqu view data static manipul processor map data implement techniqu redistribut oper would specifi map data processor sinc map techniqu work within realm logic processor architectureindepend thu could incorpor differ redistribut implement variou distributedmemori architectur section 2 provid summari data redistribut issu relat work section 3 present map techniqu appli onedimension data prove map techniqu achiev minim data movement section 4 show natur extens techniqu mdimension data section 5 discuss impact use techniqu programm compil respect addit compar data redistribut execut time perform propos map techniqu tradit dataprocessor map section 6 conclud paper 2 data redistribut mani research espous util incorpor redistribut capabl dataparallel languag kali languag 4 one first incorpor runtim data redistribut mechan dino 5 address implicit redistribut data subprogram boundari hall et al 6 discuss global optim employ set redistribut call fortran program hypertask compil 7 dataparallel c program incorpor runtim redistribut facil chapman et al 8 introduc vienna fortran dynam data distribut capabl discuss highlevel implement vienna fortran engin vfe figur present segment hpf code illustr use data redistribut hpf array real number initi distribut onto threedimension processor configur p distribut pattern block cyclic appli dimens respect p contain forti processor processor own 20theta12theta10 data element follow amount comput redistribut new set pattern cyclic cyclic block onto differ shape logic processor configur q figur 2 illustr initi distribut left subsequ redistribut right shade portion figur depict data element own processor 000 wherea 000 own data global contigu locat initi own data global noncontigu locat follow redistribut note processor configur q also consist forti processor howev p q vari shape exampl illustr one redistribut possibl mani distinct combin sourc destin distribut pattern possibl hpf requir howev data processor distribut pattern declar equal dimension eg fig 1 data processor pattern declar threedimension hpf requir sourc target processor configur equal size henceforth denot initi sourc distribut pattern program destin target distribut pattern similarli shall refer sourc target processor configur p p respect note p embed primit fig 1 real comput comput figur 1 exampl hpf figur 2 redistribut blockcycl cycliccyclicblock previou exampl illustr data redistribut result chang map data processor thu necessit data exchang among processor perspect send processor data exchang view simultan scatter data element p p ie processor p perform onetomani send oper differ data recipi p altern perspect receiv processor data exchang view simultan gather data element p p ie processor perform manytoon receiv oper distinct data sender p figur 3 illustr commun pattern jp processor particip scatter gather oper data exchang view alltoal person commun 9 order perform data redistribut processor must determin ident processor receiv data well ident processor must send data set comput processor exchang data recal defin redistribut logic processor therefor logic physic processor map dictat actual data movement among memori machin exampl logic processor map physic node would obvious requir messagepass exchang data figur 3 redistribut commun pattern sever research effort focus effici method determin send receiv processor set redistribut gupta et al 10 deriv close form express commun set base fortran ds block cyclic blockcycl distribut pattern approach global array templat indic combin knowledg use comput send receiv set altern approach node scan local data array determin destin processor element place element messag packet bound processor 11 onedimension data redistribut perform distinct algorithm differ combin distribut pattern multidimension redistribut implement seri sequenti onedimension redi tribut stichnoth et al 12 propos method comput ownership set array assign statement due similar determin send receiv set advoc comput togeth send processor commun inform togeth data receiv approach chiefli intend commun righthand side operand array assign statement incorpor data redistribut pitfal 13 mathemat represent regular distribut facilit determin processor set data redistribut pitfal robustli handl arbitrari sourc target processor configur arbitrari number data array dimens pitfal develop inclus paradigm 14 compil project univers illinoi research present 10 11 12 13 focu effici comput sendrec processor set rather effici actual data exchang portion redistribut found latter oper sever order magnitud costli term execut time send receiv set determin 15 data exchang distributedmemori machin perform either pointtopoint collect commun messagepass primit techniqu effici multipl scatter gather oper present 16 mckinley et al 17 survey issu regard alltoal commun wormholerout machin effici simultan redistribut data among physic processor affect topolog rout switch mechan underli machin techniqu communicationeffici data redistribut address messag content certain topolog present 18 author propos data redistribut commun cost model parameter number messag size network content model express commun sequenc permut may execut fix number contentionfre step multiphas redistribut defin redistribut data intermedi distribut pattern eventu arriv destin distribut model use conjunct multiphas redistribut show lower overal cost achiev compar singlephas redistribut reshap perfect poweroftwo size array hypercub discuss 19 earliest work optim data redistribut minim data exchang regular hpf distribut pattern present 20 relat redistribut optim strategi propos wakatani wolf 21 explor logic physic processor map redistribut reduc commun overhead techniqu assum underli toru topolog map data way commun processor partit nonoverlap set thu keep commun local group messag content physic network reduc drawback approach logic physic data map base local inform word techniqu impart map base sole best optim redistribut howev redistribut part larger dataparallel program logic physic data map must remain consist throughout execut program logic physic processor map determin redistribut oper may best map overal perform entir applic assert compil privi global inform ought determin logic physic processor map possibl howev manipul data elementlog processor map target ie map p long semant target distribut pattern violat topic remaind paper onedimension logic processor map begin illustr util propos map techniqu onedimension data figur 4 illustr initi block distribut sixteen element data array onto eight logic processor subsequ redistribut array use cyclic pattern map initi distribut data onto physic node machin establish program initi see arrow label 1 subsequ redistribut among processor must retain consist logic physic processor map see arrow label 2 logic processor id lpid bold ital superimpos data processor 1 own two contigu element block two noncontigu element stride p number processor cyclic lpid map increas numer order specifi 1 claim unnecessari restrict permut lpid 07 conform semant cyclic pattern sinc data distribut processor hpf requir specif order processor cyclic requir global data element own processor global indic separ stride p figur 5 illustr benefit permut lpid map data element use sixteen element array fig 4 show two altern redistribut array cyclic choic 1 show convent cyclic map lpid data result two sixteen data element mark fill rectangl remain processor follow redistribut call number data hit among processor choic 2 show altern cyclic map lpid permut 04152637 result total eight data element one per processor remain origin memori elimin exchang six data element among processor anoth advantag processor 16 must send data two processor use choic 1 one processor use choic 2 thu reduc number destin factor reduc interprocessor commun overhead reduct redistribut cost exampl fig 5 small given size exampl extend sixteen million data element permut lpid elimin exchang six million data element across processor 2 31 processor map techniqu consist minim size data transfer arbitrari data block processor set size develop systemat method determin permut lpid map data tech 1 henceforth logic processor unless otherwis state 2 assum number processor kept block size block two million block size cyclicc one million physic nodes141012145715141012145715 logic processor id onto p8 onto p8 initi distribut data physic node perform compiler135713502466 redistribut data perform runtim bold ital 1 2 figur 4 data distribut redistribut onto physic node niqu ensur processor retain maximum amount data possibl conform semant sourc target distribut pattern establish upper bound ratio amount data retain p processor total number data element n present function determin lpid data map redistribut block cyclicc 3 achiev upper bound make follow assumpt 1 let p number processor number 0p gamma 1 n total number data element number 0n gamma 1 distribut p assum processor own b element thu 2 consid redistribut block cycliccpattern blockb pattern b variabl consid assumpt 1 cyclicc assum c divid b ie integ z 3 redistribut symmetr term amount data movement among processor ie redistribut block cyclicc redistribut cyclicc block result equal amount data movement direct redistribut choic 1 choic 2 figur 5 logic processor data map altern 3 data initi distribut among p processor assum data redistribut among p processor r number data element global array remain data hit origin processor follow redistribut block cyclicc defin hitratio r n maxhitratio upper bound hitratio c block size block cyclicc pattern respect z integ maxhitratio integ everi cycl cp data element map c contigu data element cyclicc sinc complet cycl lpid map one complet data block own lpid j block pattern processor lpid j map exactli ic element ie ic data hit p processor number data hit cp ecp hitratio b case 2 integ sinc b icp complet cycl lpid map lpid 0 origin block data block thu number hit greater lpid consequ number hit across processor greater icp remaind proof follow case 1 2 maxhitratio achiev permut lpid z integ multipl number processor ie z ip howev goal achiev upper bound valu z ip order satisfi aim must consid differ permut lpid maxim number data hit figur 5 demonstr permut lpid yield maxhitratio next use semant defin function determin permut lpid map data array 32 map function defin ptupl q place holder let f function map lpid place holder q j 0 assign lpid place holder specifi permut lpid repres map lpid data element cyclicc distribut pattern p possibl permut p processor may case mani permut yield ratio maxhitratio howev exhaust test permut determin whether produc ratio would impract sinc would requir exponenti amount comput gener p therefor present function determin permut achiev maxhitratio b p c lpid map uniqu q fi equat 1 specifi function map lpid q fi intuit behind map function first view place holder q j circular list function map lpid 0 place holder q 0 map lpid 1 z place lpid 0 map lpid 2 z place lpid 1 gener map lpid i1 z mod p place lpid figur 6 illustr behavior f appli differ valu z p simplic choos reduc cyclic part show exampl case 4 map broken row better illustr distanc consecut lpid distinct two exampl onetoon part f onetoon part b part b one lpid map locat lpid map place holder formal depend valu z p possibl f yield one permut part produc six possibl permut 4 part b sinc lpid 0 lpid 3 map q 0 turn 4 optim term maxhitratio arbitrarili map two lpid place holder q 0 q 1 hold true lpid 2 lpid 5 place holder q 2 q 3 lpid 1 lpid 4 place holder q 4 q 5 shall prove result later lemma figur 6 map lpid place holder 33 optim map function given arbitrari z p gcdz p determin whether f onetoon lemmata 2 3 establish lemmata 4 5 establish f achiev maxhitratio whether onetoon z natur number z bc let gcdz p greatest common divisor z p gcdz establish onetoon map place holder q word gcdz proof proof contradict assum gcdz choos k j recal map function f map lpid i1 z mod p place lpid let map distanc rz mod p place holder lpid j sinc map place holder distanc modp zero ie rz mod must z mod impli assumpt lemma 3 let p z natur number z bc gcdz q fi proof show divid z jz p mp mod arbitrari thu second term sum jz p left establish two lemma captur behavior f equat 1 natur shown relationship gcdz p function f fig 6 thu f onetoon part b gcdz map two lpid place holder q next establish two lemma show f produc permut alway yield maxhitratio lemma 4 let p z natur number z bc gcdz determin singl permut lpid achiev maxhitratio proof sinc f map lpid i1 z place lpid lpid map cyclic first data element data block block figur 7 illustr situat arbitrari lpid thu alway least one data hit per lpid case 1 z p exactli c data hit per lpid map cycl begin ensur c data hit sinc z p equival b cp lpid map anoth c data element bsize data block would violat semant cyclic map thu exactli c hit per processor cp hit processor follow case 2 z p least c data hit per lpid establish sinc lpid map first element data block also map cp 1th element well sinc z p see fig 7 let integ divis lpid map element number total cj element 5 thu cp ec data hit per lpid b cp ecpdata hit lpid b c cp element data figur 7 lpid map first element data block lemma 5 let p z natur number z bc k lpid map q ik f remap k way place holder permut yield maxhitratio proof lemma 3 establish k 1 k lpid name lpid ijpk place holder q fi consequ k lpid map first data element lpid 0 data block block case 1 z p b cp exactli c data hit per lpid figur 8 illustr situat z p k z thu ck cz b furthermor k lpid c data element fit lpid 0 bsize data block lpid map place holder q fi remap one first ck data element regardless permut k lpid sinc lpid group c data hit lpid appear within data block sinc b cp therefor exactli c data hit lpid cp b cp ecp data hit total among p processor case 2 z p b cp least c data hit per lpid establish integ mcp 1cp mc data hit lpid sinc p lpid 5 data element number 0b gamma 1 data block own lpid c data data element element figur 8 z p map least cycl bsize data block claim 1c data hit even figur 9 illustr situat group p processor map csize block lpid 0 data block block must show lpid map one last element data block word show ck b gamma mcp proof contradict assum ck exist two integ z rk ck c z z z z sinc pk 1 contradict thu cgcdz mcp given result lpid 0 csize block must appear within last b gamma mcp element data block regardless permut order therefor data hit per lpid 1cp p processor 2 cp cp b mcp cp group element data figur 9 z p lemmata 1 5 prove follow result theorem 1 redistribut block cyclicc b c respect block integ z p number processor number global data array element logic processor map function equat 1 achiev maxhitratio multidimension logic processor map section extend logic processor map techniqu present section 3 mdimension data array specif extend techniqu optim logic processor map redistribut block block block cyclicc 0 addit demonstr approach redistribut twodimension data 1 initi distribut dimens subsequ redistribut one dimens eg blockblock cyclic 2 initi distribut one dimens redistribut dimens eg block 41 mdimension redistribut extend techniqu appli onedimension lpid map dimens global data array let mdimension data array n 0 theta n 1 theta n 2 theta theta nmgamma1 data element processor direct hpf declar processor arrang specifi name rank extent dimens let processor arrang distribut set block size cyclic block size respect dimens extend equat 1 equat 2 map rectilinear set lpid p j n j data element jth dimens mdimension data redistribut maxhitratio similar ratio present section 3 product maxhitratio dimens present lemma substanti result lemma 6 maxhitratio mdimension global array defin b 0 c map function equat 2 achiev upper bound proof lemmata 1 5 establish b maxhitratio onedimension data map function f achiev upper bound mdimension data product upper bound dimens yield maximum data hit ratio mdimension array function g gener f appli g respect dimens upper bound achiev 2 figur appli data matrix distribut across 3 theta 4 processor grid lpid bold ital 6 redistribut data matrix blockblockto cyclic3cyclic2 lpid data map dimens mark block tradit cyclic map lpid data indic trad optim techniqu use g shown opt key right figur indic data hit follow redistribut tradit optim map tradit map yield hitratio 12 map use g result hitratio 6 reduc 1 4 threefold increas number data hit tradit map theorem 1 lemma 6 prove follow result 6 subscript figur label denot number processor given dimens theorem 2 redistribut block block block cyclicc 0 respect block size z integ z number processor n number data element dimens logic processor map function equat 2 achiev theta b c block opt trad0000 map data hit data hit data element trad opt figur 10 block 3 block 4 cyclic 3 3cyclic 4 2 redistribut 42 twodimension onedimension redistribut redistribut mdimension array need necessarili involv redistribut dimen sion instanc data matrix may initi distribut blockblock redistribut cyclic row dimens distribut extend map techniqu case defin vector equat 3 defin new function map cartesian lpid placehold lemma 7 establish maxhitratio twodimension onedimension redistribut base valu gcdz p 0 lemmata 8 9 establish map function h achiev max hitratio lemma 7 redistribut block 7 cyclicc 0 let b 0 block size row dimens c 0 cyclic block size n 0 theta n 1 data matrix proof proof quit similar proof lemma 1 twodimension grid processor remap vector place holder sinc distribut one dimens substitut p 0 p 1 p lemma 1 remaind proof follow point thu b 0 count number hit row dimens number multipli block size column dimens b 1 obtain total number data hit 2 prove map function h yield maxhitratio first establish correspond lpid place holder show map yield given ratio sinc column coordin lpid relev valu comput h lpid row coordin r map place holder q hr sinc exactli lpid map place holder thu greater p 0 place holder map h lemma 8 establish h achiev maxhitratio lpid row coordin r map place holder lemma 9 establish result kp 1 lpid map place holder lemma 8 let z natur number exactli p 0 place holder map hr p 1 lpid map place holder q hr remap way place holder q hr q hrs1 q hrs2 q permut yield proof first conjectur lemma follow directli lemma 2 proof second part lemma similar proof lemma 5 instead map place holder lemma 5 p 1 lpid map place 7 denot hpf distribut pattern holder establish previous remaind proof follow make substitut maxhitratio result lemma 5 except b 1 must factor sinc number data element row contribut number data hit 2 lemma 9 let z natur number place holder map hr hr map lpid rjp 0 ks place holder q hr 1 lpid map place holder q hr remap p 1 k way place holder q permut yield proof first portion lemma follow directli lemma 3 proof second part lemma similar proof lemmata 5 8 current situat lpid map place holder remaind proof follow make substitut maxhitratio lemma 8 2 figur 11 demonstr map function h appli 24 theta 16 matrix matrix origin distribut blockblock 3 theta 2 processor grid redistribut cyclic2 six processor vector lpid bold ital lpid rs identifi processor cartesian system 1 block size row dimens blockblock eight b cyclic block size cyclic2 two fig 11 show tradit map sinc map set cartesian lpid vector lpid undefin hpf use h map lpid produc hitratio 4 distribut column inconsequenti number data hit achiev sinc row distribut word redistribut block cyclicc cyclic would result number data hit redistribut blockblock cyclic addit lpid row index r could permut data hit ratio achiev eg lpid 00 lpid 01 figur 12 show exampl gcdz processor lpid 0s lpid 2s could permut 6 possibl way optim data hit ratio would obtain lemmata 7 9 prove follow result theorem 3 redistribut block cyclicc 0 b 0 c 0 respect block size row dimens integ z b 1 block size column dimens defin grid processor n 0 theta n 1 number global data array block data element data hit figur 11 block 3 block 2 cyclic 6 2 redistribut element logic processor map function equat 3 achiev 43 onedimension twodimension redistribut anoth possibl data matrix initi distribut one dimens redistribut two dimens deriv new map function equat 4 redistribut block cyclicc 0 sinc specifi twodimension data distribut declar twodimension grid place holder q 1 function map processor lpid place holder q rs thu comput order pair establish maxhitratio onedimension twodimension redistribut base valu gcdz p 0 lemmata 11 12 prove achiev maxhitratio 106 214 203 022 0001216200246811141822 block data element data hit figur 12 block 4 block 3 cyclic 12 1 redistribut redistribut block cyclicc 0 let b 0 block size row dimens c 0 cyclic block size n 0 theta n 1 data matrix proof proof quit similar proof lemmata 1 7 singl dimens case lemma 1 redistribut block cyclicc consid current situat first dimens block cyclicc thu proof lemma 1 appli lemma 7 must includ factor b 1 second dimens 2 lemma 11 let z natur number place holder map map yield maxhitratio b 0 proof proof similar proof lemma 4 sinc map lpid i1 z place thu lpid map first row data block block pattern sinc lpid own b 1 element second dimens factor contribut maxhitratio 2 lemma 12 let z natur number place holder map place holder p 1 dimens map place holder p 0 dimens map exactli k lpid map place holder k lpid map place holder q rs remap place holder q k way achiev maxhitratio b 0 proof proof similar proof lemma 5 lemma 9 current situat second p 1 processor dimens place holder dimens map dimens case onedimension map lemma 5 sinc lpid own b 1 element second dimens factor contribut maxhitratio 2 figur 13 illustr lpid map function appli 24 theta 24 matrix matrix initi distribut block across six processor redistribut cyclic2 block across 3 theta 2 processor grid lpid bold ital superimpos matrix data hit ratio use map function 4 exampl gcdz fig 14 illustr situat gcdz 2 latter figur demonstr flexibl permut lpid instanc processor lpid 0 lpid 2 could permut data hit ratio would achiev lemmata prove follow result theorem 4 redistribut block cyclicc 0 b 0 c 0 respect block size row dimens integ z b 1 block size column dimens defin grid processor n 0 theta n 1 number global data array element logic processor map function equat 4 achiev 5 analysi perform section discuss possibl impact dataparallel programm compil respect use optim map techniqu present execut time benefit block data data hit figur 13 block 6 cyclic 3 2block 2 redistribut optim map techniqu compar tradit map techniqu number data redistribut case 51 effect optim map programm permut lpid optim map function section 3 4 may incur undesir side effect thu may suitabl redistribut instanc instanc programm may lose neighbor data relationship fig 5 tradit cyclic map lpid 1 neighbor 8 optim map lpid 0 4 neighbor suppos programm impart particular logic physic processor map given program util inform maintain neighbor data relationship physic processor order preserv relationship programm may favor tradit processordata 8 processor neighbor own data element adjac perspect global data data element data hit figur 14 block 12 cyclic 4 block 3 redistribut map optim map call data redistribut figur 15 illustr static ie vari one program execut anoth logic physic processor map logic processor alway map physic node pi tradit map ie 0123 preserv neighbor processor relationship b neighbor data relationship physic node corrupt use optim map ie 0213 make p0 p2 becom neighbor previous p0 p1 neighbor compil runtim system uniqu determin logic physic processor map howev programm unabl maintain neighbor data relationship physic node inde indirectnetwork multicomput eg ibm spx workstat cluster concept neighbor node furthermor alloc parallel job thu data may vari distinct execut program sinc differ physic processor may alloc job figur 16 illustr number program execut alloc logic processor physic node differ invoc top part fig logic physical13 p3 tradit logic physical3 p3 b optim figur 15 tradit optim map physic node map static tradit processordata map three distinct program execut bottom portion fig 16 illustr optim processordata map three distinct program execut perspect physic node machin permut lpid whether tradit optim transpar programm argu situat use optim map alway justifi sinc neighbor data relationship physic node maintain exploit programm logic physical3 p3 b second execut logic physical13 p3 first execut logic physical32 c third execut p3 optim logic physic p3 first execut logic physic c third execut p3 logic physic p3 b second execut figur 16 tradit optim map physic node map dynam permut lpid also facilit greater flexibl programm may want influenc data element redistribut processor remain onprocessor tradit map techniqu programm one choic ie lpid map increas numer order optim map techniqu often sever option sinc number lpid may map place holder see fig 6b compil support programmerspecif lpid permut user inher greater control data processor map flexibl may becom even signific data align introduc situat number data element may map local indic fewer data element map indic therefor abl influenc indic redistribut indic remain onprocessor could enhanc overal perform 52 effect optim map compil loss neighbor data relationship may complic role compil gener spmd node program hpf sourc code let us examin simpl exampl let 16element array initi distribut block redistribut cyclic see fig 5 assum follow refer pattern appear compilergener spmd code follow call redistribut cyclic use tradit map techniqu compil gener follow commun paradigm obtain offprocessor element exclud boundari processor lpid commun lpid igamma1 lpid i1 optim map techniqu neighbor processor relationship maintain thu commun paradigm use exampl lpid 3 commun lpid 6 lpid 7 lpid 4 commun lpid 0 lpid 1 problem easili overcom howev compil util place holder map inform determin optim map function let 7 array place holder lpid gener equat 1 optim map compil would specifi neighbor commun nonboundari processor follow lpid place holder q j commun lpid q j gamma1 q j1 essenti compil perform tabl lookup determin neighbor lpid relationship extens boundari processor straightforward exclud present discuss 53 perform integr optim map techniqu data redistribut librari darel 15 written c use mpif 22 perform result obtain ibm spx argonn nation laboratori 23 assess runtim perform optim map techniqu compar data redistribut execut time use optim techniqu redistribut execut time use tradit map figur 19 illustr variou perform comparison two map tech niqu figur 17 show block cyclicc redistribut 8 node rang matrix size 32thousand 34million 4byte float point element block size cyclicc pattern row dimens maintain onehalf block size b block distribut optim map techniqu appli row dimens matrix demonstr significantli lower execut time tradit map optim techniqu achiev redistribut result roughli 60 time need redistribut use tradit techniqu larger valu c respect b greater effect optim map techniqu overal execut time significantli data hit occur rel tradit map smaller valu c rel b benefit optim techniqu lessen sinc rel differ number data hit reduc redistribut instanc regardless block size b c global data size find optim techniqu outperform equal tradit map figur 19 demonstr blockblock cyclicccyclicc redistribut 12 logic 4 theta 3 24 logic 6 theta 4 processor configur respect matrix size rang 91million float point number perform plot map techniqu appli dimens matric optim map redistribut outperform tradit map case block size c maintain onesixth oneeighth respect block size row column dimens matric execut time advantag optim map techniqu remain consist larger 24 processor configur well comput send receiv processor set includ execut time plot shown execut time attribut calcul repres small fraction overal total ie order hundr microsecond small data size order ten millisecond largest data size plot see 15 detail 6 conclus distinct comput phase algorithm perform effici differ data distribut pattern effici runtim redistribut enhanc overal algorithm perform effect use data redistribut dataparallel program promot minim runtim cost perform data exchang among node machin import aspect reduc redistribut cost minim amount data move among processor memori without violat semant distribut pattern paper present techniqu map logic processor id data element data redistribut prove techniqu maxim ratio data retain local tradit optim matrix size 4byte float time sec figur 17 block cyclicc 8 theta 1 processor tradit optim matrix size 4byte float time sec figur 18 blockblock cyclicccyclicc 4 theta 3 processor tradit optim matrix size 4byte float time sec figur 19 blockblock cyclicccyclicc 6 theta 4 processor total amount data exchang among processor perform redistribut block cyclicc arbitrari number dimens architectureindepend techniqu improv data redistribut execut time perform approxim 40 wide rang data size examin impact techniqu dataparallel programm compil respect believ minim amount interprocessor data exchang effect optim data redistribut dataparallel program work remain gener techniqu arbitrari block size necessarili integ multipl number processor pad data array becom integ multipl number processor one possibl addit extend techniqu redistribut differ size processor set redistribut cyclicc 1 area futur studi acknowledg grate argonn nation laboratori staff use ibm spx also thank anonym refere construct comment r high perform fortran languag specif version 10 draft fortran languag specif languag specif version 11 program distribut memori architectur use kali dino parallel program languag interprocedur compil fortran mimd machin hypertask support dynam redistribut resiz array ipsc dynam data distribut distribut rout algorithm broadcast person commun hypercub gener effici data commun distributedmemori machin runtim array redistribut hpf program gener commun array statement design implement evalu automat gener effici array redistribut routin distribut memori multicomput commun optim use paradigm compil distributedmemori multicomput darel portabl data redistribut librari distributedmemori machin effici algorithm index oper messagepass system survey collect commun wormholerout massiv parallel comput approach communicationeffici data redistribut complex reshap array boolean cube processor map techniqu toward effici data redistribut optim redistribut array distribut memori multicomput mpi ibm sp1sp2 current statu futur direct tr ctr jihwoei huang chihp chu effici commun schedul method processor map techniqu appli data redistribut journal supercomput v37 n3 p297318 septemb 2006 wang minyi guo dame wei divideandconqu algorithm irregular redistribut parallel compil journal supercomput v29 n2 p157170 august 2004 chinghsien hsu shengwen bai yehch chung chuse yang gener basiccycl calcul method effici array redistribut ieee transact parallel distribut system v11 n12 p12011216 decemb 2000 frddric desprez cyril randriamaro jack dongarra antoni petitet yve robert schedul blockcycl array redistribut ieee transact parallel distribut system v9 n2 p192205 februari 1998 chinghsien hsu shihchang chen chaoyang lan schedul contentionfre irregular redistribut parallel compil journal supercomput v40 n3 p229247 june 2007 yehch chung chinghsien hsu shengwen bai basiccycl calcul techniqu effici dynam data redistribut ieee transact parallel distribut system v9 n4 p359377 april 1998 chinghsien hsu spars matrix blockcycl realign distribut memori machin journal supercomput v33 n3 p175196 septemb 2005 chinghsien hsu yehch chung effici method kr r r kr array redistribution1 journal supercomput v12 n3 p253276 may 1 1998 chinghsien hsu yehch chung donlin yang chyiren dow gener processor map techniqu array redistribut ieee transact parallel distribut system v12 n7 p743757 juli 2001 chinghsien hsu yehch chung chyiren dow effici method multidimension array redistribut journal supercomput v17 n1 p2346 aug 2000 minyi guo yi pan improv commun schedul array redistribut journal parallel distribut comput v65 n5 p553563 may 2005 minyi guo ikuo nakata framework effici data redistribut distribut memori multicomput journal supercomput v20 n3 p243265 novemb 2001 chinghsien hsu kunm yu compress diagon remap techniqu dynam data redistribut band spars matrix journal supercomput v29 n2 p125143 august 2004 peizong lee wenyao chen gener commun set array assign statement blockcycl distribut distribut memori parallel comput parallel comput v28 n9 p13291368 septemb 2002 stavro souravla mano roumelioti pipelin techniqu dynam data transfer multiprocessor grid intern journal parallel program v32 n5 p361388 octob 2004 antoin p petitet jack j dongarra algorithm redistribut method blockcycl decomposit ieee transact parallel distribut system v10 n12 p12011216 decemb 1999