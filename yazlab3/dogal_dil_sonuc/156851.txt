weight nearest neighbor algorithm learn symbol featur past nearest neighbor algorithm learn exampl work best domain featur numer valu domain exampl treat point distanc metric use standard definit symbol domain sophist treatment featur space requir introduc nearest neighbor algorithm learn domain symbol featur algorithm calcul distanc tabl allow produc realvalu distanc instanc attach weight instanc modifi structur featur space show techniqu produc excel classif accuraci three problem studi machin learn research predict protein secondari structur identifi dna promot sequenc pronounc english text direct experiment comparison learn algorithm show nearest neighbor algorithm compar superior three domain addit algorithm advantag train speed simplic perspicu conclud experiment evid favor use continu develop nearest neighbor algorithm domain one studi b introduct learn classifi object fundament problem artifici intellig field one attack mani angl despit mani success domain task proven difficult due either inher difficulti domain lack suffici data learn exampl instancebas learn program also call exemplarbas salzberg 1990 nearest neighbor cover hart 1967 method learn store exampl point featur space requir mean measur distanc exampl aha 1989 aha kibler 1989 salzberg 1989 cost salzberg 1990 exampl usual vector featur valu plu categori label featur numer normal euclidean distanc use compar exampl howev featur valu symbol unord valu eg letter alphabet natur interlett distanc nearest neighbor method typic resort much simpler met ric count featur match towel et al 1990 recent use metric nearest neighbor algorithm compar studi simpler metric may fail captur complex problem domain result may perform well paper present sophist instancebas algorithm design domain featur valu sym bolic algorithm construct modifi valu differ tabl style stanfil waltz 1986 produc noneuclidean distanc metric introduc idea except space result weight attach individu exampl combin two techniqu result robust instancebas learn algorithm work domain symbol featur valu describ seri experi demonstr algorithm pebl perform well three import practic classif problem comparison given show algorithm accuraci compar back propag decis tree learn algorithm result support claim nearest neighbor algorithm power classifi even featur symbol 11 instancebas learn versu model power instancebas method demonstr number import real world domain predict cancer recurr diagnosi heart diseas classif congression vote record aha kibler 1989 salzberg 1989 experi demonstr instancebas learn ibl appli effect three main featur unord symbol valu 1 predict protein secondari structur 2 word pronunci 3 predict dna promot sequenc domain receiv consider attent connectionist research employ back propag learn algorithm sejnowski rosenberg 1986 qian sejnowski 1988 towel et al 1990 addit word pronunci problem subject number comparison use machin learn algorithm stanfil waltz 1986 shavlik et al 1989 dietterich et al 1990 domain repres problem consider practic im portanc symbol featur valu make difficult convent nearest neighbor algorithm show nearest neighbor algorithm pebl base stanfil waltz 1986 valu differ method produc highli accur predict model domain intent compar ibl learn method three respect classif accuraci speed train perspicu ie eas algorithm represent understood compar perform first respect superior latter two argu ibl often prefer learn algorithm type problem domain consid paper instancebas learn shown compar favor algorithm eg decis tree rule wide rang domain featur valu either numer binari eg aha 1989 aha kibler 1989 aha et al 1991 salzberg 1989 paper present similar evid term classif accuraci domain symbol featur valu howev describ domain consid advantag instancebas learn algorithm provid train time neural net learn algorithm requir vastli time train machin learn method train normal perform repeatedli present network instanc train set allow gradual converg best set weight task use exampl back propag algorithm weiss kapoulea 1989 mooney et al 1989 shavlik et al 1989 report back propag train time mani order magnitud greater train time algorithm id3 frequent factor 100 addit neural net algorithm number paramet eg momentum paramet need tweak programm may requir much addit time weiss kapoulea experi requir mani month experiment time produc result back propag algorithm typic requir hour paramet might adjust algorithm valu r distanc metric see consid two possibl valu 2 nearest neighbor algorithm requir littl train time term experiment time process time present two version pebl algorithm one slightli complex simpler version call unweight version experiment section requir odn number exampl number featur dimens per exampl v number valu featur may gener n much larger v 2 henc complex usual odn complex version pebl increment comput weight exemplar requir train instanc 1 train classif 1 refer system take minut real time decstat 3100 train 17142 instanc experiment time limit minut defin time nearest neighbor system worst odn admittedli slow compar algorithm nearest neighbor method lend well parallel produc significantli faster classif time exampl assign separ processor parallel architectur enough processor exampl divid among number processor larg train set classif time may reduc od log n implement system set four looselycoupl transput well convent architectur recent effort fgp fertig gelernt 1991 use larger number parallel processor mbrtalk system waltz stanfil 1986 implement form knearestneighbor learn use tightlycoupl massiv parallel architectur 64000processor connect machin tm perspicu instancebas learn algorithm also transpar oper learn method algorithm noth straightforward defin measur distanc valu compar new instanc instanc memori classifi accord categori closest decis tree algorithm perhap equal straightforward classif time pass exampl tree get decis neural net algorithm fast classif time transpar weight must propag net sum pass filter eg threshold layer complic part method comput distanc ta data set ble comput via fix statist techniqu base frequenc occurr valu howev certainli complic entropi calcul decis tree method eg quinlan 1986 weight adjust routin use back propag tabl provid insight rel import differ featur notic instanc sever tabl protein fold data almost ident indic featur similar instanc memori ibl system readili access examin numer way exampl human want know particular classif made system simpli present instanc memori use classif human expert commonli use explan exampl ask justifi predict economi expert typic produc anoth similar econom situat past neural net yet provid insight made classif although recent effort explor new method understand content train network hanson burr 1990 also rel easi modifi algorithm includ domain specif knowledg rel import featur known featur may weight accordingli distanc formula salzberg 1989 taken togeth advantag list make clear ibl algorithm number benefit respect compet model howev order consid realist practic learn techniqu ibl must still demonstr good classif accuraci problem domain symbol featur obviou distanc metric ibl count number featur differ work well metric call lap metric experiment result show modifi valuediffer metric process symbol valu except well result taken togeth result domain numer featur eg aha 1990 aha et al 1991 show ibl algorithm perform quit well wide rang problem learn algorithm back propag wide use understood neural net learn algorithm use basi comparison experi paper although earlier approach notabl perceptron learn model unabl classifi group concept linearli separ back propag overcom problem back propag gradient descent method propag error signal back multilay network describ mani place eg rumelhart et al 1986 rumelhart mcclelland 1986 reader interest detail descript look use decis tree algorithm id3 quinlan 1986 basi comparison decis tree algorithm addit compar perform algorithm method use data domain describ section 3 appropri present result domainspecif classif method 21 instancebas learn instancebas learn algorithm like algorithm store seri train instanc memori use distanc metric compar new instanc store new instanc classifi accord closest exemplar memori algorithm implement program call stand parallel exemplarbas learn system 2 clariti use term exampl mean train test exampl shown system first time use term exemplar follow usag salzberg 1991 refer specif instanc previous store comput memori exemplar may addit inform attach eg weight term stanc cover exampl exemplar pebl design process instanc symbol featur val ue heart pebl algorithm way measur distanc two exampl consist essenti three compo nent first modif stanfil waltz 1986 valu differ metric vdm defin distanc differ valu given featur call method mvdm modifi valu differ metric second compon standard distanc metric measur distanc two exampl multidimension featur space fi nalli distanc modifi weight scheme weight instanc memori accord perform histori salzberg 1989 1990 compon distanc calcul describ section 22 2 parallel algorithm develop speed experiment theoret import learn model 23 pebl requir two pass train set first pass featur valu differ tabl construct instanc train set accord equat stanfil waltz vdm second pass system attempt classifi instanc comput distanc new instanc previous store one new instanc assign classif nearest store instanc system check see classif correct use feedback adjust weight old instanc weight describ detail section 23 final new instanc store memori test exampl classifi manner modif made memori distanc tabl 22 stanfillwaltz vdm 1986 stanfil waltz present power new method measur distanc valu featur domain symbol featur val ue appli techniqu english pronunci problem impress initi result stanfil waltz 1986 valu differ metric vdm take account overal similar classif instanc possibl valu featur use method matrix defin distanc valu featur deriv statist base exampl train set distanc ffi two valu eg two amino acid specif featur defin equat 1 1 equat v 1 v 2 two possibl valu featur eg protein data would two amino acid distanc valu sum n class exampl protein fold experi section 41 three categori number time v 1 classifi categori c 1 total number time valu 1 occur k constant usual set 1 use equat 1 comput matrix valu differ featur input data interest note valu differ matric comput experi quit similar overal differ featur although differ significantli valu pair idea behind metric wish establish valu similar occur rel frequenc classif term c 1 repres likelihood central residu classifi given featur question valu v 1 thu say two valu similar give similar likelihood possibl classifica tion equat 1 comput overal similar two valu find sum differ likelihood classif consid follow exampl say pool instanc examin singl featur take one three valu b c two classif ff fi possibl data construct tabl 1 tabl entri repres number time instanc tabl 1 number occurr valu class featur valu ff fi 4 3 tabl 2 valu differ tabl featur valu 0000 0571 0191 given featur valu classif inform construct tabl distanc follow frequenc occurr class ff 571 sinc 4 instanc classifi ff 7 instanc valu similarli frequenc occurr b c 286 667 respect frequenc occurrnc class fi 429 find distanc b use equat 1 yield 0571 complet tabl distanc shown tabl 2 note construct differ valu differ tabl featur 10 featur construct 10 tabl equat 1 defin geometr distanc fix finit set valu properti valu distanc zero posit distanc valu distanc symmetr distanc obey triangl inequ summar properti follow ii iii iv stanfil waltz origin vdm also use weight term w g f make version ffi nonsymmetr eg ffia b 6 ffib major differ metric vdm mvdm omit term make ffi symmetr total distanc delta two instanc given x repres two instanc eg two window protein fold domain x exemplar memori new exampl variabl x valu th featur x exampl n featur wx w weight assign exemplar describ follow section new exampl w domain numer featur manhattan distanc produc euclidean distanc experi use howev use protein secondari structur task summari four major differ mvdm stanfillwaltz vdm 1 omit weight term w g f make stanfillwaltz vdm nonsymmetr formul ffi delta symmetr 2 stanfil waltz 1986 use valu version equat 1 preliminari experi indic equal good perform achiev chose valu reason simplic 3 ad exemplar weight distanc formula describ section 23 4 stanfil waltz use 10 closest exemplar classif wherea pebl use nearest neighbor realli differ learn algorithm rather valu differ 23 weight exemplar except space store instanc reliabl classifi other intuit one would like trustworthi exemplar draw power other final differ mvdm metric origin vdm capac metric treat reliabl instanc differ accomplish weight wx distanc formula reliabl exemplar given smaller weight make appear closer new exampl weight scheme first adopt system salzberg 1989 1990 assign weight exemplar accord perform histori wx ratio number use exemplar number correct use exemplar thu accur exemplar wx 1 unreli exemplar wx 1 make appear away new exampl unreli exemplar may repres either nois except small area featur space normal rule appli time exemplar incorrectli use classif larger weight grow altern scheme handl noisi except instanc ibl framework discuss aha kibler 1989 elabor aha 1990 scheme instanc use nearestneighbor comput proven accept classifi accept instanc whose classif accuraci exceed baselin frequenc class fix amount exampl baselin frequenc class 30 instanc correct 80 time would accept wherea baselin frequenc 90 instanc would accept abl note techniqu design primarili filter noisi instanc rather identifi except instanc differ noisi instanc probabl ignor discard wherea except instanc retain use rel infrequ differ salzberg origin exemplar weight scheme one signific aspect way exemplar point weight initi origin scheme store point initi weight 11 effect featur space signific consid instanc space contain two point classifi ff fi unweight two point defin hyperplan divid ndimension space ff fi region shown figur 1 point locat left side plane classifi ff likewis fi figur 1 two unweight point instanc space pebl comput distanc new instanc weight ex emplar distanc multipli exemplar weight intuit make less like new instanc appear near exemplar exemplar weight grow figur 2 show geometr use weight creat circular envelop around exemplar larger weight defin except space shrink weight differ increas point insid circl match point larger weight weight equal special case hyperplan given gener given space mani exemplar exemplar smallest weight best classif perform partit space set hyperplan weight best exemplar ident partit use larg circl exemplar effect rule region space exemplar larger weight defin except space around figur 3 show within except space process may recur group exemplar figur 2 two weight point instanc space figur 3 partit except space approxim equal weight abil partit space larg gener rule pocket except import domain contain mani except without capabl mani point requir learn necessari surround except set nonexcept point defin edg space two point requir defin rule except capabl becom even import ibl model store subset train exampl reduc number point must store cost salzberg 1990 given discuss clear instanc initi weight 1 consid system train instanc train n th hierarchi instanc weight alreadi construct train repres structur domain instanc enter weight 1 would immedi becom one influenti classifi space found better strategi initi new instanc weight equal match exemplar adopt weight strategi experi describ weight scheme complet modifi valu differ metric domain chose comparison three domain receiv consider attent machin learn research commun word pronunci task sejnowski rosenberg 1986 shavlik et al 1989 predict protein secondari structur qian sejnowski 1988 holley karplu 1989 predict dna promot sequenc towel et al 1989 domain symbolicvalu featur thu mvdm applic wherea standard euclidean distanc section 3133 describ three databas problem present learn 31 protein secondari structur accur techniqu predict fold structur protein yet exist despit increasingli numer attempt solv problem techniqu depend part predict secondari structur primari sequenc amino acid secondari structur inform use construct final tertiari structur tertiari structur difficult deriv directli requir expens method xray crystallographi primari sequenc sequenc amino acid constitut protein rel easi discov attempt predict secondari structur involv classif residu three categori ff helix fi sheet coil three wide use approach problem robson garnier et al 1978 chou fasman 1978 lim 1974 produc classif accuraci rang 48 58 accur techniqu develop predict tertiari secondari structur eg cohen et al 1986 lathrop et al 1987 accur predict secondari structur proven extrem difficult task learn problem describ follow protein consist sequenc amino acid bond togeth chain sequenc known primari structur amino acid chain one twenti differ acid point two acid join chain variou factor includ chemic properti determin angl molecular bond angl purpos character one three differ type fold ff helix fi sheet coil word certain number consecut acid hereaft residu chain join manner call ff segment chain ff helix character fold type protein known secondari structur learn problem given sequenc residu fix length window protein chain classifi central residu window ff helix fi sheet coil setup simpli window z z gtpgksfnlnfdtg central residu qian sejnowski 1988 holley karplu 1989 formul problem exactli manner studi found optim window size approxim 17 residu 21 largest window test either studi separ statist studi cost 1990 found window size five six nearli suffici uniqu identifi residu data set indic tabl 3 tabl show percentag sequenc given size unambigu entir data set determin fold classif protein segment exampl consid window size six center residu classifi found 9941 pattern data set uniqu addit found slightli inform contain residu left point predict right point predict residu secondari structur must predict skew top column tabl indic left right shift pattern respect center window eg skew 2 mean tabl 3 percent uniqu pattern window size skew window size 3 2 6 9935 9941 9941 9941 9936 9929 9923 7 9950 9953 9954 9952 9948 9942 9936 9 9962 9963 9963 9964 9962 9958 9954 14 9972 9973 9972 9972 9971 9971 9971 19 9976 9976 9975 99 pattern center two posit left point predict tabl show quit clearli one store pattern length 6 data set one could classifi data set better 99 accuraci obstacl good perform domain includ undersampl nonloc effect consid window size five databas use 21618 32 million possibl segment repres databas 068 also protein solut form globular structur net result residu sequenti far may physic quit close signific effect reason secondari structur probabl complet determin primari structur qian claim method incorpor local inform perform much better current result 6070 rang nonhomolog protein 32 promot sequenc promot sequenc databas subject sever recent experi towel et al 1990 relat protein fold task involv predict whether given subsequ dna sequenc promot sequenc gene init process call transcript express adjac gene data set contain 106 exampl 53 posit exampl promot neg exampl gener larger dna sequenc believ contain promot see towel et al 1990 detail construct data set instanc consist sequenc 57 nucleotid alphabet c classif learn 57 nucleotid treat 57 featur one four symbol valu 33 pronunci english text word pronunci problem present interest challeng machin learn although effect practic algorithm develop task given rel small sequenc letter object learn sound stress requir pronounc part given word sejnowski rosenberg 1987 introduc task learn commun nettalk program nettalk use back propag learn method perform well task pronounc word continu spoken text although could match perform current speech synthesi program instanc represent text pronunci similar previou problem instanc sequenc letter make word task classifi central letter sequenc correct phonem use fix window seven charact experi sejnowski rosenberg stanfil waltz 1986 use window size 15 class includ 54 phonem plu 5 stress classif phonem stress predict 5 theta 54 270 possibl class although 115 actual occur dictionari experi emphas predict phonem difficulti domain aris irregular natur lan guag english languag particular rule exist except better perform data set obtain nonlearn rulebas approach kontogiorgio 1988 howev learn algorithm troubl find best set rule 4 experiment result section describ experi result three test domain comparison use previous publish result learn method order make comparison valid attempt duplic experiment design earlier studi close possibl use data use studi 41 protein secondari structur protein sequenc use experi origin brookhaven nation laboratori secondari structur assign ff helix fisheet coil made base atom coordin use method kabsch sander 1983 qian sejnowski 1988 collect databas 106 protein contain 128 protein segment call subunit use set protein segment use parallel experi sigillito 1989 use back propag ident data reproduc classif accuraci result qian initi experi divid data train set contain 100 protein segment test set contain 28 segment overlap two set tabl 4 show composit two set tabl 4 show percentag three categori tabl 4 composit train test set protein segment residu ff fi coil train 100 17142 261 195 544 test 28 4476 218 231 551 approxim test set train set 3 protein segment separ main experi ie instanc drawn one segment resid togeth either train test set pebl train describ train set use equat 2 found preliminari experi produc slightli improv accuraci domain repeat main experi varieti differ window size rang 3 21 domain pebl includ postprocess algorithm base minim sequenc length restrict use holley karplu 1989 restrict state fisheet must consist contigu sequenc fewer two residu ffhelix fewer four subsequ predict conform restrict individu residu reclassifi coil qian sejnowski 1988 use differ form postprocess call cascad neural net fed output one net anoth network attempt reclassifi residu second network design 3 qian sejnowski care balanc overal frequenc three categori train test set attempt addit use train set 18105 residu slightli smaller although databas ident access specif partit train test set use qian sejnowski tabl 5 classif accuraci window size window unweight holley qian size pebl pebl karplu sejnowski 9 647 656 623 623 19 692 710 626 take advantag correl neighbor secondari structur assign result classif accuraci given tabl 5 un weight pebl column show result use pebl without weight wx exemplar entri tabl 5 percentag correct predict test set tabl show highest accuraci produc pebl achiev 710 window size 19 qian sejnowski obtain best result 643 use cascad network ar chitectur use singl network design similar holley karplu best result 627 best perform pebl without postprocess 678 best convent techniqu report holley karplu produc accuraci 55 also perform experi use overlap metric produc accuraci tabl comparison correl coeffici algorithm correct c ff c fi c coil pebl 710 047 045 040 qian sejnowski 643 041 031 041 holley karplu 632 041 032 046 5560 rang differ window size match pair analysi reveal weight version pebl perform significantli better unweight version particular ttest show weight version better 9995 confid level 9 thu exemplar weight improv perform significantli anoth frequent use measur perform domain correl coeffici provid measur accuraci categori defin follow equat mathew p ff number time ff correctli predict n ff number time ff correctli reject ff number fals posit ff u ff number miss ff correct predict similar definit use c fi c coil coeffici pebl two back propag experi appear tabl 6 variat train set size third measur classif perform involv repeat test randomli select test set tabl 7 show tabl 7 train pebl vari percentag data set train set percent correct size test set 50 602 70 623 90 651 perform pebl weight train vari percentag randomli select instanc entir data set use window size 19 trial set exampl chosen random train ing exampl remov data set test phase use remain exampl sinc protein compris mani exampl differ part singl protein could appear train test set given trial classif accuraci tabl 7 averag ten run train set size note number report tabl 7 reflect classif perform pebl without post process minim sequenc length restrict explain post process part experi holley karplu experi thu perform compar weight algorithm use window size postprocess weight pebl figur 678 post process accuraci improv 710 thu see particular composit train test set experi construct mimic design earlier experi improv accuraci learn algorithm 4 42 promot sequenc experi run towel et al 1990 promot sequenc databas leaveoneout trial methodolog involv remov one element data train remain data test one element thu 106 instanc databas pebl train 105 test remain 1 perform instanc databas entir procedur repeat 10 time time use differ random order instanc towel et al also repeat entir leaveoneout experi 10 time use differ random initi state neural net time result shown tabl 8 compar pebl towel et al kbann algorithm addit report number obtain towel et al sever machin learn algorithm includ back propa gation id3 nearest neighbor overlap metric best method report biolog literatur oneil 1989 recal overlap metric measur distanc number featur differ valu also worth note 10 test run pebl four instanc caus error three four neg 4 one like sourc variat classif accuraci homolog train test set homolog protein structur similar algorithm may much accur predict structur protein train homolog one tabl 8 promot sequenc predict algorithm pebl 4106 kbann 4106 pebl unweight 6106 back propag 8106 id3 19106 nearest neighbor overlap 13106 oneil 12106 instanc towel note neg exampl databas data use deriv select substr fragment e coli bacteriophag believ contain promot site towel et al 1990 p 865 would suggest base result four exampl reexamin four exampl might interest except gener pattern dna promot 43 english text pronunci english pronunci task use train set defin se jnowki rosenberg 1987 nettalk program set consist instanc drawn brown corpu 1000 commonli use word english languag unabl discern differ train set somewhat restrict set shavlik shavlik et al 1989 one experiment design use train brown corpu pebl test entir 20012 word merriam webster pocket dictionari result present tabl 9 weight unweight version pebl algorithm comparison tabl 9 english text pronunci algorithm phonem accuraci phonemestress pebl 782 692 pebl unweight 791 672 back propag 770 give result nettalk program use back propag learn algorithm shavlik et al 1989 replic sejnowski rosenberg methodolog part work although result differ sejnowski rosenberg surprisingli sinc back propag network requir much tune make easier comparison properti follow fact origin sejnowski rosenberg studi use distribut output encod system produc 26bit sequenc rather one bit 115 phonemestress combin first 21 bit distribut encod 51 phonem remain 5 bit local encod stress type 21bit output vector match closest descript vector 51 phonem shavlik et al explicitli compar encod pure local encod sinc output pebl alway local ie output specif phonem phonemestress combin appropri compar method produc output tabl 10 show result 5 compar back propag perceptron id3 quinlan 5 preliminari experi use overlap metric databas abysm result desir improv result one reason develop mvdm tabl 10 phonemestress accuraci output encod algorithm local encod distribut encod correct correct back propag 630 723 id3 642 693 perceptron 492 421 1986 latter three result shavlik et al 1989 tabl show pebl perform slightli better learn method output local encod distribut encod improv result id3 back propag compar experi pebl would requir signific chang output function yet perform shavlik et al also test perform back propag id3 perceptron learn function size train set perform similar experi increas show result tabl 11 graphic figur 4 comparison result averag 10 run differ randomlychosen train set run surprisingli perform improv steadili size train set increas surpris though good perform even small train set tabl 11 pebl perform vari train set size percentag brown phonem correct corpu train full dictionari 100 782 figur 4 classif accuraci function train set size 51 classif accuraci studi show classif accuraci pebl gener equal slightli superior learn method domain featur notabl protein structur predict task pebl give consider better classif result back propaga tion without weight exemplar note consid fair test perform random select residu vari percentag dataset perform figur algorithm slightli wors albeit still quit good would inform see similar experi run neural network learn algorithm recent zhang waltz investig hybrid learn method protein structur predict combin nearest neighbor neural net learn statist inform figur yet publish method also outperform previou method waltz 1990 although accuraci exceed pebl dna promot sequenc predict towel et al 1990 report kbann techniqu integr neural net domain knowledg superior standard back propag 9995 certainti df 18 kbann design specif show ad domain knowledg could improv perform neural net learn algorithm addit kbann outperform id3 nearest neighbor nearest neighbor use overlap metric use experiment de sign pebl exactli match perform kbann measur superior back propag id3 oneil method data strong perform pebl data set demonstr nearest neighbor perform well use larg protein fold small promot sequenc train set especi signific pebl use weak gener method abl match perform kbann knowledgerich approach english pronunci domain result mix best result sejnowski rosenberg 77 superior phonemestress accuraci pebl 692 howev shavlik et al replic ex periment best result 723 note neural net result reflect distribut output encod local encod pebl produc back propag classif accuraci 630 id3 642 somewhat lower pebl conclus techniqu perform similarli learn techniqu yet come close perform good commerci system much less nativ speaker english clearli still room consider progress domain shavlik et al conclud base experi classif accuraci versu number train exampl see figur 4 nettalk data small amount train data back propag prefer decis tree construct id3 howev result indic nearest neighbor algorithm also work well train set small perform curv figur 4 show pebl need exampl achiev rel good perform 52 transpar represent oper train given domain pebl contain memori set inform rel perspicu comparison weight assign neural network exemplar provid specif refer instanc case histori may cite support particular decis inform may easili gather train even test phase shed addit light domain question instanc consid attach counter exemplar increment time exemplar use exact match compar number time exemplar use get good idea whether exemplar specif except part gener rule examin weight wx attach exemplar determin whether instanc reliabl cla sifier distanc tabl reveal order set symbol valu appar valu alon hand deriv distanc perspicu deriv global characterist train data english pronunci task distribut output encod shown produc superior perform local encod shavlik et al 1989 result point weak pebl 1nearest neighbor method allow distribut output encod neural net handl encod quit easili decis tree handl difficulti shavlik et al built separ decis tree 26 bit distribut encod phonemestress pair task rais question whether nearest neighbor method handl encod one possibl use knearest neighbor would allow one exemplar determin output bit eg exemplar contain 26bit encod predict valu bit new exampl would determin major vote k nearest neighbor bit experi requir determin strategi would advantag gener transpar oper learn classif algorithm nearest neighbor algorithm simpl basic learn routin simpli store new exampl memori pebl comput exemplar weight noth simpl recordkeep base classif perform exist exemplar ad exemplar chang weight chang way nearest neighbor algorithm partit featur space illustr except space although may hard visual three dimens nonetheless straightforward compar oper adjust weight back propag algorithm thu far research found difficult character classif perform chang connect weight chang one minor drawback pebl method nonincrement unlik back propag version decis tree method increment extens pebl would probabl quit expens sinc valu differ tabl might recomput mani time hand extend pebl handl mix symbol numer data quit straightforward algorithm could use simpl differ numer fea ture valu differ tabl symbol one final experi protein domain demonstr use weight attach exemplar improv accuraci nearest neighbor algorithm domain english pronunci weight make signific differ base result earlier result realvalu domain salzberg 1990 1991 conclud exemplar weight offer real potenti enhanc power practic learn algorithm 6 conclus demonstr seri experi instancebas learn algorithm perform except well domain featur valu symbol direct comparison implement pebl perform well better back propag id3 sever domainspecif learn algorithm sever difficult classif task addit nearest neighbor offer clear advantag much faster train represent rel easi interpret one yet know interpret network weight learn neural net decis tree somewhat easier interpret hard predict impact new exampl structur tree sometim one new exampl make differ time may radic chang larg portion tree hand neural net fix size decis tree tend quit small respect method compress data way nearest neighbor addit classif time fast depend depth net tree size input base classif accuraci though clear learn techniqu advantag nearestneighbor method respect nearest neighbor learn per se shown weight exemplar improv perform subdivid instanc space manner reduc impact unreli exampl nearest neighbor algorithm one simplest learn method known yet algorithm shown outperform consist taken togeth result indic continu research extend improv nearest neighbor learn algorithm prove fruit acknowledg thank joann houlahan david aha numer insight comment suggest thank also richard sutton three anonym review detail comment idea research support part air forc offic scientif research grant afosr890151 nation scienc foundat grant iri9116843 r studi instancebas algorithm supervis learn task predict secondari structur protein amino acid sequenc turn predict protein use pattern match approach nearest neighbor pattern classif certain aspect anatomi physiolog cerebr cortex compar studi id3 backpropag english texttospeech map fgp virtual machin acquir knowledg case empir comparison id3 backpropag analysi accuraci implic simpl method predict secondari structur globular protein connectionist model learn learn represent connectionist network protein secondari structur predict dictionari protein secondari struc ture pattern recognit hydrogenbond geometr featur automat lettertophonem transcript speech synthesi ariadn patterndirect infer hierarch abstract protein structur recogni tion algorithm predict ffhelic betastructur region globular protein comparison predict observ secondari structur t4 phage lysozym distribut model human learn memori context theori classif learn escherichia coli promot comput geometri intro duction predict secondari structur globular protein use neural network model pattern recognit categor learn represent backpropag error pdp research group nest hyperrectangl exemplarbas learn learn nest gener exemplar nearest hyperrectangl learn method nettalk parallel network learn read aloud symbol neural learn algorithm experiment comparison person commun toward memorybas reason refin approxim domain theori knowledgebas neural network massiv parallel ai empir comparison pattern recognit tr ctr walter daeleman peter berck steven gilli unsupervis discoveri phonolog categori supervis learn morpholog rule proceed 16th confer comput linguist august 0509 1996 copenhagen denmark david waltz simon kasif reason data acm comput survey csur v27 n3 p356359 sept 1995 vroniqu host walter daeleman iri hendrickx antal van den bosch dutch word sens disambigu optim local context proceed acl02 workshop word sens disambigu recent success futur direct p6166 juli 11 2002 l mangasarian j b rosen e thompson convex kernel underestim function multipl local minima comput optim applic v34 n1 p3545 may 2006 tomuro question terminolog represent question type classif coling02 computerm 2002 second intern workshop comput terminolog p17 august 31 2002 rafael alonso jeffrey bloom hua li chumki basu adapt nearest neighbor search part acquisit eport proceed ninth acm sigkdd intern confer knowledg discoveri data mine august 2427 2003 washington dc michael pazzani daniel billsu learn revis user profil identif ofinterest web site machin learn v27 n3 p313331 june 1997 luca cazzanti maya r gupta local similar discrimin analysi proceed 24th intern confer machin learn p137144 june 2024 2007 corvali oregon perrizo amal perera paramet optim vertic nearestneighborvot boundarybas classif acm sigkdd explor newslett v8 n2 p6369 decemb 2006 ping zhang brijesh verma kuldeep kumar neural vs statist classifi conjunct genet algorithm base featur select pattern recognit letter v26 n7 p909919 15 may 2005 steven salzberg arthur l delcher david heath simon kasif bestcas result nearestneighbor learn ieee transact pattern analysi machin intellig v17 n6 p599608 june 1995 paul losiewicz dougla w oard ronald n kostoff textual data mine support scienc technolog manag journal intellig inform system v15 n2 p99119 septoct 2000 v host hendrickx w daeleman van den bosch paramet optim machinelearn word sens disambigu natur languag engin v8 n4 p311325 decemb 2002 gerard escudero llu mrquez german rigau empir studi domain depend supervis word sens disambigu system proceed 2000 joint sigdat confer empir method natur languag process larg corpora held conjunct 38th annual meet associ comput linguist p172180 octob 0708 2000 hong kong kai ming ting discretis lazi learn algorithm artifici intellig review v11 n15 p157174 feb 1997 jorn veenstra antal van den bosch singleclassifi memorybas phrase chunk proceed 2nd workshop learn languag logic 4th confer comput natur languag learn septemb 1314 2000 lisbon portug piotr indyk rajeev motwani prabhakar raghavan santosh vempala localitypreserv hash multidimension space proceed twentyninth annual acm symposium theori comput p618625 may 0406 1997 el paso texa unit state amir ahmad lipika dey featur select techniqu classificatori analysi pattern recognit letter v26 n1 p4356 1 januari 2005 david w patterson mykola galushka niall rooney characteris novel index techniqu casebas reason artifici intellig review v23 n4 p359393 june 2005 jianp zhang yeesat yim jum yang intellig select instanc predict function lazylearn algorithm artifici intellig review v11 n15 p175191 feb 1997 hwee tou ng hian beng lee integr multipl knowledg sourc disambigu word sens exemplarbas approach proceed 34th annual meet associ comput linguist p4047 june 2427 1996 santa cruz california pedro domingo michael pazzani optim simpl bayesian classifi zeroon loss machin learn v29 n23 p103130 novdec 1997 naoki abe hiroshi mamitsuka predict protein secondari structur use stochast tree grammar machin learn v29 n23 p275301 novdec 1997 stephan raaijmak learn distribut linguist class proceed 2nd workshop learn languag logic 4th confer comput natur languag learn septemb 1314 2000 lisbon portug christoph j merz use correspond analysi combin classifi machin learn v36 n12 p3358 julyaugust 1999 eyal kushilevitz rafail ostrovski yuval rabani effici search approxim nearest neighbor high dimension space proceed thirtieth annual acm symposium theori comput p614623 may 2426 1998 dalla texa unit state philip k chan salvator j stolfo experi multistrategi learn metalearn proceed second intern confer inform knowledg manag p314323 novemb 0105 1993 washington dc unit state anandeep pannu use genet algorithm induct reason case legal domain proceed 5th intern confer artifici intellig law p175184 may 2124 1995 colleg park maryland unit state aristid gioni piotr indyk rajeev motwani similar search high dimens via hash proceed 25th intern confer larg data base p518529 septemb 0710 1999 belur v dasarathi data mine task method classif nearestneighbor approach handbook data mine knowledg discoveri oxford univers press inc new york ny 2002 amir ahmad lipika dey method comput distanc two categor valu attribut unsupervis learn categor data set pattern recognit letter v28 n1 p110118 januari 2007 charl x ling hangdong wang comput optim attribut weight set nearest neighboralgorithm artifici intellig review v11 n15 p255272 feb 1997 grzegorz gra arkadiusz wojna riona new classif system combin rule induct instancebas learn fundamenta informatica v51 n4 p369390 decemb 2002 mark stevenson yorick wilk interact knowledg sourc word sens disambigu comput linguist v27 n3 p321349 septemb 2001 ting liu andrew w moor alexand gray new algorithm effici highdimension nonparametr classif journal machin learn research 7 p11351158 1212006 wai lam chikin keung danyu liu discov use concept prototyp classif base filter abstract ieee transact pattern analysi machin intellig v24 n8 p10751090 august 2002 jihoon yang vasant g honavar featur subset select use genet algorithm ieee intellig system v13 n2 p4449 march 1998 jakub zavrel walter daeleman memorybas learn use similar smooth proceed eighth confer european chapter associ comput linguist p436443 juli 0712 1997 madrid spain ronni kohavi daniel sommerfield case studi public domain multipl mine task system mlc handbook data mine knowledg discoveri oxford univers press inc new york ny 2002 xin dong alon halevi jayant madhavan ema neme jun zhang similar search web servic proceed thirtieth intern confer larg data base p372383 august 31septemb 03 2004 toronto canada philip k chan salvator j stolfo accuraci metalearn scalabl data mine journal intellig inform system v8 n1 p528 janfeb 1997 antal van den bosch sabin buchholz shallow pars basi word case studi proceed 40th annual meet associ comput linguist juli 0712 2002 philadelphia pennsylvania arkadiusz wojna centerbas index vector metric space fundamenta informatica v56 n3 p285310 august fedro domingo controlsensit featur select lazi learner artifici intellig review v11 n15 p227253 feb 1997 arkadiusz wojna centerbas index vector metric space fundamenta informatica v56 n3 p285310 august filippo neri lorenza saitta explor power genet search learn symbol classifi ieee transact pattern analysi machin intellig v18 n11 p11351141 novemb 1996 ding liu strong lower bound approxim nearest neighbor search inform process letter v92 n1 p2329 16 octob 2004 harold somer bill black joakim nivr torbjrn lager annarosa multari luca gilardoni jeremi ellman alex roger multilingu gener summar job advert tree project proceed fifth confer appli natur languag process p269276 march 31april 03 1997 washington dc nivr mario scholz determinist depend pars english text proceed 20th intern confer comput linguist p64e august 2327 2004 geneva switzerland walter daeleman antal van den bosch jakub zavrel forget except harm languag learn machin learn v34 n13 p1141 feb 1999 juan manuel gimeno illa javier bjar alonso miquel snchez marr nearestneighbour time seri appli intellig v20 n1 p2135 januaryfebruari 2004 xudong luo jimmi homan lee hofung leung nichola r jen prioritis fuzzi constraint satisfact problem axiom instanti valid fuzzi set system v136 n2 p151188 june 1 omer barkol yuval rabani tighter lower bound nearest neighbor search relat problem cell probe model journal comput system scienc v64 n4 p873896 june 2002 omer barkol yuval rabani tighter bound nearest neighbor search relat problem cell probe model proceed thirtysecond annual acm symposium theori comput p388396 may 2123 2000 portland oregon unit state chaolin liu chengtsung chang jimhow ho classif cluster casebas crimin summari judgment proceed 9th intern confer artifici intellig law june 2428 2003 scotland unit kingdom roumani b skillicorn mobil servic discoveri select publishsubscrib paradigm proceed 2004 confer centr advanc studi collabor research p163173 octob 0407 2004 markham ontario canada stergio papadimitri seferina mavroudi liviu vladutu anastasio bezeriano gener radial basi function network train instanc base learn data mine symbol data appli intellig v16 n3 p223234 mayjun 2002 piotr indyk rajeev motwani approxim nearest neighbor toward remov curs dimension proceed thirtieth annual acm symposium theori comput p604613 may 2426 1998 dalla texa unit state terri r payn peter edward clair l green experi rule induct knearest neighbor method interfac agent learn ieee transact knowledg data engin v9 n2 p329335 march 1997 fink alfr kobsa user model person citi tour artifici intellig review v18 n1 p3374 septemb 2002 dietrich wettschereck david w aha takao mohri review empir evalu featur weight method aclass lazi learn algorithm artifici intellig review v11 n15 p273314 feb 1997 miquel montan beatriz lpez josep llu de la rosa taxonomi recommend agent theinternet artifici intellig review v19 n4 p285330 june sunil arya david mount nathan netanyahu ruth silverman angela wu optim algorithm approxim nearest neighbor search fix dimens journal acm jacm v45 n6 p891923 nov 1998 stergio papadimitri seferina mavroudi liviu vladutu g pavlid anastasio bezeriano supervis network selforgan map classif larg data set appli intellig v16 n3 p185203 mayjun 2002 christoph g atkeson andrew w moor stefan schaal local weight learn artifici intellig review v11 n15 p1173 feb 1997 alfr kobsa jrgen koenemann wolfgang pohl personalis hypermedia present techniqu improv onlin custom relationship knowledg engin review v16 n2 p111155 march 2001 francisco azuaj werner dubitzki norman black kenni adamson retriev strategi casebas reason categoris bibliographi knowledg engin review v15 n4 p371379 decemb 2000 francisco azuaj werner dubitzki norman black kenni adamson retriev strategi casebas reason categoris bibliographi knowledg engin review v15 n4 p371379 decemb 2000