mesh partit effici use distribut system mesh partit homogen system studi extens howev mesh partit distribut system rel new area research ensur effici execut distribut system heterogen processor network perform must taken consider partit process equal size subdomain small cut set size result convent mesh partit longer primari goal paper address variou issu relat mesh partit distribut system issu includ metric use compar differ partit effici applic execut distribut system advantag exploit heterogen network perform present tool call part automat mesh partit distribut system novel featur part consid heterogen applic distribut system simul anneal use part perform backtrack search desir partit wellknown simul anneal comput intens describ parallel version simul anneal use part result parallel exhibit superlinear speedup case nearli perfect speedup remain case experiment result also present partit regular irregular finit element mesh explicit nonlinear finit element applic call whams2d execut distribut system consist two ibmsp differ processor result regular problem indic 33 46 percent increas effici processor perform consid compar convent even partit result indic 5 15 percent increas effici network perform consid compar consid processor perform signific given optim improv 15 percent applic result irregular problem indic 36percent increas effici processor network perform consid compar even partit b introduct distribut comput regard futur high perform comput nationwid high speed network vbn 25 becom wide avail interconnect highspe comput virtual environ scientif instrument larg data set project globu 15 legion 20 develop softwar infrastructur integr distribut comput inform resourc paper present mesh partit tool distribut system tool call part take consider heterogen processor network found distribut system well heterogen found applic mesh partit requir effici parallel execut finit element finit differ applic wide use mani disciplin biomed engin structur mechan fluid dynam applic distinguish use mesh procedur discret problem domain execut meshbas applic parallel distribut system involv partit mesh subdomain assign individu processor parallel distribut system mesh partit homogen system studi extens 2 4 14 31 36 37 41 howev mesh partit distribut system rel new area research brought recent avail system ensur effici execut distribut system heterogen processor network perform must taken consider partit process equal size subdomain small cut set size result convent mesh partit longer desir part take advantag follow heterogen system featur 1 processor speed 2 number processor 3 local network perform wide area network perform differ finit element applic consider may differ comput complex differ commun pattern differ element type also must taken consider partit paper discuss major issu mesh partit distribut system particular identifi good metric use compar differ partit result present measur effici distribut system discuss optim number cut set remot commun metric use part identifi good effici estim execut time also present parallel version part significantli improv perform partit process simul anneal use part perform backtrack search desir partit howev well known simul anneal comput intens parallel part use asynchron multipl markov chain approach parallel simul anneal 21 part use partit six irregular mesh 8 16 100 subdomain use 64 client processor ibm sp2 machin result show superlinear speedup case nearli perfect speedup rest result also indic parallel version part produc partit consist sequenti version part use partit part ran explicit 2d finit element code two geograph distribut ibm sp machin use globu softwar commun two sp compar partit part gener use widelyus partit tool meti 26 consid processor perform result regular problem indic increas effici processor perform consid compar convent even partit result indic 5 gamma 15 increas effici network perform consid compar consid processor perform signific given optim 15 applic result irregular problem indic 36 increas effici processor network perform consid compar even partit remaind paper organ follow section 2 provid background section 3 discuss issu section 4 describ part detail section 5 experiment result section 6 give previou work final conclus background 21 meshbas applic finit element method fundament numer analysi techniqu solv partial differenti equat engin commun past three decad 24 3 three basic procedur finit element method problem first formul variat weight residu form second step problem domain discret complex shape call element last major step solv result system equat procedur discret problem domain call mesh applic involv mesh procedur refer meshbas applic meshbas applic natur suitabl parallel distribut system implement finit element method parallel involv partit global domain element connect subdomain distribut among p processor processor execut numer techniqu assign subdomain commun among processor dictat type integr method solver method explicit integr finit element problem requir use solver sinc lump matrix diagon matrix use therefor commun occur among neighbor processor common data rel simpl implicit integr finit element problem howev commun determin type solver use applic applic use paper explicit nonlinear finit code call whams2d 6 use analyz elast plastic materi focu whams2d code concept gener implicit well meshbas applic 22 distribut system distribut comput consist platform network resourc resourc may cluster workstat cluster person comput parallel machin resourc mayb locat one site distribut among differ site figur 1 show exampl distribut system distribut system provid econom altern costli massiv parallel comput research longer limit comput resourc individu site distribut comput environ also provid research opportun collabor share idea use collabor technolog distribut system defin group set processor share one interconnect network perform group smp parallel comput cluster workstat person comput commun occur within group group refer commun within group local commun processor differ group remot commun number group distribut system repres term supercomput smp supercomput figur 1 distribut system 23 problem formul mesh partit homogen system view graph partit problem goal graph partit problem find small vertex separ equal size subset mesh partit distribut system howev variat graph partit problem goal differ regular graph partit problem equal size subset may desir distribut system partit problem state follow given graph e jv maximum cost function f v minim min paper cost function f estim execut time given applic distribut system function discuss section 4 graph partit proven npcomplet mesh partit problem distribut system also npcomplet proven appendix 1 therefor focu heurist solv problem 3 major issu section discuss follow major issu relat mesh partit problem distribut system comparison metric effici number cut group 31 comparison metric de facto metric compar qualiti differ partit homogen parallel system equal subdomain minimum interfac cut set size although object counter exampl 14 metric use extens compar qualiti differ partit obviou equal subdomain size minimum interfac valid compar partit distribut system one may consid obviou metric distribut system unequ subdomain pro portion processor perform small cut set size problem metric heterogen network perform consid given local wide area network use distribut system case big differ local remot commun especi term latenc argu use estim execut time applic target heterogen system alway lead valid comparison differ partit estim use rel comparison differ partit method henc coars approxim execut appropri comparison metric import make estim repres applic system estim includ paramet correspond system heterogen processor perform local remot commun also reflect applic comput complex 32 effici effici distribut system equal ratio rel speedup effect number processor v ratio given 1 e1 sequenti execut time one processor e execut time distribut system term v equal summat processor perform rel perform processor use sequenti execut term follow 2 k processor use sequenti execut exampl two processor processor perform f 2 effici would processor 1 use sequenti execut effici processor 2 instead use sequenti execut 33 network heterogen wellknown heterogen processor perform must consid distribut system section identifi condit heterogen network perform must consid distribut system recal defin group collect processor perform share local interconnect network remot commun correspond commun two group given processor requir remot local commun other requir local commun dispar execut time processor correspond differ remot local commun assum equal comput load 331 ideal reduct execut time retrofit step use part tool reduc comput load processor local remot commun equal execut time among processor group step describ detail section 62 reduct execut time occur retrofit demonstr consid simpl case stripe partit commun occur two neighbor processor assum exist two group processor local network perform group locat geograph distribut site requir wan interconnect figur 2 illustr one case g processor local commun local commun remot commun figur 2 commun pattern stripe partit processor well processor local remot commun differ two commun time x percentag differ cr cl total execut time e assum e repres execut time take consider processor perform sinc assum processor perform entail even partit mesh time written consid case partit take consider heterogen network perform achiev decreas load assign processor increas load processor group 1 appli processor j group 2 amount load redistribut cr gamma cl xe amount distribut g processor illustr figur 6 discuss retrofit step part execut time g differ e e 0 x g g therefor take network perform consider partit percentag reduct execut time approxim xedenot delta1 g includ follow 1 percentag commun applic 2 differ remot local commun factor determin applic partit maximum number processor among group remot commun reduct execut time follow g delta exampl whams2d applic experi calcul ideal reduct 15 regular mesh 8 partit one processor group local remot commun therefor rel easi calcul ideal perform improv 332 number processor group local remot commun major issu address reduct partit domain assign group maxim reduct particular issu entail tradeoff follow two scenario 1 mani processor group local remot commun result small messag size execut time without retrofit step smaller case 2 howev given mani processor group remot local commun fewer processor avail redistribut addit load illustr figur 3 mesh size n theta 2n partit 2p block block oe oe figur 3 size n theta 2n mesh partit 2p block n theta n assum processor equal perform mesh partit two group group p processor processor group boundari incur remot commun well local commun part comput load processor need move processor local commun compens longer commun time assum overlap commun messag messag aggreg use commun one node diagon processor commun time processor group boundari approxim localremot processor local commun commun time approxim messag aggreg overlap assum local therefor commun time differ processor local remot commun processor local commun approxim local total p number processor local remot commun fore use equat 1 ideal reduct execut time group 1 group 2 n figur 4 size n theta 2n mesh partit 2p stripe block 2 one processor group local remot commun result larg messag size result execut time without retrofit step larger case 1 howev processor avail redistribut addit load illustr figur 4 mesh partit stripe one processor group local remot commun follow similar analysi figur 3 commun time differ processor local remot commun processor local commun approxim local one processor remot commun group henc use equat 1 ideal reduct execut time stripe comm stripe 15 therefor total execut time stripe block partit stripe reduct block reduct differ total execut time block stripe partit deltat blocksgammastrip ff l therefor differ total execut time block stripe partit determin term c posit sinc p 1 term b neg 4 block partit higher execut time ie stripe partit advantag p 4 howev block partit still higher execut time unless n larg absolut valu term b larger sum absolut valu c note ff l ff r one two order magnitud larger fi l experi calcul block partit lower execut time n 127kb mesh use howev largest n 10kb 4 descript part part consid heterogen applic system particular part take consider differ mesh base applic may differ comput complex mesh may consist differ element type distribut system part take consider heterogen processor network perform figur 5 show flow diagram part part consist interfac program simul anneal program finit element mesh fed interfac program produc propos commun graph fed simul anneal program final partit comput partit graph translat requir input file format applic section describ initi interfac program step requir partit graph problem domain group processor per group comput model commun model partit data finit element mesh interfac program graph simul anneal partit graph interfac program input processor figur 5 part flowchart 41 mesh represent use weight commun graph repres finit element mesh natur extens commun graph commun graph vertic repres element origin mesh weight ad vertex repres number node within element commun graph edg repres connect element weight commun graph weight also ad edg repres number node inform need exchang two neighbor element 42 partit method part entail three step partit mesh distribut system step 1 partit mesh subdomain group take consider heterogen processor perform element type 2 partit subdomain g part g processor group take consider heterogen network perform element type 3 necessari global retrofit partit among group take consider heterogen local network among differ group step describ detail follow subsect subsect includ descript object function use simul anneal key good partit simul anneal cost function cost function use part estim execut time one particular supercomput let e execut time ith processor 1 p goal minim varianc execut time processor run simul anneal program found best cost function instead sum 2 20 actual cost function use simul anneal program cost function ecomm includ commun cost partit element need commun element remot processor therefor execut time balanc paramet need tune accord applic problem size partit first step gener coars partit distribut system group get subdomain proport number processor perform processor comput complex applic henc comput cost balanc across group cost function given number group system 422 step 2 retrofit second step subdomain assign group step 1 partit among processor within group simul anneal use balanc execut time step varianc network perform consid processor entail inter group commun reduc comput load compens longer commun time step illustr figur 6 two supercomput sc1 sc2 sc1 four processor use two processor use sc2 comput load reduc p3 sinc commun remot processor amount reduc comput load repres ffi amount equal distribut three processor assum cut size remain unchang commun time chang henc execut time balanc shift comput load comm comp comm p3 p3 comp retrofit d4 figur illustr retrofit step two supercomput assum two nearest neighbor commun step entail gener imbalanc partit group take consider processor commun local remot processor commun local imbal repres term delta term ad processor requir local remot commun ad term result decreas ecomm compar processor requir local commun cost function given follow equat p number processor given group delta differ estim local remot commun time processor commun local 423 step 3 global retrofit third step address global optim take consider differ local interconnect perform variou group goal minim varianc execut time across processor step element boundari partit move accord execut time varianc neighbor processor step execut larg differ perform differ local interconnect case signific number element move group step 3 second step execut equal execut time group given new comput load step 2 processor group balanc execut time howev execut time differ group may balanc may occur larg differ commun time differ group balanc execut among group take weight averag execut time group weight group equal comput power group versu total comput power comput power particular group multipl ratio processor perform respect slowest one among group number processor use group denot weight averag e assumpt commun time chang much ie separ step 1 incur larg chang size e optim execut time achiev balanc execut time group execut time e first comput differ e e gamma ad e comp cost function commun cost ecomm remot commun cost group cost function therefor given number group system group whose domain increas group whose domain decreas step 3 necessari step 2 perform partit within group 5 parallel simul anneal part use simul anneal partit mesh figur 7 show serial version simul anneal algorithm algorithm use metropoli criteria line 8 13 figur 7 accept reject move move reduc cost function accept move increas cost function may accept probabl e gamma delta avoid trap local minima probabl decreas temperatur lower simul anneal comput intens therefor parallel version simul anneal use parallel version part three major class parallel simul anneal 19 serial like 32 39 parallel move 1 multipl markov chain 5 21 34 serial like algorithm essenti break move subtask parallel subtask parallel line 6 7 figur 7 parallel move algorithm processor gener evalu move cost function calcul may inaccur sinc processor awar move processor period updat normal use address effect cost function error parallel move algorithm essenti parallel loop figur 7 line 5 14 multipl markov chain algorithm multipl simul anneal process start variou processor differ random seed processor period exchang solut best select given processor continu anneal process 5 multipl markov chain approach shown effect vlsi cell placement reason parallel version part use multipl markov chain approach given p processor straightforward implement multipl markov chain approach would initi simul anneal p processor differ seed processor perform move independ final best solut comput 1 get initi solut 2 get initi temperatur 0 3 stop criteria met f 4 number move per temperatur 5 6 gener random move 7 evalu chang cost function delta 8 delta 9 accept move updat solut 10 g els f 11 accept probabl 12 updat solut accept 13 g 14 g end loop 15 16g end loop figur 7 simul anneal processor select approach howev simul anneal essenti perform p time may result better solut speedup achiev speedup p processor perform independ simul anneal differ seed processor perform mp move number move perform simul anneal temperatur processor exchang solut end temperatur exchang data occur synchron asynchron synchron multipl markov chain approach processor period exchang solut asynchron approach client processor exchang solut server processor report synchron approach easili trap local optima asynchron 21 therefor parallel version part use asynchron approach solut exchang client solut better server processor updat better solut server solut better client get updat better solut continu processor exchang solut server processor end temperatur ensur subdomain connect check disconnect compon end part subdomain disconnect compon parallel simul anneal repeat differ random seed process continu disconnect subdomain number trial exceed three time warn messag given output disconnect subdomain 6 experi section present result two differ experi first experi focus speedup parallel version part second experi focus qualiti partit gener part 61 speedup result tabl 1 parallel part execut time second 8 partit proc barth4 barth5 inviscid labarr spiral viscou 4 444 547 466 324 412 537 21 22 24 25 15 29 part use partit six 2d irregular mesh triangular element barth4 11451 labarr 14971 elem spiral 1992 elem viscou 18369 elem run time partit six irregular mesh 8 100 parallel part speedup 8 partitions2060100140 barth4 barth5 inviscid labarr spiral viscou figur 8 parallel part speedup 8 partit subdomain given tabl 1 2 respect assum subdomain execut distribut system consist two ibm sp equal number processor differ processor perform machin interconnect via vbn perform network given tabl 4 discuss section 62 tabl column 1 number client processor use part column 2 6 run time part second differ mesh solut qualiti use two client processor within 5 use one client processor case solut qualiti estim execut time whams2d figur 8 9 graphic represent speedup parallel version part rel one client processor figur show mesh partit 8 subdomain superlinear speedup occur case mesh partit 100 subdomain superlinear speedup occur case two smallest mesh spiral inviscid case show slightli less perfect speedup superlinear speedup attribut use multipl client processor conduct search processor benefit result good solut found one client inform given client quickli therebi reduc effort continu search solut superlinear tabl 2 parallel part execut time second 100 partit proc barth4 barth5 inviscid labarr spiral viscou 4 39822 46664 30823 42735 13044 39740 2883 4263 1928 2912 627 3917 speedup result consist report 33 62 qualiti partit 621 regular mesh part appli explicit nonlinear finit code call whams2d 6 use analyz elast plastic materi code use mpi built top nexu interprocessor commun within supercomput supercomput nexu runtim system allow multipl protocol within applic comput complex linear size problem code execut ibm sp machin locat argonn nation laboratori cornel theori center two machin connect internet macro benchmark use determin network processor perform result network perform analysi given tabl 3 experi conduct determin cornel node 16 time faster argonn node problem mesh consist 3 regular mesh execut time given 100 time step correspond 0005 second applic time gener applic may execut 10 000 100 000 time step record execut time repres 100 run take data run standard deviat less 3 regular problem execut parallel part speedup 100 partitions1030507090barth4 barth5 inviscid labarr spiral viscou figur 9 parallel part speedup 100 partit tabl 3 valu ff fi differ network argonn sp vulcan switch ff machin configur 8 processor 4 anl ibm sp 4 ctc ibm sp tabl 4 present result regular problem column 1 mesh configur column 2 execut time result convent equal partit particular use chaco spectral bisect column 3 result partit taken end first step varianc processor perform comput complex consid column 4 execut time result partit taken end second step varianc network perform consid result tabl 4 show approxim increas effici achiev balanc comput cost anoth 5 gamma 16 effici increas achiev consid varianc network perform small increas effici consid network perform tabl 4 execut time use internet 8 processor 4 anl 4 ctc case chaco proc perf local retrofit 9 theta 1152 mesh 10299 7802 6881 effici 046 061 071 effici 047 061 068 36 theta 288 mesh 10388 7321 7022 effici 046 067 070 due commun small compon whams2d applic howev recal optim increas perform 15 regular problem describ earlier global optim step last step part balanc execut time across supercomput give signific increas effici includ tabl 4 expect sinc two supercomput use argonn ibm sp cornel ibm sp interconnect network similar perform indic tabl 3 result indic perform gain achiev step comparison convent method evenli partit mesh given obviou consid processor perform result signific gain follow section irregular mesh consid perform gain result consid network perform 622 irregular mesh experi irregular mesh perform gusto testb avail experi regular mesh testb includ two ibm sp machin one locat argonn nation laboratori anl locat san diego supercomput center sdsc two machin connect vbn high speed backbon network servic use globu 15 16 softwar allow multimod commun within applic macro benchmark use determin network processor perform result network perform analysi given tabl 5 experi conduct determin sdsc sp processor node 16 time fast anl one tabl 5 valu ff fi differ network anl sp vulcan switch ff sdsc sp vulcan switch ff part use partit five 2d irregular mesh triangular element barth4 11451 labarr 14971 elem viscou 18369 elem inviscid 6928 call part without restrict sightli modifi version part call part restrict use partit mesh one processor remot commun group meti 30 26 use gener partit take consider processor perform processor comput power use one input three partition use identifi perform impact consid heterogen network addit processor three partition highlight differ forc remot commun occur one processor group versu multipl processor remot commun group consid 6 configur two machin 4 anl 4 sdsc 8 anl 8 sdsc 20 anl 20 sdsc two group correspond two ibm sp anl sdsc use 20 processor sp due limit coschedul comput resourc execut time given 100 time step record execut time repres averag 10 run standard deviat less 3 tabl 6 tabl 8 show experiment result 3 configur column one identifi irregular mesh number element mesh includ parenthesi column two execut time result partit part restrict one processor per group entail remot commun column 2 4 number indic number processor remot commun group column three similar column two except partit restrict remot commun one processor column four execut time result meti take comput power consider processor comput power use one tabl execut time use vbn 8 processor 4 anl 4 sdsc mesh part w restrict part wo restrict proc perf meti effici viscou 18369 elem 1500 1 1690 3 1700 3 effici 086 075 075 labarr 14971 elem 1330 1 1420 2 1460 3 effici 079 073 071 effici 079 068 068 inviscid 6928 elem 732 1 855 3 885s3 effici 066 056 055 input meti program result show use part without restrict slight decreas 13 execut time achiev compar meti forc remot commun one processor retrofit step achiev signific reduct execut time result tabl 6 tabl 8 show effici increas 36 compar meti execut time reduc 30 compar meti reduct come fact even high speed network vbn differ messag start cost remot local commun larg tabl 5 see differ two order magnitud messag start compar approxim one order magnitud bandwidth restrict remot commun one processor allow part redistribut load among processor therebi achiev close ideal reduct execut time 7 previou work problem domain partit finit element mesh equival partit graph associ finit element mesh graph partit proven tabl 7 execut time use vbn processor 8 anl 8 sdsc mesh part w restrict part wo restrict proc perf meti effici 072 062 059 viscou 18369 elem 829s1 1008s4 1060s5 effici 077 064 061 labarr 14971 elem 758s1 837s3 886s3 effici 069 062 059 effici 074 050 048 inviscid 6928 elem 422s1 628s3 672s4 effici 057 039 036 npcomplet problem 17 mani good heurist static partit method propos kernighanlin 31 propos local optim partit method farhat 13 14 propos automat domain decompos base greedi algorithm berger bokhari 4 propos recurs coordin bisect rcb util spatial nodal coordin inform nouromid et al 35 40 propos recurs inerti bisect rib simon 37 propos recurs spectral bisect rsb comput fiedler vector graph use lanczo algorithm sort vertic accord size entri fiedler vector recurs graph bisect rgb propos georg liu 18 use sparspak rcm algorithm comput level structur sort vertic accord rcm level structur barnard et al 2 propos multilevel version rsb faster hendrickson leland 23 22 also report similar multilevel partit method karypi kumar 27 28 30 propos new coarsen heurist improv multilevel method aforement decomposit method avail one three autom tool chaco 22 meti 26 29 topdomdec 38 chaco versatil implement inerti spectral kernighanlin multilevel algorithm algorithm use tabl 8 execut time use vbn 40 processor 20 anl 20 sdsc mesh part w restrict part wo restrict proc perf meti effici 069 054 045 viscou 18369 elem 387s1 586s5 649s7 effici 067 044 040 labarr 14971 elem 338s1 512s3 535s6 effici 062 041 040 effici 039 034 032 inviscid 6928 elem 3351 347s4 468s5 effici recurs bisect problem equal size subproblem meti use method fast partit spars matric use coarsen heurist provid speed topdomdec interact mesh partit tool tool produc equal size partit tool applic system processor one interconnect network tool meti produc partit unequ weight howev none tool take network perform consider partit process reason tool applic distribut system crandal quinn 7 8 9 10 11 12 develop partit advisori system network workstat advisori system three builtin partit method contigu row contigu point block given inform problem space machin speed network advisori system provid rank three partit method advisori system take consider varianc processor perform among workstat problem howev linear comput complex assum applic case implicit finit element problem wide use varianc network perform consid 8 conclus paper address issu mesh partit problem distribut system issu includ comparison metric effici cut set present tool part automat mesh partit distribut system novel featur part consid heterogen applic distribut system heterogen distribut system includ processor network perform heterogen applic includ comput complex also demonstr use parallel version part distribut system novel part parallel part use asynchron multipl markov chain approach parallel simul anneal mesh partit parallel part use partit 6 irregular mesh 8 16 100 subdomain use 64 client processor ibm sp2 machin result show superlinear speedup case nearli perfect speedup rest use globu softwar run explicit 2d finit element code use mesh partit parallel part testb includ two geograph distribut ibm sp machin experiment result present 3 regular mesh 4 irregular finit element mesh whams2d applic execut distribut system consist two ibm sp result regular problem indic increas effici processor perform consid compar even partit result also indic addit 5 gamma 16 increas effici network perform consid result irregular problem indic 38 increas effici processor network perform consid compar even partit experiment result irregular problem also indic 36 increas effici compar use partit take processor perform consider improv come fact even high speed network vbn messag start cost remot local commun still larg differ appendix 1 proof npcomplet mesh partit problem distribut system partit problem distribut system npcomplet proof 1 transform proven npcomplet problem minimum sum squar 17 partit problem distribut system let set 2 arbitrari instanc minimum sum squar shall construct graph desir partit exist g sum squar basic unit minimum sum squar instanc n local replac substitut 2 collect e 3 edg shown figur 10 therefor e defin follow 2 1 figur 10 local replac 2 transform minimum sum squar partit problem distribut system easi see instanc partit problem distribut system construct polynomi time minimum sum squar instanc disjoint k partit sum squar minim correspond k disjoint partit v given take fa 1 2 g everi subset also restrict cost function f minimum sum squar 2 ensur partit sum squar cost function convers disjoint k partit g minimum sum squar cost function correspond disjoint k partit set given choos vertic fa 1 2 henc minimum sum squar cost function k disjoint partit ensur sum squar sa k disjoint set also minim conclud partit problem distribut system npcomplet appendix 2 nomenclatur estim execut time processor estim comput time processor estim commun time processor perform processor measur comput kernel ff l per messag cost local commun messag cost remot commun cost local commun cost remot commun size messag cl local commun time processor cr remot commun time processor differ cr cl one processor differ cr cl processor maximum number processor group local remot commun coeffici comput complex paramet use equal contribut comput commun execut time number element partit number processor system number processor group g number processor particular group p number group system ith group system ratio speed processor rel slowest processor system r parallel simul anneal algorithm cell placement hypercub multiprocessor fast multilevel implement recurs spectral bisect partit unstructur problem finit element procedur engin analysi partit strategi nonuniform problem multiproc sor evalu parallel simul anneal strategi applic standard cell placement whams3d project progress report pr2 data partit network parallel process problem decomposit parallel network block data partit partialhomogen parallel network evalu decomposit techniqu highspe cluster comput partit advisori system network dataparallel process simpl effici automat fem domain decompos automat partit unstructur mesh parallel solut problem comput mechan manag multipl commun method highperform network comput system softwar infrastructur iway metacomput experi comput intract guid theori np complet comput solut larg spars posit definit system parallel simul anneal techniqu legion team simul anneal base parallel state assign finit state machin chaco user guid multilevel algorithm partit graph finit element method internet fast lane research educ fast high qualiti multilevel scheme partit irregular graph fast high qualiti multilevel scheme partit irregular graph multilevel kway partit scheme irregular graph multilevel kway partit scheme irregular graph parallel multilevel kway partit scheme irregular graph effici heurist procedur partit graph placement simul anneal multiprocessor introduct parallel comput design analysi algorithm asynchron commun multipl markov chain parallel simul anneal solv finit element equat concurr comput partit spars matric eigenvector graph partit unstructur problem parallel process topdomdec softwar tool mesh partit parallel process parallel nari specul comput simul anneal studi factor fillin parallel implement finit element method retrofit base methodolog fast gener optim largescal mesh partit beyond minimum interfac size criterion tr partit strategi nonuniform problem multiprocessor partit spars matric eigenvector graph parallel simul anneal techniqu introduct parallel comput threedimension grid partit network parallel process legion vision worldwid virtual comput manag multipl commun method highperform network comput system simul anneal base parallel state assign finit state machin comput solut larg spars posit definit comput intract parallel simul anneal algorithm cell placement hypercub multiprocessor parallel nari specul comput simul anneal mesh partit distribut system problem decomposit parallel network ctr kyungmin lee dongman lee scalabl dynam load distribut scheme multiserv distribut virtual environ system highlyskew user distribut proceed acm symposium virtual realiti softwar technolog octob 0103 2003 osaka japan zhile lan valeri e taylor greg bryan dynam load balanc samr applic distribut system scientif program v10 n4 p319328 decemb 2002 zhile lan valeri e taylor greg bryan dynam load balanc samr applic distribut system proceed 2001 acmiee confer supercomput cdrom p3636 novemb 1016 2001 denver colorado