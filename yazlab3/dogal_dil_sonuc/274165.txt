factori hidden markov model hidden markov model hmm proven one wide use tool learn probabilist model time seri data hmm inform past convey singl discret variableth hidden state discuss gener hmm state factor multipl state variabl therefor repres distribut manner describ exact algorithm infer posterior probabl hidden state variabl given observ relat forwardbackward algorithm hmm algorithm gener graphic model due combinatori natur hidden state represent exact algorithm intract intract system approxim infer carri use gibb sampl variat method within variat framework present structur approxim state variabl decoupl yield tractabl algorithm learn paramet model empir comparison suggest approxim effici provid accur altern exact method final use structur approxim model bach choral show factori hmm captur statist structur data set unconstrain hmm b introduct due flexibl simplic effici paramet estim algorithm hidden markov model hmm emerg one basic statist tool model discret time seri find widespread applic area speech recognit rabin juang 1986 comput molecular biolog krogh brown mian sjoland haussler 1994 hmm essenti mixtur model encod inform histori time seri valu singl multinomi variableth hidden statewhich take one k discret valu multinomi assumpt support effici paramet estim algorithmth baumwelch algorithmwhich consid k set hidden state time step howev multinomi assumpt also sever limit represent capac hmm exam ple repres bit inform histori time sequenc hmm would need distinct state hand hmm distribut state represent could achiev task binari state z ghahramani mi jordan variabl william hinton 1991 paper address problem construct effici learn algorithm hidden markov model distribut state represent need distribut state represent hmm motiv two way first represent let model automat decompos state space featur decoupl dynam process gener data second distribut state represent simplifi task model time seri known priori gener interact multipl looselycoupl process exampl speech signal gener superposit multipl simultan speaker potenti model architectur william hinton 1991 first formul problem learn hmm distribut state represent propos solut base determinist learn 1 approach present paper similar william hinton also view framework statist mechan mean field theori howev learn algorithm quit differ make use special structur hmm distribut state represent result significantli effici learn procedur anticip result section 3 learn algorithm obviat need twophas procedur boltzmann machin exact step make use forwardbackward algorithm classic hmm subroutin differ approach come saul jordan 1995 deriv set rule comput gradient requir learn hmm distribut state space howev method appli limit class architectur hidden markov model distribut state represent particular class probabilist graphic model pearl 1988 lauritzen spiegelhalt 1988 repres probabl distribut graph node correspond random variabl link repres condit independ relat relat hidden markov model graphic model recent review smyth heckerman jordan 1997 although exact probabl propag algorithm exist gener graphic model jensen lauritzen olesen 1990 algorithm intract denselyconnect model one consid paper one approach deal issu util stochast sampl method kanazawa et al 1995 anoth approach provid basi algorithm describ current paper make use variat method cf saul jaakkola jordan 1996 follow section defin probabilist model factori hmm section 3 present algorithm infer learn section 4 describ empir result compar exact approxim algorithm learn basi time complex model qualiti also appli factori hmm real time seri data set consist melodi line collect choral j bach discuss sever gener probabilist model factori hidden markov model 247 t1 t1 2 2 t1 2 t1 b figur 1 direct acycl graph dag specifi condit independ relat hidden markov model node condit independ nondescend given parent b dag repres condit independ relat factori hmm underli markov chain section 5 conclud section 6 necessari detail deriv provid appendix 2 probabilist model begin describ hidden markov model sequenc observ model specifi probabilist relat observ sequenc hidden state fs g markov transit structur link hidden state model assum two set condit independ relat independ observ state given independ markov properti use independ relat joint probabl sequenc state observ factor condit independ specifi equat 1 express graphic form figur 1 state singl multinomi random variabl take one k discret valu kg state transit probabl specifi k theta k transit matrix observ discret symbol take one valu observ probabl fulli specifi k theta observ matrix continu observ vector p js model mani differ form gaussian mixtur gaussian even neural network 2 present paper gener hmm state represent let state repres collect state variabl z ghahramani mi jordan take k valu refer model factori hidden markov model state space consist cross product state variabl simplic assum k although result present trivial gener case differ k given state space factori hmm consist k combin variabl place constraint state transit structur would result k theta k transit matrix unconstrain system uninterest sever reason equival hmm k state unlik discov interest structur k state variabl variabl allow interact arbitrarili time complex sampl complex estim algorithm exponenti therefor focu factori hmm underli state transit constrain natur structur consid one state variabl evolv accord dynam priori uncoupl state variabl graphic represent model present figur 1 b transit structur system repres distinct k theta k matric gener allow coupl state variabl briefli discuss section 5 shown figur 1 b factori hmm observ time step depend state variabl time step continu observ one simpl form depend linear gaussian observ gaussian random vector whose mean linear function state variabl repres state variabl k theta 1 vector k discret valu correspond 1 one posit 0 elsewher result probabl densiti theta 1 observ vector ae oe w matrix theta k matrix whose column contribut mean set c theta covari matrix 0 denot matrix transpos j delta j matrix determin oper one way understand observ model equat 4a 4b consid margin distribut obtain sum possibl state k set state variabl thu factori hidden markov model 249 k possibl mean vector obtain form sum column one column chosen w matric result margin densiti thu gaussian mixtur model k gaussian mixtur compon constant covari matrix c static mixtur model without inclus time index markov dynam factori parameter standard mixtur gaussian model interest right zemel 1993 hinton zemel 1994 ghahramani 1995 model consid current paper extend model allow markov dynam discret state variabl underli mixtur unless otherwis state assum gaussian observ model throughout paper hidden state variabl one time step although margin independ becom condit depend given observ sequenc determin appli semant direct graph particular dsepar criterion pearl 1988 graphic model figur 1 b consid gaussian model equat 4a4b given observ vector posterior probabl set hidden state variabl proport probabl gaussian mean sinc function state variabl probabl set one state variabl depend set state variabl 3 depend effect coupl hidden state variabl purpos calcul posterior probabl make exact infer intract factori hmm 3 infer learn infer problem probabilist graphic model consist comput probabl hidden variabl given observ context speech recognit exampl observ may acoust vector goal infer may comput probabl particular word sequenc phonem hidden state problem solv effici via forwardbackward algorithm rabin juang 1986 shown special case jensen lauritzen olesen 1990 algorithm probabl propag gener graphic model smyth et al 1997 case rather probabl distribut hidden state desir infer singl probabl hidden state sequenc achiev via viterbi 1967 algorithm form dynam program close relat forwardbackward algorithm also analogu graphic model literatur dawid 1992 learn problem probabilist model consist two compon learn structur model learn paramet structur learn topic current research graphic model machin learn commun eg heckerman 1995 stolck omohundro 1993 current paper deal exclus problem learn paramet given structur z ghahramani mi jordan 31 em algorithm paramet factori hmm estim via expect maxim em algorithm dempster laird rubin 1977 case classic hmm known baumwelch algorithm baum petri soul weiss 1970 procedur iter step fix current paramet comput posterior probabl hidden state e step step use probabl maxim expect log likelihood observ function paramet step sinc e step em exactli infer problem describ subsum discuss infer learn problem descript em algorithm factori hmm em algorithm follow definit expect log likelihood complet observ hidden data qoe new log q function paramet oe new given current paramet estim oe observ sequenc fy g factori hmm paramet model consist comput q expand 5 use equat 14b find q express function three type expect hidden state variabl hs hdeltai use abbrevi e fdeltajo fy gg hmm notat rabin juang 1986 hs correspond fl vector state occup probabl hs correspond k theta k matrix state occup probabl two consecut time step hs analogu singl underli markov model step use expect maxim q function oe new use jensen inequ baum petri soul weiss 1970 show iter e step increas likelihood p fy gjoe converg local optimum hidden markov model exact step factori hmm simpl tractabl particular step paramet output model describ equat 4a4b found solv weight linear regress problem similarli step prior state transit matric ident one use baumwelch algorithm detail step given appendix turn substanti difficult problem comput expect requir e step 32 exact infer unfortun exact e step factori hmm comput intract fact best shown make refer standard algorithm prob factori hidden markov model 251 abilist infer graphic model lauritzen spiegelhalt 1988 although also deriv readili direct applic bay rule consid comput requir calcul posterior probabl factori hmm shown figur 1 b within framework lauritzen spiegelhalt algorithm moral triangul graphic structur factori hmm result junction tree fact chain cliqu size m1 result probabl propag algorithm time complex otmk m1 singl observ sequenc length present forwardbackward type recurs implement exact e step appendix b naiv exact algorithm consist translat factori hmm equival hmm k state use forwardbackward algorithm time complex otk 2m like model multipl denselyconnect hidden variabl exponenti time complex make exact learn infer intract thu although markov properti use obtain forwardbackward like factor expect across time step sum possibl configur hidden state variabl within time step unavoid abl intract due inher cooper natur model gaussian output model exampl set state variabl one time step cooper determin mean observ vector 33 infer use gibb sampl rather comput exact posterior probabl one approxim use mont carlo sampl procedur therebi avoid sum exponenti mani state pattern cost accuraci although mani possibl sampl scheme review see neal 1993 present one simplestgibb sampl geman geman 1984 given observ sequenc fy g procedur start random set hidden state fs g step sampl process state vector updat stochast accord probabl distribut condit set state vector graphic model use node condit independ node given markov blanket defin set children parent parent children node sampl typic state variabl need examin state neighbor node sampl p sampl tm hidden variabl model result new sampl hidden state model requir otmk oper sequenc overal state result pass gibb sampl defin markov chain state space model assum probabl bound away zero markov chain guarante converg z ghahramani mi jordan posterior probabl state given observ geman geman 1984 thu suitabl time sampl markov chain taken approxim sampl posterior probabl first secondord statist need estim hs collect use state visit probabl estim sampl process use approxim e step em 4 34 complet factor variat infer also exist second approxim posterior probabl hidden state tractabl determinist basic idea approxim posterior distribut hidden variabl p fs gjfi g tractabl distribut qf g approxim provid lower bound log likelihood use obtain effici learn algorithm argument formal follow reason saul jaakkola jordan 1996 distribut hidden variabl qf g use defin lower bound log likelihood log log qf g log made use jensen inequ last step differ lefthand side righthand side inequ given kullbackleibl diverg cover thoma 1991 qf g log complex exact infer approxim given q determin condit independ relat paramet thu chose q tractabl structurea graphic represent elimin depend p given structur free vari paramet q obtain tightest possibl bound minim 6 refer gener strategi use parameter approxim distribut variat approxim refer free paramet distribut variat paramet illustr consid simplest variat approxim state variabl assum independ given observ distribut written factori hidden markov model 253 2 2 2 2 2 2 b figur 2 complet factor variat approxim assum state variabl independ condit observ sequenc b structur variat approxim assum state variabl retain markov structur within chain independ across chain qs variat paramet g mean state variabl state variabl repres kdimension vector 1 k th posit 0 elsewher th markov chain state k time element vector therefor defin state occup probabl multinomi variabl distribut q qs tk tk complet factor approxim often use statist physic provid basi simpl yet power mean field approxim statist mechan system parisi 1988 make bound tight possibl vari separ observ sequenc minim kl diverg take deriv 6 respect set zero obtain set fix point equat see appendix c defin new ae oe residu error given predict state variabl includ 6m z ghahramani mi jordan delta vector diagon element w 0 c fdeltag softmax oper map vector vector b size element log p denot elementwis logarithm transit matrix p first term 9a project error reconstruct observ onto weight state vector mthe particular set state vector reduc error larger associ variat paramet second term aris fact second order correl hs evalu variat distribut diagon matrix compos element last two term introduc depend forward backward time 5 therefor although posterior distribut hidden variabl approxim complet factor distribut fix point equat coupl paramet associ node paramet markov blanket sens fix point equat propag inform along pathway defin exact algorithm probabl propag follow may provid intuit interpret approxim made distribut given particular observ sequenc hidden state variabl markov chain time step stochast coupl stochast coupl approxim system hidden variabl uncorrel coupl mean variat meanfield equat solv determinist coupl mean best approxim stochast coupl system hidden state vector updat turn use 9a time complex otmk 2 per iter converg determin monitor kl diverg variat distribut success time step practic converg rapid 2 10 iter 9a fix point equat converg expect requir e step obtain simpl function paramet equat c6c8 appendix c 35 structur variat infer approxim present previou section factor posterior probabl state variabl statist independ contrast rather extrem factor exist third approxim tractabl preserv much probabilist structur origin system scheme factori hmm approxim uncoupl hmm shown figur b within hmm effici exact infer implement via forwardbackward algorithm approach exploit tractabl substructur first suggest machin learn literatur saul jordan 1996 factori hidden markov model 255 note argument present previou section hing form approxim distribut therefor structur variat approxim obtain use structur variat distribut q q provid lower bound log likelihood use obtain learn algorithm write structur variat approxim qs qs zq normal constant ensur q integr one qs qs tk tk tk tk last equal follow fact vector 1 one posit 0 elsewher paramet distribut gthe origin prior state transit matric factori hmm timevari bia state variabl compar equat 11a11c equat 1 see k theta 1 vector h play role probabl observ p js 1 k set exampl qs 1joe correspond observ time state 1j intuit approxim uncoupl markov chain attach state variabl distinct fictiti observ probabl fictiti observ vari minim kl diverg q p appli argument obtain set fix point equat h minim klqkp detail appendix h new ae oe delta defin redefin residu error 6m z ghahramani mi jordan paramet h obtain fix point equat observ probabl associ state variabl hidden markov model use probabl forwardbackward algorithm use comput new set expect hs fed back 12a 12b algorithm therefor use subroutin minim kl diverg note similar equat 12a12b equat 9a9b complet factor system complet factor system sinc hs fix point equat written explicitli term variat paramet structur approxim depend hs h comput via forwardbackward algorithm note also 12a contain term involv prior transit matrix p term cancel choic approxim 36 choic approxim theori em algorithm present dempster et al 1977 assum use exact e step model exact e step intract one must instead use approxim like describ choic among approxim must take account sever theoret practic issu mont carlo approxim base markov chain gibb sampl offer theoret assur sampl procedur converg correct posterior distribut limit although mean one come arbitrarili close exact e step practic converg slow especi multimod distribut often difficult determin close one converg howev sampl use e step em time tradeoff number sampl use number em iter seem wast wait converg earli learn posterior distribut sampl drawn far posterior given optim paramet practic found even approxim step use gibb sampl eg around ten sampl hidden variabl tend increas true likelihood variat approxim offer theoret assur lower bound likelihood maxim minim kl diverg e step paramet updat step guarante decreas lower bound therefor converg defin term bound altern view given neal hinton 1993 describ em term neg free energi f function paramet oe observ posterior probabl distribut hidden variabl qs eq denot expect use distribut qs exact e step em maxim f respect q given oe variat e step use factori hidden markov model 257 maxim f respect q given oe subject constraint q particular tractabl form given view seem clear structur approxim prefer complet factor approxim sinc place fewer constraint q cost tractabl 4 experiment result investig learn infer factori hmm conduct two experi ment first experi compar differ approxim exact method infer basi comput time likelihood model obtain synthet data second experi sought determin whether decomposit state space factori hmm present advantag model real time seri data set might assum complex intern structurebach choral melodi 41 experi 1 perform time benchmark use data gener factori hmm structur underli markov model k state compar time per em iter train test set likelihood five model ffl hmm train use baumwelch algorithm ffl factori hmm train exact infer e step use straightforward applic forwardbackward algorithm rather effici algorithm outlin appendix b ffl factori hmm train use gibb sampl e step number sampl fix 10 sampl per variabl 6 ffl factori hmm train use complet factor variat approxima tion ffl factori hmm train use structur variat approxim factori hmm consist underli markov model k state wherea hmm k state data gener factori hmm structur state variabl could take k discret valu paramet model except output covari matrix sampl uniform 0 1 distribut appropri normal satisfi sumtoon constraint transit matric prior covari matrix set multipl ident matrix train test set consist 20 sequenc length 20 observ fourdimension vector randomli sampl set paramet separ train set test set gener algorithm run z ghahramani mi jordan fifteen set paramet gener four problem size algorithm run maximumof 100 iter em converg defin iter k log likelihood lk approxim log likelihood approxim algorithm use satisfi end learn log likelihood train test set comput model use exact algorithm also includ comparison log likelihood train test set true model gener data test set log likelihood n observ sequenc defin log p n obtain maxim log likelihood train set disjoint test set provid measur well model gener novel observ sequenc distribut train data result averag 15 run algorithm four problem size total 300 run present tabl 1 even smallest problem size standard hmm k state suffer overfit test set log likelihood significantli wors train set log likelihood expect overfit problem becom wors size state space increas particularli seriou factori hmm log likelihood three approxim em algorithm compar exact algorithm gibb sampl appear poorest perform three smaller size problem log likelihood significantli wors exact algorithm train set test set p 005 may due insuffici sampl howev soon see run gibb sampler 10 sampl potenti improv perform make substanti slower variat method surprisingli gibb sampler appear quit well largest size problem although differ method statist signific perform complet factor variat approxim statist significantli differ exact algorithm either train set test set problem size perform structur variat approxim statist differ exact method three four problem size appear better one problem size 005 although result may fluke aris random variabl anoth interest specul explan exact em algorithm implement unconstrain maxim f defin section 36 variat method maxim f subject constrain distribut constraint could presum act regular reduc overfit larg amount variabl final log likelihood model learn algorithm subtract log likelihood true gener model train model elimin main effect randomli sampl gener model reduc variabl due train test set one import remain sourc varianc random seed use factori hidden markov model 259 tabl 1 comparison factori hmm four problem vari size neg log likelihood train test set plu minu one standard deviat shown problem size algorithm measur bit per observ log likelihood bit divid nt rel log likelihood true gener model data set 7 true true gener model log likelihood per symbol defin zero model measur hmm hidden markov model k state exact factori hmm train use exact e step gibb factori hmm train use gibb sampl cfva factori hmm train use complet factor variat approxim sva factori hmm train use structur variat approxim k algorithm train test hmm 119 sigma 067 229 sigma 102 exact 088 sigma 080 105 sigma 072 gibb 167 sigma 123 178 sigma 122 cfva 106 sigma 120 120 sigma 111 sva 091 sigma 102 104 sigma 101 hmm 076 sigma 067 981 sigma 255 exact 102 sigma 104 126 sigma 099 gibb 221 sigma 091 250 sigma 087 cfva 124 sigma 150 150 sigma 153 exact 229 sigma 119 251 sigma 121 gibb 325 sigma 117 335 sigma 114 cfva 173 sigma 134 207 sigma 174 exact 423 sigma 228 449 sigma 224 gibb 363 sigma 113 395 sigma 114 cfva 485 sigma 068 514 sigma 069 260 z ghahramani mi jordan iter em log likelihood bit iter em log likelihood bit b iter em log likelihood bit c iter em log likelihood bit figur 3 learn curv five run four learn algorithm factori hmm exact b complet factor variat approxim c structur variat approx imat gibb sampl singl train set sampl size use run solid line show neg log likelihood per observ bit rel true model gener data calcul use exact algorithm circl denot point converg criterion met run end three approxim algorithm dash line show approxim neg log likelihood 8 train run determin initi paramet sampl use gibb algorithm algorithm appear sensit random seed suggest differ run train set found differ local maxima plateau likelihood figur 3 variabl could elimin explicitli ad regular term view prior paramet maximuma posteriori paramet estim altern bayesian ensembl method could use averag variabl integr paramet space time comparison confirm fact standard hmm exact extrem slow model larg state space fig factori hidden markov model 261 timeiter hmm figur 4 time per iter em silicon graphic r4400 processor run matlab ure 4 gibb sampl slower variat method even limit ten sampl hidden variabl per iter em sinc one pass variat fix point equat time complex one pass gibb sampl sinc variat fix point equat found converg quickli experi suggest gibb sampl competit timewis variat method time per iter variat method scale well larg state space 42 experi 2 bach choral music piec natur exhibit complex structur mani differ time scale furthermor one imagin repres state music piec given time would necessari specifi conjunct mani differ featur reason chose test whether factori hmm would provid advantag regular hmm model collect music piec data set consist discret event sequenc encod melodi line j bach choral obtain uci repositori machin learn origin discuss conklin witten 1995 event sequenc repres six attribut describ tabl 2 sixtysix choral 40 event divid train set 30 choral test set 36 choral use first set hidden markov model state space rang 2 100 state train converg 30 sigma 12 step em factori hmm vari size k rang 2 6 rang 2 also train data 262 z ghahramani mi jordan tabl 2 attribut bach choral data set key signatur time signatur attribut constant durat choral attribut treat real number model lineargaussian observ 4a attribut descript represent pitch pitch event int 0 127 fermata event fermata binari st start time event int 116 note dur durat event int 116 note approxim e step factori hmm use structur variat ap proxim choic motiv three consider first size state space wish explor exact algorithm prohibit slow second gibb sampl algorithm appear present advantag speed perform requir heurist method determin number sampl third theoret argument suggest structur approxim gener superior complet factor variat approxim sinc depend origin model preserv test set log likelihood hmm shown figur 5 exhibit typic ushap curv demonstr tradeoff bia varianc ge man bienenstock doursat 1992 hmm fewer 10 state predict well hmm 40 state overfit train data therefor provid poor model test data 75 run highest test set log likelihood per observ gamma90 bit obtain hmm hidden state 9 factori hmm provid satisfactori model choral three point view first time complex possibl consid model significantli larger state space particular fit model 1000 state second given componenti parametr factori hmm larg state space requir excess larg number paramet rel number data point particular saw evid overfit even largest factori hmm seen figur 5 c final approach result significantli better predictor test set likelihood best factori hmm order magnitud larger test set likelihood best hmm figur 5 reveal factori hmm clearli better predictor singl hmm acknowledg neither approach produc model easili interpret musicolog point view situat reminisc speech recognit hmm prove valu predict model speech signal without necessarili view causal gener model speech factori hmm clearli impoverish represent factori hidden markov model 263 log likelihood bit b size state space c figur 5 test set log likelihood per event bach choral data set function number state hmm factori hmm b dash line line symbol repres singl run line indic mean perform thin dash line bd indic log likelihood per observ best run factori hmm train use structur approxim method true likelihood comput use exact algorithm music structur promis perform predictor provid hope could serv step way toward increasingli structur statist model music complex multivari time seri 5 gener model section describ four variat gener factori hmm 51 discret observ probabilist model present paper assum realvalu gaussian observ one advantag aris assumpt condit densiti ddimension observ p js 1 compactli specifi mean matric dimens theta k one theta covari matrix furthermor step model reduc set weight least squar equat model gener handl discret observ sever way consid singl dvalu discret observ analog tradit hmm output probabl could model use matrix howev case factori hmm matrix would theta k entri combin state variabl observ thu compact represent would entir lost standard method graphic model suggest approxim larg matric noisyor pearl 1988 sigmoid neal 1992 model interact exampl softmax model gener sigmoid model 2 p js 1 multinomi mean proport 264 z ghahramani mi jordan exp like gaussian model specif com pact use matric size theta k lineargaussian model w overparametr sinc model overal mean shown appendix nonlinear induc softmax function make e step step algorithm difficult iter numer method use step wherea gibb sampl variat method use e step see neal 1992 hinton et al 1995 saul et al 1996 discuss differ approach learn sigmoid network 52 introduc coupl architectur factori hmm present section 2 assum underli markov chain interact observ constraint relax introduc coupl hidden state variabl cf saul jordan 1997 exampl depend equat 3 replac follow factor similar exact variat gibb sampl procedur defin architectur howev note coupl must introduc caution may result exponenti growth paramet exampl factor requir transit matric size k 2 theta k rather specifi higherord coupl probabl transit matric one introduc secondord interact term energi log probabl function term effect coupl chain without number paramet incur full probabl transit matrix graphic model formal correspond symmetr undirect link make model chain graph jensen lauritzen olesen 1990 algorithm still use propag inform exactli chain graph undirect link caus normal constant probabl distributionth partit functionto depend coupl paramet boltzmann machin hinton sejnowski 1986 clamp unclamp phase therefor requir learn goal unclamp phase comput deriv partit function respect paramet neal 1992 53 condit input like hidden markov model factori hmm provid model uncondit densiti observ sequenc certain problem domain observ better thought input explanatori variabl factori hidden markov model 265 other output respons variabl goal case model condit densiti output sequenc given input sequenc machin learn terminolog uncondit densiti estim unsupervis condit densiti estim supervis sever algorithm learn hidden markov model condit input recent present literatur cacciator nowlan 1994 bengio frasconi 1995 meila jordan 1996 given sequenc input vector g probabilist model inputcondit factori hmm theta model depend specif p js condit discret state variabl possibl con tinuou input vector approach use bengio frasconi input output hmm iohmm suggest model p separ neural network one set decomposit ensur valid probabl transit matrix defin point input space sumtoon constraint eg softmax nonlinear use output network use decomposit condit probabl k network infer inputcondit factori hmm straightforward gener algorithm present factori hmm exact forwardbackward algorithm appendix b adapt use appropri condit probabl similarli gibb sampl procedur complex condit input final complet factor structur approxim also gener readili approxim distribut depend input similar model probabl transit structur decompos complex depend previou state variabl input infer may becom consider complex depend form input condit maxim step learn may also chang consider gener output transit probabl model neural network step longer solv exactli gradientbas gener em algorithm must use loglinear model step solv use inner loop iter reweight leastsquar mccullagh nelder 1989 54 hidden markov decis tree interest gener factori hmm result one condit input x order state variabl depend n 266 z ghahramani mi jordan 2 2 2 figur 6 hidden markov decis tree figur 6 result architectur seen probabilist decis tree markovian dynam link decis variabl consid probabilist model would gener data first time step given top node 1 take k valu stochast partit x space k decis region next node hierarchi 2 subdivid region k subregion output 1 gener input x 1 kway decis hidden node next time step similar procedur use gener data model except decis tree depend decis taken node previou time step thu hierarch mixtur expert architectur jordan jacob 1994 gener includ markovian dynam decis hidden markov decis tree provid use start point model time seri tempor spatial structur multipl resolut explor gener factori hmm jordan ghahramani saul 1997 6 conclus paper examin problem learn class gener hidden markov model distribut state represent gener provid richer model tool method incorpor prior structur inform state variabl underli dynam system gener data although exact infer class model gener intract provid structur variat approxim comput tractabl approxim form basi expect step em algorithm learn paramet model empir comparison sever approxim exact algorithm show approxim effici comput accur final shown factori hidden markov model 267 factori hmm represent provid advantag tradit hmm predict model complex tempor pattern bach choral appendix step step equat paramet obtain set deriv q respect paramet zero start expand q use equat 14b tr c tr log p hs log z a1 tr trace oper squar matric z normal term independ state observ ensur probabl sum one set deriv q respect output weight zero obtain linear system equat w 0 a2 assum dtheta1 vector let mk theta1 vector obtain concaten vector w theta mk matrix obtain concaten w matric size theta k solv a2 result moorepenros pseudoinvers note model overparameter sinc theta 1 mean w matric add singl mean use pseudoinvers remov need explicitli subtract overal mean w estim separ anoth paramet estim prior solv q subject constraint sum one obtain 268 z ghahramani mi jordan similarli estim transit matric solv qp subject constraint column p sum one element new final reestim equat covari matrix deriv take deriv respect c gamma1 first term aris normal gaussian densiti function z proport jcj t2 jcjc substitut a2 reorgan get equat reduc baumwelch reestim equat hmm gaussian observ step present case singl observ sequenc extens multipl sequenc straightforward appendix exact forwardbackward algorithm specifi exact forwardbackward recurs comput posterior probabl hidden state factori hmm differ straightforward applic forwardbackward algorithm equival k state hmm depend k theta k transit matrix rather make use independ underli markov chain sum transit matric size k theta k use notat fy g r mean observ sequenc ff 1 ff factori hidden markov model 269 obtain forward recurs end forward recurs likelihood observ sequenc sum k element ff similarli obtain backward recurs defin obtain posterior probabl state time obtain multipli ff algorithm shown equival jensen lauritzen olesen algorithm probabl propag graphic model probabl defin collect state variabl correspond cliqu equival junction tree inform pass forward backward sum set separ neighbor cliqu tree result forwardbackwardtyp recurs order otmk m1 use ff fi fl quantiti statist requir e step z ghahramani mi jordan appendix complet factor variat approxim use definit probabilist model given equat 14b posterior probabl state given observ sequenc written z z normal constant ensur probabl sum one similarli probabl distribut given variat approxim 7 8 written expfgammah log use notat denot expect respect variat distribut use angular bracket hdeltai kl diverg three fact verifi definit variat approxim diagf factori hidden markov model 271 diag oper take vector return squar matrix element vector along diagon zero everywher els kl diverg therefor expand log c tr c trf log p take deriv respect obtain log c gammalog delta vector diagon element w 0 c c term aris log zq ensur sum one set deriv equal 0 solv give equat 9a appendix structur approxim structur approxim hq defin log h use c2 write kl diverg tr c tr c diag log z d2 z ghahramani mi jordan sinc kl independ p first thing note paramet structur approxim remain equal equival paramet true system take deriv respect log h n get log h n log h 6m c log h n last term obtain make use fact log zq log h n cancel first term set term insid bracket d3 equal zero yield equat 12a acknowledg thank lawrenc saul help discuss geoffrey hinton support project support part grant mcdonnellpew foundat grant atr human inform process research laboratori gift siemen corpor grant n000149410777 offic naval research zoubin ghahramani support grant ontario inform technolog research centr note 1 relat work infer distribut state hmm see dean kanazawa 1989 2 speech neural network gener use model p jy probabl convert observ probabl need hmm via bay rule 3 column w w n orthogon everi pair state variabl n c diagon covari matrix state variabl longer depend given observ case explain away state variabl model variabl observ along differ subspac 4 bayesian treatment learn problem paramet also consid hidden random variabl handl gibb sampl replac step sampl condit distribut paramet given hidden variabl exampl see tanner wong 1987 5 first term replac log second term appear 6 sampl use learn sampl discard begin run although ten sampl even approach converg provid runtim roughli compar variat method goal see whether impati gibb sampler would abl compet approxim method factori hidden markov model 273 7 lower valu suggest better probabilist model valu one exampl mean would take one bit true gener model code observ vector standard deviat reflect variat due train set test set random seed algorithm standard error mean factor 38 smaller 8 variat method dash line equal minu lower bound log likelihood except normal term intract comput vari learn result appar occasion increas bound 9 sinc attribut model real number log likelihood measur rel code cost comparison likelihood meaning wherea obtain absolut cost code sequenc necessari specifi discret level 10thi analog fullyconnect boltzmann machin n unit hinton sejnowski 1986 everi binari unit coupl everi unit use 2 paramet rather o2 n paramet requir specifi complet probabl tabl r maxim techniqu occur statist analysi probabilist function markov chain inputoutputhmm architectur mixtur control jump linear nonlinear plant multipl viewpoint system music predict element inform theori applic gener propag algorithm probabilist expert system model reason persist causat maximum likelihood incomplet data via em algorithm neural network biasvari dilemma stochast relax factori learn em algorithm learn relearn boltzmann machin bayesian updat recurs graphic model local comput hierarch mixtur expert em algorithm neural comput stochast simul algorithm dynam probabilist network hidden markov model comput biolog applic protein model local comput probabl graphic structur applic expert system gener linear model learn fine motion markov mixtur expert uci repositori machin learn databas connectionist learn belief network probabilist infer use markov chain mont carlo method technic report crgtr931 new view em algorithm justifi increment variant statist field theori probabilist reason intellig system network plausibl infer ca morgan kaufmann introduct hidden markov model mix memori markov model mean field theori sigmoid belief network journal artifici intellig research boltzmann chain hidden markov model exploit tractabl substructur intract network probabilist independ network hidden markov probabl model hidden markov model induct bayesian model merg calcul posterior distribut data augment discuss bound convolut code asymptot optim decod algorithm mean field network learn discrimin tempor distort string minimum descript length framework unsupervis learn receiv tr probabilist reason intellig system network plausibl infer model reason persist causat learn relearn boltzmann machin element inform theori connectionist learn belief network neural network biasvari dilemma hierarch mixtur expert em algorithm probabilist independ network hidden markov probabl model hidden markov model induct bayesian model merg minimum descript length framework unsupervis learn ctr p xing michael jordan stuart russel graph partit strategi gener mean field infer proceed 20th confer uncertainti artifici intellig p602610 juli 0711 2004 banff canada ricardo silva jiji zhang jame g shanahan probabilist workflow mine proceed eleventh acm sigkdd intern confer knowledg discoveri data mine august 2124 2005 chicago illinoi usa andrew howard toni jebara dynam system tree proceed 20th confer uncertainti artifici intellig p260267 juli 0711 2004 banff canada raul fernandez rosalind w picard model driver speech stress speech commun v40 n12 p145159 april robert jacob wenxin jiang martin tanner factori hidden markov model gener backfit algorithm neural comput v14 n10 p24152437 octob 2002 terri caelli andrew mccabe garri brisco shape track product use hidden markov model hidden markov model applic comput vision world scientif publish co inc river edg nj 2001 agnieszka betkowska koichi shinoda sadaoki furui robust speech recognit use factori hmm home environ eurasip journal appli signal process v2007 n1 p1010 1 januari 2007 yunhua hu hang li yunbo cao dmitriy meyerzon qinghua zheng automat extract titl gener document use machin learn proceed 5th acmieeec joint confer digit librari june 0711 2005 denver co usa yunhua hu hang li yunbo cao li teng dmitriy meyerzon qinghua zheng automat extract titl gener document use machin learn inform process manag intern journal v42 n5 p12761293 septemb 2006 toni jebara risi kondor andrew howard probabl product kernel journal machin learn research 5 p819844 1212004 fine yoram singer naftali tishbi hierarch hidden markov model analysi applic machin learn v32 n1 p4162 juli 1998 charl sutton khashayar rohanimanesh andrew mccallum dynam condit random field factor probabilist model label segment sequenc data proceed twentyfirst intern confer machin learn p99 juli 0408 2004 banff alberta canada wang nan zheng yan li yingq xu heungyung shum learn kernelbas hmm dynam sequenc synthesi graphic model v65 n4 p206221 juli jie tang hang li yunbo cao zhaohui tang email data clean proceed eleventh acm sigkdd intern confer knowledg discoveri data mine august 2124 2005 chicago illinoi usa cen li gautam biswa bayesian approach structur learn hidden markov model scientif program v10 n3 p201219 august 2002 sophi denev bayesian spike neuron infer neural comput v20 n1 p91117 januari 2008 lawrenc k saul michael jordan mix memori markov model decompos complex stochast process mixtur simpler one machin learn v37 n1 p7587 oct 1999 yong cao petro faloutso frdric pighin unsupervis learn speech motion edit proceed acm siggrapheurograph symposium comput anim juli 2627 2003 san diego california hung h bui svetha venkatesh geoff west track surveil widearea spatial environ use abstract hidden markov model hidden markov model applic comput vision world scientif publish co inc river edg nj 2001 r anderson pedro domingo daniel weld relat markov model applic adapt web navig proceed eighth acm sigkdd intern confer knowledg discoveri data mine juli 2326 2002 edmonton alberta canada tom ingliar milo hauskrecht noisyor compon analysi applic link analysi journal machin learn research 7 p21892213 1212006 martin v butz kernelbas ellipsoid condit realvalu xc classifi system proceed 2005 confer genet evolutionari comput june 2529 2005 washington dc usa ying wu thoma huang robust visual track integr multipl cue base coinfer learn intern journal comput vision v58 n1 p5571 june 2004 michael jordan zoubin ghahramani tommi jaakkola lawrenc k saul introduct variat method graphic model machin learn v37 n2 p183233 nov11999 andrea torsello antonio robleskelli edwin r hancock discov shape class use tree editdist pairwis cluster intern journal comput vision v72 n3 p259285 may 2007 charl sutton andrew mccallum khashayar rohanimanesh dynam condit random field factor probabilist model label segment sequenc data journal machin learn research 8 p693723 512007 h attia independ factor analysi neural comput v11 n4 p803851 may 15 1999 jinhai cai zhiqiang liu hidden markov model spectral featur 2d shape recognit ieee transact pattern analysi machin intellig v23 n12 p14541458 decemb 2001 john binder daphn koller stuart russel keiji kanazawa adapt probabilist network hidden variabl machin learn v29 n23 p213244 novdec 1997 xiangdong dawn jutla nick cercon privaci intrus detect use dynam bayesian network proceed 8th intern confer electron commerc new ecommerc innov conquer current barrier obstacl limit conduct success busi internet august 1316 2006 fredericton new brunswick canada akio utsugi ensembl independ factor analyz applic natur imag analysi neural process letter v14 n1 p4960 august 2001 zoubin ghahramani introduct hidden markov model bayesian network hidden markov model applic comput vision world scientif publish co inc river edg nj 2001 cristian sminchisescu atul kanaujia dimitri metaxa condit model contextu human motion recognit comput vision imag understand v104 n2 p210220 novemb 2006 hichem snoussi ali mohammaddjafari bayesian unsupervis learn sourc separ mixtur gaussian prior journal vlsi signal process system v37 n23 p263279 junejuli 2004 inna stainva david low gener probabilist orient wavelet model textur segment neural process letter v17 n3 p217238 june russel greiner christian darken n iwan santoso effici reason acm comput survey csur v33 n1 p130 march 2001