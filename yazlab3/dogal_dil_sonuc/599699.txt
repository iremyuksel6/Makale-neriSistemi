nearoptim reinforc learn polynomi time present new algorithm reinforc learn prove polynomi bound resourc requir achiev nearoptim return gener markov decis process observ number action requir approach optim return lower bound mix time optim polici undiscount case horizon time discount case give algorithm requir number action total comput time polynomi number state action undiscount discount case interest aspect algorithm explicit handl explorationexploit tradeoff b introduct reinforc learn agent interact unknown environ attempt choos action maxim cumul payo sut ton barto 1998 barto et al 1990 bertseka tsitsikli 1996 environ typic model markov decis process mdp assum agent know paramet process learn act directli experi thu reinforc learn agent face fundament tradeo exploit explor bertseka 1987 kumar varaiya 1986 thrun 1992 agent exploit cumul experi far execut action current seem best execut dierent action hope gain inform experi could lead higher futur payo littl explor prevent agent ever converg optim behavior much explor prevent agent gain nearoptim payo time fashion larg literatur reinforc learn grow rapidli last decad mani dierent algorithm propos solv reinforc learn problem variou theoret result converg properti algorithm proven exampl watkin qlearn algorithm guarante asymptot converg optim valu optim action deriv provid everi state mdp visit innit number time watkin 1989 watkin dayan 1992 jaakkola et al 1994 tsitsikli 1994 asymptot result specifi strategi achiev innit explor provid solut inher exploitationexplor tradeo address singh et al 1998 specifi two explor strategi guarante sucient explor asymptot converg optim action asymptot exploit qlearn sarsa algorithm variant qlearn rum meri niranjan 1994 singh sutton 1996 sutton 1995 gullap barto 1994 jalali ferguson 1989 present algorithm learn model environ experi perform valu iter estim model innit explor converg optim polici asymptot result best knowledg result reinforc learn gener mdp asymptot natur provid guarante either number action comput time agent requir achiev nearoptim perform hand nonasymptot result becom avail one consid restrict class mdp model learn modi standard one one chang criteria success thu saul singh 1996 provid algorithm learn curv converg rate interest special class mdp problem design highlight particular exploitationexplor tradeo fiechter 1994 1997 whose result closest spirit consid discountedpayo case make learn protocol easier assum avail reset button allow agent return set startstat arbitrari time other provid nonasymptot result predict uncontrol markov process schapir warmuth 1994 singh dayan 1998 thu despit mani interest previou result reinforc learn literatur lack algorithm learn optim behavior gener mdp provabl nite bound resourc action comput time requir standard model learn agent wander continu unknown environ result present paper void essenti strongest possibl sens present new algorithm reinforc learn prove polynomi bound resourc requir achiev nearoptim payo gener mdp observ number action requir approach optim return lower bound algorithm mix time optim polici undiscountedpayo case horizon time discountedpayo case give algorithm requir number action total comput time polynomi number state undiscount discount case interest aspect algorithm rather explicit handl exploitationexplor tradeo two import caveat appli current result well prior result mention first assum agent observ state environ may impract assumpt reinforc learn problem second address fact state space may larg resort method function approxim result avail reinforc learn function approxim sutton 1988 singh et al 1995 gordon 1995 tsitsikli roy 1996 partial observ mdp chrisman 1992 littman et al 1995 jaakkola et al 1995 asymptot natur extens result case left futur work outlin paper follow section 2 give standard denit mdp reinforc learn section 3 argu mix time polici must taken consider order obtain nitetim converg result undiscount case make relat technic observ denit section 4 make similar argument horizon time discount case provid need technic lemma heart paper contain section 5 state prove main result describ algorithm detail provid intuit proof converg rate section 6 elimin technic assumpt made conveni main proof section 7 discuss extens main theorem defer exposit final section 8 close discuss futur work preliminari denit begin basic denit markov decis process denit 1 markov decis process mdp state action transit probabl p action state j specifi probabl reach state j execut action state thu state action payo distribut state mean rm r max rm 0 varianc var var max distribut determin random payo receiv state visit simplic assum number action k constant easili veri k paramet resourc requir algorithm scale polynomi k sever comment regard benign technic assumpt make payo order first common assum payo actual associ stateact pair rather state alon choic latter entir technic simplic result paper hold standard stateact pay os model well second assum xed upper bound r max var max mean varianc payo distribut restrict necessari nitetim converg result third assum expect payo alway nonneg conveni easili remov ad sucient larg constant everi payo note although actual payo experienc random variabl govern payo distribut paper abl perform analys term mean varianc except section 55 need translat high expect payo high actual payo move standard denit stationari determinist polici mdp denit 2 let markov decis process state action polici map g later occas dene use nonstationari polici polici action chosen given state also depend time arriv state mdp combin polici yield standard markov process state say ergod markov process result ergod wellden stationari distribu tion develop exposit easiest consid mdp everi polici ergod socal unichain mdp put erman 1994 unichain mdp stationari distribut polici depend start state thu consid unichain case simpli allow us discuss stationari distribut polici without cumbersom technic detail turn result unichain alreadi forc main technic idea upon us result gener nonunichain multichain mdp small necessari chang denit best perform expect learn algo rithm gener multichain mdp given section 7 meantim howev import note unichain assumpt impli everi polici eventu visit everi state even exist singl polici quickli thu exploitationexplor dilemma remain us strongli follow denit nitelength path mdp repeat technic use analysi denit 3 let markov decis process let polici tpath sequenc p probabl p travers upon start state 1 execut polici pr dene two standard measur return polici denit 4 let markov decis process let polici let p path expect undiscount return along r expect discount return along p 1 discount factor make futur reward less valuabl immedi reward tstep undiscount return state u pr tstep discount return state pr case sum path p start dene u unichain case u independ simpli write u furthermor dene optim tstep undiscount return u fu similarli optim tstep discount return also u unichain case u independ simpli write u exist limit guarante unichain case final denot maximum possibl step return g undiscount case g discount case g tr max 3 undiscount case mix time easi see seek result undiscount return learn algorithm nite number step need take account notion mix time polici mdp put simpli undiscount case move asymptot return nitetim return may longer wellden notion optim polici may polici eventu yield high return instanc nalli reach remot highpayo state take mani step approach high return polici yield lower asymptot return higher shortterm return polici simpli incompar best could hope algorithm compet favor polici amount time compar mix time polici standard notion mix time polici markov decis process quanti smallest number step requir ensur distribut state step within stationari distribut induc distanc distribut measur kullbackleibl diverg variat distanc standard metric furthermor wellknown method bound mix time term second eigenvalu transit matrix p also term underli structur properti transit graph conduct sinclair 1993 turn state result weaker notion mix requir expect return step approach asymptot return denit 5 let markov decis process let ergod polici return mix time smallest 0 ju suppos simpli told polici whose asymptot return u exce valu r unknown mdp reward r reward 0 1 figur 1 simpl markov process demonstr nitetim converg result must account mix time return mix time principl sucient clever learn algorithm instanc one manag discov quickli could achiev return close u much step convers without assumpt reason expect learn algorithm approach return u mani fewer step simpli may take assum polici order step approach asymptot return exampl suppos two state one action see figur 1 state 0 payo 0 selfloop probabl 1 probabl go state 1 absorb state 1 payo r 0 small return mix time order 1 start state 0 realli requir order 1 step reach absorb state 1 start approach asymptot return r relat notion return mix time standard notion mix time markov decis process n state let ergod polici let smallest valu 0 state probabl state 0 step within stationari probabl return mix time 3trmax proof lemma follow straightforward way linear expect omit import point return mix time polynomi bound standard mix time may case substanti smaller would happen instanc polici quickli settl subset state common payo take long time settl stationari distribut within subset thu choos state result undiscount return term return mix time alway translat standard notion via lemma 1 notion return mix time precis type result reason expect undiscount case would like learn algorithm number action polynomi return learn algorithm close achiev best polici among mix time motiv follow denit denit 6 let markov decis process dene class ergod polici whose return mix time let opt denot optim expect asymptot undiscount return among polici thu goal undiscount case compet polici time polynomi 1 n eventu give algorithm meet goal everi simultan interest special case mix time asymptot optim polici whose asymptot return u time polynomi 1 n algorithm achiev return exceed u high probabl clear modulo degre polynomi run time result best one could hope gener mdp 4 discount case horizon time discount case quantic polici learn algorithm compet straightforward sinc discount make possibl principl compet polici time proport horizon time word unlik undiscount case expect discount return polici 11 step approach expect asymptot discount return made precis follow lemma markov decis process let polici state call valu lower bound given horizon time discount mdp proof lower bound v follow trivial denit sinc expect payo nonneg upper bound x innit path p let r 1 expect payo along path path prex innit path p solv yield desir bound sinc inequ hold everi xed path also hold distribut path induc polici discount case must settl notion compet slightli dierent undiscount case reason undiscount case sinc total return alway simpli averag learn algorithm recov youth mistak low return earli part learn possibl discount case due exponenti decay eect discount factor ask time polynomi horizon time learn algorithm polici current state discount return within asymptot optim state thu time reiniti 0 current state start state learn polici would nearoptim expect return goal algorithm achiev gener mdp discount case 5 main theorem readi describ new learn algorithm state prove main theorem name new algorithm gener mdp achiev nearoptim perform polynomi time notion perform paramet run time mix horizon time describ preced section eas exposit rst state theorem assumpt learn algorithm given input target mix time optim return opt achiev polici mix within step undiscount case optim valu function v discount case simpler case alreadi contain core idea algorithm analysi assumpt entir remov section 6 theorem 3 main theorem let markov decis process n state undiscount case recal class ergod polici whose return mix time bound opt optim asymptot expect undiscount return achiev exist algorithm take input nt opt total number action comput time taken polynomi 1 1 n r max probabl least 1 total actual return exce opt discount case let v denot valu function polici optim expect discount return exist algorithm take input n v total number action comput time taken polynomi 1 1 n horizon time r max probabl least 1 halt state output polici remaind section divid sever subsect describ dierent central aspect algorithm proof full proof theorem rather technic underli idea quit intuit sketch rst outlin 51 highlevel sketch proof algorithm although dierenc algorithm analys undiscount discount case easiest think singl algorithm algorithm commonli refer indirect modelbas name rather maintain current polici valu function algorithm actual maintain model transit probabl expect payo subset state unknown mdp import emphas although algorithm maintain partial model may choos never build complet model necessari achiev high return easiest imagin algorithm start call balanc wander mean algorithm upon arriv state never visit take arbitrari action state upon reach state visit take action tri fewest time state break tie action randomli state visit algorithm maintain obviou statist averag payo receiv state far action empir distribut next state reach estim transit probabl crucial notion algorithm analysi known state intuit state algorithm visit mani time therefor due balanc wander tri action state mani time transit probabl expect payo estim state close true valu import aspect denit weak enough mani time still polynomi bound yet strong enough meet simul requir outlin shortli fact denit known state achiev balanc shown section 52 state thu divid three categori known state state visit still unknown due insuci number visit therefor unreli statist state even visit import observ balanc wander indenit least one state becom known pigeonhol principl soon start accumul accur statist state fact state formal section 55 perhap import denit knownstat mdp set current known state current knownstat mdp simpli mdp natur induc full mdp brie transit state preserv transit redirect lead singl addit absorb state intuit repres unknown unvisit state although learn algorithm direct access virtu denit known state approxim rst two central technic lemma prove section 52 show appropri denit known state good simul accuraci expect step return polici close expect step return either mix time compet undiscount case horizon time discount case thu time partial model part algorithm know well second central technic lemma section 53 perhap enlighten part analysi name explor exploit lemma formal rather appeal intuit either optim step polici achiev high return stay high probabl set current known state importantli algorithm detect replic nding highreturn exploit polici partial model optim polici signic probabl leav within step algorithm detect replic nding explor polici quickli reach addit absorb state partial model thu perform two olin polynomialtim comput section 54 algorithm guarante either nd way get nearoptim return quickli nd way improv statist unknown unvisit state pigeonhol principl latter case occur mani time new state becom known thu algorithm alway make progress worst case algorithm build model entir mdp happen analysi guarante happen polynomi time follow subsect esh intuit sketch provid full proof theorem 3 section 6 show remov assum knowledg optim return 52 simul lemma section prove rst two key technic lemma mention sketch section 51 name one sucient accur approxim anoth mdp actual approxim step return polici quit accur step return eventu appeal lemma show accur assess return polici induc knownstat mdp comput return algorithm approxim appeal lemma 4 use set import technic point good approxim requir depend polynomi 1t thu denit known state requir polynomi number visit state begin denit approxim requir denit 7 let markov decis process state space say approxim state rm r state j action p state prove simul lemma say provid sucient close sens dene step return polici similar lemma 4 simul lemma let markov decis process state undiscount ontg polici t2 1 state u discount case let 11 let ontg approxim polici state 1 note lemma undiscount case state respect polici whose 2return mix time oppos return mix time howev 2 return return mix time linearli relat standard eigenvalu argument proof let us x polici start state let us say transit state 0 state j 0 action small p probabl step state follow polici cross least one small transit nt total probabl small transit state 0 action 0 n independ opportun cross transit impli total expect contribut either u walk cross least one small transit ntg similarli sinc p impli p approxim total contribut either u walk cross least one small transit thu bound dierenc u u restrict walk eventu determin choic solv thu restrict attent walk length cross small transit note transit satisfi p convert addit approxim p multipl approxim 1 p thu path p cross small transit path p approxim error payo yield sinc inequ hold xed path travers small transit also hold take expect distribut path induc thu addit 4 term account contribut path travers small transit bound equat 19 upper bound use follow taylor expans complet analysi undiscount case need two condit hold 2 rst condit would satis solv obtain t2 8g 4t g valu also impli constant therefor satisfi second condit would requir recal earlier constraint given equat 19 choos nd 4t g satis choic given lemma similar argument yield desir lower bound complet proof undiscount case analysi discount case entir analog except must addit appeal lemma 2 order relat step return asymptot return 2 simul lemma essenti determin denit known state one visit enough time ensur high probabl estim transit probabl estim payo state within ontg valu follow lemma whose proof straightforward applic cherno bound make translat number visit state desir accuraci transit probabl payo estim lemma 5 let markov decis process let state visit least time action execut least bmkc time let p ij denot empir probabl transit estim obtain visit probabl least 1 p state j action rm rm var maximum varianc random payo state thu get formal denit known state denit 8 let markov decis process say state known action execut least time 53 explor exploit lemma lemma 4 indic degre approxim requir sucient simul accuraci led denit known state let denot set known state specifi straightforward way known state dene induc mdp induc mdp addit new state intuit repres unknown state transit denit 9 let markov decis process let subset state induc markov decis process denot state fs 0 g transit payo dene follow state 2 rm payo determinist zero varianc even payo stochast action p ms absorb state state action p ms ij thu transit state preserv state 2 action p ms 0 2s p ij thu transit state redirect 0 denit 9 describ mdp directli induc true unknown mdp preserv true transit probabl state cours algorithm approxim transit probabl lead follow obviou approxim denot obviou empir approxim natur approxim follow lemma establish simul accuraci immedi lemma 4 lemma 5 lemma 6 let markov decis process let set current known state probabl least 1 undiscount case polici t2 ms state u ms ms ms discount case let 11 polici state ms v ms v ms 37that state simpli state visit far transit probabl observ transit frequenc reward observ reward let us also observ return achiev thu approxim achiev achiev real world lemma 7 let markov decis process let set current known state polici state 2 u ms ms proof follow immedi fact ident expect payo nonneg outsid payo possibl 2 heart analysi identi part unknown mdp algorithm know well form approxim key lemma follow demonstr fact thu simul lemma must alway provid algorithm either polici yield larg immedi return true mdp polici allow rapid explor unknown state lemma 8 explor exploit lemma let markov decis pro cess let subset state let induc markov decis process 2 1 0 either exist polici u ms respect v ms exist polici probabl walk step follow termin proof give proof undiscount case argument discount case analog let polici satisfi u u suppos u ms wit claim lemma may write u pr pr r pr sum respect path p start state path q start state everi state q path r start state least one state keep interpret variabl p q r xed may write pr pr ms qu ms q u ms equal follow fact path q everi state pr ms q um inequ fact u ms take sum path avoid absorb state 0 thu pr impli x r pr r pr r pr x r pr desir 2 54 olin optim polici comput let us take moment review synthes combin lemma 6 7 8 establish basic line argument time set current known state step return polici lower bound step return extens time must either polici whose step return nearli optim must polici quickli reach absorb state case polici execut quickli reach state current known set section discuss two olin polynomialtim comput nd polici highest return exploit polici one highest probabl reach absorb state step explor polici essenti follow fact standard valu iter algorithm dynam program literatur abl nd step optim polici arbitrari mdp n state 2 comput step discount undiscount case sake complet present undiscount discount valu iter algorithm bertseka tsitsikli 1989 optim step polici may nonstationari denot sequenc optim action taken state th step tstep undiscount valu iter initi u ms ms iju t1 j ms ms iju t1 j undiscount valu iter work backward time rst produc optim polici time step optim polici time step 1 observ nite polici maxim cumul step return also maxim averag step return tstep discount valu iter initi ms ms ijv t1 j ms ms ijv t1 j discount valu iter work backward time rst produc optim polici time step optim polici time step 1 note total comput involv 2 discount undiscount case use valu iter straightforward certain point execut algorithm perform valu iter olin twice use either undiscount discount version depend measur return second time denot comput use undiscount valu iter regardless measur return transit probabl dierent payo absorb state 0 payo r max state payo 0 thu reward explor repres visit 0 rather exploit polici return valu iter 0 polici return valu iter guarante either step return current known state approach optim achiev assum know thu detect probabl execut 0 reach unknown unvisit state step signic probabl also detect put togeth technic piec need place give detail descript algorithm tie loos end section 6 remov assumpt know optim return achiev sequel use express balanc wander denot step algorithm current state known state algorithm execut action tri fewest time current state note state becom known denit never involv step balanc wander use known denot number visit requir state becom known state dierent undiscount discount case given denit 8 call algorithm explicit explor exploit whenev algorithm engag balanc wander perform explicit olin comput partial model order nd step polici guarante either exploit explor descript follow freeli mix descript step algorithm observ make ensu analysi easier digest explicit explor exploit initi initi set known state empti balanc wander time current state algorithm perform balanc wander discoveri new known state time state visit known time balanc wander enter known set longer particip balanc wander observ clearli nm known 11 step balanc wan dere pigeonhol principl state becom known worst case term time requir least one state becom known gener total number step balanc wander algorithm perform ever exce nm known everi state known even step balanc wander consecut known state account known step balanc wander olin optim upon reach known state 2 balanc wander algorithm perform two olin optim polici comput describ section 54 attempt exploit result exploit polici achiev return least u 2 respec tive discount case least v 2 algorithm execut next step respect halt output given 2mix time given algorithm input respect horizon time attempt explor otherwis algorithm execut result explor polici deriv olin comput step lemma 8 guarante probabl least 2g leav set balanc wander time attempt exploit attempt explor visit state algorithm immedi resum balanc wander observ thu everi action taken algorithm either step balanc wander part step attempt exploit attempt explor conclud descript algorithm wrap analysi one main remain issu handl condenc paramet statement main theorem undiscount discount case theorem 3 ensur certain perform guarante met probabl least 1 essenti three dierent sourc failur algorithm known state algorithm actual poor approxim nextstat distribut action thu sucient strong simul accuraci repeat attempt explor fail yield enough step balanc wander result new known state undiscount case repeat attempt exploit fail result actual return near u handl failur probabl simpli alloc 3 sourc failur fact make probabl rst sourc failur bad known state control small quanti lemma 6 formal use lemma 6 meet requir state known simultan second sourc failur fail attempt explor standard cherno bound analysi suce lemma 8 attempt explor view independ bernoulli trial probabl least 2g least one step balanc wander worst case must make everi state known exploit requir nm known step balanc wander probabl fewer nm known step balanc wander smaller 3 number step attempt explor og nish analysi discount case discount case ever discov polici whose return current state close v attempt exploit algorithm nish argument alreadi detail sinc high probabl accur approxim part must nearoptim polici well lemma 7 long algorithm nish must engag balanc wander attempt explor alreadi bound number step high probabl everi state known set contain state actual accur approxim entir mdp lemma 8 ensur exploit must possibl sinc explor emphas case eventu contain state worst case analysi algorithm may discov abl halt nearoptim exploit polici long ever occur use valu known given discount case denit 8 total number action execut algorithm discount case thu bound time maximum number attempt explor given equat 45 bound total comput time bound 2 time requir olin comput time maximum number attempt explo ration give undiscount case thing slightli complic sinc want simpli halt upon nding polici whose expect return near u want achiev actual return approach u third sourc failur fail attempt exploit enter alreadi argu total number step attempt explor algorithm perform contain state polynomi bound action algorithm must account step attempt exploit step attempt exploit expect return least u 2 probabl actual return restrict attempt exploit less u 34 made smaller 3 number block exce o1 2 log1 standard cherno bound analysi howev also need make sure return restrict exploit block sucient domin potenti low return attempt explor dicult show provid number attempt exploit exce og time number attempt explor bound equat 45 condit satis total number action bound ot time number attempt explor total comput time thu 2 time number attempt explor thu bound conclud proof main theorem remark seriou attempt minim worstcas bound made immedi goal simpli prove polynomi bound straightforward manner possibl like practic implement base algorithm idea given would enjoy perform natur problem consider better current bound indic see moor atkeson 1993 relat heurist algorithm 6 elimin knowledg optim return mix time order simplifi present main theorem made assumpt learn algorithm given input target mix time optim return opt achiev mix time undiscount case valu function v discount case horizon time impli knowledg discount factor section sketch straightforward way assumpt remov without chang qualit natur result brie discuss altern approach may result practic version algorithm let us begin note knowledg optim return opt v use attempt exploit step algorithm must compar return possibl current state best possibl entir unknown mdp absenc knowledg explor exploit lemma lemma ensur us safe bia toward explor precis time arriv known state rst perform attempt explor olin comput modi knownstat mdp describ section 54 obtain optim explor polici 0 sinc simpl matter comput probabl 0 reach absorb state 0 step compar probabl lower bound long lower bound exceed may 0 attempt visit state lower bound guarante olin comput attempt exploit step must result exploit polici close optim discount case halt output undiscount case execut continu note explorationbias solut remov knowledg result algorithm alway explor state reach reason amount time ex ploitat although simpl way remov knowledg keep polynomialtim algorithm practic variant algorithm might pursu balanc strategi standard approach strong bia toward exploit instead enough explor ensur rapid converg optim perform instanc maintain schedul 2 0 1 total number action taken algorithm far upon reach known state algorithm perform attempt exploit execut attempt explor execut choic analys ensur still explor enough polynomi time contain polici whose return near optim return enjoy meantim may much greater explorationbias solut given note approach similar spirit greedi method augment algorithm qlearn explor compon crucial dierenc greedi explor probabl attempt singl action design visit rare visit state propos probabl execut multistep polici reach unknown state polici provabl justi undiscount case still remain remov assumpt algorithm know target mix time inde would like state main theorem valu long run algorithm number step polynomi paramet total return exceed opt probabl easili accomplish ignor paramet alreadi algorithm given input run p step xed polynomi p meet desir criterion propos new algorithm 0 need input simpli run sequenti amount time 0 must run 0 execut still polynomi need run 0 sucient mani step rst step domin lowreturn period took place p 0 step similar analysi done undiscount case toward end section 55 note solut sucient polynomi time far one would implement practic instanc would clearli want modifi algorithm mani sequenti execut share accumul common partial model 7 multichain case main issu extend result arbitrari multichain mdp asymptot undiscount return polici independ start state make undiscount case multichain mdp look lot like usual discount case inde result extend arbitrari multichain mdp discount case without modic therefor one way deal undiscountedcas multichain mdp ask given polynomi time algorithm state polici expect return nearoptim state anoth way modifi expect compet polici instead expect compet largest asymptot return start state polici compet lowest asymptot return start state polici thu modifi denit 5 6 follow markov decis process let polici return mix time smallest ij definit 6 let arbitrari markov decis process dene class polici whose return mix time let opt optim expect asymptot undiscount return among polici rene denit undiscountedcas result unichain mdp extend without modic arbitrari mdp 8 futur work number interest line research practic implement although polynomi bound proven far larg immedi claim practic relev algorithm feel underli algorithm idea promis eventu result competit algorithm current examin practic issu choic aris implement discuss brie section 6 hope report implement experi soon modelfre version partial relat last item would nice nd algorithm similar requir maintain partial model polici perhap sever current investig well larg state space would interest studi applic recent method deal larg state space function approxim algorithm recent investig context factor mdp kearn koller 1999 acknowledg give warm thank tom dean tom dietterich tommi jaakkola lesli kaelbl michael littman lawrenc saul terri sejnowski rich sutton valuabl comment satind singh support nsf grant iis9711753 portion work done univers colorado boulder r sequenti decis problem neural network dynam program determinist stochast model parallel distribut compu tation numer method athena scienti expect mistak bound model onlin reinforc learn stabl function approxim dynam program ming converg indirect adapt asynchron valu iter algorithm converg stochast iter dynam program algorithm reinforc learn algorithm partial observ markov decis problem distribut asynchron algorithm expect averag cost dynam program stochast system estim markov decis process learn curv bound markov decis process undiscount reward worstcas analysi temporaldier learn algorithm machin learn algorithm random gener count markov chain approach analyt mean squar error curv tempor di reinforc learn soft state aggreg converg result singlestep onpolici reinforc learn algo rithm reinforc learn replac elig trace gener reinforc learn success exampl use spars coars code reinforc learn introduc tion role explor learn control asynchron stochast approxim q learn learn delay reward tr ctr david wingat kevin seppi p3vi partit priorit parallel valu iter proceed twentyfirst intern confer machin learn p109 juli 0408 2004 banff alberta canada alexand l strehl michael l littman theoret analysi modelbas interv estim proceed 22nd intern confer machin learn p856863 august 0711 2005 bonn germani alexand l strehl lihong li eric wiewiora john langford michael l littman pac modelfre reinforc learn proceed 23rd intern confer machin learn p881888 june 2529 2006 pittsburgh pennsylvania shie mannor duncan simest peng sun john n tsitsikli bia varianc valu function estim proceed twentyfirst intern confer machin learn p72 juli 0408 2004 banff alberta canada revelioti theologo bountour effici pac learn episod task acycl state space discret event dynam system v17 n3 p307327 septemb 2007 carlo diuk alexand l strehl michael l littman hierarch approach effici reinforc learn determinist domain proceed fifth intern joint confer autonom agent multiag system may 0812 2006 hakod japan eyal evendar shie mannor yishay mansour action elimin stop condit multiarm bandit reinforc learn problem journal machin learn research 7 p10791105 1212006 daniela pucci de faria nimrod megiddo combin expert advic reactiv environ journal acm jacm v53 n5 p762799 septemb 2006 pieter abbeel andrew ng explor apprenticeship learn reinforc learn proceed 22nd intern confer machin learn p18 august 0711 2005 bonn germani daniela pucci de faria benjamin van roy costshap linear program averagecost approxim dynam program perform guarante mathemat oper research v31 n3 p597620 august 2006 amol deshpand zachari ive vijayshankar raman adapt queri process foundat trend databas v1 n1 p1140 januari 2007