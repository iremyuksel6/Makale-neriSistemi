dimension reduct unsupervis learn condit gaussian network abstractthi paper introduc novel enhanc unsupervis learn condit gaussian network benefit featur select propos base assumpt absenc label reflect cluster membership case databas featur exhibit low correl rest featur consid irrelev learn process thu suggest perform process use relev featur everi irrelev featur ad learn model obtain explanatori model origin databas primari goal simpl thu effici measur assess relev featur learn process present addit form measur allow us calcul relev threshold automat identifi relev featur experiment result report synthet realworld databas show abil propos distinguish relev irrelev featur acceler learn howev still obtain good explanatori model origin databas b introduct one basic problem aris great varieti eld includ pattern recognit machin learn statist socal data cluster problem 1 2 10 11 18 22 despit dierent interpret expect give rise gener data cluster problem involv assumpt addit observ variabl also refer predict attribut simpli featur hidden variabl last unobserv variabl would ect cluster membership everi case databas thu data cluster problem also refer exampl learn incomplet data due exist hidden variabl incomplet data repres special case miss data miss entri concentr singl variabl hidden cluster variabl refer given databas incomplet case unlabel point view adopt paper data cluster problem may dene infer gener joint probabl densiti function given databas concret focu learn condit gaussian network data cluster 25 26 27 36 37 roughli speak condit gaussian network graphic model encod condit gaussian distribut 25 26 27 variabl domain appli data cluster encod multivari normal distribut observ variabl condit state cluster variabl aim automat recov gener joint probabl densiti function given incomplet databas learn condit gaussian network paper concern understand data cluster descript task rather predict task thu order encod descript origin databas learnt model must involv origin featur instead subset unsupervis learn algorithm focu predict task featur select proven valuabl techniqu increas predict abil elicit model paper demonstr even focus descript featur select also known dimension reduct protabl tool improv perform unsupervis learn gener framework propos show unsupervis learn condit gaussian network benet featur select straightforward consist three step ident relev featur learn ii unsupervis learn condit gaussian network databas restrict relev featur iii addit irrelev featur learnt network obtain explanatori model origin databas accord framework featur select consid preprocess step accompani postprocess step fulll object postprocess step consist addit everi irrelev featur learnt model nal model encod gener joint probabl densiti function origin data complet dene framework one decid automat dimension reduct scheme identifi relev featur learn paper introduc simpl relev measur assess relev featur learn process order select subset contain salient one addit propos heurist method automat qualifi everi featur complet relev irrelev learn process carri automat calcul relev threshold featur relev measur valu higher relev threshold consid relev learn pro cess wherea rest quali irrelev experiment result report paper show framework depict provid us good explanatori model origin databas reduc cost learn process relev featur use process addit eectiv simplic automat dimension reduct scheme propos repres valuabl advantag allow framework reduc dimension databas perform learn ecient besid scheme tie particular learn algorithm therefor adapt remaind paper organ follow section 2 introduc condit gaussian network data cluster section 3 dedic explain detail automat dimension reduct scheme present new relev measur well automat discov relev irrelev featur calcul relev threshold section also present propos unsupervis learn condit gaussian network framework alreadi outlin experiment result show abil propos identifi relev featur acceler learn process compil section 4 final draw conclus section 5 condit gaussian network data cluster ing section start introduc notat use throughout paper give formal denit condit gaussian network also present bayesian structur em algorithm 13 use explanatori purpos well experi present section 4 due good perform unsupervis learn condit gaussian network 21 notat follow usual convent denot variabl uppercas letter state letter lowercas use letter letter boldfac uppercas design set variabl boldfac lowercas letter letter denot assign state variabl given set gener joint probabl densiti function x repres x addit gener condit probabl densiti function x given variabl x discret joint probabl mass function x thu denot condit probabl mass function x given hand variabl x continu joint probabl densiti function x thu fx j denot condit probabl densiti function x given 22 condit gaussian network alreadi mention face data cluster problem assum exist random variabl x partit ndimension continu variabl unidimension discret hidden cluster variabl c x said follow condit gaussian distribut 25 26 27 distribut condit state c multivari normal distribut c ndimension mean vector c n n varianc matrix posit denit dene condit gaussian network cgn x graphic model encod condit gaussian distribut x 25 26 27 36 37 essenti cgn belong class mix graphic model introduc rst time lauritzen wermuth 27 develop 25 26 class group model discret continu variabl present condit distribut continu variabl given discret variabl restrict multivari gaussian recent cgn success appli data cluster 36 37 concret cgn dene direct acycl graph model structur determin condit independ among variabl set local probabl densiti function multinomi distribut variabl c model structur yield factor gener joint probabl densiti function x follow denot congur parent consist x local probabl densiti function multinomi distribut previou equat assum depend nite set paramet therefor equat 2 rewritten follow model structur multinomi distribut local probabl densiti function c1 c1 c1 c2 c2 c2 figur 1 structur local probabl densiti function multinomi distribut cgn three continu variabl one binari cluster variabl c denot paramet local probabl densiti function h denot hypothesi condit independ assert impli hold true gener joint probabl densiti function x obtain equat 3 order encod condit gaussian distribut x local probabl densiti function cgn linearregress model thu normal distribut mean standard deviat 0 given form miss arc j impli b c linearregress model local paramet c b c column vector loop 1 run em algorithm comput map paramet b l given 2 perform search model structur evalu model structur l l 3 let l1 model structur highest score among encount search 4 score l l figur 2 schemat bsem algorithm interpret compon local paramet c follow given uncondit mean v c condit varianc given pa b c linear coecient ect strength relationship j see figur 1 exampl cgn three continu variabl one binari cluster variabl note model structur independ valu cluster variabl c thu model structur valu c howev paramet local probabl densiti function depend valu c may dier distinct valu variabl c 23 learn condit gaussian network incomplet data one method learn cgn incomplet data wellknown bayesian structur em bsem algorithm develop friedman 13 due good perform algorithm receiv special attent literatur motiv sever variant 32 34 35 41 use bsem algorithm explanatori purpos well experi present section 4 appli bsem algorithm data cluster problem assum databas n case everi case repres assign n observ variabl involv problem domain n 1n random variabl describ databas let denot set observ variabl nn variabl assign valu similarli let h denot set hidden unobserv variabl n variabl ect unknown cluster membership case learn cgn incomplet data bsem algorithm perform search space cgn base wellknown em algorithm 7 29 direct optim bayesian score shown figur 2 bsem algorithm compris two step optim cgn paramet structur search model select concret bsem algorithm altern step nd maximum posteriori map paramet current cgn structur usual mean em algorithm step search cgn structur iter bsem algorithm attempt maxim expect bayesian score instead true bayesian score interest solv data cluster problem consider size direct applic bsem algorithm appear figur 2 may unrealist ineci solut opinion reason possibl ineci comput score l impli huge comput expens take account everi possibl complet databas common use relax version present bsem algorithm consid like complet databas comput score l instead consid everi possibl complet thu relax version bsem algorithm compris iter parametr optim current model structur search databas complet like complet use best estim gener joint probabl densiti function data far current model posterior probabl distribut cluster variabl c case databas l calcul case assign cluster maximum posterior probabl distribut c reach use relax version experi section 4 complet specifi bsem algorithm decid structur search procedur step 2 figur 2 usual approach perform greedi hillclimb search cgn structur consid possibl addit remov revers singl arc point search structur search procedur desir exploit decomposit properti cgn factor properti bayesian score complet data howev structur search procedur exploit properti use log margin likelihood expect complet data log j h usual chosen score guid structur search make use experi accord 15 assumpt databas restrict cluster variabl c c multinomi sampl ii databas complet iii paramet multinomi distribut c independ follow dirichlet distribut c databas restrict continu variabl case set valu cluster variabl c take term pd correspond margin likelihood trivial bayesian network singl node c calcul close form reason assumpt accord 5 moreov term form f c c j h c 2 v alc repres margin likelihood domain contain continu variabl assumpt continu data sampl multivari normal distribut term evalu factor close form reason assumpt accord 15 16 19 3 automat dimension reduct unsupervis learn condit gaussian network section devot detail present new automat dimension reduct scheme appli unsupervis learn cgn section start introductori revis gener problem featur select brief discuss problem appear adapt supervis featur select unsupervis paradigm 31 supervis unsupervis featur select mani data analysi applic size data larg larg due excess number featur huge number instanc learn algorithm work ecient even sometim eectiv one may need reduc data size featur select proven valuabl techniqu achiev reduct dimension data select subset featur focu attent subsequ learn process gener form featur select consid problem search optim subset origin featur accord certain criterion 3 23 28 criterion speci detail measur good featur subset well relev featur choic criterion uenc purpos featur select howev share dierent purpos desir improv perform subsequ learn algorithm usual term speed learn predict abil learnt model andor comprehens learnt model roughli speak featur select involv algorithm explor space potenti featur subset evalu function measur qualiti featur subset sinc space featur subset n featur size 2 n featur select mechan typic perform nonexhaust search one popular techniqu use simpl hillclimb search known sequenti select may either forward backward 3 23 28 former search start empti set select featur time add best featur among unselect one accord evalu function process stop improv made similarli backward sequenti select begin full set featur time remov worst featur base evalu function improv found address doak 9 featur select mechan base sequenti select requir great deal process time databas larg number featur also complex eectiv search algorithm use explor space potenti featur subset main advantag algorithm sequenti select avoid get stuck local maxima mean random howev approach usual involv huge comput eort one recent work eld report 20 paper author propos explor space featur subset accord evolutionari populationbas random search algorithm repres instanc estim distribut algorithm eda approach 24 23 author distinguish two approach evalu function featur select wrapper lter wrapper approach impli search optim featur subset tailor perform function subsequ learn algorithm consid feedback perform function particular subsequ learn algorithm part function evalu featur subset hand lter approach reli intrins properti data presum aect perform learn algorithm direct function perform lter approach tri assess merit dierent featur subset data ignor subsequ learn algorithm appli supervis learn main object featur select improv classic accuraci class label predict accuraci model elicit subsequ learn algorithm consid relev featur task independ approach use lter wrapper approach requir class label present data order carri featur select filter approach evalu featur subset usual assess correl everi featur class label use dierent measur 3 28 hand wrapper approach reli perform learn algorithm measur classic accuraci valid set evalu good dierent featur subset 3 23 28 evid supervis featur select research wrapper approach outperform lter approach 21 although featur select central problem data analysi suggest grow amount research area vast major research carri supervis learn paradigm supervis featur select pay littl attent unsupervis learn unsupervis featur select work exist address latter problem 6 author present method rank featur accord unsupervis entropi measur algorithm work lter approach plu backward sequenti select search devaney ram 8 propos wrapper approach combin either forward backward sequenti select search perform conceptu cluster 39 talavera introduc lter approach combin search one step wrapper approach combin either forward backward sequenti select search featur select mechan hierarch cluster symbol data lter approach use featur depend measur dene fisher 11 wherea perform criterion consid 39 multipl predict accuraci measur averag accuraci predict valu featur present test data 40 appli mechan compris lter approach search one step present 39 featur select conceptu cluster symbol data consid class label predict accuraci perform criterion opinion two main problem translat supervis featur select unsupervis featur select firstli absenc class label ect membership everi case databas inher unsupervis paradigm make imposs use evalu function supervis featur select secondli standard accept perform task unsupervis learn due lack uni perform criterion mean optim featur subset may vari task task natur solut problem propos 39 interpret perform task unsupervis learn multipl predict accuraci seem reason approach extend standard accept perform task supervis learn unsupervis learn wherea former learn compris predict one featur class knowledg mani latter aim predict mani featur knowledg mani 12 hand 6 8 40 evalu unsupervis featur select mechan measur class label predict accuraci learnt model case test set perform learn train set class label mask speed learn comprehens learnt model also studi 8 39 although consid less import perform criteria 32 learn condit gaussian network data cluster benet featur select motiv perform unsupervis featur select dier motiv previous refer paper due distinct point view data cluster problem learnt model data cluster primarili evalu regard multipl class label predict accuraci occur 6 8 39 40 featur select proven valuabl techniqu reduc dimension databas perform learn usual pursu improv perform learnt model consid relev featur task howev main goal data cluster happen paper descript rather predict learnt model must involv featur origin databas order encod descript databas wellknown unsupervis learn cgn solv data cluster problem dicult time consum task even focus descript origin featur usual consid learn process aim solv handicap propos framework learn cgn data cluster benet featur select framework straightforward consist three step ident relev featur learn ii unsupervis learn cgn databas restrict relev featur iii addit irrelev featur learnt cgn obtain explanatori model origin databas thu featur select consid preprocess step accompani postprocess step achiev object postprocess step consist addit everi irrelev featur elicit model condit independ rest given cluster variabl make framework applic unsupervis learn cgn dene relev howev mean relev depend particular purpos dimension reduct due lack uni perform criterion data cluster concret case object reduc dimension databas learn cgn data cluster decreas cost learn process still obtain good explanatori model origin data achiev goal assess compar term explanatori power runtim learn process cgn learnt given origin databas cgn elicit use dimension reduct learn process assess achiev object lead us make follow assumpt consider featur either relev irrelev learn process absenc label ect cluster membership case databas featur exhibit low correl rest featur consid irrelev learn process implicitli assumpt dene relev accord purpos perform dimension reduct import note assumpt independ cluster data readili appli without requir previou cluster databas justic previou assumpt straightforward featur low correl rest like remain condit independ rest featur given cluster variabl learn cgn origin databas thu cgn elicit origin databas restrict featur highli correl rest like encod set condit depend assert cgn learnt origin databas paramet local probabl densiti function featur appear cgn similar well furthermor low correl featur ad cgn elicit restrict databas condit independ rest given cluster variabl nal cgn like encod set condit depend independ assert cgn learnt origin data thu explanatori power cgn almost model like similar work success made use similar assumpt 11 39 40 although three work present assumpt gener form valid conceptu cluster symbol data paper rst knowledg veri continu domain 321 relev measur order assess relev evalu follow simpl thu ecient relev measur ijjrest n number featur databas n number case databas r ijjrest sampl partial correl j adjust remaind variabl last quantiti express term maximum likelihood estim element invers varianc matrix r ijjrest relev measur valu featur averag likelihood ratio test statist exclud edg featur graphic gaussian model 38 mean featur like remain condit independ rest given cluster variabl learn progress receiv low relev measur valu thu measur show reason behavior accord denit relev 322 relev threshold calcul relev measur valu everi featur databas decreas relev rank featur obtain would like know mani need perform learn appropri would like identifi relev rank relev featur learn process knew k featur need could simpli choos rst k featur relev rank name k featur highest relev measur valu howev kind knowledg usual propos novel automat solut problem relev measur valu featur interpret averag valu likelihood ratio test statist exclud singl edg featur graphic gaussian model thu propos follow evalu relev measur featur calcul relev threshold let rel featur subset contain relev featur loop 1 run em algorithm comput map paramet b rel l rel l given 2 perform search model structur evalu model structur l rel l l rel l l 3 let rel l1 model structur highest score among encount search 4 exit loop score rel l l let final nal model obtain ad irrelev featur rel l calcul map paramet b final final return final b figur 3 schemat automat dimension reduct scheme bsem algorithm framework present heurist relev threshold calcul reject region boundari edg exclus test graphic gaussian model likelihood ratio test statist see 38 detail heurist agre purpos perform dimension reduct quali irrelev featur like remain condit independ rest given cluster variabl learn progress shown 38 distribut function likelihood ratio test statist follow distribut function x 2 1 random variabl thu 5 test reject region boundari consid relev threshold given resolut follow equat simpl manipul resolut previou equat turn nding root equat newtonraphson method use experi exampl suitabl method solv equat featur exhibit relev measur valu higher relev threshold quali relev learn process rest featur treat irrelev 323 fit automat dimension reduct learn subsect present automat dimension reduct scheme bsem algorithm gener framework previous introduc howev notic scheme coupl particular learn algorithm could adapt figur 3 show preprocess step consist automat dimension reduct scheme bsem algorithm appli usual restrict origin databas relev featur rel hidden cluster variabl c databas perform learn consist n case figur 4 exampl tanb model structur seven predict attribut g everi case repres assign relev featur r 1n random variabl describ databas r number relev featur denot set observ variabl restrict relev featur set hidden variabl rel jo rel respect obvious figur 3 rel l repres model structur relev featur consid learn pro cess rel l denot hypothesi condit independ assert impli rel l hold true joint probabl densiti function rel learn end postprocess step compris addit everi irrelev featur model return bsem algorithm condit independ rest given cluster variabl result explanatori model origin databas local paramet node nal model associ irrelev featur easili estim complet origin databas last complet restrict databas rel 4 experiment evalu section dedic show abil propos perform automat dimension reduct acceler unsupervis learn cgn without degrad explanatori power nal model order reach conclus perform 2 sort experi synthet realworld databas rst evalu relev measur introduc section 321 mean assess relev featur learn process second evalu abil relev threshold calcul appear section 322 automat distinguish relev irrelev featur learn address use bsem algorithm unsupervis learn algorithm current experi limit bsem algorithm learn tree augment naiv bay tanb model 14 30 36 sensibl usual decis reduc otherwis larg search space cgn moreov allow solv ecient data cluster problem consider size wellknown diculti involv learn dens connect cgn larg databas pain slow probabilist infer work tanb model constitut class compromis cgn dene follow con dition predict attribut may one predict attribut parent figur 4 show exampl tanb model structur tanb model cgn interest tradeo ecienc eectiv achiev balanc cost learn process qualiti learnt cgn 36 41 databas involv 2 synthet 2 realworld databas involv experiment evalu knowledg cgn use gener synthet databas allow us assess accur achiev object besid realworld databas provid us realist evalu framework obtain 2 synthet databas construct 2 tanb model dierent complex sampl rst tanb model involv 25 predict continu attribut 1 3valu cluster variabl rst 15 25 predict attribut relev rest irrelev 14 arc relev attribut randomli chosen uncondit mean everi relev attribut xed 0 rst valu cluster variabl 4 second 8 third linear coecient randomli gener interv 1 1 condit varianc xed 1 see equat 5 multinomi distribut cluster variabl c uniform everi irrelev attribut follow univari normal distribut mean 0 varianc 1 3 valu cluster variabl second tanb model involv predict continu attribut 1 3valu cluster variabl rst 15 predict attribut relev rest irrelev 14 arc relev attribut randomli chosen uncondit mean everi relev attribut xed 0 rst valu cluster variabl 4 second 8 third linear coecient randomli gener interv 1 1 condit varianc xed 2 see equat 5 multinomi distribut cluster variabl c uniform everi irrelev attribut follow univari normal distribut mean 0 varianc 5 3 valu cluster variabl second model consid complex rst due higher degre overlap probabl densiti function cluster higher number irrelev attribut 2 tanb model sampl 4000 case learn databas 1000 case test databas forthcom learn databas sampl 2 tanb model refer synthetic1 synthetic2 respect obvious discard entri correspond cluster variabl 2 learn databas 2 test databas anoth sourc data evalu consist 2 wellknown realworld databas uci repositori machin learn databas 33 waveform artici databas consist 40 predict featur last 19 predict attribut nois attribut turn irrelev describ underli 3 cluster use data set gener uci repositori obtain 4000 case learn 1000 case test pima real databas contain 768 case 8 predict featur 2 cluster use rst 700 case learn last 68 case test rst databas chosen due interest work databas consider size thousand case ten featur addit repres opportun evalu eectiv approach true irrelev featur known beforehand second databas consider shorter number case number featur chosen get feedback scalabl dimension reduct scheme obvious delet cluster entri 2 learn databas 2 test databas 42 perform criteria exist 2 essenti purpos focu explanatori power generaliz learnt model rst purpos summar given databas learnt model second purpos elicit model abl predict unseen instanc 28 thu explanatori power learnt cgn assess evalu achiev purpos log margin likelihood sc nal multipl predict accuraci ltest learnt cgn seem sensibl perform measur rst second purpos respect multipl predict accuraci measur logarithm score rule good 17 jd test j y2d test log fy test set test case jd test j number test case higher valu criterion higher multipl predict accuraci learnt cgn note ltest primari perform measur 1 2 measur assess explanatori power learnt cgn focus descript ltest extrem necessari detect model suer overt high sc nal valu although abl gener learn data unseen instanc note equat 10 repres kind probabilist approach standard multipl predict accuraci understand latter averag accuraci predict valu featur present test data data cluster problem consid infer gener joint probabl densiti function learn data via unsupervis learn cgn probabilist approach present equat 10 appropri standard multipl predict accuraci illustr simpl exampl let us imagin 2 dierent cgn exhibit standard multipl predict accuraci dierent multipl predict accuraci measur logarithm score rule good would ect gener joint probabl densiti function encod 2 cgn dierent moreov would impli 1 2 cgn gener learn data unseen instanc better ie likelihood unseen instanc higher although standard multipl predict accuraci thu standard multipl predict accuraci would appropri perform criterion context would unabl distinguish model work made use logarithm score rule good assess multipl predict accuraci 31 34 36 37 41 runtim overal learn process runtim also consid valuabl inform everi runtim report includ runtim preprocess step dimension reduct learn algorithm postprocess step addit irrelev featur result report averag 10 independ run synthetic1 synthetic2 waveform databas 50 independ run pima databas due shorter size experi run pentium 366 mhz comput featur synthetic12216104relevance503010featur synthetic2251791 relevance3010featur waveform31197relevance1062featur pima7531 figur 5 relev measur valu featur databas use dash line correspond relev threshold 43 result relev rank figur 5 plot relev measur valu featur 4 databas consid addit show relev threshold dash line databas case synthet databas 10 true irrelev featur synthetic1 databas 15 synthetic2 databas clearli appear lowest relev measur valu case waveform databas may interest compar graph figur 5 graph report 4 40 42 databas caution use detail comparison advis due fact relev dene dierent way depend particular purpos work moreov work talavera 40 limit conceptu cluster symbol data origin waveform databas previous discret howev notic 19 true irrelev featur appear plot low relev valu 4 graph although shape graph restrict 21 relev featur vari 3 work report 4 40 42 agre graph consid rst last relev featur less import rest 21 shape graph slightli closer appear 4 42 one plot 40 conclud relev measur propos exhibit desir behavior databas true irrelev featur known clearli assign low relev valu follow subsect evalu valu low enough automat distinguish relev irrelev featur calcul relev threshold figur 6 show log margin likelihood sc nal multipl predict accuraci number select featur synthetic12216104scfin 74000 76000 78000 80000 82000 84000 86000 88000 number select featur synthetic12216104ltest number select featur synthetic2251791 scfinal 122000 132000 number select featur synthetic2251791 number select featur waveform31197scfin 122000 number select featur waveform31197ltest number select featur pima7531 scfinal 9000 number select featur pima7531 figur log margin likelihood sc nal multipl predict accuraci ltest nal cgn databas use function number featur select relev decreas relev rank ltest nal cgn 4 databas consid function number featur select relev learn addit figur 7 report runtim need learn nal cgn function number featur select number select featur synthetic12216104runtim number select featur synthetic2251791 runtim seconds1006020numb select featur waveform31197runtim seconds2000 number select featur pima7531 runtim figur 7 runtim need learn nal cgn databas use function number featur select relev decreas relev rank relev learn select k featur relev mean select k rst featur decreas relev rank obtain featur concret databas accord relev measur valu thu rst part experiment evalu perform automat dimension reduct instead aim studi perform function number featur involv learn allow us evalu abil relev measur assess relev featur learn process gener term figur 6 conrm relev measur abl induc eectiv decreas relev rank featur databas consid addit featur low relev measur valu last featur rank impli signic increas qualiti nal model even case hurt explanatori power thu gure conrm assumpt low correl featur irrelev learn process work well continu domain consid hand addit irrelev featur tend increas cost learn process measur runtim see figur 7 particularli interest result synthet databas origin model known select true irrelev featur take part learn produc better model increas runtim learn process also known last 19 40 featur waveform databas true irrelev featur accord relev measur valu featur waveform databas see figur 5 19 true irrelev featur would appear last 21 posit decreas relev rank furthermor appreci figur 6 addit 19 irrelev featur signicantli increas tabl 1 comparison perform achiev learn cgn origin databas automat dimension reduct scheme appli featur origin dimension dimension reduct databas origin relev sc nal ltest runtim sc nal ltest runtim synthetic2 explanatori power nal cgn result obtain pima databas knowledg exist true irrelev featur share fact use featur learn process degrad qualiti nal model well make learn process slower thu explanatori power nal cgn appear monoton respect addit featur relev learn henc need automat tool discov irrelev featur may degrad eectiv enlarg runtim learn 44 result automat dimension reduct figur 5 show relev threshold dash line calcul shown section 322 databas consid featur exhibit relev measur valu higher relev threshold quali relev rest featur consid irrelev learn interest notic 2 synthet databas true irrelev featur identi independ complex sampl model rememb synthetic2 databas sampl model complex one use gener synthetic1 databas result obtain waveform databas also special appeal 19 true irrelev featur correctli identi moreov scheme consid 8 featur remaind 21 featur also irrelev appear sensibl decis 8 featur correspond rst 4 last 4 21 relev featur rememb 4 40 42 agre point rst last 21 relev featur less import rest relev featur tabl 1 compar 4 databas consid perform achiev dimension reduct carri perform achiev automat dimension reduct scheme appli learn cgn column relev indic number relev featur automat identi scheme databas see figur 5 clearli appear tabl scheme abl automat set relev threshold induc save runtim still obtain good explanatori model applic scheme preprocess step bsem algorithm figur provid us save runtim origin bsem algorithm achiev 22 synthetic1 databas synthetic2 databas moreov explanatori power cgn elicit origin synthet databas cgn obtain use automat dimension reduct scheme exactli waveform databas automat dimension reduct scheme propos reduct number featur 68 13 40 origin featur consid relev reduct induc gain term runtim 58 wherea scheme signicantli hurt qualiti learnt model hand cgn learnt help automat dimension reduct scheme pima databas exhibit averag desir behavior cgn elicit origin pima databas higher log margin likelihood multipl predict accuraci wherea runtim learn process shorten conclus main contribut paper twofold first propos novel automat scheme perform unsupervis dimension reduct compris simpl ecient measur assess relev everi featur learn process ii heurist calcul relev threshold automat distinguish relev irrelev featur second present framework unsupervis learn cgn benet propos scheme order obtain model describ origin databas framework propos perform learn take account relev featur identi automat dimension reduct scheme present everi irrelev featur incorpor learnt model order obtain explanatori cgn origin databas experiment result synthet realworld domain suggest great advantag deriv use automat dimension reduct scheme unsupervis learn cgn huge decreas runtim learn process achiev nal model appear good sometim even better model obtain use featur learn process addi tional experiment result proven assumpt made relev dene accord purpos perform dimension reduct work fairli well continu domain consid paper primarili focus gain ecienc without degrad explanatori power nal model deriv use refer scheme preprocess learn process howev worth notic identica tion relev irrelev featur learn process allow us reach better comprehens readabl problem domain elicit model work address problem unsupervis featur select preprocess step 6 8 39 40 howev dier work wherea focu descript origin databas 6 8 40 interest class label predict accuraci 39 multipl predict accuraci imposs fair comparison dierent approach moreov automat dimension reduct scheme oer seri advantag exist mechan addit simplic ecienc scheme coupl particular learn algorithm could adapt hand exist unsupervis featur select mechan base wrapper approach tailor perform criterion particular subsequ learn algorithm see 8 39 thu usual requir great deal process time larg databas furthermor 6 40 propos featur select mechan base l ter approach provid user rank featur leav open problem determin mani featur use perform proper learn scheme abl automat distinguish relev irrelev featur relev rank one line futur research could extens current contribut categor data order overcom problem determin number featur use subsequ learn algorithm awar contribut present paper unabl deal properli domain redund featur exist ie featur whose valu exactli determin rest featur reason relev measur introduc section 321 score featur separ instead group featur thu redund featur would consid relev although would provid learn process addit inform true relev fea ture detect featur necessari eect runtim learn process one line research current explor concern extens gener framework depict paper case redund featur exist current work focus deriv new relev measur assess gain relev featur relat featur consid relev far acknowledg jm pena wish thank dr steve ellacott interest work use comment also made possibl visit school comput mathemat scienc univers brighton brighton unit kingdom author would also like thank two anonym review whose use comment previou version paper help us improv manuscript work support spanish ministri educ cultur min isterio de educacion cultura ap97 44673053 grant r analysi applic pattern classi cluster algorithm find group data estim distribut algorithm featur select knowledg discoveri data mine em algorithm extens uci repositori machin learn databas tr ctr martin h c law mario figueiredo anil k jain simultan featur select cluster use mixtur model ieee transact pattern analysi machin intellig v26 n9 p11541166 septemb 2004 lanc parson ehtesham haqu huan liu subspac cluster high dimension data review acm sigkdd explor newslett v6 n1 p90105 june 2004 j pea j lozano p larraaga global multimod problem optim via estim distribut algorithm base unsupervis learn bayesian network evolutionari comput v13 n1 p4366 januari 2005