natur languag grammat infer recurr neural network abstractthi paper examin induct infer complex grammar neural networksspecif task consid train network classifi natur languag sentenc grammat ungrammat therebi exhibit kind discriminatori power provid principl paramet linguist framework governmentandbind theori neural network train without divis learn vs innat compon assum chomski attempt produc judgment nativ speaker sharpli grammaticalungrammat data recurr neural network could possess linguist capabl properti variou common recurr neural network architectur discuss problem exhibit train behavior often present smaller grammar train initi difficult howev implement sever techniqu aim improv converg gradient descent backpropagationthroughtim train algorithm signific learn possibl found certain architectur better abl learn appropri grammar oper network train analyz final extract rule form determinist finit state automata investig b introduct paper consid task classifi natur languag sentenc grammat ungrammat attempt train neural network without bifurc learn vs innat compon assum chomski produc judgment nativ speaker sharpli grammaticalungrammat data recurr neural network investig comput reason comput recurr neural network power feedforward network recurr architectur shown least ture equival 53 54 investig properti variou popular recurr neural network architectur particular elman narendra parthasarathi np william zipser recurr network also frasconigorisoda fg local recurr network find elman wz recurr neural network abl learn appropri grammar implement techniqu improv converg gradient descent base backpropagationthroughtim train algorithm analyz oper network investig rule approxim recurr network learn specif extract rule form determinist finit state automata previou work 38 compar neural network machin learn paradigm problem work focus recurr neural network investig addit network analyz oper network train algorithm investig rule extract paper organ follow section 2 provid motiv task attempt section 3 provid brief introduct formal grammar grammat infer describ data section 4 list recurr neural network model investig provid detail data encod network section 5 present result investig variou train heurist investig train simul anneal section 6 present main result simul detail investig oper network extract rule form determinist finit state automata investig section 7 section 8 present discuss result conclus motiv 21 represent power natur languag tradit handl use symbol comput recurs process success stochast languag model base finitest descript ngram hidden markov model howev finitest model repres hierarch structur found natur languag 1 48 past year sever recurr neural network architectur emerg 1 insideoutsid reestim algorithm extens hidden markov model intend use learn hierarch system algorithm current practic rel small grammar 48 use grammat infer 9 21 19 20 68 recurr neural network use sever smaller natur languag problem eg paper use elman network natur languag task includ 1 12 24 58 59 neural network model shown abl account varieti phenomena phonolog 23 61 62 18 22 morpholog 51 41 40 role assign 42 58 induct simpler grammar address often eg 64 65 19 learn tomita languag 60 task consid differ grammar complex recurr neural network investig paper constitut complex dynam system shown recurr network represent power requir hierarch solut 13 ture equival 22 languag acquisit certainli one import question studi human languag peopl unfailingli manag acquir complex rule system system complex resist effort linguist date adequ describ formal system 8 coupl exampl kind knowledg nativ speaker often take grant provid section instanc nativ speaker english know adject eager obligatorili take complement sententi complement contain overt subject verb believ moreov eager may take sententi complement nonovert ie impli understood subject believ eager john believ john eager john believ john eager believ grammat judgment sometim subtl unargu form part nativ speaker languag compet case judgment fall accept aspect languag compet interpret consid refer embed subject predic talk follow exampl john stubborn mari talk john stubborn talk john stubborn talk bill first sentenc clear mari subject embed predic everi nativ speaker know strong contrast corefer option understood subject second 2 convent asterisk use indic ungrammat third sentenc despit surfac similar third sentenc john must impli subject predic talk contrast john understood object predic second sentenc subject arbitrari refer word sentenc read john stubborn arbitrari person talk john point emphas languag faculti impress discriminatori power sens singl word seen exampl result sharp differ accept alter interpret sentenc consider furthermor judgment shown robust sens virtual nativ speaker agre data light exampl fact contrast crop english languag exampl stubborn contrast also hold dutch linguist chiefli chomski 7 hypothes reason knowledg partial acquir lack variat found across speaker inde languag certain class data suggest exist fix compon languag system word innat compon languag faculti human mind govern languag process languag obey socal univers principl sinc languag differ regard thing like subjectobjectverb order principl subject paramet encod systemat variat found particular languag innat hypothesi languag paramet plu languagespecif lexicon acquir speaker particular principl learn base assumpt studi languageindepend principl becom known principlesandparamet framework governmentandbind gb theori paper investig whether neural network made exhibit kind discriminatori power sort data gblinguist examin precis goal train neural network scratch ie without divis learn vs innat compon assum chomski produc judgment nativ speaker grammaticalungrammat pair sort discuss instead use innat knowledg posit neg exampl use second argument innat possibl learn grammar without neg exampl 3 data first provid brief introduct formal grammar grammat infer natur languag thorough introduct see harrison 25 fu 17 detail dataset use experi 31 formal grammar grammat infer briefli grammar g four tupl fn set termin nontermin compris alphabet grammar p set product rule start symbol everi exist languag l set string termin symbol grammar gener recogn also exist automata recogn gener grammar grammat infer concern mainli procedur use infer syntact product rule unknown grammar g base finit set string lg languag gener g possibl also finit set string complement lg 17 paper consid replac infer algorithm neural network grammar english languag simpl grammar use elman 13 shown tabl 1 contain structur complet english grammar eg agreement verb argument structur interact rel claus recurs cat mari tabl 1 simpl grammar encompass subset english languag 13 phrase full sentenc chomski hierarchi phrase structur grammar simplest grammar associ automata regular grammar finitestateautomata fsa howev firmli establish 6 syntact structur natur languag parsimoni describ regular languag certain phenomena eg center embed compactli describ contextfre grammar recogn pushdown automata other eg crossedseri depend agreement better describ contextsensit grammar recogn linear bound automata 50 32 data data use work consist 552 english posit neg exampl taken introductori gblinguist textbook lasnik uriagereka 37 exampl organ minim pair like exampl eager john wini eager john win minim natur chang involv suggest dataset may repres especi difficult task model due small sampl size raw data name word first convert use exist parser major syntact categori assum gbtheori tabl 2 summar part speech use partofspeech tag repres sole grammat inform suppli model particular sentenc addit grammat statu import refin implement categori exampl noun n john book destruct verb v hit sleep adject eager old happi preposit p without complement c thought eager determin man man adverb adv sincer sincer believ john want marker mrkr possess john mother destruct want help tabl 2 part speech includ subcategor inform major predic name noun verb adject preposit experi show ad subcategor bare categori inform improv perform model exampl intransit verb sleep would place differ class obligatorili transit verb hit similarli verb take sententi complement doubl object seem give persuad would repres class 3 flesh subcategor requir along line lexic item train set result 9 class verb 4 noun adject 2 preposit exampl input data shown tabl 3 sentenc encod grammat statu eager john n4 v2 a2 c n4 v2 adv 1 eager john n4 v2 a2 n4 v2 adv 0 eager n4 v2 a2 v2 adv 1 tabl 3 exampl partofspeech tag tag done complet contextfre manner obvious word eg may part one partofspeech tag result sever contradictori duplic sentenc variou method 3 follow classic gb theori class synthes thetagrid individu predic via canon structur realiz csr mechan pesetski 49 test deal case howev remov altogeth result report addit number posit neg exampl equal randomli remov exampl higher frequenc class train test set order reduc effect due differ priori class probabl number sampl per class vari class may bia toward predict common class 3 2 4 neural network model data encod follow architectur investig architectur 1 3 topolog restrict 4 number hidden node equal sens may represent capabl model 4 expect frasconigorisoda fg architectur unabl perform task includ primarili control case 1 frasconigorisoda local recurr network 16 multilay perceptron augment local feedback around hidden node localoutput version use fg network also studi 43 network call fg paper line 63 2 narendra parthasarathi 44 recurr network feedback connect output node hidden node np network architectur also studi jordan 33 34 network call np paper line 30 3 elman 13 recurr network feedback hidden node hidden node train elman network backpropagationthroughtim use rather truncat version use elman ie paper elman network refer architectur use elman train algorithm 4 william zipser 67 recurr network node connect node diagram architectur shown figur 1 4 input neural network data encod fix length window made segment contain eight separ input correspond classif noun verb adject etc subcategori class linearli encod input manner demonstr specif valu noun input noun 0 noun class class 1 linear order defin accord similar variou subcategori 4 two output use neural network correspond grammat ungrammat classif data input neural network window pass sentenc tempor 4 fix length window made segment contain 23 separ input correspond classif noun class 1 noun class 2 verb class 1 etc also test prove inferior figur 1 frasconigorisoda local recurr network connect shown fulli figur 2 narendra parthasarathi recurr network connect shown fulli figur 3 elman recurr network connect shown fulli figur 4 william zipser fulli recurr network connect shown fulli order begin end sentenc see figur 5 size window variabl one word length longest sentenc note case input window small greater interest larger input window greater capabl network correctli classifi train data without form grammar exampl input window equal longest sentenc network store inform simpli map input directli classif howev input window rel small network must learn store inform shown later network implement grammar determinist finit state automaton recogn grammar extract network thu interest small input window case network requir form grammar order perform well gradient descent simul anneal learn backpropagationthroughtim 66 5 use train global recurr network 6 gradient descent algorithm describ author 16 use fg network standard gradient descent algorithm found impract problem 7 techniqu describ improv converg investig due depend initi paramet number simul perform differ initi weight train settest set combin howev due comput complex task 8 possibl perform mani simul 5 backpropagationthroughtim extend backpropag includ tempor aspect arbitrari connect topolog consid equival feedforward network creat unfold recurr network time 6 realtim 67 recurr learn rtrl also test show signific converg present problem modifi standard gradient descent algorithm possibl train network oper larg tempor input window network forc model grammar memor interpol train data 8 individu simul section took averag two hour complet sun figur 5 depict neural network input come input window sentenc window move begin end sentenc desir standard deviat nmse valu includ help assess signific result tabl 4 show result use use techniqu list except note result section elman network use two word input 10 hidden node quadrat cost function logist sigmoid function sigmoid output activ one hidden layer learn rate schedul shown initi learn rate 02 weight initi strategi discuss million stochast updat target valu provid end sentenc standard nmse std dev variat nmse std dev updat batch 0931 00036 updat stochast 0366 0035 learn rate constant 0742 0154 learn rate schedul 0394 0035 activ logist 0387 0023 activ tanh 0405 014 section 0367 0011 section ye 0573 0051 cost function quadrat 0470 0078 cost function entropi 0651 00046 tabl 4 comparison use use variou converg techniqu paramet constant case elman network use two word input ie slide window current previou word 10 hidden node quadrat cost function logist activ function sigmoid output activ one hidden layer learn rate schedul initi learn rate 02 weight initi strategi discuss 1 million stochast updat nmse result repres averag four simul standard deviat valu given standard deviat four individu result 1 detect signific error increas nmse increas significantli train network weight restor previou epoch perturb prevent updat point techniqu found increas robust algorithm use learn rate larg enough help avoid problem due local minima flat spot error surfac particularli case william zipser network 2 target output target output 01 09 use logist activ function 08 08 use tanh activ function help avoid satur sigmoid function target set asymptot sigmoid would tend drive weight infin b caus outlier data produc larg gradient due larg weight c produc binari output even incorrect lead decreas reliabl confid measur 3 stochast versu batch updat stochast updat paramet updat pattern presen tation wherea true gradient descent often call batch updat gradient accumul complet train set batch updat attempt follow true gradient wherea stochast path follow use stochast updat stochast updat often much quicker batch updat especi larg redund dataset 39 addit stochast path may help network escap local minima howev error jump around without converg unless learn rate reduc second order method work well stochast updat stochast updat harder parallel batch 39 batch updat provid guarante converg local minima work better second order techniqu howev slow may converg poor local minima result report train time equal reduc number updat batch case equal number weight updat batch updat would otherwis much slower batch updat often converg quicker use higher learn rate optim rate use stochast updat 9 henc alter learn rate batch case investig howev signific converg obtain shown tabl 4 4 weight initi random weight initi goal ensur sigmoid start satur small correspond flat part error surfac 26 ad dition sever 20 set random weight test set provid best perform train data chosen experi current problem found techniqu make signific differ 5 learn rate schedul rel high learn rate typic use order help avoid slow converg local minima howev constant learn rate result signific paramet perform fluctuat entir train cycl perform network 9 stochast updat gener toler high learn rate batch updat due stochast natur updat alter significantli begin end final epoch moodi darken propos search converg learn rate schedul form 10 11 1 jt learn rate time j 0 initi learn rate constant found learn rate final epoch still result consider paramet fluc henc ad addit term reduc learn rate final epoch specif learn rate schedul found later section found use learn rate schedul improv perform consider shown tabl 4 6 activ function symmetr sigmoid function eg tanh often improv converg standard logist function particular problem found differ minor logist function result better perform shown tabl 4 7 cost function rel entropi cost function 4 29 57 26 27 receiv particular attent natur interpret term learn probabl 36 investig use quadrat rel entropi cost function 1 quadrat cost function defin rel entropi cost function defin 3where correspond actual desir output valu k rang output also pattern batch updat found quadrat cost function provid better perform shown tabl 4 possibl reason use entropi cost function lead increas varianc weight updat therefor decreas robust paramet updat 8 section train data investig divid train data subset initi one subset use train 100 correct classif obtain prespecifi time limit expir addit subset ad work set continu work set contain entir train set data order term sentenc length result obtain epoch involv stochast updat mislead surpris find quit signific differ onlin nmse calcul compar static calcul even algorithm appear converg shortest sentenc first enabl network focu simpler data first elman suggest initi train constrain later train use way 13 howev problem use section consist decreas perform shown tabl 4 also investig use simul anneal simul anneal global optim method 32 35 minim function downhil step accept process repeat new point uphil step may also accept therefor possibl escap local minima optim process proce length step declin algorithm converg global optimum simul anneal make assumpt regard function optim therefor quit robust respect nonquadrat error surfac previou work shown use simul anneal find paramet recurr network model improv perform 56 comparison gradient descent base algorithm use simul anneal investig order train exactli elman network success train 100 correct train set classif use backpropagationthroughtim detail section 6 signific result obtain trial 11 use simul anneal found improv perform simard et al 56 howev problem pariti problem use network four hidden unit wherea network consid paper mani paramet result provid interest comparison gradient descent backpropagationthroughtim bptt method bptt make implicit assumpt error surfac amen gradient descent optim assumpt major problem practic howev although difficulti encount bptt method significantli success simul anneal make assumpt problem 6 experiment result result four neural network architectur given section result base multipl trainingtest set partit multipl random seed addit set japanes control data use test set consid train model japanes data larg enough dataset japanes english opposit end spectrum regard word order japanes sentenc pattern differ english particular japanes sentenc typic sov subjectobjectverb verb less fix argument less avail freeli permut english data cours svo argument permut gener avail exampl canon japanes word order simpli ungrammat english henc would extrem surpris englishtrain model accept japanes ie expect network train 11 adapt simul anneal code lester ingber 31 32 use english gener japanes data find model result signific gener japanes data 50 error averag five simul perform architectur simul took approxim four hour summar result obtain variou network order make number weight architectur approxim equal use singl word input wz model two word input other reduct dimension wz network improv perform network contain 20 hidden unit full simul detail given section 6 goal train network use small tempor input window initi could done addit techniqu describ earlier possibl train elman network sequenc last two word input give 100 correct 996 averag 5 trial classif train data gener test data result 742 correct classif averag better perform obtain use network howev still quit low data quit spars expect increas gener perform obtain amount data increas well increas difficulti train addit dataset handdesign gb linguist cover rang grammat structur like separ train test set creat test set contain mani grammat structur cover train set william zipser network also perform reason well 713 correct classif test set note test set perform observ drop significantli extend train indic use valid set control possibl overfit would alter perform significantli train classif std dev elman 996 084 fg 671 122 wz 917 226 english test classif std dev elman 742 382 fg 590 152 wz 713 075 tabl 5 result network architectur comparison classif valu report averag five individu simul standard deviat valu standard deviat five individu result complet detail sampl elman network follow network differ topolog except wz better result obtain use input window one word network contain three layer includ input layer hidden layer contain 20 node hidden layer node recurr connect hidden layer node network train total 1 million stochast updat input within rang zero one target output either 01 09 bia input use best 20 random weight set chosen base train set perform weight initi shown haykin 26 weight initi node node basi uniformli distribut random number rang gamma24f fanin neuron logist output activ function use quadrat cost function use search converg learn rate schedul use learn rate learn rate train epoch current train epoch c 065 train set consist 373 noncontradictori exampl describ earlier english test set consist 100 noncontradictori sampl japanes test set consist 119 noncontradictori sampl take closer look oper network error train sampl network architectur shown figur 6 error point graph nmse complet train set note natur william zipser learn curv util detect correct signific error increas 12 figur 7 show approxim complex error surfac base first deriv error criterion respect weight sum weight network nw total number weight valu plot epoch train note complex natur plot william zipser network figur 8 11 show sampl plot error surfac variou network error surfac mani dimens make visual difficult plot sampl view show variat error two dimens note plot indic quantit conclus drawn test error plot plot shown figur respect two randomli chosen dimens case center plot correspond valu paramet train taken togeth plot provid approxim indic natur error surfac differ network type fg network error surfac appear smoothest howev result indic solut found perform well indic minima found poor compar global optimum andor network capabl implement map low error william zipser fulli connect network greater represent capabl elman architectur sens perform greater varieti comput number hidden unit howev compar elman wz network error surfac plot observ wz network greater percentag flat spot graph conclus show two dimens plot around one point weight space howev back 12 learn curv william zipser network made smoother reduc learn rate tend promot converg poorer local minima epoch epoch epoch epoch wz figur 6 averag nmse log scale train set train top bottom frasconigorisoda elman narendra parthasarathi william zipser hypothesi wz network perform wors error surfac present greater difficulti train method 7 automata extract extract symbol knowledg train neural network allow exchang inform connectionist symbol knowledg represent great interest understand neural network actual 52 addit symbol knowledg insert recurr neural network even refin train 15 47 45 order tripl discret markov process fstate input nextstateg extract rnn epoch epoch epoch epoch wz figur 7 approxim complex error surfac train top bottom frasconigorisoda elman narendra parthasarathi william zipser use form equival determinist finit state automata dfa done cluster activ valu recurr state neuron 46 automata extract process recogn regular grammar 13 howev natur languag 6 parsimoni describ regular languag certain phenomena eg center embed compactli describ contextfre grammar other eg crossedseri depend agreement better describ contextsensit grammar henc network may implement parsimoni version grammar unabl extract techniqu 13 regular grammar g 4tupl start symbol n nontermin termin symbol respect p repres product form ab b 2 n 2 weight 0 04992 6 5 4 3 2 1 weight 0 04992 weight 41 007465 5 4 3 2 1 weight 162 049550708096 5 4 3 2 weight 109 00295 5 4 3 2 115 12 weight 28 001001 6 5 4 3 2 weight 43 0075290675068506950705 weight 96 0013050684 0685 0686 0687 0688 0689 0690692 0693 weight 146 01471 6 5 4 3 2 1weight 45 012570685 069 weight 81 01939 6 5 4 3 2 15 weight weight 144 01274 5 4 3 2 1 weight 67 07342066 068072 074 weight 5 4 3 2 weight 101 003919 6 5 4 3 2 weight 76 038080682 0684 0686 0688 069 weight 13 1463 5 4 3 2 1 0weight 7 06851 weight 60 0334 5 4 3 2 1 weight figur 8 error surfac plot fg network plot respect two randomli chosen dimens case center plot correspond valu paramet train 2 1weight 0 3033 2 weight 0 3033122124126128 weight 132 1213 13 12 11 10 9 8 7 6 5 4 3 2 weight 100 7007124 126 128132 134 136 1385 4 3 2 weight 152 06957 8 7 6 5 4 3 2 1weight 47 2201 124 125 126 127 128 129 136 5 4 3 2 weight 84 01601 5 4 3 2 1 weight 152 06957122124126128 weight 24 01485 124 125 126 127 128 129131 weight 160 05467 6 5 4 3 2 weight 172 07099 124 125127 12813132 weight 105 2075 9 8 7 6 5 4 3 2 weight 13 1695 7 6 5 4 3 2 14 weight 8 10081245125512651275 weight 195 01977 3 2 1 weight 5 2464 124 126 128 13 weight weight 155 20941225 123 1235 124 1255 126 weight 163 2216 4 3 2 1 07 weight 138 1867 122 123 124 125 126 127 128 weight figur 9 error surfac plot np network plot respect two randomli chosen dimens case center plot correspond valu paramet train algorithm use automata extract 19 work follow network train even train appli procedur extract network learn ie network current concept dfa learn dfa extract process includ follow step cluster recurr network activ space form dfa state 2 construct transit weight 0 3612 9 8 7 6 5 4 3 2 weight 0 36120602 06040608 0610614 0616 weight 102 07384 3 4 weight 122 832059061063065 weight 244 8597 8 7 6 5 4 3 2 1 weight 81 2907 weight weight 151 1307 weight 263 9384 10 9 8 7 6 5 4 3 2 1 weight 215 4061058 059061 062 weight 265 8874 5 4 3 2 1 weight weight 62 5243 7 6 5 4 3 2 1weight 226 1218056 057 058 059 06 weight 128 1025 14 13 12 11 10 9 8 7 6 5 4 weight 162 8527059061063 064066 weight 118 9467 weight weight 118 9467059061063065 weight 228 4723 5 4 3 2 1 0weight 8 0370705850595 06061062 weight weight 228 47230595060506150625 figur 10 error surfac plot elman network plot respect two randomli chosen dimens case center plot correspond valu paramet train weight 0 3879 2 14 weight 0 3879141514251435 weight 65 2406 15 14 13 12 11 10 9 8 7 6 5 weight 153 1014335 1434 weight weight 105 1408 weight 122 3084 15 14 13 12 11 10 9 8 7 6 5 weight 158 9731143352 143354 weight 74 8938 112 111 110 109 108 107 106 105 104 103 102 weight 204 10714351445 145146147 weight 192 958 15 14 13 12 11 10 9 8 7 6 5 weight 86 9935 weight 170 8782 weight 2 8368 weight 214 8855 5 4 3 2 1 weight 39 055991415 142 43 weight 147 3869 1 2 weight 223 5656141142 weight 210 1329 15 14 13 12 11 10 9 8 7 6 5 weight 86 993514142144146 weight 63 3889 8 7 6 5 4 3 2 13 weight 81 21651425143514451455 weight 88 8262 6 7 weight 85 9631139 14142 143145 146 figur 11 error surfac plot wz network plot respect two randomli chosen dimens case center plot correspond valu paramet train diagram connect state togeth alphabet label arc put transit togeth make full digraph form loop 4 reduc digraph minim represent hypothesi train network begin partit quantiz state space fairli wellsepar distinct region cluster repres correspond state finit state automaton recent prove arbitrari dfa stabli encod recurr neural network 45 one simpl way find cluster divid neuron rang q partit equal width thu n hidden neuron exist q n possibl partit state dfa construct gener state transit diagram ie associ input symbol partit state left partit state activ initi partit state start state dfa determin initi valu t0 next input symbol map partit state valu assum loop form otherwis new state dfa form dfa thu construct may contain maximum q n state practic usual much less sinc partit state reach eventu process must termin sinc finit number partit avail practic mani partit never reach deriv dfa reduc minim dfa use standard minim algorithm 28 note dfa extract method may appli discretetim recurr net regardless order hidden layer recent extract process proven converg extract dfa learn encod neural network 5 extract dfa depend quantiz level q extract dfa use valu q start 3 use standard minim techniqu compar result automata 28 pass train test data set extract dfa found extract automata correctli classifi 95 train data 60 test data 7 smaller valu q produc dfa lower perform larger valu q produc significantli better perform sampl extract automata seen figur 12 difficult interpret extract automata topic futur research analysi extract automata view aid interpret addit import open question well extract automata approxim grammar implement recurr network may regular grammar automata extract may also use improv perform system via iter combin rule extract rule insert signific learn time improv achiev train network prior knowledg 46 may lead abil train larger network encompass target grammar paper investig use variou recurr neural network architectur fg np elman wz classifi natur languag sentenc grammat ungrammat therebi exhibit kind discriminatori power provid principl paramet linguist framework governmentandbind theori best worst perform architectur elman wz np fg surpris elman network outperform fg np network comput power elman network shown least ture equival 55 np network shown ture equival 54 within linear slowdown fg network figur 12 automata extract elman network train perform natur languag task start state state 1 bottom left accept state state 17 top right string reach accept state reject recent shown comput limit 14 elman network special case wz network fact elman wz network top perform surpris howev theoret elman network outperform wz network open question experiment result suggest train issu represent issu backpropag throughtim bptt iter algorithm guarante find global minima cost function error surfac error surfac differ elman wz network result suggest error surfac wz network less suitabl bptt train algorithm use howev architectur learn represent grammar network learn grammar hierarchi architectur increas comput power given number hidden node give insight whether increas power use model complex structur found grammar fact power elman wz network provid increas perform suggest abl find structur data may possibl model fg network addit investig data suggest 100 correct classif train data two word input would possibl unless network abl learn signific aspect grammar anoth comparison recurr neural network architectur gile horn 30 compar variou network randomli gener 6 64state finit memori machin local recurr narendra parthasarathi network prove good superior power network like elman network indic either task requir increas power vanilla backpropag throughtim learn algorithm use unabl exploit paper shown elman wz recurr neural network abl learn appropri grammar discrimin sharpli grammaticalungrammat pair use gblinguist howev gener limit amount natur data avail expect increas difficulti encount train model data use clear consider difficulti scale model consid larger problem need continu address converg train algorithm believ improv possibl address natur paramet updat gradient descent howev point must reach improv gradient descent base algorithm requir consider natur error surfac relat input output encod rare chosen specif aim control error surfac abil paramet updat modifi network behavior without destroy previous learn inform method network implement structur hierarch recurs relat acknowledg work partial support australian telecommun electron research board sl r sequenti connectionist network answer simpl question microworld comparison criterion function linear classifi supervis learn probabl distribut neural network dynam discretetim comput three model descript languag lectur govern bind knowledg languag natur finit state automata simpl recurr network note learn rate schedul stochast optim toward faster stochast gradient search structur represent connectionist model distribut represent comput capabl localfeedback recurr network act finitest machin unifi integr explicit rule learn exampl recurr network local feedback multilay network syntact pattern recognit applic network learn phonolog learn extract finit state automata secondord recurr neural network extract learn unknown grammar recurr neural network higher order recurr network role similar hungarian vowel harmoni connectionist account connectionist perspect prosod structur repres variabl inform simpl recurr network introduct formal languag theori neural network introduct theori neural comput introduct automata theori learn algorithm probabl distribut feedforward feedback network gile experiment comparison recurr neural network fast simul reanneal adapt simul anneal asa attractor dynam parallel connectionist sequenti machin serial order parallel distribut process approach simul anneal inform theori statist cours gb syntax lectur bind empti categori gile natur languag grammat infer comparison recurr neural network machin learn method effici learn second order method learn past tens english verb use recurr neural network languag learn cue rule encod inputoutput represent connectionist cognit system focus backpropag algorithm tempor pattern recognit control dynam system use neural network construct determinist finitest automata recurr neural network extract rule discretetim recurr neural network rule revis recurr neural network path categori induct dynam recogn learn past tens english verb combin symbol neural learn comput beyond ture limit comput capabl recurr narx neural network comput power neural net analysi recurr backpropag acceler learn layer neural network learn appli contextu constraint sentenc comprehens learn featurebas semant simpl recurr network dynam construct finitest automata exampl use hillclimb rule map connectionist symbol process mani map local recurr global feedforward network critic review architectur induct finit state languag use secondord recurr network induct finitest languag use secondord recurr network effici gradientbas algorithm onlin train recurr network trajec tori learn algorithm continu run fulli recurr neural network learn finit state machin selfclust recurr network tr ctr michal eransk matej makula ubica beukov organ state space simpl recurr network train recurs linguist structur neural network v20 n2 p236244 march 2007 marshal r mayberri iii risto miikkulainen broadcoverag pars neural network neural process letter v21 n2 p121132 april 2005 peter tio ash j mill learn beyond finit memori recurr network spike neuron neural comput v18 n3 p591613 march 2006 edward kei shiu ho lai wan chan analyz holist parser implic robust pars systemat neural comput v13 n5 p11371170 may 2001 peter c r lane jame b henderson increment syntact pars natur languag corpora simpl synchroni network ieee transact knowledg data engin v13 n2 p219231 march 2001 juan c vallelisboa florencia reali hctor anastasa eduardo mizraji elman topolog sigmapi unit applic model verbal hallucin schizophrenia neural network v18 n7 p863877 septemb 2005 henrik jacobsson rule extract recurr neural network taxonomi review neural comput v17 n6 p12231263 june 2005