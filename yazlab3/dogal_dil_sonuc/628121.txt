gener abil fold network abstractth inform theoret learnabl fold network success approach capabl deal tree structur input examin find bound vc pseudo fat shatter dimens fold network variou activ function consequ valid gener fold network guarante howev distribut independ bound gener error exist principl propos two approach take specif distribut account allow us deriv explicit bound deviat empir error real error learn algorithm first approach requir probabl larg tree limit priori second approach deal situat maximum input height concret learn exampl restrict b introduct one particular problem connectionist method deal structur object find possibl make process data priori unlimit size possibl connectionist method often use distribut represent object vector space fix dimens wherea list tree logic formula term graph etc consist unlimit number simpl element connect structur way possibl unlimit size admit direct represent finit dimension vector space often structur data possess recurs natur case process structur possibl standard neural network enlarg recurr connect mimic recurs natur structur 9 10 network capabl deal tree list arbitrari height length exampl dynam propos larg number approach deal adapt process structur data raam lraam fold network name 11 24 28 method differ singl process step look train share method entir tree process simpl map appli recurs input tree accord tree structur tree encod recurs distribut represent code use standard connectionist method regard fold network encod train simultan classif tree learn use modif backpropag 11 approach use success sever area applic 7 11 21 22 25 raam lraam train encod simultan dual decod composit yield ident 24 28 classif encod tree train separ focu capabl learn dynam principl consid inform theoret learnabl ie question whether finit number exampl contain enough inform learn given finit set data function dynam identifi mirror underli regular decid underli regular model function question whether valid gener finit set exampl underli regular possibl function class standard feedforward network question answer affirm combinatori quantiti call vc dimens finit fix architectur socal pac learnabl guarante moreov learn algorithm small empir error gener well one find explicit bound accuraci gener depend number paramet number train pattern independ concret distribut 3 order use fold network learn mechan necessari establish analog result recurr case question whether valid gener architectur possibl answer neg none approach learn principl question answer posit learn algorithm small empir error good learn algorithm inform theoret point view cours may exist differ effici algorithm algorithm may turn comput intract wherea anoth approach yield good solut short time howev mainli differ empir error optim number exampl necessari valid gener least case depend function class use learn learn algorithm unfortun situat turn difficult recurs case standard feedforward network exist work estim vc dimens recurr fold network 14 19 combinatori quantiti finit character distribut independ learnabl arbitrari input dimens infinit due unlimit input length ie abil deal input arbitrari size even lead abil store arbitrari inform finit number paramet sinc unlimit input space use purpos way consequ distribut independ bound gener error exist situat order take specif distribut account modifi two approach literatur guarante learnabl even infinit vc dimens adequ stratif function class instead 2 27 approach formul binari valu function class consid gener error algorithm zero empir error gener situat function class arbitrari error appli fold network standard learn algorithm well allow us establish inform theoret learnabl fold network first defin dynam fold network formal mention fact learn theori add mention two formal allow us obtain concret bound deviat empir error real error concret situat estim socal vc pseudo fat shatter dimens fold architectur quantiti play key role concern learnabl bound tell us distribut independ learnabl guarante principl deriv concret distribut data depend bound gener error fold network complet recal definit standard feedforward network feedforward neural network consist finit set neuron connect acycl graph connect equip weight w ij 2 r input neuron neuron without predecessor neuron call comput unit nonempti subset comput unit specifi output unit comput unit output neuron call hidden neuron comput unit equip bia activ function input n output comput function output unit defin recurs neuron ae x input unit term call activ neuron architectur network weight bias specifi allow vari r obviou way architectur stand set network result specifi weight bias consequ network comput map compos sever simpl function comput singl neuron activ function f singl comput unit often ident drop subscript case follow activ function consid ident id perceptron activ ae standard sigmoid function polynomi activ function feedforward network handl real vector fix dimens complex object tree label real vector space assum follow tree fix fanout k mean nonempti node exactli k successor consequ tree either empti tree consist root label valu subtre 1 k latter case denot tree set tree defin denot r k one use recurs natur tree construct induc map deal tree input vector valu map appropri ariti assum r l use encod label taken r map g r theta r k deltal r l initi context 2 r induc map l defin recurs follow definit use formal defin recurr fold network fold network consist two feedforward network comput function respect initi context 2 r l comput map fold architectur given two feedforward architectur input l output l input n output respect context specifi either input neuron 1 k delta l g call context neuron g refer recurs part network h feedforward part input neuron fold network architectur neuron 1 g follow assum network contain one output neuron h understand fold network comput function valu one think recurs part encod part tree encod recurs real vector r l start empti tree encod initi context leaf encod via g use code proceed way subtre c e f c f recurr context neuron recurr part feedforward part input tree lead comput figur 1 exampl comput fold network specif tree serv input network unfold accord structur input tree output valu simpli comput unfold network via g use alreadi comput code k subtre 1 k feedforward part map encod tree desir output valu see fig 1 exampl comput list dealt fold network reduc recurr network except separ feedforward recurr part definit coincid standard definit partial recurr neural network literatur 12 practic recurr fold network train gradient descent method like backpropag structur backpropag time respect 11 30 use success sever area applic includ time seri predict control search heurist classif chemic data graphic object 7 22 25 similar mechan propos process structur data lraam possibl defin analog encod function g map deltal set ae r l induc decod k ae mean complementari dynam decod valu recurs order obtain label root via g 0 code k subtre via g 1 g k definit 2 lraam consist two feedforward network comput g r mk deltal r l respect vector 2 r l set ae r l comput map frequent lraam use follow way one choos fix architectur network g g train weight composit yield ident consid tree afterward g g combin standard network order approxim map tree real vector space vice versa second step feedforward architectur train encod decod lraam remain fix although lraam train differ way process dynam use classif structur data encod part lraam combin standard network train specif learn problem consid entir process obtain function class repres fold network restrict learn function tree real vector space henc follow argument appli lraam mechan process dynam well howev situat chang fix encod learn feedforward network combin neural encod lraam situat reduc learn standard feedforward network tree identifi fix real input vector 3 foundat learn theori learn deal possibl learn abstract regular finit set data given fix input space x exampl set list tree equip oealgebra fix set f function x 0 1 network architectur exampl unknown learn f purpos finit set independ ident distribut data drawn accord probabl distribut p x learn algorithm map x theta 0 1 select function f pattern set function hope nearli coincid function learn write hm f x hm algorithm tri minim real error p f hm f x p f z cours error unknown gener sinc probabl p function f learn unknown concret learn algorithm often simpli minim empir error dm f hm f x x dm f exampl standard train algorithm network architectur fit weight mean gradient descent surfac repres empir error depend weight first consid distribut depend set ie given fix probabl distribut p x algorithm call probabl approxim correct pac sup hold ffl 0 weakest condit follow hold bound number exampl guarante valid gener exist learn algorithm bound independ unknown function learn practic somewhat stronger condit desir exist one mayb ineffici algorithm satisfactori want use learn algorithm effici yield small empir error properti learn algorithm empir error repres real error captur properti uniform converg empir distanc uce short ie dm f hold ffl 0 f possess uce properti learn algorithm small empir error highli probabl good algorithm concern gener uce properti desir sinc allow us use algorithm small empir error rank sever algorithm depend empir error quantiti ffl refer accuraci probabl explicitli bound ffi refer ffi confid exist equival character uce properti allow us test properti concret class f furthermor allow us deriv explicit bound number exampl empir real error deviat ffl confid least ffi set pseudometr cover number nffl denot smallest number n n point x 1 x n exist close ball respect radiu ffl center x cover character uce properti possibl uce properti hold lim dm f jx denot restrict f input x dm refer empir distanc explicit bound deviat empir error real error given inequ fg2f jd p f dm f see 29exampl 55 corollari 56 theorem 57 need possibl estim socal empir cover number f occur inequ often done estim combinatori quantiti associ f measur sens capac function class first assum f concept class ie function valu contain binari set f0 1g vapnikchervonenki dimens vc dimens vcf f largest number point shatter f ie binari map point obtain restrict function f real valu function class f fflfat shatter dimens fat ffl f largest size set fflfat shatter f ie point x 1 x n exist refer point r 1 r n 2 r binari map x function f 2 f exist everi quantiti call pseudodimens psf hold arbitrari p 29 theorem 42 consequ finit vc pseudodimens respect ensur uce properti hold bound term lead bound number exampl guarante valid gener moreov ensur distribut independ uce properti well ie uce properti hold even prefix sup p fat shatter dimens may smaller pseudodimens yield inequ even finit fat shatter dimens guarante distribut independ uce properti lead bound gener error follow assum constant function 0 contain f usual case f function class comput fold architectur shown finit vc dimens f concept class finit fat shatter dimens f function class respect even necessari f possess distribut independ uce properti 1 18 gener class socal loss function correl f finit fat shatter dimens f possess uce properti howev constant function 0 contain f class loss function contain f f finit fat shatter dimens well central role combinatori quantiti estim vc fat shatter dimens fold architectur next paragraph turn infinit gener order ensur uce properti argument need refin fortun input space x divid tree height case fold architectur vc dimens architectur finit restrict input x allow us deriv bound probabl high tree restrict priori purpos use follow theorem theorem 3 assum f function class input x output 0 1 assum measur 2 n x ae x t1 assum p probabl measur x ffl ffi 2 0 1 chosen p fg2f jd p f dm f finit hold even proof estim deviat real error empir error dm f m1 gamma ffl4 point x contain x p probabl induc p x hold p differ p 3ffl8 fraction ffl4 drop x dm chang ffl2 use chebychef inequ limit first term mention earlier second term limit expect cover number limit 512e entir term limit ffi oe respect 2 howev necessari know probabl high tree priori order get bound number exampl suffici valid gener hold even maximum input height tree concret train set restrict therefor like larger tree occur lucki framework 27 turn use allow us substitut prior bound probabl high tree posterior bound maximum input height sinc want get bound uce properti gener approach 27 function class follow way assum f 0 1valu function class input x function socal lucki function function measur quantiti allow stratif entir function class subclass finit capac l output small valu lucki sens concret output function learn algorithm contain subclass small capac need exampl correct gener case simpli measur maximum height input tree concret train set defin correspond function measur number function least lucki f x note deal real output consequ quantiz accord valu ff output necessari ensur finit number f ff refer function class output f0 ff obtain output f 2 f kff gamma ff2 kff ff2 identifi kff k 2 n lucki function l smooth respect j phi map n theta r indic fraction delet x obtain x 0 0 technic condit need proof smooth condit allow us estim number function least lucki g doubl sampl xy larg part know lucki g first half sampl x sinc lucki situat number function consid limit henc good gener bound obtain condit character kind smooth enlarg sampl set stronger condit smooth requir 27 consider restrict function g coincid x sinc want get result learn algorithm small empir error necessarili consist gener possibl estim lucki doubl sampl know first half appropri case analog 27 state follow theorem guarante kind uce properti situat turn lucki concret learn task purpos set split differ scenario less lucki occur probabl depend concret scenario gener bound obtain theorem 4 suppos p 2 n posit number l lucki function class f smooth respect j phi inequ valid learn algorithm h real valu ffi ff 0 fflm ffi henc lucki situat phi limit 2 0 1 deviat real error empir error limit term order high probabl proof f 2 f bound probabl fulfil ffl defin 29 theorem 57 step 1 suffici bound probabl latter set singl p ffi 2 intersect set singl set occur definit smooth l complement respect obtain bound denot event consid uniform distribut u group permut swap element j thu r x oe vector obtain appli permut oe 29 theorem 57 step 2 latter probabl bound sup xy length x 0 0 f ff denot quantiz version f output identifi kff denot event probabl u measur b defin equival class c permut two permut belong class map indic valu unless x 0 0 contain index find restrict event c definit consid permut swap element x 0 0 bound latter probabl u 0 denot uniform distribut swap common indic x 0 0 latter probabl bound use hoeffd inequ random variabl valu fsigmaerror x 0 term total therefor obtain desir bound choos ffl jm fulfil r note bound ffl tend 0 decreas j way becom small furthermor obtain bound differ real empir error instead deal consist algorithm 27 consid function instead concept class caus increas bound ff due quantiz decreas converg use hoeffd inequ function case furthermor dual formul unlucki function l 0 possibl correspond substitut definit l formula hold manner use unlucki framework later gener abil fold network want appli gener result learn theori fold network purpos first estim combinatori quantiti vcf jx psf jx fat ffl fold architectur f restrict set x input tree height denot oe activ function architectur w number adjust paramet ie weight bias compon initi context n number neuron h depth feedforward architectur induc fold architectur ie maximum length path graph defin network structur upper bound vc pseudodimens f jx obtain first substitut input tree equival input tree maximum number node unfold network input appli bound feedforward case unfold network detail see 14 15 lead follow bound found 8 14 19 ow lnth oe linear ow th ln oe polynomi degre 2 w lnw 2 note bound differ polynomi activ function respect lower bound found 8 14 19 oe nonlinear polynomi sinc standard feedforward perceptron architectur exist point sigmoid function approxim perceptron activ arbitrarili well combin architectur sigmoid case feedforward architectur obtain addit summand lower bound sgd detail construct describ 14 theorem 11 follow theorem yield slight improv sigmoid case theorem 5 input set r 2 sgd architectur exist shatter proof restrict argument case 2 consid tt tree depth contain binari number length first compon label leav binari number length label next layer number 0 1 first layer number 0 root tree ij 2 second compon label 0 except one layer 1 label alreadi defin coeffici 1 jth digit 21 tree 0 00 000 0 01 0 1 010 1 11 1 depth purpos definit coeffici enumer binari string use extract bit number 1 tt 12 effici way context vector simpli compar context number first bit correspond cut prefix subtract number context obtain next bit next iter step coeffici label specifi digit context vector respons input tree ij name digit definit recurs architectur construct output input ij respons bit initi context therefor shatter tree appropri choic initi context precis architectur induc map f 01 lead map comput third compon respons bit ij initi context 0 role first context neuron store remain bit initi context recurs comput step context shift multipli 2 drop first bit subtract appropri label tree correspond layer second context neuron comput valu 10 height remain tree cours substitut valu scale version contain rang sgd third context neuron store bit respons obtain output 1 first bit appropri context coincid binari number entri 1 posit respons tree posit indic x 2 f approxim arbitrarili well architectur sigmoid activ function fix number neuron shatter tree combin w architectur obtain architectur shatter w tt tree ow weight proce first simul initi context addit weight ad w architectur describ 14 theorem 11 detail addit summand w ln w obtain describ earlier 2 unfortun lower bound still differ upper bound exponenti term nevertheless interest due follow reason bound linear polynomi case differ compar 2 sigmoid case real upper bound expect order wn lower bound obtain order 2 consequ capac increas tree structur input sigmoid case compar list contrast linear polynomi case activ function bound becom infinit arbitrari input allow perceptron activ function prohibit restrict input list tree node finit alphabet 19 gener one could restrict absolut valu weight input consid fat shatter dimens instead pseudodimens turn use deal svm ensembl network exampl 4 13 27 unfortun even activ function coincid ident function lower bound omegagammaun ln found fat shatter dimens restrict weight input 15 sigmoid activ lower found fat shatter dimens 15 follow theorem gener result larg number activ function theorem 6 activ function oe twice continu differenti nonvanish second deriv neighborhood least one point obtain lower bound fat 01 recurr architectur f 3 comput neuron activ function oe recurs part one linear neuron feedforward part input list entri unari alphabet weight restrict constant depend activ function oe proof function properti x therefor function class f 01shatter set sequenc mutual differ length start longest sequenc set valu 01 04 06 09 respect correspond desir output sequenc one recurs choos invers imag 01 04 06 09 order get output shorter sequenc final appropri initi context sinc even 0 1 entir contain 09 hold continu function differ f 01 oe twice continu differenti nonvanish second deriv neighborhood least one point consequ point x 0 x 1 ffl 0 exist maximum deviat 0007 2 01 consequ gx differ fx 01 input 0 1 henc fg j 20 1g shatter sequenc mutual differ length well g implement fold network without hidden layer 3 context neuron activ function oe recurs part one linear neuron feedforward part context close set depend activ function oe hold g linear combin neuron activ function oe constant ident hold map f 1 appropri ariti vector therefor linear map g integr network structur except initi context chosen compact set weight architectur fix set shatter depend activ function oe 2 henc distribut independ uce properti hold fix fold architectur distribut independ pac learnabl realist condit larg number activ function includ standard sigmoid function fact reli learn algorithm use characterist function class possibl deal input arbitrari size unlimit size use store sens dichotomi input regard argument situat even wors architectur small use differ length input particular train set typic occur time seri predict shatter ie tablelookup possibl input howev shown 14 distribut depend pac learnabl guarante argument last section allow us deriv bound deviat empir error real error learn algorithm corollari 7 denot f fix fold architectur input x x set tree height assum p probabl measur x assum chosen learn algorithm h jd p f hm f valid number exampl chosen specifi theorem 3 bound polynomi 1ffl 1ffi p x order fat ffl512 proof bound follow immedi theorem 3 polynomi 1ffl 1ffi vc pseudo fat shatter dimens polynomi 1ffl 1ffi condit inequ deriv 2 argument lead bound limit probabl p x furthermor bound polynomi probabl larg tree tend 0 suffici fast necessari rate converg depend fold architectur consid substitut prior inform use lucki framework learn concret train sampl deriv bound depend maximum height tree train sampl capac architectur corollari 8 assum f 0 1valu function class tree x p probabl distribut x tree height finit everi tree height x maximum height tree sampl x proof want appli theorem 4 use notat theorem unlucki ness function l 0 x f maxfheight tree xg smooth respect phim l 0 x f ffi tree height l 0 x f jm l 0 x f ffi 2 seen follow number function jfgjx 0 height l 0 x f bound phim l 0 x f ffi ff number bound quantiti length x 0 0 latter probabl equal z u uniform distribut swap permut 2m element event want bound number swap xy first half tree higher fix valu wherea second half least mj tree higher may swap mj indic arbitrarili obvious probabl bound 2 gammamj ffi j lg1ffim choos insert valu inequ obtain lucki framework get bound sm chosen phim lx f ffi ff 2 t1 henc suffici choos least 2 conclus inform theoret learnabl fold architectur examin purpos bound vc pseudo fat shatter dimens play key role learnabl cite improv respect sinc fat shatter dimens infinit even restrict weight input exist bound number exampl guarante valid gener independ special distribut sinc result depend concret learn algorithm dynam principl drawback mani mechan propos learn structur data list tree structur process recurs accord recurs structur data priori unlimit length height input offer possibl use space store desir dichotomi input way upper bound vc pseudodimens given term number paramet network maximum input height propos two approach allow stratif situat via input space output concret learn algorithm concern fold network divis input space set tree restrict height fit first approach allow us deriv bound deviat empir real error learn algorithm probabl distribut probabl high tree restrict priori second approach appli train situat height input tree restrict concret learn exampl allow us deriv bound deviat empir error real error depend concret learn set mean maximum height input tree note approach bound rather conserv yet tri improv constant occur consequ structur risk learn algorithm control fold network method process dynam well real error learn algorithm estim empir error network architectur number pattern addit probabl high tree maximum input height train set known although fact hold learn algorithm algorithm prefer compar other sinc algorithm small empir error gener well need number exampl question aris whether learn algorithm capabl minim empir error effici way algorithm prefer manag task effici fold architectur seem superior raam exampl use classif structur data tri find encod decod appropri classif encod classif mean function class consid deal lraam instead fold network difficult minim task solv furthermor algorithm start prior knowledg avail exampl form automata rule 23 highli probabl find small empir error faster algorithm start scratch start point closer optimum valu first case function class remain initi train process adequ actual train recurr network gradient descend method proven particularli difficult 5 16 hold fold network deal high input tree well make investig altern method learn necessari alreadi mention method start appropri initi network rather scratch 20 23 use appropri modif architectur train algorithm 6 16 algorithm bound number train sampl appli algorithm howev complic abl yield valid gener number exampl independ underli regular r suffici condit polynomi distributiondepend learnabl probabilist analysi learn artifici neural network pac model variant valid gener learn longterm depend gradient descent difficult credit assign time altern backpropag topolog transform hidden recurs model sampl complex learn recurr perceptron map gener framework adapt process data sequenc adapt process sequenc data structur learn taskdepend distribut represent backpropag structur special issu recurr neural network sequenc process approxim learn convex superposit learnabl recurs data gener elman network long shortterm memori polynomi bound vc dimens sigmoid neural network effici distributionfre learn probabilist concept correspond neural fold architectur tree automata induct learn symbol domain use structuredriven neural network neural net architectur tempor sequenc process construct determinist finitest automata recurr neural network recurs distribut represent relat chemic structur activ structur process neural fold architectur experi applic fold architectur guid theorem prove structur risk minim data depend hierarchi label raam theori learn gener root backpropag tr ctr k rahman wang pi yang tommi w chow sitao wu flexibl multilay selforgan map gener process treestructur data pattern recognit v40 n5 p14061424 may 2007 barbara hammer alessio mich alessandro sperduti univers approxim capabl cascad correl structur neural comput v17 n5 p11091159 may 2005 barbara hammer peter tio recurr neural network small weight implement definit memori machin neural comput v15 n8 p18971929 august