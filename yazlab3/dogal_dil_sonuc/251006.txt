accuraci metalearn scalabl data mine paper describ gener approach scale data mine applic come call metalearn metalearn refer gener strategi seek learn combin number separ learn process intellig fashion desir metalearn architectur exhibit two key behavior first metalearn strategi must produc accur final classif system mean metalearn architectur must produc final outcom least accur convent learn algorithm appli avail data second must fast rel individu sequenti learn algorithm appli massiv databas exampl oper reason amount time paper focuss primarili issu relat accuraci efficaci metalearn gener strategi number empir result present demonstr metalearn technic feasibl widearea network comput environ b introduct mani believ pois radic shift way learn work amount new knowledg acquir come age high perform network comput wide avail data highway transform inform age knowledg age provid new opportun defens commerc educ scienc share util inform howev new technolog capabl come along number hard technic problem mani center issu scale perhap obviou massiv amount data inform avail anywher anytim enabl mani new opportun acquir new knowledg field data mine studi precis achiev effici transpar fashion one mean acquir new knowledg databas appli variou machin learn algorithm comput descript represent data well pattern may exhibit data field machin learn made substanti progress year number algorithm popular appli host applic divers field thu may simpli appli current gener learn algorithm larg databas wait respons howev question long might wait inde current gener machin learn algorithm scale task common today includ thousand data item new learn task encompass much two order magnitud data physic distribut furthermor mani exist learn algorithm requir data resid main memori clearli unten mani realist databas certain case data inher distribut local one machin varieti practic reason situat infeas inspect data one process site comput one primari global classifi call problem learn use new knowledg larg inher distribut databas scale problem machin learn approach solv scale problem execut number learn process implement distinct serial program number data subset data reduct techniqu parallel eg network separ process site integr collect result process call metalearn 6 without integr discuss later individu result gener data subset far desir metalearn serv mean glu multipl knowledg sourc togeth note interest gener metalearn approach independ underli learn algorithm may employ furthermor independ comput platform use thu metalearn approach intend scalabl well portabl extens howev may abl guarante accuraci final result good individu learn algorithm appli entir data set sinc consider amount inform may access separ learn process primari issu studi paper 2 relat work relat databas context typic data mine task explain predict valu attribut data given collect tupl known attribut valu exist relat attribut valu drawn domain thu treat train data learn algorithm comput logic express concept descript classifi later use predict valu desir attribut test datum whose desir attribut valu unknown detail approach discuss first summar close relat work other improv accuraci learn algorithm appli larg amount data machin learn research clearli desir accur learn algorithm one recent approach focuss integr mean multipl strategi multipl algorithm research concentr method improv exist algorithm use algorithm gener purpos bias distribut train data notabl work area due schapir 17 schapir prove theoret pac probabilist approxim correct learn model 20 boost techniqu improv weak learner achiev arbitrari high accuraci research propos implement learn system integr fashion number differ algorithm boost overal accuraci basic notion behind integr complement differ underli learn strategi embodi differ learn algorithm effect reduc space incorrect classif learn concept mainli two strategi may consid integr differ learn strategi one strategi increas amount knowledg learn system exampl work report integr induct explanationbas learn 12 explanationbas techniqu integr provid appropri domain knowledg complement induct learn knowledg poor approach requir complic new algorithm implement strategi learn singl system anoth strategi loos integr number differ induct learn algorithm integr collect output concept fashion techniqu describ later evalu empir result mani simpler techniqu aim combin multipl evid singular predict base vote first scheme examin simpl vote base predict differ base classifi final predict chosen classif plural vote variat simpl vote weight vote classifi associ weight determin accur classifi perform valid set valid set set exampl randomli select avail data sinc classifi train one subset exampl subset contribut valid set provid measur predict predict weight classifi assign weight weight classif sum final predict classif weight littleston warmuth 14 propos sever weight major algorithm integr differ classifi work classifi differ predict algorithm necessarili learn train data use calcul weight integr algorithm similar weight vote method describ main differ weight obtain basic algorithm call wm associ learn classifi initi weight exampl train set process classifi final predict exampl gener weight vote final predict wrong weight classifi whose predict incorrect multipli fix discount fi 0 fi 1 decreas contribut final predict variat basic wm algorithm call wml allow weight discount beyond predefin limit xu et al 22 develop method integr predict multipl classifi base bayesian formal belief function deriv equat simplifi belclass classifi x instanc classifi k x classif instanc x predict classifi k final predict class j belclass largest among class experi report estim condit probabl frequenc gener valid set interest approach loos combin learn program learn combin independ learn concept stolfo et al 18 propos learn rule train weight vote scheme merg differ phonem output represent multipl train speech recogn wolpert 21 present theori stack gener combin sever classifi inde work closest mean metalearn describ later sever level classifi first learn train set predict made classifi train set correct classif form train set next level level 1 classifi instanc classifi level classifi first make predict instanc predict present level 1 classifi make final predict zhang et al 23 work util similar approach learn combin base predict made three differ classifi latter idea suggest gener approach may exhibit favor scale characterist discuss later research investig differ characterist success integr multipl classifi ali pazzani 1 empir show classifi fewer uncorrel error reduc error rate integr model krogh vedelsbi 13 prove overal error rate reduc classifi gener highli independ predict next go detail metalearn approach 3 metalearn metalearn loos defin learn metaknowledg learn knowl edg work concentr learn output concept learn system case metalearn mean learn predict classifi common train data thu interest output classifi intern structur strategi learn algorithm selv moreov sever scheme defin train data present learn algorithm initi also avail metalearn certain circumst 31 comput initi base classifi consid two distinct phase metalearn data reduct appli two differ fashion first phase base level classifi comput 5from initi input databas thu initi input databas n j j divid random unbias subset train data roughli size ns subset input learn algorithm execut concurr second phase metalearn number comput base classifi may similarli partit metadata across subset classifi integr smaller group howev may compos distribut metalevel train data purpos bias classif underli base classifi ie filter data accord predict precomput classifi howev sever import consider must concern bia introduc particular distribut form data reduct method exampl data partit class attribut ie target concept induct learn result classifi would specif singl class other may poor strategi least two import reason first scheme import inform distinguish two class avail learn algorithm thu nearmiss counterfactu avail learn algorithm may lead overli gener induct infer descript data put heavier burden metalearn correct mistak base classifi inde mani discrimin base learn algorithm requir neg train exampl comput use result secondli independ subset train data may still larg process effici exampl larg n rel small number class c quantiti nc may larg num ber impli attribut data must particip data reduct scheme distribut comput must concern choos good distribut minim potenti sever bia skew may lead faulti mislead classifi import choos right attribut result impact learn underst random select partit data set uniform distribut class perhap sensibl solut may attempt maintain frequenc distribut class attribut partit repres good smaller model entir train set otherwis total random select strategi may result absenc class wish discrimin among train subset sever experi conduct report explor issu 32 integr base classifi sinc differ learn algorithm employ differ knowledg represent search heurist differ search space may explor henc potenti divers result obtain mitchel 15 refer phenomenon induct bia outcom run algorithm bias toward certain outcom furthermor differ partit data set differ statist characterist perform singl learn algorithm might differ substanti partit observ impli great care must taken design appropri distribut metalearn architectur number issu explor paper precis integr number separ learn classifi bayesian statist theori provid one possibl approach combin sever learn classifi base upon statist behavior classifi train set given set classifi c featur vector x seek comput class label x bay theorem suggest optim strategi follow probabl c predict correctli ie probabl true probabl x class given c cours make sens probabl inde known classifi probabilist categor best estim calcul appropri statist observ behavior classifi train set approxim actual probabl may quit inaccur furthermor bay theorem would optim knew possibl classifi happen comput inform howev provid statist classifi behavior respect train set inform classifi relat exampl learn two classifi rare agre predict class label mean one classifi predict might much predict valu eg combin third classifi mere know two classifi predict equal probabl view pure bayesian approach baselin use method deriv approach bay 9 bayesianbelief 22 compar purpos experi report later mani approach might imagin base upon learn relationship classifi manner learn relationship classifi learn new classifi metalevel classifi whose input set predict two classifi common data latter view call metalearn follow section detail metalearn arbitr combin case varieti induct learn algorithm employ gener appropri metaclassifi strategi treat great detail includ varieti train data distribut gener scheme number import question poorli understood substanti experiment evid suggest direct futur explor ffl metalearn data partit maintain boost accuraci singl global classifi ffl vote bayesian techniqu compar metalearn accuraci ffl arbit compar combin accuraci ffl metalearn classifi may treat base classifi thu might hierarch metalearn classifi perform better singl layer meta learn architectur ffl much train data distribut arbit combin provid order produc accur result substanti number exploratori evalu complet report number paper suggest answer question sever result repeat complet exposit discov experiment three interest behavior exhibit variou metalearn strategi warrant elabor demonstr certain circumst metalearn architectur learn effect fraction total avail inform one site accuraci boost global classifi train avail data maxim parallel effect exploit metalearn disjoint data partit without substanti loss accuraci 8 result suggest strongli field test techniqu real world network comput environ eg databas server site web technic feasibl also import next step develop idea next section present metalearn arbitr combin follow present hierarch metalearn devot consider depth topic demonstr rang issu involv attempt scale machin learn system 4 metalearn arbitr combin distinguish base classifi arbiterscombin follow base classifi outcom appli learn algorithm directli raw train data base classifi program given test datum provid predict unknown class arbit combin detail program gener learn algorithm train predict produc set base classifi raw train data arbitercombin also classifi henc arbit combin comput set predict arbiterscombin 41 arbit strategi arbit 7 learn learn algorithm arbitr among predict gener differ base classifi arbit togeth arbitr rule decid final classif outcom base upon base predict figur 1 instanc final predict arbit arbitr rule arbit predict combin instanc final predict figur 1 arbit combin two classifi depict final predict made predict input two base classifi singl arbit let x instanc whose classif seek c 1 x c 2 x c k x predict classif x k base classifi c 1 c 2 c k ax classif x predict arbit one arbitr rule studi report follow ffl return class plural occurr c 1 x c 2 x c k x ax prefer given arbit choic case tie detail arbit learn train set arbit gener pick exampl valid set e valid set e randomli select avail data prior onset arbit train choic exampl select e dictat select rule purpos bias arbit train data one version select rule studi follow ffl instanc e select none class k base predict gather major classif k2 vote ie majorityc 1 purpos rule choos data sens confus ie major classifi agre data classifi figur 2 provid abstract exampl three base classifi train data three class b c later discuss refer set arbit train data disagr train set form arbit gener learn algorithm use train base classifi togeth arbitr rule learn arbit resolv conflict among classifi necessari class attribut vector exampl base classifi predict classx attrvec 1 x 1 b c c attrvec 3 x 3 c b train set arbit scheme instanc class attribut vector train set classcombin scheme instanc class attribut vector 1 figur 2 sampl train set gener combin arbit strategi 42 combin strategi combin 5 strategi predict learn base classifi train set form basi metalearn train set composit rule vari differ scheme determin content train exampl metalearn exampl metalearn gener meta classifi call combin classifi instanc base classifi first gener predict base composit rule new instanc gener predict classifi combin see figur 1 aim strategi coalesc predict base classifi learn relationship predict correct predict combin comput predict may entir differ propos base classifi wherea arbit choos one predict base classifi arbit experi two scheme composit rule first predic tion c 1 x c 2 x c k x exampl x valid set exam ple e gener k base classifi predict classif use form new set metalevel train instanc use input learn algorithm comput combin manner comput vari defin follow definit classx attribut vectorx denot correct classif attribut vector exampl x specifi valid set e 1 return metalevel train instanc correct classif pre diction ie scheme also use wolpert 21 refer scheme denot classcombin 2 return metalevel train instanc classcombin addit attribut vector ie attribut scheme denot classattribut combin note differ train data arbit comput form distinguish bias subset data select input databas use train base classifi combin howev train predict classif data gener base classifi well data 43 issu sever issu aris metalearn strategi detail follow number size train subset number initi partit train data subset larg depend number processor avail inher distribut data across multipl platform possibl mobil period disconnect total size avail train set complex learn algorithm avail resourc process site natur defin upper bound size subset number subset exce number processor avail processor simul work multipl one serial execut task processor anoth consider desir accuraci wish achiev see experiment result may tradeoff number subset final accuraci metalearn system moreov size subset small suffici data must avail learn process produc effect base classifi initi stage train distribut exampl disjoint replic sinc total random distribut exampl may result absenc one class partit data subset classifi form subset ignor class disagr may occur classifi lead larger arbit train set maintain class distribut subset total avail train set may allevi problem classifi gener subset may closer behavior global classifi produc entir train set train random class distribut addit disjoint data subset promot maximum amount parallel henc desir yet partial replic 8 may mitig problem extrem bia potenti introduc disjoint data strategi inde mani strategi arbitr combin detail impact size train data requir implement effect sever experi run determin rel effect strategi vari type inform bias distribut 12 34 14 classifi train data subset arbit figur 3 sampl arbit tree train data arbit allow see thu far metalearn strategi discuss appli sole singl collect base classifi call onelevel metalearn also studi build hierarch structur recurs fashion ie metalearn arbit combin collect lower level arbit combin hierarch classifi attempt improv predict accuraci may achiev onelevel metalearn classifi 5 hierarch metalearn onelevel metalearn learn techniqu may produc highli accur classifi explor hierarch techniqu appli metalearn strategi recurs 51 arbit tree arbit tree hierarch structur compos arbit comput bottomup binarytre fashion choic binari tree simplifi discuss higher order tree also studi arbit initi learn output pair base classifi recurs arbit learn output two arbit k subset k classifi log 2 gener instanc classifi arbit tree predict flow leav root first leaf classifi produc initi classif test instanc pair predict parent arbit predict anoth predict produc arbitr rule process appli level final predict produc root tree proceed describ build arbit tree detail suppos initi four train data subset process learn algorithm l first four classifi c gener four instanc l appli union subset 1 2 classifi c 1 c 2 gener two set predict p 1 select rule detail earlier gener train set 12 arbit predict p 1 subset u 12 arbit train set 12 algorithm l similarli arbit 34 gener 3 henc firstlevel arbit produc u 14 form union subset 1 4 classifi arbit tree root 12 34 similarli 14 14 root arbit gener arbit tree complet result tree depict figur 3 process gener arbit tree higher order higher order shallow tree becom parallel environ translat faster execut howev logic increas number disagr henc data item select train higher commun overhead level tree due arbitr mani predict singl arbitr site note interest distribut comput environ union set need form one process site rather classifi subset transmit learn classifi site use scan local data set label classifi predict classifi comput object far smaller size train set deriv exampl network comput environ classifi may encapsul agent commun among site experi sever differ arbit strategi besid one describ section 41 entir set result obtain variou strategi report 7 next discuss comput effici variou strategi explor 511 discuss sinc arbit train set construct result arbit two subtre node arbit tree synchron point arbitrari subtre run asynchron commun pair subtre join parent time learn arbit tree proport longest path tree bound path train data reduc complex learn arbit tree size train set arbit purpos restrict larger train set use comput base classifi thu parallel process time level tree rel equal throughout tree howev sever experi restrict allow size train set arbit remov explor two key issu whether higher accuraci could achiev provid inform arbit might number disagr gener henc size train data would natur form select rule notic maximum train set size doubl one move one level tree equal size entir train set root reach obvious desir form train set root larg origin train set inde metalearn case use great expens therefor desir mean control size arbit train set move tree without signific reduct accuraci final result sinc train set select arbit node depend classif result two descend subtre run time configur arbit tree optim compil time size set ie number disagr known base classifi first comput howev may optim configur tree run time clever pair classifi configur result tree depend upon manner classifi arbit pair order level goal devis pair strategi favor smaller train set near root one strategi may consid pair classifi arbit level would produc fewest disagr henc smallest arbit train set denot minsiz anoth possibl strategi pair classifi produc highest number disagr maxsiz first glanc first strategi would seem attract howev disagr classifi resolv bottom tree data commonli classifi surfac near root tree also fewer choic pair classifi control growth train set henc may advantag resolv disagr near leav produc fewer disagr near root may desir pair classifi arbit produc largest set lower tree perhap counterintuit sophist pair scheme might decreas arbit train set size might also increas commun overhead distribut comput environ also creat synchron point level instead node special pair perform compromis strategi might perform pair leaf level indirectli affect subsequ train set level synchron occur node level 52 combin tree way combin tree learn use similar arbit tree combin tree train bottomup combin instead arbit comput nonleaf node combin tree simplifi discuss describ binari combin tree use train experi report later includ higher order tree well classifi instanc leaf classifi produc initi predict pair predict composit rule use gener metalevel instanc classifi parent combin process appli level final predict produc root tree anoth signific departur arbit tree combin tree random set exampl valid set select level learn gener combin tree instead choos set union underli data subset learn commenc random set exampl pick underli subset level combin tree ensur effici process size random train set limit size initi subset use train base classifi base classifi learn leaf level disjoint train data pair base classifi produc predict random train set first level follow composit rule metalevel train set gener predict train exampl combin learn metalevel train set appli learn algorithm process repeat level root combin creat network comput environ classifi may repres remot agent process distribut metalearn process arbit combin tree strategi differ impact effici arbit tree approach implement requir classif possibl entir data set root level signific speed might easili obtain combin tree approach howev alway classifi set data bound size rel small valid set therefor combin tree gener effici arbit tree howev remain seen impact accuraci either scheme may exhibit larg number experi conduct evalu sever metalearn strategi vari particular learn algorithm distribut scheme three learn task result report next 6 experiment result evalu one common techniqu use evalu accuraci learn program crossvalid 2 techniqu entir data set divid train set disjoint test set classifi comput train set evalu test set process repeat n time case use entir differ train test set accuraci n differ classifi measur n differ test set averag final predict accuraci learn algorithm employ learn algorithm learn task evalu cross valid detail follow page experiment result report averag 10fold cross valid run plot repres hundr experiment run variou metalearn strategi intoto also statist signific differ averag measur use onesid ttest 90 confid valu 61 learn algorithm four induct learn algorithm use experi obtain id3 16 cart 2 part ind packag 3 nasa ame research cen algorithm comput decis tree wpebl weight version pebl 10 nearestneighbor learn algorithm bay bayesian classifi base comput condit probabl describ 9 latter two algorithm reimplement c 62 learn task two molecular biolog sequenc analysi data set obtain uci machin learn databas use studi dna splice junction sj data set 19 courtesi towel shavlik ordewi contain 3190 sequenc nucleotid type splice junction center sequenc three class sequenc 60 nucleotid eight differ valu four base one plu four combin protein code region pcr data set 11 courtesi craven shavlik contain 20000 dna nucleotid sequenc binari classif code noncod sequenc 15 nucleotid four differ valu two data set chosen experi repres two differ kind data set one difficult learn pcr 70 easi learn sj 90 although larg data set provid us idea strategi behav practic sinc data set suffici small abl gener base line statist accuraci learn algorithm chosen use studi otherwis use massiv databas would impli unbound resourc time order comput baselin statist note well 4 might take mani year comput furthermor scale studi possibl smaller set simpli vari number size subset form initi data reduct scheme extrapol howev larger data set sought use studi focu work exhaust experi possibl smaller test case state anoth way display use interest result metalearn small test case would much point write paper first place 63 vote metalearn first consid whether metalearn perform well common vote bayesian techniqu report literatur experi vari number equis subset train data 2 64 ensur disjoint proport distribut exampl class size valid accuraci number subset splice junction id3 vote weightedvot wml wmr avgbas accuraci number subset splice junction id3 vote arbit classcombin classattcombiner8595 accuraci number subset splice junction cart vote weightedvot wml wmr avgbas maxbas accuraci number subset splice junction cart vote arbit classcombin classattcombin figur 4 accuraci onelevel integr techniqu splice junction domain set use gener integr structur weightsprobabilitiesarbiterscombin twice size underli train set base classifi predict accuraci separ test set primari comparison measur differ strategi run two data set two learn algorithm result splice junction data set plot figures4 protein code region data set figur 5 figur first row graph depict result differ integr techniqu use id3 second row use cart accuraci global classifi plot one subset mean learn algorithm appli entir train set produc baselin accuraci result comparison averag accuraci base classifi number subset also plot label avgbas way com parison averag accuraci accur base classifi plot maxbas plot accuraci averag 10fold crossvalid run accuraci number subset protein code region id3 vote weightedvot wml wmr avgbas accuraci number subset protein code region id3 vote arbit classcombin classattcombiner6575 accuraci number subset protein code region cart vote weightedvot wml wmr avgbas accuraci number subset protein code region cart vote arbit classcombin classattcombin figur 5 accuraci onelevel integr techniqu protein code region domain experi run splice junction data set indic method sustain drop accuraci number subset increas ie size distinct subset train data decreas either algorithm class combin classattributecombin scheme exhibit higher accuraci techniqu differ statist signific id3 subset size cart subset size 64 subset 45 exampl method sustain significantli 10 accuraci degrad combin method incur around 10 less decreas accuraci weightedmajorityrandom method perform worst significantli wors other protein code region data set arbit scheme maintain sometim exce origin accuraci level techniqu suffer signific drop accuraci 2 subset climb back origin accuraci level number subset increas weightedmajorityrandom method perform much wors other gener method except weightedmajorityrandom scheme consider outperform averag base classifi avgbas gap statist signific furthermor outperform averag accur base classifi except cart splice junction domain random sampl train data definit suffici gener accur classifi two data set studi henc combin techniqu necessari result experi indic metalearn strategi domin weight vote techniqu across domain learner use studi howev metalearn techniqu alway outperform weight vote scheme sj domain combin techniqu favor pcr domain arbit techniqu clear circumst particular metalearn strategi perform better addit studi underway attempt gain understand circumst observ sj domain none scheme maintain baselin accuraci number subset increas techniqu present far character onelevel method perform one level process gener integr structur next consid behavior hierarch metalearn structur 64 arbit tree first examin result bound arbit train set arbit tree differ order binari tree 8ari tree follow result achiev case arbit train set unbound differ pair strategi 641 order arbit tree train set size limit perform experi splice junction protein code region data evalu arbit tree approach vari number subset 2 64 measur predict accuraci disjoint test set plot result figur 6 averag 10fold crossvalid run vari order arbit tree two eight sj data set plot display drop accuraci number subset increas also higher order tree gener less accur lower one howev pcr data set experi accuraci maintain exceed circumst regardless order tree recal tree level size arbit train set fix size data subset use train base classifi relax restrict size data set train arbit might expect improv accuraci expens process time test hypothesi set accuraci number subset splice junction id3 binari 4ari 8ari binari max x2 4ari max x2 8ari max x28595 accuraci number subset splice junction cart binari 4ari 8ari binari max x2 4ari max x2 8ari max x26575 accuraci number subset protein code region id3 binari 4ari 8ari binari max x2 4ari max x2 8ari max x26575 accuraci number subset protein code region cart binari 4ari 8ari binari max x2 4ari max x2 8ari max x2 figur 6 accuraci arbit tree techniqu experi perform doubl maximumtrain set size arbit observ figur 6 doubl arbit train set size origin accuraci roughli maintain binari tree sj domain regardless learner 4ari 8ari tree accuraci result show signific improv howev multilevel arbit tree approach demonstr accuraci improv onelevel techniqu gener maintain accuraci obtain whole data set experi 642 largest arbit train set size classifi pair observ figur 7 restrict size train set arbit lift level accuraci achiev sj data set empir result show singl largest arbit train set exhibit largest arbit train set size number subset id3 sj random dist random dist 2 random dist 2 maxsiz pair random dist 2 minsiz pair uniform dist2060100 largest arbit train set size number subset random dist random dist 2 random dist 2 maxsiz pair random dist 2 minsiz pair uniform dist2060100 largest arbit train set size number subset wpebl sj random dist random dist 2 random dist 2 maxsiz pair random dist 2 minsiz pair uniform dist2060100 largest arbit train set size number subset bay sj random dist random dist 2 random dist 2 maxsiz pair random dist 2 minsiz pair uniform dist figur 7 arbit train set size differ class distribut pair strategi tree 30 entir train set signific result less time memori serial version need reach accuraci arbitr demonstr signific scale properti percentag total train data exhibit tree depend sever factor predict accuraci algorithm given data set distribut data subset pair learn classifi arbit level mention section 43 pair classifi arbit affect arbit train set size sever experi perform two pair strategi maxsiz minsiz appli leaf level result display figur 7 experi conduct sj data set initi random class distribut uniform class distribut second random class distribut use train base classifi second random distribut compos way ensur half learn arbit tree ignor one class case initi random distribut differ pair strategi use uniform distribut second random distribut shown figur 7 uniform distribut achiev smaller train set two random distribut largest train set size case approxim 10 total avail data number subset larger eight except bay 64 subset bay seem abl gather enough statist small subset note number subset eight fewer train set leaf classifi larger 10 origin data set becom largest arbit tree two pair strategi affect size uniform distribut shown figur one possibl explan uniform distribut produc smallest train set possibl pair strategi matter howev maxsiz pair strategi gener reduc size train subset second random distribut minsiz pair strategi hand affect sometim even increas size gener subset summari uniform class distribut tend produc smallest train set maxsiz pair strategi reduc size subset random class distribut 65 combin tree consid accuraci combin tree experi vari number equis subset train data 2 64 ensur disjoint proport distribut exampl class also vari order combin tree two eight result experi combin tree two differ train strategi display figur 8 9 baselin accuraci compar evalu plot one subset mean learn algorithm appli entir train set intoto produc global classifi plot deriv averag 10fold crossvalid run result classcombin tree strategi display figur 8 show drop accuraci data set case compar global classifi number subset increas drop vari 3 15 percentag decreas amount data train subset far larger binari combin tree seem less accur higher order tree case might due lack inform find correl among two set predict experi arbit tree doubl size metalevel train set statist signific improv observ sj data set cart learner anoth experi use classattributecombin tree strategi figur 9 suggest binari tree appear maintain accuraci global classifi except splice junction data set cart learner higherord tree gener less accur note interest doubl size train set combin improv accuraci significantli protein code region data set accu accuraci number subset splice junction id3classcombin binari 4ari 8ari binari max x2 4ari max x2 8ari max x28595 accuraci number subset splice junction cartclasscombin binari 4ari 8ari binari max x2 4ari max x2 8ari max x26575 accuraci number subset protein code region id3classcombin binari 4ari 8ari binari max x2 4ari max x2 8ari max x2 accuraci number subset protein code region cartclasscombin binari 4ari 8ari binari max x2 4ari max x2 8ari max x2 figur 8 accuraci classcombin tree techniqu raci binari tree consist higher global classifi ie metalearn strategi demonstr mean boost accuraci singl classifi train entir data set improv statist signific particularli interest find sinc inform loss due data partit recov combin tree thu scheme demonstr mean integr collect knowledg distribut among individu base classifi summari experiment result suggest increas size metalevel train set improv accuraci learn tree like result simpl observ data avail train lead better inform correl among base classifi experiment data convincingli demonstr doubl train set size metalevel partit rel underli subset use train base classifi suffici accuraci number subset splice junction id3classattributecombin binari 4ari 8ari binari max x2 4ari max x2 8ari max x28595 accuraci number subset splice junction cartclassattributecombin binari 4ari 8ari binari max x2 4ari max x2 8ari max x26575 accuraci number subset protein code region id3classattributecombin binari 4ari 8ari binari max x2 4ari max x2 8ari max x2 accuraci number subset protein code region cartclassattributecombin binari 4ari 8ari binari max x2 4ari max x2 8ari max x2 figur 9 accuraci classattributecombin tree techniqu maintain level accuraci global classifi inde may boost accuraci well 7 conclud remark way summari demonstr sever interest behavior variou metalearn architectur studi date result base mani empir experi use differ combin small number data set learn algorithm ffl metalearn strategi show consist improv classif accuraci base classifi train subset avail train data studi show classifi train individu random subset larg data set accur integr collect separ learn classifi ffl metalearn strategi outperform common onelevel votingbas bayesian techniqu learn task domain studi onelevel metalearn scheme consist maintain high accuraci number subset increas amount avail data thu decreas howev result show hierarch metalearn approach abl sustain level accuraci global classifi train entir data set distribut among number site ffl arbit tree strategi allow unbound metalevel train set determin varieti algorithm employ 30 certain case 10 entir train data requir one process site maintain equival predict accuraci singl global classifi comput avail data word arbit tree strategi site process larger learn task least 3 time domain studi without increas memori resourc ffl unbound metalevel train set necessari achiev good result limit metalevel train set size twice size data subset use comput base classifi usual yield system abl maintain level accuraci achiev global classifi import complex perspect ffl combin arbit tree lower order perform better one higher order seem mainli attribut increas number opportun correct base classifi sinc level lower order tree filter compos good train data ffl final combin tree strategi demonstr consist boost predict accuraci global classifi certain circumst suggest properli configur metalearn strategi combin multipl knowledg sourc provid accur view avail data one learn algorithm alon achiev believ concept embodi term metalearn propos provid import first step understand develop system learn massiv wide dispers databas scale metalearn architectur may provid mean use larg number low cost network comput collect learn massiv databas use import new knowl edg would otherwis prohibit expens cost time achiev believ metalearn system import contribut technolog futur infrastructur envis intellig integr inform system realiz acknowledg work perform columbia univers partial support grant nsf iri9413847 cda9024735 new york state scienc technolog foundat citicorp thank david wolpert mani use discuss work r reduct learn multipl descript introduct ind recurs partit test flight experi multistrategi learn metalearn toward parallel distribut learn metalearn scale learn metalearn disjoint partial replic data cn2 induct algorithm weight nearest neighbor algorithm learn symbol featur learn repres codon challeng problem construct induct studi explanationbas mehtod induct learn neural network ensembl weight major algorithm need bias learn generalizaion induct decis tree strength weak learnabl speech recognit parallel refin approxim domain theori knowledgebas neural network theori learnabl stack gener method combin multipl classifir applic handwrit recognit hybrid system protein secondari structur predict tr theori learnabl strength weak learnabl stack gener weight nearest neighbor algorithm learn symbol featur experi multistrategi learn metalearn reduct learn multipl descript studi explanationbas method induct learn cn2 induct algorithm induct decis tree weight major algorithm supersed 8916 ctr rueyhsia li geneva g belford instabl decis tree classif algorithm proceed eighth acm sigkdd intern confer knowledg discoveri data mine juli 2326 2002 edmonton alberta canada vincent cho beat wthrich distribut mine classif rule knowledg inform system v4 n1 p130 januari 2002 aleksandar lazarev zoran obradov boost algorithm parallel distribut learn distribut parallel databas v11 n2 p203229 march 2002 philip k chan salvator j stolfo david wolpert guest editor introduct machin learn v36 n12 p57 julyaugust 1999 leo breiman past small vote classif larg databas onlin machin learn v36 n12 p85103 julyaugust 1999 ljupo todorovski sao deroski combin classifi meta decis tree machin learn v50 n3 p223249 march jaideep vaidya chri clifton privaci preserv associ rule mine vertic partit data proceed eighth acm sigkdd intern confer knowledg discoveri data mine juli 2326 2002 edmonton alberta canada w nick street yongseog kim stream ensembl algorithm sea largescal classif proceed seventh acm sigkdd intern confer knowledg discoveri data mine p377382 august 2629 2001 san francisco california puuronen alexey tsymbal local featur select dynam integr classifi fundamenta informatica v47 n12 p91117 januari 2001 guido heumer heni ben amor bernhard jung grasp recognit uncalibr data glove machin learn approach presenc teleoper virtual environ v17 n2 p121142 april 2008 christoph giraudcarri ricardo vilalta pavel brazdil introduct special issu metalearn machin learn v54 n3 p187193 march 2004 jaideep vaidya chri clifton secur set intersect cardin applic associ rule mine journal comput secur v13 n4 p593622 juli 2005 chihfong tsai ken mcgarri john tait clair modular support vector imag index classif system acm transact inform system toi v24 n3 p353379 juli 2006 ron kohavi llew mason rajesh parekh zijian zheng lesson challeng mine retail ecommerc data machin learn v57 n12 p83113 octobernovemb 2004 foster provost venkateswarlu kolluri data mine task method scalabl handbook data mine knowledg discoveri oxford univers press inc new york ny 2002 yifeng zhang siddhartha bhattacharyya genet program classifi largescal data ensembl method inform scienc intern journal v163 n13 p85101 14 june 2004 perspect view survey metalearn artifici intellig review v18 n2 p7795 octob 2002 foster provost venkateswarlu kolluri survey method scale induct algorithm data mine knowledg discoveri v3 n2 p131169 june 1999