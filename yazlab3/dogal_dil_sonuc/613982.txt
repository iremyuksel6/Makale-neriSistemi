assess perform new ibm sp2 commun subsystem ibm recent launch upgrad commun subsystem sp2 parallel comput chang affect hardwar softwar element highperform switch messag interfac adapt new implement mpi messagepass librari character extent chang affect execut time parallel applic author run collect benchmark sp2 old commun subsystem machin upgrad benchmark includ pointtopoint collect commun test well set complet parallel applic perform indic latenc throughput exhibit basic commun test execut time case real applic result indic certain circumst signific perform increas result b introduct long time pass sinc highperform comput commun realiz highest comput speed reason cost reach via massiv parallel eighti earli nineti sort build euphoria led work done r beivid ja gregorio depart electr comput engin univers california irvin visit research design use massiv parallel processor mpp thousand even ten thousand process element howev nowaday commun seem think hundr processor repres upper limit feasibl parallel comput system addit mainli due lack stabil supercomput sector constructor make rel conserv decis regard hardwar softwar element mpp way guarante system surviv market reason period ibm sp2 parallel comput built around workstationbas hardwar softwar repres one success approach mpp seem capabl surviv difficult time hp96 current mpp includ sp2 shown suitabl run coars grain parallel applic interchang larg data structur mani case system use simpli increas throughput sequenti job multipl user share machin howev big challeng sp2 similar machin distribut memori parallel comput effici run communicationdemand medium fine grain parallel applic reduc execut time nowaday follow principl allow product portabl softwar mpp program use convent imper languag enhanc commun librari pvm mpi implement messag pass synchron among process gei94 mpi94 environ effici parallel applic maxim workload evenli distribut among processor overhead introduc parallel process minim cost commun synchron oper must kept low possibl order achiev interconnect subsystem use support interchang messag must fast enough avoid becom bottleneck tradit latenc throughput two paramet use indic perform interconnect network mpp throughput increas one gener mpp next signific reduct latenc seem tougher problem design industri unfortun perform parallel applic sensit latenc increas throughput help process long messag help significantli interchang reasonablys messag ten thousand bytesi sever order magnitud smaller requir reach maximum achiev throughput current mpp current attempt measur character differ compon latenc modern mpp lead conclus interconnect network router wire overdimens compar node abil send receiv messag word largest compon latenc network network interfac commun softwar caus messag pass overhead two element io hardwar softwar must greatli improv reduc messag latenc mpp thu achiev better perform run parallel applic import issu highlight sever research develop project includ cul96 myr96 recent ibm develop new version highperform switch constitut interconnect network sp2 upgrad affect switch also io adapt connect processor switch ibm upgrad oper system aix 3 aix 4 latter includ new version mpi special tailor sp2 aix 3 mpi avail addit layer softwar mpl nativ ibm messag pass librari chang aim increas parallel applic perform paper report experiment data obtain run collect benchmark applic sp2 first old new commun subsystem inform use make perform comparison two sp2 comput whose differ commun subsystem dont tri report exhaust evalu new subsystem provid exact character latenc componentsthat done futur paper howev import allow commun know preliminari experiment data especi search machin run parallel applic design new mpp redesign exist one paper structur follow next section brief introduct ibm sp2 parallel comput made includ limit inform avail recent chang commun subsystem describ collect program use test machin section 3 result run program analyz section 4 paper end conclus section 5 acknowledg section 6 reader find refer access path parallel code use elabor paper overview ibm sp2 sp2 distribut memori parallel comput processor node interconnect commun subsystem ibm offer sever altern subsystem possibl provid commun among processor use standard network technolog ethernet fddi atm howev highend sp2 system ibm offer highperform switch provid better characterist parallel comput figur 1 show highlevel system structur sp2 age95 micro channel control switch adapt system io bu memori processor highperform switch adapt detail node node figur 1 sp2 system ibm also offer sever altern node access sp2 c4 see acknowledg section compos node power 66 mhz processor attach 256 mb memori mean 128bit wide bu node 32 kb instruct cach 128 kb data cach 2 mb l2 cach micro channel control govern io bu connect processormemori subset devic disk ethernet network highperform switch mean appropri adapt sp2 commun subsystem compos highperform switch plu adapt connect node switch element need care design allow cooper without introduc bottleneck adapt contain onboard microprocessor offload work pass messag node node memori provid buffer space dma engin use move inform node adapt memori switch link highperform switch describ ibm anytoani packetswitch multistag indirect network similar omega network advantag network bisect bandwidth increas linearli size system contrast direct network ring mesh tori guarante system scalabl core network crossbar chip offer 8 bidirect port use build small sp2 system larger system board compos two stage 4 chip total bidirect port use system alway least one stage necessari order provid redund path least 4 pair node reader find age95 stu95 sketch configur system 16 48 64 128 node frame build block sp2 system frame contain switch board node sp2 avail c4 two frame 16 node one frame use run sequenti batch job second one avail run parallel program one use experi report paper origin system commun subsystem follow characterist adapt built around intel i860 processor 8 mb ram theoret peak transfer abil adapt 80 mb although achiev maximum 52 mb due overhead associ access manag micro channel link highperform switch provid 40 mb peak bandwidth direct 80 mb bidirect nodetonod latenc 05 system 80 node recent system upgrad inform new system avail write report limit abl confirm new adapt built around powerpc 601 processor allow doubl peak transfer bandwidthreach 160 mb new switch offer 300 mb peak bidirect bandwidth latenc less 12 system 80 node gar96 regard softwar environ sp2 node run full version aix ibm version unix includ unix featur plu specif tool librari program execut parallel program first test c4 sp2 carri version 3 aix includ mpl ibmdesign librari parallel program use messagepass paradigm sni95 mpl actual interfac implement sever commun subsystem particular run softwar layer design make best use highperform switch altern mpl might also run ip thu almost commun media includ switch environ mpi librari mpich see bri95 also avail addit softwar layer mpl introduct aix version 4 ibm decid adopt mpi nativ languag program sp2 system replac nonstandard mpl way applic program mpi could run less overhead compar previou version introduct new switch accompani new version mpi implement although aix remain without signific chang order take advantag characterist new hardwar mpl still avail eas migrat ensur usabl old code mpi mpl use fortran 77 c program paper report result obtain run experi bechmark describ section 3 follow three configur c4 sp2 tabl 1 commun subsystem softwar old switch adapt aix v3 mpich implement mpi top mpl old switch adapt aix v4 nativ version mpi nv4 new switch adapt aix v4 nativ version mpi tabl 1 sp2 configur test 3 experi benchmark order make fastbut faircomparison two version sp2 select ran small collect test program alreadi use research other prepar us select program aim perform progress evalu machin start simpl point point commun go collect commun numer kernel finegrain nonnumer applic measur obtain experi provid inform latenc throughput commun channel complet network inform use firstord indic predict chang commun subsystem affect execut time parallel applic next subsect select benchmark briefli describ 31 point point commun first group test base code provid dongarra character point point commun use mpi dd95 two processor 0 1 engag sort ping pong processor 0 charg measur processor read valu walltim clock invok mpisend oper block mpirecv meanwhil processor 1 perform symmetr oper latter oper finish processor 0 clock read thu delay twomessag interchang one direct measur latenc comput one half time achiev throughput also comput consid latenc messag size oper done time avoid warmup effect anoth 1000 time averag result messag size provid input paramet program measur minimum maximum averag latenc throughput consid averag valu repres perform user obtain machin author xh96 consid minimum valu latenc suppos free influenc oper system user case minimum averag valu close case 32 collect commun sever mpi collect oper object detail measur similar taken point point commun pay special attent two broadcast reduct experi perform other exhibit similar behavior addit random traffic test perform experi sever messag might compet network resourc well access common destin next describ detail test broadcast test involv processor perform broadcast processor root oper includ test perform 1000 iter averag valu iter processor perform broadcast mpibcast processor 0 design root barriersynchron mpibarri comput broadcast delay root measur time moment broadcast start time barrier finish subtract precomput barrier delay obtain throughput figur indic amount inform receiv particip reduct test like previou one use mpireduc instead mpibcast case throughput figur consid amount inform sent particip mpi offer collect predefin oper mpimax mpimin mpisum etc perform reduct also allow user defin new oper test userdefin null oper perform last test group random2 set separ avail processor two group even rank odd rank even processor send data anyon odd processor randomli chosen respond immedi delay twoway interchang achiev throughput comput like point point case 33 parallel applic order assess behavior sp2 run complet applic select four benchmark mg lu sp bt na parallel benchmark npb version 2 bai95 shallow water model code swm parkbench suit benchmark wf95 parallel simul ps develop group mig95 next briefli describ program version 2 npb obtain sourc code form contrast version 1 written fortran 77 plu mpi program compil execut without chang sourc code benchmark oper sever input problem size number grid point npb specifi three class b c depend problem size experi use class b ie mediums problem mg use multigrid method comput solut threedimension scalar poisson equat lu simul comput fluid dynam applic use symmetr success overrelax solv block lower triangularblock upper triangular system equat result unfactor implicit finitediffer discret navierstok equat three dimens sp bt simul comput fluid dynam applic solv system equat result approxim factor implicit finitediffer discret navierstok equat bt solv blocktriangular system 5x5 block sp solv scalar pentadiagon system result full diagon approxim factor scheme bai95 swm parallel algorithm testb solv nonlinear shallow water equat rotat sphere use spectral transform method program use messagepass paradigm fortran 77 plu mpi form part parkbench benchmark suit develop patrick h worley oak ridg nation laboratori ian foster argonn nation laboratori benchmark includ sever input file select problem size algorithm use use mediums problem requir approxim 1 gb workspac run 64 bit precis 1000 gflop default parallel algorithm distribut fourier transform distribut legendr transform ps parallel discreteev simul develop group evalu characterist messag pass network 2d toru topolog cutthrough flow control paramet simul basic problem size term number switch element load network term percentag maximum theoret bandwidth network bisect number time step appropri makefil distribut code simul result report paper toru 32x32 router simul 40000 cycl load vari 5 90 mean number messag need perform simul vari 25 million nearli 9 million process organ logic 4x4 toru process alway commun four logic neighbor commun follow particular tempor pattern messag short byte code written c plu mpi perform evalu latenc throughput basic paramet character applic view perform commun subsystem conjunct determin adequaci given system execut given type parallel applic first messag latenc impos restrict granular applic second throughput impos limit maximum amount inform process interchang per unit time reason follow section analyz result describ benchmark term two paramet order show upgrad sp2 commun subsystem affect preliminari attempt evalu perform improv achiev new sp2 commun subsystem intend perform exhaust analysi everi mpi function result discuss minimum kernel aim offer first overview potenti perform chang applic might experi 41 point point commun first approach assess commun perform parallel comput measur minimum time requir send messag two process locat differ node reason mani perform evalu studi concentr point point commun order establish initi comparison point among differ platform cul96 dd95 hoc94 xh96 tabl 2 figur 2 show result obtain run point point test describ previou section observ tabl introduct new version mpi impli reduct softwar overhead consequ startup time lower latenc notic reduc messag short howev messag 4 kb new version perform wors explan phenomenon found fra95 deal later section maximum achiev throughput remain old new mpi implement introduct new switch clearli bring reduct latenc length rang latenc reduct special signific medium long messag thu allow achiev higher throughput latenc throughput mb byte ov3 ov4 nv4 ov3 ov4 nv4 128 6454 5054 4958 198 253 258 512 9559 7816 6195 536 655 827 tabl 2 averag valu latenc throughput point point commun l messag length configur ov3 ov4 nv4 describ tabl latenc messag thov3 thov4 thnv4 throughput mb messag length figur 2 latenc throughput point point commun data tabl 2 preliminari analysi result easili establish character latenc gener latenc messag length l decompos two compon startup time h time use messag header reach destin spool time time requir transmit remaind messag path origin destin alreadi establish spool time usual model linear function l thu tl express repres time requir transmit byte path establish paramet interest throughput defin gener asymptot behavior function l follow l thmax maximum asymptot throughput achiev commun subsystem equat 3 4 see thmax calcul 1t b latenc tabl 1 fit equat 2 obtain maximum throughput unidirect point point commun mb system behavior short messag long messag see clearli detail set experi carri new switch result shown figur 3 case minimum latenc valu repres averag valu noisi produc clear graph curv fit done use least squar latenc messag length kb103050700 throughput mb messag length kb figur 3 minimum latenc maximum throughput new switch messag smaller 192 kb detail analysi data indic exist three differ region defin messag length see figur 4 one 3 region fit equat 2 differ paramet latenc messag length kb region 1200400600800 latenc messag length kb region 25001500250032 64 96 128 160 192 latenc messag length kb region 3 figur 4 messag latenc three differ messag length region interest observ h smaller region correspond smaller messag b smaller region correspond longer messag mean system optim minim latenc short messag maxim throughput long messag mention refer fra95 explain chang region 1 region 2 perfectli short messag eager protocol use ie messag sent destin immedi longer messag rendezv protocol use ie messag sent receiv node agre receiv switch rendezv protocol incur higher startup cost reduc number time inform copi thu reduc cost per byte chang protocol come ibm nativ version mpi discuss also valid experi ov4 third region behavior latenc linear region 1 2 figur show clear sawedg effect jump everi 16 kb moment without inform manufactur unabl offer reason hypothesi explain behavior set equat easi see maximum throughput achiev region 3 long messag comput paramet b equat 7 minimum latenc obtain region 1 short messag domin paramet h equat 5 go back comparison among sever sp2 configur consid exampl data tabl 1 byte quit short messag seen latenc basic old new switch contrast long messag eg latenc notic reduc throughput increas short effect commun subsystem upgrad hn ho thmax n 24 thmax therefor upgrad basic effect spool time reduc less one half origin howev startup time use messag header reach destin almost summar channel throughput doubl startup time remain result first approach tell us coars grain parallel applic requir interchang larg data structur experi signific perform improv improv also notic oper system level use system increas throughput batch taskswhen node run decoupl fashion contrast applic requir frequent interchang short messag experi signific reduct execut time upgrad fact happen messag size requir obtain reason perform increas chang hockney defin l 12 length messag allow util one half maximum channel bandwidth hoc94 old subsystem l 12 aprox 3000 byte see tabl 2 increas reach around 32 kb see figur 3 therefor minimum messag size requir effici run parallel applic increas order magnitud goal increas perform sp2 run applic requir frequent interchang short messag imper reduc startup time proport even maximum throughput increas tabl 3 summar paramet h thmax l 12 sever wellknown mpp includ three consid sp2 configur machin h thmax mb convex spp1000 pvm 76 11 1000 convex spp1200 pvm 63 15 1000 cray t3d pvm 21 27 1502 intel paragon 29 154 7236 meiko cs2 83 43 3559 sp2ov4 44 35 3000 tabl 3 latenc asymptot throughput parallel comput data convex cray paragon ksr1 meiko taken dd95 42 collect commun measur characterist repres commun pattern involv two processor sever mpi collect oper random traffic two set regard broadcast collect oper experi offer us inform summar figur 5 seen behavior much like point point case chang softwar mean chang behavior latenc curv use nativ mpi alway reduc latenc regard hardwar upgrad graph show signific improvementalthough signific messag longer 4 kb even larg enough messag latenc reduct complet broadcast oper spectacular point point case therefor throughput increas impress perform remain collect oper includ reduct describ previou section measur appli strategi use broadcast show similar behavior therefor comment applicableso repeat latenc messag thov3 thov4 thnv4 throughput mb messag length figur 5 latenc throughput broadcast oper result obtain run experi random commun two set 8 processor plot figur 6 abovement broadcast case signific latenc reduct achiev messag size 4 kb2000600010000 latenc messag thov3 thov4 thnv4 throughput mb messag length figur 6 latenc throughput random2 set test 43 parallel applic comment section 3 execut time 6 benchmark applic measur order get insight chang commun subsystem may affect real parallel applic combin comput commun data analyz previou section reader infer reduct execut time mark case parallel applic requir process interchang long messages5001500mg lu sp bt swm timeold timenew execut time benchmark2006001000 mg lu sp bt swm mflopsold mflopsnew mflop benchmark figur 7 result npb swm benchmark graph figur 7 show execut time second perform mflop bt lu mg sp swm mention nativ version mpi use test therefor basic chang hardwar affect perform result expect improv quit modest case thing better parallel simul perform increas measur figur 8 show execut time vari load simul model indic number messag applic manag refer point comparison execut time experi 16 node intel paragon 25 time achiev sp250150250350 old execut time load figur 8 execut time parallel simul summar section state chang commun subsystem effect run parallel applic design abl achiev import reduct startup time ie latenc short medium messag conclus aim write paper offer preliminari evalu effect upgrad introduc ibm commun subsystem sp2 perform parallel applic clear dont pretend analysi upgrad system comprehensiveit mainli offer snapshot behavior collect experi perform use sp2 frame run benchmark measur perform specif commun oper plu other repres typic parallel applic purpos experi analyz progress effect upgrad overal system perform point point commun collect oper kernel numer applic na parallel benchmark swm nonnumer applic fine grain characterist analyz obtain measur conclud import improv achiev bandwidth commun channel allow applic reach throughput valu much higher achiev old networkalthough possibl applic interchang long enough messag 10 kb 1 mb absolut term point point commun asymptot throughput new commun subsystem doubl previou one contrast messag short improv bandwidth translat better perform startup time reduc word latenc short medium messag chang significantli old new system consequ applic requir frequent interchang massiv amount inform experi clear reduct execut time result shown report confirm conclus research studi real bottleneck mpp commun lie messagepass softwar messag interfac attach node interconnect network therefor point mpp design focu interest opinion 1 overhead messag pass softwar minim even mean introduc chang architectur node processor 2 messag interfac locat close processor possibl connect system bu even bu offchip cach processor acknowledg want express grate acknowledg follow institut c4 centr de computaci comunicacion de catalunya provid access machin test technic support cicyt research done support comisin interministeri de ciencia tecnologa spain contract tic950378 dgicyt direccin gener de investigacin cientfica tcnica grant allow ja gregorio stay uci visit associ research depart electr comput engin univers california irvin provid support equip access r sp2 system architectur na parallel benchmark 20 user guid mpich assess fast network interfac mpi program environ ibm sp1sp2 ibm power parallel divis pvm user guid tutori network parallel comput commun challeng mpp intel paragon meiko cs2 comput architectur empir evalu techniqu parallel simul messag pass network messag pass interfac forum myricom myrinet inform commun softwar parallel environ ibm sp2 sp2 highperform switch pstswm v4 model commun overhead mpi mpl perform ibm sp2 tr ctr sangman moh chansu yu ben lee hee young youn dongsoo han dongman lee fourari treebas barrier synchron 2d mesh without nonmemb involv ieee transact comput v50 n8 p811823 august 2001 jess labarta sensit perform predict messag pass program journal supercomput v17 n3 p291298 nov 2000 zoltan johasz analyt method predict perform parallel imag process oper journal supercomput v12 n12 p157174 janfeb 1998 j gregorio r beivid f vallejo model interconnect subsystem massiv parallel comput perform evalu v47 n2 p105129 februari 2002 manuel prieto ignacio llorent francisco tirado data local exploit decomposit regular domain problem ieee transact parallel distribut system v11 n11 p11411150 novemb 2000