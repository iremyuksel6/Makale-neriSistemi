effici spars lu factor partial pivot distribut memori architectur abstracta spars lu factor base gaussian elimin partial pivot gepp import mani scientif applic still open problem develop high perform gepp code distribut memori machin main difficulti partial pivot oper dynam chang comput nonzero fillin structur elimin process paper present approach call parallel problem distribut memori machin approach adopt static symbol factor avoid runtim control overhead incorpor 2d lu supernod partit amalgam strategi improv cach perform exploit irregular task parallel embed spars lu use asynchron comput schedul paper discuss compar algorithm use 1d 2d data map scheme present experiment studi crayt3d t3e perform result set nonsymmetr benchmark matric encourag achiev 6878 gflop 128 t3e node best knowledg highest perform ever achiev challeng problem previou record 2583 gflop share memori machin 8 b current comput scienc depart univers illinoi urbanachampaign pivot oper interchang row base numer valu matrix element elimin process imposs predict precis structur l u factor without actual perform numer factor adapt irregular natur spars lu data structur make effici implement algorithm hard even modern sequenti machin memori hierarchi sever approach use solv nonsymmetr system one approach unsymmetricpattern multifront method 5 25 use elimin graph model irregular parallel guid parallel comput anoth approach 19 restructur spars matrix border block upper triangular form use special pivot techniqu preserv structur maintain numer stabil accept level method implement illinoi cedar multiprocessor base aliant share memori cluster paper focus parallel issu given column order row interchang maintain numer stabil parallel spars lu partial pivot also studi 21 share memori machin use static symbol lu factor overestim nonzero fillin avoid dynam variat lu data structur approach lead good speedup 6 processor sequent machin work need assess perform sequenti code far know publish result parallel spars lu popular commerci distribut memori machin crayt3dt3 intel paragon ibm sp2 tmc cm5 meiko cs2 one difficulti parallel spars lu machin util sophist uniprocessor architectur design sequenti algorithm must take advantag cach make previous propos techniqu less effect hand parallel implement must util fast commun mechan avail chine easi get speedup compar parallel code sequenti code fulli exploit uniprocessor capabl easi parallel highli optim sequenti code one sequenti code superlu 7 use supernod approach conduct sequenti spars lu partial pivot supernod partit make possibl perform numer updat use blas2 level dens matrixvector multipl therefor better exploit memori hierarchi superlu perform symbol factor gener supernod fli factor proce umfpack anoth competit sequenti code problem neither superlu umfpack alway better 3 4 7 ma41 code spars matric symmetr pattern regard high qualiti deliv excel megaflop perform paper focu perform analysi comparison superlu code sinc structur code closer superlu paper present approach call consid follow key strategi togeth parallel spars lu algorithm 1 adopt static symbol factor scheme elimin data structur variat caus dynam pivot 2 data regular spars structur obtain symbol factor scheme effici dens oper use perform comput impact nonzero fillin overestim overal elimin time minim 3 develop schedul techniqu exploit maximum irregular parallel reduc memori requir solv larg problem observ current commod processor memori hierarchi highli optim blas3 subroutin usual outperform blas2 subroutin implement numer oper 6 9 afford introduc extra blas3 oper redesign lu algorithm new algorithm easi parallel sequenti perform code still competit current best sequenti code use static symbol factor techniqu first propos 20 21 predict worst possibl structur l u factor without know actual numer valu develop 2d lu supernod partit techniqu identifi dens structur l u factor maxim use blas3 level subroutin dens structur also incorpor supernod amalgam 1 10 techniqu increas granular comput exploit irregular parallel redesign spars lu algorithm experi two map method one use 1d data map use 2d data map one advantag use 1d data map correspond lu algorithm easili model direct acycl task graph dag graph schedul techniqu effici runtim support avail schedul execut dag parallel 15 16 schedul execut dag parallel difficult job parallel spars problem irregular execut must asynchron import optim overlap comput commun balanc processor load elimin unnecessari commun overhead graph schedul excel job exploit irregular parallel lead extra memori space per node achiev best perform also 1d data map expos limit parallel due restrict also examin 2d data map method asynchron execut scheme exploit parallel memori constraint implement spars lu algorithm conduct experi set nonsymmetr benchmark matric crayt3d t3e experi show approach quit effect deliv good perform term high megaflop number particular 1d code outperform current 2d code processor suffici memori 2d code potenti solv larger problem produc higher megaflop number rest paper organ follow section 2 give problem definit section 3 describ structur predict 2d lu supernod partit spars lu factor section 4 describ program partit data map scheme section 5 address asynchron comput schedul execut section 6 present experiment result section 7 conclud paper find ja mk 03 singular stop row k row ik 6 0 kj 6 0 ik 6 0 figur 1 spars gaussian elimin partial pivot lu factor preliminari figur show nonsingular matrix factor two matric l u use gepp elimin step control loop index k element manipul step k use row index j column index convent use rest paper step elimin process row interchang may need maintain numer stabil result lu factor process express l unit lower triangular matrix u upper triangular matrix p permut matrix contain row interchang inform solut linear system solv two triangular solver triangular solver much less time consum gaussian elimin process cach behavior play import role achiev good perform scientif comput better exploit memori hierarchi modern architectur supernod partit import techniqu exploit regular spars matrix comput util bla routin speed comput success appli choleski factor 26 30 31 difficulti nonsymmetr factor supernod structur depend pivot choic factor thu determin advanc superlu perform symbol factor identifi supernod fli also maxim use bla level oper improv cach perform spars lu howev challeng parallel superlu distribut memori machin use precis pivot inform elimin step certainli optim data space usag reduc commun improv load balanc benefit could offset high runtim control commun overhead strategi static data structur predict 20 valuabl avoid dynam symbol factor identifi maximum data depend pattern minim dynam control overhead use static strategi approach overestim introduc extra fillin lead substanti amount unnecessari oper numer factor observ superlu 7 dgemv routin blas2 level dens matrix vector multipl account 78 98 float point oper exclud symbol factor part also fact blas3 routin dgemm matrixmatrix multipl usual much faster blas1 blas2 routin 6 crayt3d matrix size theta 25 dgemm achiev 103 mflop dgemv reach 85 mflop thu key idea approach could find way maxim use dgemm use static symbol factor even overestim nonzero extra numer oper overal code perform could still competit superlu mainli use dgemv 3 storag predict dens structur identif 31 storag predict purpos symbol factor obtain structur l u factor sinc pivot sequenc known numer factor way alloc enough storag space fillin gener numer factor phase overestim given spars matrix zerofre diagon simpl solut use choleski factor l c shown structur l c use upper bound structur l u factor regardless choic pivot row step 20 turn bound tight often substanti overestim structur l u factor refer tabl 1 instead consid anoth method 20 basic idea static consid possibl pivot choic step space alloc possibl nonzero would introduc pivot sequenc could occur numer factor summar symbol factor method briefli follow nonzero structur row defin set column indic nonzero fillin present given n theta n matrix sinc nonzero pattern row chang factor proce use r k denot structur row step k factor k denot structur matrix step k k ij denot element ij k notic structur row whole matrix cover structur l u factor addit process symbol factor assum exact numer cancel occur thu ij structur nonzerog also defin set candid pivot row step k follow ik structur nonzerog assum kk alway nonzero nonsingular matrix zerofre diagon alway possibl permut row matrix permut matrix zerofre diagon 11 though symbol factor work matrix contain zero entri diagon prefer make overestim gener symbol factor process iter n step step k row structur updat r essenti structur candid pivot row step k replac union structur candid pivot row except column indic less k way guarante result structur n abl accommod fillin introduc possibl pivot sequenc simpl exampl figur 2 demonstr whole process nonzero fillin figur 2 first 3 step symbol factor sampl 5 theta 5 spars matrix structur remain unchang step 4 5 symbol factor appli order perform matrix reduc fillin order current use multipl minimum degre order also permut row matrix use transvers obtain duff algorithm 11 make zerofre diagon transvers often help reduc fillin 12 test storag impact overestim number nonsymmetr test matric variou sourc result list tabl 1 fourth column tabl origin number nonzero fifth column measur symmetri structur origin matrix bigger symmetri number nonsymmetr origin matrix unit symmetri number indic matrix symmetr matric nonsymmetr numer valu compar number nonzero obtain static approach number nonzero obtain superlu well choleski factor matric result tabl 1 show overestim usual lead less 50 extra nonzero superlu scheme extra nonzero impli addit comput cost exampl one either check symbol nonzero actual nonzero numer factor directli perform arithmet oper could unnecessari aggreg float point oper maxim use blas3 subroutin sequenti code perform still competit even fifth column tabl 1 show float oper overestim approach high 5 time result section 6 show actual ratio run time much less thu necessari benefici identifi dens structur spars matrix static symbol factor note case static symbol factor lead excess overestim exampl memplu matrix 7 case static scheme produc 119 time mani nonzero superlu fact case order superlu appli base instead otherwis overestim ratio 234 use superlu also anoth matrix wang3 7 static scheme produc 4 time mani nonzero superlu code still produc 1 gflop 128 node t3e paper focus develop high perform parallel code overestim ratio high futur work studi order strategi minim overestim ratio factor entriesjaj superlu matrix 9 e40r0100 17281 553562 1000 1476 1732 2648 117 311 tabl 1 test matric statist 32 2d lu supernod partit dens structur identif supernod partit commonli use techniqu improv cach perform spars code 2 symmetr spars matrix supernod defin group consecut column nest structur l factor matrix excel perform achiev 26 30 31 use supernod partit choleski factor howev definit directli applic spars lu nonsymmetr matric good analysi defin unsymmetr supernod l factor avail 7 notic supernod may need broken smaller one fit cach expos parallel superlu approach l supernod partit regular dens structur u factor could make possibl use blas3 routin see figur 3a howev approach dens column subcolumn u factor identifi static symbol factor see figur 3b u partit strategi explain follow l supernod partit obtain spars matrix ie set column block possibl differ block size partit appli row matrix break supernod panel submatric offdiagon submatrix l part either dens block contain dens block furthermor follow theorem identifi dens structur pattern u factor key maxim use blas3 subroutin algorithm b figur 3 illustr dens structur u factor superlu approach b dens structur u factor approach follow theorem show 2d lu partit strategi success rich set dens structur exploit follow notat use rest paper ffl l u partit divid column n column block row n row block whole matrix divid n theta n submatric submatric u factor denot u ij 1 submatric l factor denot l ij 1 denot diagon submatrix use ij denot submatrix necessari distinguish l u factor ffl defin si start column row number ith column row block conveni defin sn ffl subcolumn subrow column row submatrix simplic use global column row index denot subcolumn subrow submatrix exampl subcolumn k submatrix block u ij mean subcolumn submatrix global column index k 1 similarli use ij indic individu nonzero element base global indic compound structur l u submatrix subcolumn subrow ffl compound structur nonzero contain least one nonzero element fillin use ij 6 0 indic block ij nonzero notic algorithm need oper nonzero compound structur compound structur structur dens element nonzero fillin follow differenti nonzero fillin entri consid nonzero element theorem 1 given spars matrix zerofre diagon static symbol factor 2d lu supernod partit perform nonzero submatrix u factor contain structur dens subcolumn proof recal p k set candid pivot row symbol factor step k given supernod span column k k definit fact step k static symbol factor affect nonzero pattern submatrix k1nk1n zerofre diagon notic step k final structur row 2 p k updat symbol factor procedur r structur row k k interest nonzero pattern u part exclud part belong l kk call partial structur ur thu ur seen kth step updat ur k know structur row k unchang step k need prove ur k ks shown infer nonzero structur row k k subcolumn u part either structur dens zero sinc p k oe p k1 clear similarli show ur ks k theorem show lu partit gener rich set structur dens subcolumn even structur dens submatric u factor also incorpor result supernod amalgam section 33 experi indic 64 numer updat perform blas3 routin dgemm show effect lu partit method figur 4 demonstr result supernod partit 7 theta 7 sampl spars matrix one see submatric upper triangular part matrix contain structur dens subcolumn base theorem show structur relationship two submatric supernod column block use implement algorithm detect nonzero structur effici numer updat corollari 1 given two nonzero submatric u ij u k u ij structur dens subcolumn k u 0 j also structur dens nonzero figur 4 exampl lu supernod partit proof corollari illustr figur 5 sinc l 0 nonzero must structur dens subrow l 0 lead nonzero element subcolumn k u subcolumn k u ij structur dens accord theorem 1 subcolumn k u 0 j structur dens u figur 5 illustr corollari 1 corollari 2 given two nonzero submatric u ij u structur dens u must structur dens proof straightforward use corollari 1 33 supernod amalgam test spars matric averag size supernod lu partit small 15 2 column result fine grain task amalgam small supernod lead great perform improv parallel sequenti spars code improv cach perform reduc interprocessor commun overhead could mani way amalgam supernod 7 30 basic idea relax restrict column supernod must exactli nonzero structur diagon amalgam usual guid supernod elimin tree parent could merg children merg introduc mani extra zero entri supernod row column permut need parent consecut children howev column permut introduc amalgam method could undermin correct static symbol factor use simpler approach requir permut approach amalgam consecut supernod nonzero structur differ small number entri perform effici manner time complex 27 control maximum allow differ amalgam factor r experi show r rang give best perform test matric lead improv execut time sequenti code reason get bigger supernod get larger dens structur although may zero entri take advantag blas3 kernel notic appli supernod amalgam dens structur identifi theorem 1 strictli dens call almostdens structur still use result theorem 1 minor revis summar follow corollari result present section 6 obtain use amalgam strategi corollari 3 given spars matrix supernod amalgam appli static symbol factor 2d lu supernod partit perform nonzero submatrix u factor contain almoststructurallydens subcolumn 4 program partit task depend processor map divid spars matrix submatric use lu supernod partit need partit lu code accordingli defin coars grain task manipul partit dens data structur program partit column block partit follow supernod structur typic two type task one f actork factor column kth column block includ find pivot sequenc associ column updatek j appli pivot sequenc deriv f actork jth column block modifi jth column block use kth column block instead perform row interchang right part matrix right pivot search techniqu call delayedpivot use 6 techniqu pivot sequenc held factor kth column block complet pivot sequenc appli rest matrix ie interchang row delayedpivot import especi parallel algorithm equival aggreg multipl small messag larger one owner kth column block send column block pack togeth pivot inform processor outlin partit spars lu factor algorithm partial pivot describ figur 6 code f actork summar figur 7 use blas1 bla subroutin comput cost numer factor mainli domin task function task updatek j present figur 8 line 05 use dens matrix multipl 2 perform task f actork perform task updatek j figur partit spars lu factor partial pivot 3 find pivot row column row row column block k 5 scale column updat rest column column block figur 7 descript task f actork use direct acycl task graph dag model irregular parallel aris partit spars lu program dag construct static numer factor previou work exploit task parallel spars choleski factor use elimin tree eg 28 30 good way expos avail parallel pivot requir spars lu elimin tree directli reflect avail paral lelism dynam creat dag use model parallel guid runtim execut nonsymmetr multifront method 5 25 given task definit figur 6 7 8 defin structur spars lu task graph follow four properti necessari ffl n task f actork 1 k n ffl task updatek dens matrix total nn gamma 12 updat task ffl depend edg f actork task updatek j 02 interchang row accord pivot sequenc lower triangular part l kk 04 submatrix u kj dens els dens subcolumn c u u kj nonzero submatrix ij submatrix u kj dens els dens subcolumn c u u kj b correspond dens subcolumn ij figur 8 descript task updatek j ffl depend updatek k 0 f actork 0 exist task updatet k 0 add one properti necessari simplifi implement properti essenti allow exploit commut among updat task howev accord experi choleski factor 16 perform loss due properti substanti 6 averag graph schedul use ffl depend updatek j updatek exist task updatet j figur 9a show nonzero pattern partit matrix shown figur 4 figur 9b correspond task depend graph 1d data map 1d data map submatric l u part column block resid processor column block map processor cyclic manner base schedul techniqu graph schedul task assign base ownercomput rule ie task modifi column block assign processor own column block one disadvantag map serial comput singl f actork word singl f actork updatek task perform b3 4 5125 figur 9 nonzero pattern exampl matrix figur 4 b depend graph deriv partit result conveni f use denot f actor u use denot updat one processor map strategi advantag pivot search subrow interchang done local without commun anoth advantag parallel model depend structur effect exploit use graph schedul techniqu data map literatur 2d map shown scalabl 1d spars choleski 30 31 howev sever difficulti appli 2d blockori map case spars lu factor even static structur predict firstli pivot oper row interchang requir frequent wellsynchron interprocessor commun submatric column block assign differ processor effect exploit limit irregular parallel 2d case requir highli effici asynchron execut mechan delic messag buffer manag secondli difficult util schedul possibl irregular parallel spars lu lastli manag low space complex anoth issu sinc exploit irregular parallel maximum degre may need buffer space 2d algorithm use simpl standard map function scheme p avail processor view two dimension grid c nonzero submatrix block ij could l block u block assign processor p mod pr j mod pc 2d data map consid scalabl 1d data map enabl parallel singl f actork updatek j task p r processor discuss 2d parallel exploit use asynchron schedul execut 5 parallel exploit 51 schedul runtim support 1d method discuss 1d spars lu task schedul execut parallel time minim georg ng 21 use dynam load balanc algorithm share memori machin distribut memori machin dynam adapt load balanc work well problem coars grain comput still open problem balanc benefit dynam schedul runtim control overhead sinc task data migrat cost expens spars problem mix granular use task depend graph guid schedul investig two type schedul scheme ffl computeahead schedul ca use blockcycl map task computeahead execut strategi demonstr figur 10 idea use speed parallel dens factor 23 execut numer factor layer layer base current submatrix index parallel exploit concurr updat order overlap comput commun f actork execut soon f actork updatek k pivot sequenc column block k next layer commun earli possibl ffl graph schedul order task execut within processor use graph schedul algorithm 36 basic optim balanc processor load overlap comput commun hide commun latenc done util global depend structur critic path inform 01 column block 1 local 02 perform task f actor1 broadcast column block 1 pivot sequenc local receiv column block k pivot choic row accord pivot sequenc perform task f actork broadcast column block k pivot sequenc local column block k receiv receiv column block k pivot choic row accord pivot sequenc perform task updatek j figur 10 1d code use computeahead schedul graph schedul shown effect exploit irregular parallel applic eg 15 16 graph schedul outperform ca schedul spars lu constraint order f actor task demonstr point use lu task graph figur 9 exampl gantt chart ca schedul schedul deriv graph schedul algorithm list figur 11 assum task comput weight 2 edg commun weight 1 easi see schedul approach produc better result ca schedul look ca schedul care see reason ca look ahead one step execut task f actor3 place update1 5 hand graph schedul algorithm detect f actor3 execut update1 5 lead better overlap commun comput p1a figur 11 schedul deriv graph schedul algorithm b computeahead schedul conveni f use denot f actor u use denot updat howev implement ca algorithm much easier sinc effici execut spars task graph schedul requir sophist runtim system support asynchron commun protocol use rapid runtim system 16 parallel spars lu use graph schedul key optim use remot memori accessrma commun data object two processor incur copyingbuff data transfer sinc low commun overhead critic spars code mix granular rma avail modern multiprocessor architectur crayt3d 34 t3e 32 meiko cs2 15 sinc rma directli write data remot address possibl content remot address still use task execut remot processor could incorrect thu gener comput permiss write remot address need obtain issu remot write howev rapid system handshak process avoid care design task commun protocol 16 properti greatli reduc task synchron cost shown 17 rapid spars code deliv 70 speedup predict schedul crayt3d addit use rapid system greatli reduc amount implement work parallel spars lu 01 let rno cno 2d coordin processor perform scaleswapk perform updat perform updat 2dk j figur 12 spmd code 2d asynchron code 52 asynchron execut 2d code discuss previous 1d data map expos parallel maximum extent anoth issu timeeffici schedul may spaceeffici specif support concurr among multipl updat stage rapid ca code multipl buffer need keep pivot column block differ stage processor therefor given problem per processor space complex 1d code could high os 1 1 space complex sequenti algorithm spars lu processor worst case may need space hold entir matrix rapid system 16 also need extra memori space hold depend structur base observ goal 2d code reduc memori space requir exploit reason amount parallel solv larg problem instanc effici way section present asynchron 2d algorithm substanti overlap multistag updat memori requir much smaller 1d method figur 12 show main control algorithm spmd code style figur 13 show spmd code f actork execut processor column k mod p c recal algorithm use 2d blockcycl data map coordin processor own mod also divid function updat figur 8 two part scaleswap scale delay row interchang submatrix kn k1n shown figur 14 updat 2d submatrix updat shown figur 15 figur statement involv interprocessor commun mark seen comput flow 2d code still control pivot task f actork order execut f actork sequenti updat 2d task comput come execut parallel among processor asynchron parallel come two level first singl stage task updat 2dk find local maximum element column 05 send subrow within column block k contain local maximum processor p k mod pr k mod pc 06 processor own l kk 07 collect local maxima find pivot row 08 broadcast subrow within column block k along processor column interchang subrow subrow necessari scale local entri column updat local subcolumn column 12 multicast pivot sequenc along processor row 13 processor own l kk multicast l kk along processor row 14 multicast part nonzero block l k1n k own processor along processor row figur 13 parallel execut f actork 2d asynchron code execut concurr processor addit differ stage updat 2d task updat 2dk also overlap idea computeahead schedul also incorpor ie f actork 1 execut soon updat finish detail explan pivot scale swap given line 5 figur 13 whole subrow commun processor report local maximum processor own l kk block let current global column number pivot conduct without synchron processor local swap subrow subrow contain select pivot element shorten wait time conduct updat littl commun volum howev line 08 processor p k mod pr k mod pc must send origin subrow owner subrow swap select subrow processor well updat ing f actor task synchron take place line 05 07 08 processor report local maximum p k mod pr k mod pc p k mod pr k mod pc broadcast subrow contain global maximum along processor column task scaleswap main role scale u k k1n perform delay row interchang remain submatric k1n k1n examin degre parallel exploit algorithm determin number updat stage overlap use inform also determin extra buffer space need per processor execut algorithm correctli defin stage overlap degre receiv pivot sequenc p rno k mod pc 04 processor part row pivot row column 05 interchang nonzero part row row own processor 08 cno 6 k mod p c receiv l kk p rno k mod pc scale nonzero block u k kn own processor 10 multicast scale result along processor column 11 cno 6 k mod p c receiv l kn k p rno k mod pc 12 rno 6 k mod p r receiv u k kn p k mod pr cno figur 14 task scaleswapk 2d asynchron code 1 updat 2dk use l ik u kj figur 15 updat 2dk j 2d asynchron code updat task exist task updat 2dk updat 2dk execut concurrentlyg updat 2dk denot set updat 2dk task theorem 2 asynchron 2d algorithm p processor p 1 reachabl upper bound overlap degre p c among processor reachabl upper bound overlap degre within processor column minp r gamma proof use follow fact prove theorem ffl fact 1 f actork execut processor column number k mod p c processor column synchron processor complet f actork processor still updat shown figur 13 updat task belong processor 1 must complet processor ffl fact 2 scaleswapk execut processor row number k mod p r processor complet scaleswapk updat task belong processor 0 must complet processor part 1 first show updat 2d task overlap degre p c among processor trivial base fact 1 p c 1 imagin scenario processor column 0 finish task f actork still work updat processor column 1 could go ahead execut updat 2dk task processor column 1 finish updat 2dk k1 task execut f actork1 finish updat 2dk task processor column 2 could execut updat 2dk final processor column p c gamma 1 could execut f actork moment processor column 0 may still work updat thu overlap degre p c show contradict maximum overlap degre p c assum moment exist two updat stage execut concurr updat 2dk updat must complet without loss gener assum processor column 0 execut f actork 0 accord fact 1 updat complet moment sinc block cyclic map use easi see processor column perform one f actorj task complet processor concurr stage updat 2dk k must satisfi contradict part 2 first show overlap degre minp r gamma achiev within processor column conveni illustr consid scenario delay row interchang scaleswap take place local without commun within processor column therefor interprocessor synchron go within processor column except f actor task assum imagin moment processor column 0 complet f actor p 00 finish scaleswap start execut updat 2d mod processor column 1 execut updat 1 p 10 start scaleswap updat 2d follow reason updat 2d finish processor column could complet previou updat 2d task scaleswapsp r gamma 1 start updat 2dsp r gamma 1 p 00 may still work updat 2d thu overlap degre obvious reason stop processor column f actor 1 case p pc gamma10 start updat 2d pr gamma10 could still work updat 2d gamma comput ahead schedul henc overlap degre p c need show upper bound overlap degre within processor column alreadi shown proof part 1 overal overlap degre less p c overlap degre within processor column prove also less 1 use similar proof part 1 except use scaleswapk replac f actork use fact 2 instead fact 1 know degre overlap import determin amount memori space need accommod commun buffer processor support asynchron execu tion buffer space addit data space need distribut origin matrix four type commun need buffer 1 pivot along processor column line 05 07 08 figur 13 includ commun pivot posit multicast pivot row call buffer purpos pbuffer 2 multicast along processor row line 12 13 14 figur 13 commun data includ l kk local nonzero block l k1n k pivot sequenc call buffer purpos cbuffer 3 row interchang within processor column line 05 figur 14 call buffer ibuff 4 multicast along processor column line 10 figur 14 data includ local nonzero block row panel call buffer rbuffer assum p r base experiment result set p r alway lead better perform thu overlap degre updat 2d task within processor row p c overlap degre within processor column p r gamma 1 need p c separ cbuffer overlap among differ column rbuffer overlap among differ row estim size cbuffer rbuffer follow assum sparsiti ratio given matrix fillin maximum block size bsize cbuffer size maxfspac local nonzero block l knk similarli rbuffer size local nonzero block u ignor buffer size pbuffer ibuff small size pbuffer bsize delta bsize size ibuff delta np c thu total buffer space need asynchron execut c notic sequenti space complex practic set p c p 2 therefor buffer space complex processor 25 small larg matrix benchmark matric test buffer space less 100 k word given spars matrix matrix data evenli distribut onto p processor total memori requir per processor 1 p o1 consid n ae p n ae bsize lead us conclud 2d asynchron algorithm space scalabl 6 experiment studi experi origin conduct crayt3d distribut memori machin san supercomput center node t3d includ dec alpha ev421064 processor 64 mbyte memori size intern cach 8 kbyte per processor blas3 matrixmatrix multipl routin dgemm achiev 103 mflop blas2 matrixvector multipl routin dgemv reach 85 mflop number obtain assum data cach use cach readahead optim t3d matrix block size chosen 25 commun network t3d 3d toru cray provid share memori access librari call shmem achiev 126 mbytess bandwidth 27 commun overhead use shmem put primit 34 use shmem put commun implement also conduct experi newli acquir crayt3 san diego supercomput center t3e node clock rate 300 mhz 8kbyte intern cach 96kbyte second level cach 128 mbyte main memori peak bandwidth node report 500 mbytess peak round trip commun latenc 05 2 33 observ block size 25 dgemm achiev 388 mflop dgemv reach 255 mflop use block size 25 experi sinc block size larg avail parallel reduc section mainli report result t3e occas absolut perform concern also list result t3d see approach scale underlin architectur upgrad result obtain t3e unless explicitli state calcul mflop achiev parallel algorithm includ extra float point oper introduc overestim use follow formula achiev oper count obtain superlu parallel time algorithm t3d t3e oper count matrix report run superlu code sun workstat larg memori sinc superlu code run larg matric singl t3d t3e node due memori constraint also compar sequenti code superlu make sure code use static symbol factor slow prevent parallel version deliv high megaflop 61 impact static symbol factor sequenti perform studi introduct extra nonzero element static factor substanti affect time complex numer factor compar perform sequenti code superlu code perform tabl 2 1 matric tabl 1 1 time tabl includ symbol preprocess cost time superlu includ symbol factor superlu fli implement static symbol preprocess execut singl t3d t3e node also introduc two matric show well method work larger matric denser matric one two matric b33 5600 truncat bcsstk33 current sequenti implement abl handl entir matrix due memori constraint one dense1000 matrix approach superlu exec time ratio second mflop second mflop superlu sherman5 287 094 881 269 239 078 1057 324 121 122 sherman3 606 203 1018 304 427 168 1446 367 156 121 jpwh991 211 069 824 252 162 056 1066 310 134 123 goodwin 4372 170 153 394 dense1000 1048 404 636 1650 196 839 340 794 053 048 tabl 2 sequenti perform versu superlu impli data avail due insuffici memori though static symbol factor introduc lot extra comput shown tabl 1 perform 2d lu partit consist competit highli optim superlu absolut singl node perform achiev approach t3d t3e consist rang 5 gamma 10 highest dgemm perform matric small medium size consid fact spars code usual suffer poor cach reus perform reason addit amount comput test matric tabl 2 small rang 107 million doubl precis float oper sinc characterist approach explor dens structur util blas3 kernel better perform expect larger denser matric verifi matrix b33 5600 even larger matric vavasis3 run one node shown later 2d code achiev 328 mflop per node 16 t3d processor notic megaflop perform per node spars choleksi report 24 16 t3d node around 40 mflop also good indic singlenod perform satisfactori present quantit analysi explain competit superlu assum speed blas2 kernel 2 secondf lop speed blas3 kernel 3 secondf lop total amount numer updat c f lop superlu c 0 f lop appar simplic ignor comput scale part within column contribut littl total execut time henc effici exampl preprocess time 276 second singl node t3e largest matrix test vavasis3 symbol time spent dynam symbol factor superlu approach ae percentag numer updat perform dgemm let j ratio symbol factor time numer factor time superlu simplifi equat 1 follow estim j 082 test matric base result 7 17 also measur ae approxim ae 067 ratio number float point oper perform superlu test matric avail tabl 1 averag valu c 0 398 plug typic paramet equat 2 3 lop get 193 t3e lop get 168 estim close ratio obtain tabl 2 discrep caus fact submatrix size supernod nonuniform lead differ cach perform submatric uniform size expect predict accur instanc dens case c 0 exactli 1 ratio calcul 048 t3d 042 t3e almost ratio list tabl 2 analysi show use blas3 much possibl make competit superlu suppos machin dgemm outperform dgemv substanti ratio comput perform dgemm high enough could faster superlu matric last two entri tabl 2 alreadi shown 62 parallel perform 1d code subsect report set experi conduct examin overal parallel perform 1d code effect schedul supernod amalgam perform list mflop number 1d rapid code obtain variou number processor sever test matric tabl 3 entri impli data avail due memori constraint know megaflop dgemm t3e 37 time larg t3d rapid code use upgrad machin speed 3 time averag satisfactori machin perform rapid code increas number processor increas speedup compar pure sequenti code applic reach 177 64 t3d node 241 64 t3e node 32 64 node perform gain small except matric goodwin e40r0100 b33 5600 much larger problem rest reason small test matric enough amount comput parallel satur larg number processor elimin process proce toward end belief better scalabl perform obtain larger matric current avail memori node t3d t3e limit problem size solv current version rapid code matrix p2 p4 p8 p16 p32 p64 sherman5 147 444 258 790 408 1331 538 1686 649 2107 684 2299 sherman3 164 514 300 907 457 1435 611 1928 643 1990 663 2127 jpwh991 133 414 232 756 405 1242 512 1739 580 1932 600 2173 orsreg1 174 534 306 906 512 1603 687 2156 753 2233 753 2316 goodwin 296 736 540 1357 879 2380 1364 3737 1820 5226 2181 6558 tabl 3 absolut perform mflop 1d rapid code effect graph schedul compar perform 1d ca code 1d rapid code figur 16 axi stand parallel time 2 4 processor certain case computeahead code slightli faster rapid code number processor 4 rapid code run faster processor involv bigger perform gap tend reason small number processor suffici task make processor busi comput ahead schedul perform well rapid code suffer certain degre system overhead larger number processor schedul optim becom import sinc limit parallel exploit effect supernod amalgam examin effect supernod amalgam strategi use 1d rapid code let pt pt parallel time without supernod amalgam respect parallel time improv ratio t3e sever test matric list tabl 4 similar result t3d 17 appar supernod amalgam brought signific improv due increas supernod size impli increas task granular import obtain good parallel perform 22 comparison rapid code 1d ca code proc sherman5 sherman3 0101030507comparison rapid code 1d ca code proc jpwh991 x goodwin figur impact differ schedul strategi 1d code approach matrix p1 p2 p4 p8 p16 p32 sherman5 47 47 46 50 40 43 sherman3 20 25 23 28 22 14 jpwh991 48 48 48 50 47 40 tabl 4 parallel time improv obtain supernod amalgam 63 2d code perform mention 2d code exploit parallel maintain lower space complex much potenti solv larg problem show absolut perform obtain larg matric t3d tabl 5 sinc matric fit small number processor list result 16 processor maximum absolut perform achiev 64 node t3d 148 gflop translat 231 mflop per node node pernod perform 328 mflop tabl 6 show perform number t3e 2d code achiev 6878 gflop 128 node 64 node megaflop t3e 31 34 time larg t3d consid dgemm megaflop t3e 37 time larg t3d code perform use upgrad machin good notic 1d code solv last six matric tabl 6 matric solvabl use 1d rapid 2d code compar averag parallel time differ comput result figur 17 1d rapid code achiev matrix timesec mflop timesec mflop timesec mflop goodwin 60 1107 46 1452 36 1848 ex11 879 3050 534 5018 334 8026 raefsky4 1298 2429 760 4138 432 7192 tabl 5 perform result 2d code larg matric t3d matrix p8 p16 p32 p64 p128 time mflop time mflop time mflop time mflop time mflop goodwin 31 2152 19 3446 13 4963 11 5992 09 7152 ex11 507 5288 283 9462 162 16542 99 27031 64 41822 raefsky4 794 3912 432 7189 241 12907 139 22333 86 35929 inaccura 168 2446 99 4152 63 6558 39 10480 30 13914 af23560 223 2854 129 4929 812 7843 57 11232 42 15127 tabl perform result 2d asynchron algorithm t3e time second better perform use sophist graph schedul techniqu guid map column block order task result better overlap commun comput perform differ larger matric list left figur 17 compar right figur 17 partial explain reason analyz load balanc factor 1d rapid code 2d code figur 18 load balanc factor defin work total p delta work max 31 count work updat part major part comput 2d code better load balanc make impact lack effici task schedul verifi figur 17 figur 18 one see load balanc factor 2d code close rapid code eg lnsp3937 perform rapid code much better 2d code load balanc factor 2d code significantli better rapid code eg jpwh991 orserg1 perform differ smaller synchron versu asynchron 2d code use global barrier 2d code elimin step simplifi implement overlap comput among differ updat stage compar parallel time reduct asynchron code synchron code test matric tabl 7 show asynchron design improv perform significantli especi larg number processor t3e demonstr import exploit parallel use asynchron execut experi 0204comparison rapid code 2d code proc sherman5 sherman3 comparison rapid code 2d code proc jpwh991 x goodwin figur 17 perform improv 1d rapid 2d code balanc comparison rapid vs 2d proc load balanc factor x sherman3 balanc comparison rapid vs 2d proc load balanc factor x jpwh991 figur 18 comparison load balanc factor 1d rapid code 2d code data t3d 14 7 conclud remark paper present approach parallel spars lu factor partial pivot distribut memori machin major contribut paper integr sever techniqu togeth static symbol factor schedul asynchron parallel 2d lu supernod partit techniqu effect identifi dens structur maxim use blas3 subroutin algorithm design use idea abl exploit data regular open irregular problem achiev 6878 gflop 128 t3e node highest perform known challeng problem previou record 2583 gflop share memori machin 8 matrix p2 p4 p8 p16 p32 p64 sherman5 77 64 194 281 259 241 sherman3 102 124 203 227 260 250 jpwh991 87 100 238 333 357 286 orsreg1 61 77 175 280 205 282 goodwin 54 141 142 246 260 302 tabl 7 perform improv 2d asynchron code 2d synchron code comparison result show 2d code better scalabl 1d code 2d map expos parallel care design buffer scheme 1d rapid code still outperform 2d code suffici memori sinc schedul execut techniqu 2d code simpl competit graph schedul recent conduct research develop space effici schedul algorithm retain good time effici 18 still open problem develop advanc schedul techniqu better exploit parallel 2d spars lu factor partial pivot issu relat work need studi exampl altern parallel spars lu base schur complement 13 static estim parallel exploit spars qr 29 35 note static symbol factor could fail practic input matrix nearli dens row lead almost complet fillin whole matrix might possibl use differ matrix reorder avoid fortun case matric test therefor approach applic wide rang problem use simpl order strategi interest futur studi order strategi minim overestim ratio consist deliv good perform variou class spars matric acknowledg work support nsf ria ccr9409695 nsf cda9529418 uc micro grant match sun nsf career ccr9702640 arpa dabt6393c0064 rutger hpcd project would like thank kai shen effici implement static symbol factor algorithm xiaoy li jim demmel help discuss provid us test matric superlu code cleve ashcraft tim davi apostolo gerasouli esmond ng ed rothberg rob schreiber horst simon chunguang sun kathi yelick anonym refere valuabl comment r influenc relax supernod partit multifront method progress spars matrix method larg spars linear system vector supercomput user guid unsymmetricpattern multifront packag umfpack person commun unsymmetricpattern multifront method spars lu factor izat numer linear algebra parallel processor supernod approach spars partial pivot asynchron parallel supernod algorithm spars gaussian elimin extend set basic linear algebra subroutin multifront solut indefinit spars symmetr system equat algorithm obtain maximum transvers person commun structur represent schur complement spars matric comparison 1d 2d data map spars lu factor partial pivot effici runtim support irregular task comput mix granular spars lu factor partial pivot distribut memori machin space time effici execut parallel irregular comput parallel solut nonsymmetr spars linear system use h symbol factor spars gaussian elimin partial pivot parallel spars gaussian elimin partial pivot granular cluster direct acycl task graph scientif comput introduct parallel comput compil highli scalabl parallel algorithm spars matrix factor parallel unsymmetricpattern multifront method parallel algorithm spars linear system parallel spars gaussian elimin partial pivot 2d data map comput model task schedul parallel spars choleski factor distribut spars gaussian elimin orthogon factor exploit memori hierarchi sequenti parallel spars choleski factor improv load distribut parallel spars choleski fac toriz synchron commun t3e multiprocess cray t3e network adapt rout high perform 3d toru decoupl synchron data transfer messag pass system parallel comput parallel spars orthogon factor distributedmemori multiprocessor pyrro static task schedul code gener messagepass multiprocessor tr ctr kai shen xiangmin jiao tao yang elimin forest guid 2d spars lu factor proceed tenth annual acm symposium parallel algorithm architectur p515 june 28juli 02 1998 puerto vallarta mexico xiaoy li jame w demmel make spars gaussian elimin scalabl static pivot proceed 1998 acmiee confer supercomput cdrom p117 novemb 0713 1998 san jose ca patrick r amestoy iain duff jeanyv lexcel xiaoy li analysi comparison two gener spars solver distribut memori comput acm transact mathemat softwar tom v27 n4 p388421 decemb 2001 xiaoy li jame w demmel superludist scalabl distributedmemori spars direct solver unsymmetr linear system acm transact mathemat softwar tom v29 n2 p110140 june