use scalea perform analysi distribut parallel program paper give overview scalea new perform analysi tool openmp mpi hpf mix paralleldistribut program scalea instrument execut measur program comput varieti perform overhead base novel overhead classif sourc code hwprofil combin singl system significantli extend scope possibl overhead measur examin rang hwcounter number cach miss float point oper complex perform metric control loss parallel moreov scalea use new represent code region call dynam code region call graph enabl detail overhead analysi arbitrari code region instrument descript file use relat perform inform code region input program reduc instrument overhead sever experi realist code cover mpi openmp hpf mix openmpmpi code demonstr use scalea b introduct hybrid architectur eg smp cluster becom mainstay distribut parallel process market comput commun busili develop languag softwar tool machin besid openmp 27 mpi 13 hpf 15 mix program paradigm openmpmpi increasingli evalu paper introduc new perform analysi system scalea distribut parallel program cover mention program paradigm scalea base novel classica tion perform overhead share distribut memori parallel program includ data move ment synchron control parallel addit comput loss parallel unidenti head scalea among rst perform analysi tool combin sourc code hw prole singl system signicantli extend scope possibl overhead measur examin includ use hw counter cach analysi complex perform metric control loss parallel specic instrument perform analysi conduct determin categori overhead individu code region instrument done fulli automat usercontrol direct postexecut perform analysi done base perform tracel novel represent code region name dynam code region call graph drg drg ect dynam relationship code region subregion enabl detail overhead analysi everi code gion drg restrict function call also cover loop io commun statement etc moreov allow arbitrari code region alyz code region vari singl statement entir program unit contrast exist approach frequent use call graph consid function call prototyp scalea implement present sever experi realist program includ molecular dynam applic openmp version nancial model hpf openmpmpi version materi scienc code mpi version demonstr use scalea rest paper structur follow section describ overview scalea 24 section 3 present novel classic perform overhead base scalea instrument code analys perform dynam code region call graph describ next section experi shown section 5 relat work outlin section 6 conclus futur work discuss section 7 scalea postexecut perform tool strument measur analys perform behavior distribut memori share memori mix parallel program figur 1 show architectur scalea consist two main compon scalea instrument system si post execut perform analysi tool set si integr vfc 3 compil translat fortran program mpi openmp hpf mix program tran90mpi mix openmpmpi program input program scalea process compil frontend gener abstract syntax tree ast si enabl user select direct commandlin option code region interest base preselect code region si automat insert probe code collect relev perform inform set proletrac le execut program target architectur si also gener instrument descript le see section 22 enabl gather perform data relat back input program reduc instrument overhead si 25 target perform measur system base tau perform framework tau integr toolkit perform instrument mea surement analysi parallel multithread pro gram tau measur librari provid portabl prole trace capabl support access hardwar counter si automat instrument parallel program vfc use tau instrument librari build abstract syntax tree vfc tau measur system creat dynam code region call graph see section 4 main function si given follow automat instrument preden code region loop procedur io statement hpf independ loop openmp parallel loop openmp section mpi sendrec etc variou perform overhead use commandlin option manual instrument si direct insert program direct also allow dene user dene code region instrument control instrument overhead size perform data gather execut program postexecut analysi sisprofil sisoverhead pprof raci vampir dynam code region call graph raw perform data instrument descript file preprocess profiletrac file compil link instrument code automat instrument si manual instrument input program fortran mpi program openmp program hybrid program execut program user instrument control execut si runtim system sisprofil tausi papi load imbal time result hardwar counter result visual raci vampir target machin subselect select command visual perform data intermedi databas train set databas perform databas data repositori data object physic resourc data process data flow extern input control control flow diagram legend figur 1 architectur scalea manual instrument turn ono prole given code region preprocess phase scalea lter extract relev perform inform prolestrac le yield ltere perform data dynam code region call graph drg drg ect dynam relationship code region subregion use precis overhead analysi everi individu subregion contrast exist approach base convent call graph consid function call code region postexecut perform analysi also employ train set method determin specic inform eg time penalti everi cach miss head overhead probe time access lock etc everi target machin interest follow describ scalea instrument system instrument descript le detail si scalea postexecut perform analysi found 24 25 21 scalea instrument system base userprovid commandlin option di rectiv si insert instrument code program collect perform data inter est si support programm control prol ingtrac gener perform data select instrument specic code region type loop procedur io statement hpf independ loop openmp parallel loop openmp section openmp critic mpi barrier state ment etc si also enabl instrument arbitrari code region final instrument turn specic instrument direct order measur arbitrari code region si provid follow instrument code region direct si cr begin si cr end must respect insert programm region start nish note sever entri exit node code gion appropri direct must insert idf entri descript id code region identi type code region type le sourc le identi unit program unit identi enclos region line start line number region start column start column number start line end line number end column end column number end perform data perform data collect comput region aux auxiliari inform tabl 1 content instrument descript le idf programm everi entri exit node given code region altern compil analysi use automat determin entri exit node furthermor si provid specic direct order control tracingprol direct measur enabl measur disabl allow programm turn tracingprol program code region instanc follow exampl instrument portion openmp price code version see section 52 sake demonstr call function random path measur use facil control prolingtrac mention end note si direct insert programm base scalea automat instrument code 22 instrument descript file crucial aspect perform analysi relat perform inform back origin input program instrument program si gener instrument descript le idf correl prole trace overhead inform correspond code region idf maintain everi instrument code region varieti inform see tabl 1 code region type describ type code gion instanc entir program outermost loop read statement openmp section openmp parallel loop mpi barrier etc program unit correspond subroutin function enclos code region idf entri perform data actual link separ repositori store inform note inform store idf actual made runtim data structur comput perform overhead properti execut program idf also help keep instrument code minim everi probe insert singl identi allow relat associ probe timer counter correspond code region 3 classic tempor accord amdahl law 1 theoret best sequenti algorithm take time nish program p time requir execut parallel version p processor tempor overhead parallel program dene ect dierenc achiev optim parallel divid u overhead identi u overhead fraction could analyz detail theori never neg impli speedup p never exceed p 16 ever practic occur tempor overhead becom neg due super linear speedup applica tion eect commonli caus increas avail cach size figur 2 give classic tempor overhead base perform analysi scalea conduct tempor overhead data movement local memori access remot memori access level 2 level 1 level 3 level 2 level n level sendrec putget synchronis barrier lock condit variabl control parallel schedul inspector executor forkjoin addit comput algorithm chang compil chang frontend normal loss parallel unparallelis code replic code partial parallelis code unidentifi figur 2 tempor overhead classic data movement correspond data transfer within singl address space process local memori access process remot memori access synchron eg barrier lock use coordin process thread access data maintain consist comput data etc control parallel eg forkjoin oper loop schedul use control manag parallel program caus runtim librari user compil oper addit comput ect chang origin sequenti program includ algorithm compil chang increas parallel eg elimin data depend data local eg chang data access pattern loss parallel due imperfect parallel program classi follow unparallel code execut one processor replic code execut proce sor partial parallel code execut one processor unidenti overhead correspond overhead cover categori note mention classic stimul 6 dier sever respect 6 synchron part inform movement load imbal separ overhead local remot memori merg singl overhead class loss parallel split two class unidenti overhead consid load imbal opinion overhead repres perform properti caus one overhead 4 dynam code region call graph everi program consist set code region rang singl statement entir program unit code region respect enter exit multipl entri exit control ow point see figur 3 case howev code region singleentri singleexit code region order measur execut behavior code region instrument system detect entri exit node code region insert probe node basic task done support compil guid manual insert direct figur 3 show exampl code region entri exit node select arbitrari code region user respect mark two statement entri exit statement time entri exit node code region eg use si direct 25 compil analysi si automat tri determin entri exit node code region node repres statement program figur 3 show exampl code region multipl entri exit node instrument tri detect node automat insert probe entri exit node respect code region overlap scalea current support instrument overlap code gion current implement scalea support mainli instrument singleentri multipleexit code region enhanc si support also multipleentri multipleexit code region 41 dynam code region call graph scalea set preden code region classi common eg program procedur loop function call statement program paradigm specic code region mpi call hpf independ loop openmp parallel region loop section etc moreov si provid direct dene arbitrari code region see section 21 input program base code region dene new data structur call dynam code region call graph drg dynam code region call graph drg program q dene direct ow graph set node r set edg e node r 2 r repres code region execut least runtim q edg r indic code region r 2 call insid r 1 execut q r 2 dynam subregion r 1 rst code region execut execut q dene drg use key data structur conduct detail perform overhead analysi scalea notic time overhead code region r explicitli instrument subregion r 1 r n given r time overhead explicitli instrument code region r 1 n start r correspond overhead begin eg fork thread redistribut data etc end join thread barrier synchron process reduct oper etc r remain correspond code region explicitli instrument entri point statement begin select code region statement end select code region exit point statement control flow control flow exit point entri point figur 3 code region sever entri exit point howev easili comput remain region r instrument well figur 4 show excerpt openmp code togeth associ drg call graph techniqu wide use perform analysi tool vampir 20 gprof 11 10 cxperf 14 support call graph show much time spent function children 7 call graph use improv search strategi autom perform diagnosi howev node call graph tool repres function call 10 14 contrast drg dene node arbitrari code region eg function function call loop statement etc integerx print input n read n call 0 call i1n a1 end call call sisfstart5 call sisfstop5 call end program r 4 r 5 r 4 r 5 figur 4 openmp code excerpt drg 42 gener build dynam code region call graph call code region r 2 insid code region r 1 execut program establish parentchildren relationship r 1 r 2 instrument librari captur relationship maintain execut program code region r 2 call insid r 1 data entri repres relationship r 1 r 2 gener store appropri prolestrac le code region r encount isnt child code region eg code region execut rst abstract code region assign parent everi code region uniqu identi includ probe insert si store instrument descript le drg data structur maintain inform code region instrument execut everi thread process build maintain subdrg execut preprocess phase cf figur 1 drg applic built base individu subdrg thread subdrg thread comput process prolestrac le contain perform data thread algorithm gener drg describ detail 24 figur 5 execut time md applic 5 experi implement prototyp scalea control commandlin option user direct code region includ arbitrari code region select specic si direct insert input program tempor perform overhead accord classic shown figur 2 select commandlin option visual capabl current restrict textual put plan build graphic user interfac end 2001 graphic output except tabl follow experi gener manual inform use si postexecut analysi found 25 24 overhead 2cpu 3cpu 4cpu loss parallel 0025 0059 0066 control parallel 1013 0676 0517 synchron 1572 127 0942 total execut time 146754 98438 74079 tabl 2 overhead sec md applic identi unidenti total overhead respect section present sever experi demonstr use scalea experi conduct gescher 23 smp cluster 6 smp node connect fasteth ernet compris 4 intel pentium iii xeon 700 mhz cpu 1mb fullspe l2 cach 2gbyte ecc ram intel pro100fast ethernet ul tra160 36gb hard disk run linux 2218smp patch perfctr hardwar counter perform use mpich 12 pgf90 compil version 33 portland group inc 51 molecular dynam md applica tion md program implement simpl molecular dynam simul continu real space program obtain 27 implement openmp program written bill magro kuck associ inc kai perform md applic measur singl smp node gescher figur 5 tabl 2 show execut time behavior measur overhead respect result demonstr good speedup behavior nearli linear see tabl 2 total overhead small larg portion tempor overhead identi time sequenti code region unpar leliz doesnt chang alway execut figur l2 cach missescach access ratio omp region md applic one processor loss parallel unparallel code region r program q dene r processor use execut q r sequenti execut time r increas p easili shown loss parallel increas well also conrm measur shown tabl 2 control parallel mostli caus loop schedul actual decreas increas number processor possibl explan eect larger number processor master thread process less loop schedul phase smaller number processor load balanc improv increas number processorsthread one smp node time decreas synchron time examin cach miss ratio dene number l2 cach miss divid number l2 cach access two import omp code region name omp comput omp updat shown figur 6 ratio nearli use singl processor impli good cach behavior sequenti execut code data seem l2 cach case howev parallel version cach miss ratio increas substanti thread process data global array kept privat l2 cach cach coher protocol caus mani cach line figur 7 execut time hpf openmpmpi version backward price applic exchang privat cach induc cach miss unclear howev master thread consider higher cach miss ratio thread overal cach behavior littl impact speedup code 52 backward price applic backward price code 8 implement backward induct algorithm comput price interest rate depend nancial product variabl coupon bond two parallel code version creat first hpf version exploit data parallel compil mpi program second mix version combin hpf openmp latter version vfc gener openmpmpi program hpf direct use distribut data onto set smp node within node openmp program execut commun among smp node realiz mpi call execut time version shown figur 7 term legend denot entir program wherea loop refer main comput loop hpf independ loop openmp parallel loop version 1 2 spectiv hpf version perform wors openmpmpi version show almost linear speedup 2 node overal 8 processor tabl 3 5 display overhead hpf mix openmpmpi version respect case largest overhead caus control parallel overhead rise signicantli hpf version increas number node eect less sever openmpmpi version order nd caus high control parallel overhead use scalea determin individu compon overhead see tabl 4 6 two routin updat halo mpi init mainli respons high control parallel overhead version updat halo updat overlap area distribut array caus commun one process requir data own anoth process dierent node mpi init initi mpi runtim system also involv commun version impli much higher overhead two routin compar openmpmpi reason employ separ process everi cpu smp node wherea openmpmpi version use one process per node 53 lapw0 lapw0 4 materi scienc program calcul eectiv potenti kohnsham eigenvalu problem lapw0 implement fortran mpi code run across sever smp node pgf90 compil take care exchang data processor within across smp node use scalea local import code region lapw0 subdivid sequenti code region fft rean0 fft rean3 fft rean4 parallel code region interstiti potenti loop 50 energi output execut time behavior speedup base sequenti execut time code region code region shown figur 8 9 respect lapw0 examin problem size 36 atom distribut onto processor set smp node clearli use 8 16 24 processor cant reach optim load balanc wherea processor display much better load imbal eect conrm scalea see figur comput intens routin lapw0 interstiti potenti loop 50 scale poorli due load imbal larg overhead due loss parallel data move ment synchron see tabl 7 lapw0 use mani bla scalapack librari call current instrument scalea reason larg fraction unidenti overhead see tabl 7 main sourc control parallel overhead caus mpi init see figur 10 scalea also discov main subroutin caus loss parallel overhead fft rean0 fft rean3 fftp rean4 sequenti 6 relat work parav 26 perform analysi tool openmpmpi tool dynam instrument binari code determin variou perform param eter tool cover rang perform overhead support scalea moreov tool use dynam intercept mechan commonli problem relat perform data back input program ovaltin 2 measur analys varieti perform overhead fortran77 openmp program paradyn 18 automat perform analysi tool use dynam instrument search perform bottleneck base specic languag function call graph employ improv perform tune 7 recent work openmp perform interfac 19 base direct rewrit similar si instrument approach scalea scala 9 predecessor system scalea implement interfac eg perform measur librari tau allow prole trace per form conceiv interfac could use gener perform data rest scalea system could analyz tau 22 17 perform framework integr toolkit perform instrument mea surement analysi parallel multithread pro gram scalea use tau instrument librari one trace librari papi 5 speci standard api access hardwar perform counter avail modern mi croprocessor scalea use papi librari measur hardwar counter gprof 11 10 compilerbas prole framework mostli analys execut behavior count function function call vampir 20 perform analysi tool process trace le gener vampirtrac 21 support variou perform display includ timelin static visual togeth call graph sourc code 7 conclus futur work paper describ scalea perform analysi system distribut parallel pro gram scalea current support perform analysi openmp mpi hpf mix parallel program eg openmpmpi scalea base novel classic perform overhead share distribut memori parallel program scalea among rst perform analysi tool combin sourc code hw prole singl system signicantli extend scope possibl overhead measur examin specic instrument perform analysi conduct determin categori overhead individu code region instrument done fulli automat usercontrol direct postexecut perform analysi done base perform tracel novel represent code region name dynam code region call graph drg drg ect dynam relationship code region subregion enabl detail overhead analysi everi code region drg restrict function call also cover loop io commun statement etc allow analyz arbitrari code region vari singl statement entir program unit processor sequenti 1n 1p 1n 4p 2n 4p 3n 4p 4n4p 5n4p 6n4p data movement control parallel 0 0244258 659928 172419 289781 414966 564554 707302 tu 3139742 1726465 1835957 2047059 23549 299739 25173 3384 833775 1910787 310459 438749 5948315 732829 total execut time 316417 319801 87442 5866 57414 63651 75304 86467 tabl 3 overhead hpf version backward price applic u identi unidenti total overhead respect 1n 4p mean 1 smp node 4 processor processor 1n 1p 1n 4p 2n 4p 3n 4p 4n4p 5n4p 6n4p inspector work distribut 0000258 0000285 updat halo 0149 3114 9110 16170 24060 33868 43830 mpi init 0005 3462 8113 12784 17420 22568 26860 tabl 4 control parallel overhead hpf version backward price applic contrast exist approach frequent use call graph consid function call base prototyp implement scalea present sever experi realist code implement mpi hpf mix openmpmpi experi demonstr use scalea nd perform problem caus current integr scalea databas store deriv perform data moreov plan enhanc scalea perform specic languag order support automat perform bottleneck analysi sisprofil measur librari drg overhead prole extens tau prol ing capabl hope integr featur futur releas tau perform system featur oer portabl instrument tool access api r valid singl processor approach achiev larg scale comput capabili tie automat overhead pro vfc vienna fortran compil scalabl crossplatform infrastructur applic perform tune use hardwar counter hierarch classi callgraphbas search strategi autom perform diagnosi price constant matur floater embeed option use mont carlo simul gnu gprof call graph execut pro mpi standard messag pass ing cxperf user guid high perform fortran forum introduct parallel comput ingdesign analysi parallel algorithm perform technolog complex parallel distribut sy tem toward perform tool interfac openmp approach base direct rewrit vampir visual analysi mpi resourc vampirtrac 20 instal user guid portabl pro gescher system scalea version 10 user guid tr introduct parallel comput highperform portabl implement mpi messag pass interfac standard portabl profil trace parallel scientif applic use c executiondriven perform analysi distribut parallel system perform technolog complex parallel distribut system scalabl crossplatform infrastructur applic perform tune use hardwar counter paradyn parallel perform measur tool callgraphbas search strategi autom perform diagnosi distinguish paper hierarch classif overhead parallel program mpi standard messag pass gprof ctr thoma fahring clvi seragiotto jnior model detect perform problem distribut parallel program javapsl proceed 2001 acmiee confer supercomput cdrom p3535 novemb 1016 2001 denver colorado ming wu xianh sun grid harvest servic perform system grid comput journal parallel distribut comput v66 n10 p13221337 octob 2006