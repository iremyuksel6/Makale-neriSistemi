nest fgmre method parallel calcul nuclear reactor transient semiit method base nest applic flexibl gener minimum residualfgmr develop solv linear system result applic discret twophas hydrodynam equat nuclear reactor transient problem complex threedimension reactor problem decompos simpler manag problem recombin sequenti gmre algorithm mathemat method consist use inner level gmre solv precondition equat outer level gmre applic perform practic threedimension model oper pressur water reactor pwr serial parallel applic perform reactor model two differ detail core represent appropri tight converg enforc gmre level result semiit solver agreement exist direct solut method larger model test serial perform gmre factor 3 better direct solver parallel speedup 4 use 13 processor intel paragon thu larger problem order magnitud reduct execut time achiev indic use semiit solver parallel comput consider reduc comput load practic pwr transient calcul b introduct analysi nuclear reactor transient behavior alway one difficult comput problem nuclear engin comput load calcul detail threedimension solut field equat prohibit variat power flow temperatur distribut treat approxim reactor calcul result consider conservat reactor oper research estim use exist method comput load calcul threedimension distribut would exceed teraflop per time step 16 6 last sever year comput speed memori increas dramat motiv rethink limit exist reactor transient analysi code research begun adapt threedimension hydrodynam neutron kinet code advanc comput architectur begun investig advanc numer method take full advantag potenti high perform comput overal goal work report reduc comput burden threedimension reactor core model therebi enabl high fidel reactor system model specif object investig krylov subspac method parallel solut linear system result reactor hydrodynam equat follow section provid brief descript hydrodynam model reactor problem use work nest gmre method precondition develop work describ section 3 serial parallel applic present section 4 5 respect work support electr power research institut school nuclear eng purdu univers w lafayett 47907 2 hydrodynam model reactor problem nuclear reactor analysi problem involv solut coupl neutron kinet heat conduct hydrodynam equat twophas flow hydrodynam gener comput demand focu work hydrodynam method use consist reactor system code retran03 7 wide use nuclear industri analysi reactor transient behavior method base semiimplicit solut mass momentum equat phase energi equat fluid mixtur solut scheme use finit differ represent fluidflow balanc equat convect quantiti sourc term linear ie expand use first order taylor seri result coupl finit differ equat implicitli includ system coupl equat method consid semiimplicit sinc linear equat use rather origin nonlinear partial differenti equat standard newtonraphson techniqu use solv nonlinear equat 21 hydrodynam model applic spatial finit differenc set govern partial differenti equat perform use concept volum connect junction result system ordinari differenti differ equat may express dy dt 1 column vector nodal variabl f column vector function number dependentsolut variabl solut vector consist n j junction mass flow rate w slip veloc v sl volum total mass total energi u vapor mass g inventori 2 vector fy linear first order time differ approxim use result follow linear system ident matrix j matrix jacobian deltay deltat valu time level n1 n respect semiimplicit natur formul linear system aris linear discret tightli coupl also high degre stabil formul impos less stringent limit size time step indic equat 3 larger time step reduc diagon domin result illcondit linear system sever author note 5 3 illcondit linear system welldefin structur difficult solv effici mani case special handl requir obtain accept solut present linear solver retran03 direct method base gaussian elimin thetan 3 execut time complex n number unknown model domin onedimension flow direct solut complement type matrix reduct techniqu effici howev case high fidel model reactor three dimension core rep resent direct method becom ineffici demonstr use model standard 4loop presur water reactor aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa aaa system model model see model side view top view channel upper plenum 26 33 28 14 31 17 43 23 48 36 343111 355769 7177737579656 4 pass fig 1 nodal pwr model 22 pressur water reactor problem reactor system model use work consist 4loop pressur water reactor model detail three dimension represent reactor core schemat shown figur 1 loop system contain steam gener heat transfer pressur primari loop secondari loop contain steam turbin core model consist sever volum stack one upon along channel assum layout c c channel axial volum number volum ac 2 number cross flow junction number vertic junction c 2 a1 first problem use work core model 9 channel 12 axial level per channel shown figur construct two version core model one cross flow horizont flow volum without cross flow cross flow import reactor transient break steam line one loop result horizont mix hot cold water core model cross flow consist 173 volum 339 junction result linear system size 1173 core model cross flow contain 108 volum 261 junction form nearli 70 system model without cross flow contain number volum 183 junction result linear system size 885 coeffici matrix problem cross flow shown schemat figur 2 structur shown figur correspond sequenti order junction volum variabl conveni reductionelimin method current use code reorder suitabl precondit discuss follow section column row fig 2 pwr model 3d core sparsiti pattern coeffici matrix initi test problem simpl core reactiv event model simul insert withdraw control rod 20 second rod insert core rate 008 dollar reactiv per second 5 second immedi withdrawn rate comput perform problem singl processor intel paragon summar tabl 1 model without cross flow direct linear system solut perform solv modul indic tabl tabl computatin summari pwr exampl problem direct linear solver modul wo cross flow w cross flow cpu time percent cpu time percent 15589 467 19617 49 total 33388 1000 493171 1000 immedi appar tabl 1 use cross flow core increas comput time order magnitud primarili problem without cross flow result linear system contain predominantli tridiagon submatric lend effici reductionelimin method larg increas execut time discourag model cross flow gener high fidel reactor simul although reduct oper count requir direct solut primari motiv work reason consid iter linear solut method first actual problem solv retran03 nonlinear direct solut result linear equat wast float point oper sinc usual accuraci consider less machin precis adequ secondli unlik direct method semiit solut method acceler inform previou time step initi guess precondition final direct method lend easili parallel comput distribut memori mimd multicomput wherea mani new semiit linear solver effici parallel 3 semiit linear solver 3d hydrodynam last sever year consider research perform nonstationari class techniqu collect known krylov subspac method includ classic conjug gradient cg method shown effici symmetr posit definit system equat method call krylov method base build solut vector krylov sub space span fr residu initi solut coeffici matrix coeffici solut vector case cg method base minim energi norm error gen eral linear system encount hydrodynam problem symmetr posit definit therefor solv use cg method numer krylov method solv nonsymmetr problem propos year sever consid work includ gener minim residualgmres10 method biconjug gradient bicg method 1 conjug gradient squar cg method 12 biconjug gradient stabil bicg method 15 particular interest work gmre method solv minim problem iter therefor guarante monoton decreas residu well known converg rate krylov method depend spectral properti coeffici matrix proper precondit consider improv rate converg precondit involv addit cost initi per iter tradeoff cost implement precondition gain converg speed sinc mani tradit precondition larg sequenti compon tradeoff serial perform precondition parallel effici sever altern precondition examin work 31 applic krylov method reactor problem previou work applic semiit solver reactor core hydrodynam calcul 13 8 focus primarili solut linear pressur equat result singl phase onedimension flow problem krylov method found perform well result tridiagon system equat particular turner achiev excel converg conjug gradient squar method ilu precondition work provid use insight linear system result twophas threedimension flow problem significantli differ perform variou krylov method differ figur 3 show behavior absolut residu l2 norm applic pwr problem variou krylov method precondition perform bicgstab bicg cg irregular cg shown residu increas sever order magnitud first iter continu increas gmre method demonstr accept behavior monoton decreasingli residu expect basi minim principl linear system time step examin behavior similar shown figur 3 observ although float point oper count per iter higher gmre particularli larger number iter attract applic inher robust precondit techniqu examin could improv converg behavior gmre 32 domain decomposit precondit knowledg physic characterist system invalu choos good precondition evid figur 1 core excor system interact upper lower plenum suggest natur way decompos problem domain system reorder solut variabl belong core place contigu structur result matrix given ac u block l u repres interact core excor vari abl core excor interact upper lower plenum matric non zero element sever option exist use equat 4 precondition one possibl precondition neglect l u block entir result block jacobi precondition given ac gmre iter residu fig 3 figur perform krylov solver pwr 3d core problem block ae repres interact variabl excor region block ac repres interact variabl core region precondition jacobi two block may solv individu attract parallel comput note earlier size core block consider larger excor block later section method discuss solv core problem block jacobi domain decomposit precondition appli 3d pwr exampl problem first use direct solut precondition equat tabl 2 contain result linear system first two time step transient describ section 22 first case correspond problem result shown figur 3 addit number iter two measur effect precondition shown tabl first condit origin precondit matrix shown second third column fourth column shown anoth measur effect precondit suggest dutto4 use ratio frobeniu norm remaind matrix defin frobeniu norm origin matrix frobeniu norm defin diagon shown tabl measur indic jacobi precondition effect toler 10 gamma6 rel residu converg achiev less tabl result applic domain decomposit pwr problem precondition kakf iter 219 gauss seidel 830 219 modifi gauss seidel 219 block jacobi precondition impact submatric l u complet neglect solv precondition equat anoth possibl employ gaussseidel like techniqu precondition equat solv sequenti z c z c z c z e respect two approach examin solv equat precondition one approach first equat solv neglect coupl ie z c symmetri sequenc solut matter precondition form ac result appli techniqu shown gaussseidel precondition tabl 2 indic minor chang measur effect precondit howev converg achiev fewer iter second approach estim excor solut z e form use decoupl excor equat ae use solv sequenti core excor equat given equat 7 precondition approach express approach shown tabl 2 modifi gausssiedel precondition indic littl differ observ effect precondi tion block jacobi method natur parallel use work even though gausssiedel method show slightli better numer perform result shown precondition equat solv directli use gaussian elimin excor problem predominantli onedimension flow direct solut use reductionelimin prove effici con vers threedimension core model lend direct solut follow section examin use second level gmre solv core problem 33 precondit core problem origin matrix order shown figur 2 conduc precondit core problem ac z sever altern order examin junction physic volum reorder solut vector bear resembl physic layout would help decreas profil matrix goal increas densiti matrix region around diagon ie reduc bandwidth use portion matrix precondit purpos eg block jacobi etc structur exist core exploit defin supernod consist volum junction physic domain discret sever supernod introduc homogen structur matrix supernod could consid fractal repres smallest unit structur present system shown figur 4 supernod core problem consist volum 3 junction one junction vertic junction upstream volum two crossflow junction lead volum differ direct note use supernod lead problem extra junction exterior channel junction dummi junction repres matrix appear solut henc size system increas condit number remain volum vertic junction crossflow junction crossflow junction fig 4 structur supernod sever order supernod consid eg channelwis plane wise cuthillmcke etc order planewis found best purpos precondit plane repres two dimension grid supernod plane form one dimension structur link two neighbor result block tridiagon matrix shown figur 5 equat 10 case outer level precondition sever option exist use equat 10 precondition inner level gmre 12002006001000column row fig 5 structur coeffici matrix plane wise order block diagon precondition neglect planar coupl consid first tabl 3 show result solv second level gmre first outer iter four differ block size examin converg criterion 10 gamma6 two case shown 1 c 2 c taken differ time step tabl result applic jacobi precondit inner level precondition block size condit krkf kakf iter condit krkf kakf iter result outer iter slightli differ sinc sourc henc initi residu second level gmre differ howev result case consist gener trend shown tabl 3 expect larger block size reduc number iter howev cost solv block directli would increas n 3 offset reduct iter also smaller block advantag scalabl parallel comput domain decomposit scheme incorpor interact diagon block also examin approxim solut z c comput solv block jacobi system neglect coupl adjac plane z 0 prime notat use distinguish z c r c vector occur outer iter equat 7 inner gmre precondition equat solv precondition md indic tabl 4 improv precondit reduc number iter solv precondition becom expens number iter block size 81 reduc half twice number float point oper requir form md tabl result applic domain decomposit precondit inner level kakf iter block size precondition block jacobi investig core problem primarili illcondit natur coeffici matrix popular scheme ssor ilu ineffect exampl incomplet lu factor scheme test linear system aris 3d core prob lem band construct frobeniu norm ilu precondition compar norm exact invers shown frobeniu norm approxim precondit system gamma1 three order magnitud larger frobeniu norm gamma1 suggest ilu precondition would effect 34 nest gmre method describ previou section implement nest gmre algorithm consist use inner level gmre solv precondition equat outer level gmre strategi suggest van der vorst demonstr success sever model problem 14 precondition inner level gmre could solv use third level gmre applic direct solver prove effici schemat nest gmre algorithm shown figur 6 physic interpret algorithm view overal problem decompos three simpler manag problem recombin sequenti gmre algorithm highest level take advantag natur loos coupl core excor compon solv separ linear system core excor region solut recombin use highest outer level gmre second inner level gmre use solv 3d core flow problem focu coupl vertic flow channel core final third lowest level gmre direct solver use restor coupl node plane core problem system problem gmre gmre ii gmre iii direct e z c z fig 6 nest gmre algorithm retran03 linear system solut 35 flexibl gener minimum residu fgmre solut precondit equat gmre method anoth gmre algorithm pose potenti problem due finit precis inner level solut precondit gmre algorithm solut first built precondit subspac transform solut space matrix set orthonorm vector case iter solut precondition transform approxim extent determin converg criterion case illcondit matric approxim could especi troublesom tight converg requir minim error propag problem inexact transform allevi work use slight variant gmre algorithm extra set vector store use updat solut modif gmre algorithm suggest saad call flexibl gener minimum residualfgmr 9 algorithm allow complet variat precondition one iter next store result precondit basi vector use krylov subspac instead use equat 15 final transform solut subspac perform use matrix fgmre algorithm given appendix implement nest gmre method 4 serial applic 41 static problem nest gmre algorithm first appli linear system aris first time step rod withdrawalinsert transient parametr perform converg criterion number iter level maximum iter limit set inner gmre sinc case rate converg slow sometim term critic slow variat outer highest level residu iter shown figur 7 differ maximum number inner iter miter result indic rate decreas residu substanti greater miter 20 howev shown figur 8 result subsequ timestep indic differ rate decreas residu miter gradual diminish residu achiev inner second level gmre timestep correspond figur 8 shown figur 9 maximum iter limit residu increas first iter one possibl explan desir search direct outer level gmre becom harder resolv lead degrad perform inner level howev diminish qualiti precondit inner level gmre appear deleteri effect converg outer iter base converg result analysi time step strategi formul implement nest gmre transient analysi code retran03 follow section discuss result appli gmre transient problem outer iter number outer residu fig 7 reduct outer residu first time step outer iter number outer residu fig 8 reduct outer residu subsequ time step outer iter number inner residu fig 9 perform inner gmre subsequ time step 42 transient problem timedepend iter solut hydrodynam equat error incomplet converg one time step propag coeffici matrix subsequ timestep preliminari assess effect converg criteria qualiti solut perform use null transient steadyst condit continu sever second disturb system sever outer level converg criteria investig accept perform achiev toler 10e 09 rel residu higher toler minor deviat observ eg less 1 rel error perform paramet core power level provid initi guidanc set toler iter limit transient problem pwr rod withdrawalinsert transient describ section 22 analyz use retran03 nest fgmre model cross flow core studi use iter solver transient time 20 second requir step perform iter algorithm analyz first vari number outer iter use direct solver inner eg problem vari number outer inner gmre iter result shown tabl 5 cpu second per time step maximum rel residu error outer iter shown fourth column tabl purpos comparison retran03 solut tabl 1 use direct solut linear system repeat case a1 tabl 5 first two case a2 a3 inner two gmre level see figur 6 replac direct solver gmre use outer highest level iter object isol impact number outer iter tabl serial perform retran gmre 9 channel 12 axial case iter number cpu secstim step case outer inner max krk a1 direct direct gammagamma 318 1489 1565 a3 12 direct 4 a4 a6 qualiti solut seen tabl 5 maximum residu error decreas number outer iter increas howev case a2 a3 signific error observ import physic paramet number outer iter reduc four minor deviat began occur solut 15 second transient note execut time case a2 a3 gener compar direct solver gmre algorithm employ outer inner iter keep direct solut innermost third level order gain insight relat converg inner outer gmre algorithm number outer inner iter vari shown tabl 5 case a4 a5 a6 a7 accuraci solut examin import physic paramet solut normal core power pressur pressur volum 58 figur 1 plot versu time figur 10 direct solut a1 gmre solut a4 a7 minor deviat observ solut iter execut time greater direct a1 gmre outer direct inner a3 solut howev discuss next section algorithm inner level gmre attract larger problem parallel comput 5 parallel applic one attract featur precondit krylov method potenti parallel comput emphasi work use distribut memori parallel architectur applic perform intel paragon section describ map nest gmre onto paragon execut time reduct achiev pwr sampl problem natur map processor pwr model one processor excor one 12 plane core model matric stripe row wise impli l ae equat 4 store processor assign excor ideal ac u partit among 12 processor repartit data found expens henc copi ac u maintain pe oper distribut among processor one primari concern distribut data comput parallel process commun overhead incur transfer data pro cessor time necessari perform transfer consist two part time necessari initi transfer refer latenc time necessari actual transfer data depend amount data core power retran solut 20 inner 20 outer psia retran solut 20 inner 20 outer fig 10 result retran03 gmre pwr transient gmre inner 40 sec machin bandwidth follow section discuss parallel inner outer level nest algorithm special emphasi given commun issu 51 parallel outer iter one domin oper outer iter matrix vector product iter product form matrix eq1 residu vector oper broken four part first part involv product ae v e involv commun sinc data resid processor next two part involv u v c l v e involv transfer part vector processor 0 assign excor processor 1 12 assign bottommost topmost plane core respect fourth part matrix vector product involv ac v c requir commun processor 1 12 two processor gener matrix vector product requir commun select processor specif pattern vector inner product hand requir global commun mean everi processor requir inform processor necessari sum product element first vector correspond element second vector element vector first multipli processor form partial sum element resid domain sum product requir individu product number transfer requir consider reduc employ well known method treesum commun cost vari dlog 2 n e oppos n case commun gramschimdt orthogon process use gmre involv sever inner product iter howev result inner product depend other therefor partial sum inner product perform one time reduc number transfer requir 2 least squar solut gmre involv reduc linear system involv suffici oper merit parallel 11 commun relat least squar problem avoid entir perform least squar solut simultan processor also estim residu gmre consequ least squar solut termin criterion could evalu without need addit commun 52 parallel inner iter sever oper requir parallel inner iter similar outer iter matrix ac stripe row wise row correspond plane store correspond processor plane link immedi neighbor matrix vector product requir two set data transfer vector inner product least squar solut treat manner outer level sinc precondition second level block jacobi precondition equat could implement without data transfer irrespect whether solv use direct solver iter solver 53 result pwr model 9 channel 12 axial core parallel version retran03 execut paragon 13 processor section present result rod withdraw case pwr model 9 channel12 axial volum core transient analyz 20 second nest fgmre solut perform use toler 10 gamma11 iter limit 20 inner outer iter tabl serial perform second level 9 channel12 axial time step outer inner serial execut time precond total tabl parallel perform second level 9 channel12 axial time step outer inner parallel execut time precond comm total speedup tabl 6 7 show result serial parallel implement respect inner second level gmre fourth column tabl head pre cond indic time requir precondit second level gmre iii direct solv figur 6 perform applic use direct solver mean lu factor first timestep involv addit initi cost consist comparison parallel serial version retran possibl differ code structur initi high speedup 622 first iter second timestep due lu factor perform concurr parallel implement note bulk time spent third level sinc part code natur paralleliz one would expect high effici howev sinc problem size rel small communicationoverhead implement remaind gmre significantli reduc effici would case larger problem seen next section tabl 8 9 show result serial parallel implement outer level gmre bulk time spent precondit similar execut time obtain timestep speedup slightli greater two obtain outer iter execut time first four time step summar tabl 10 speedup order two achiev serial perform outer level 9 channel12 axial timestep outer serial execut time core excor total tabl parallel perform outer level 9 channel12 axial timestep outer parallel execut time core excor comm total speedup 54 result pwr model 25 channel 12 axial core order examin scalabl result pwr model use instead 9 channel core linear system nearli three time size aris form 9 channel 12 axial case anticip shown tabl 11 time requir solv linear system use direct solver order magnitud larger 9 channel 12 axial case compar case a1 tabl 5 problem first execut use retran nest fgmre solver singl processor paragon problem larg execut ordinari node 32mb ram special fat node paragon use addit memori 128 mb support larger applic shown tabl 11 result singl processor encourag sinc factor 2 improv achiev gmre compar direct solver problem execut use parallel version nest gmre solver domain decomposit method exactli 9 channel 12 axial case 12 processor assign core 1 excor expect parallel effici much better speedup factor 4 respect serial version gmre tabl 11 show comparison result first two time step indic tabl 11 parallel execut nest gmre provid order magnitud reduct execut time compar serial execut direct solver furthermor sinc memori requir per node less 32mb parallel version could execut use standard size node paragon thu parallel key execut time reduct also allevi memori constraint larger problem 6 summari conclus nest fgmre method develop solv linear system result threedimension hydrodynam equat applic perform practic pressur water reactor problem threedimension core model serial parallel applic perform overal perform nest fgmre 9 channel12 axial timestep number execut time speedup outer serial parallel tabl execut time 25 channel 12 axial case case timestep execut time per iter outer total per inner timestep core excor precond direct solver 1 343310 9 channel 25 channel version reactor core appropri tight converg enforc gmre level semiit solver perform satisfactorili durat typic transient problem serial execut time 9 channel model compar direct solver parallel speedup intel paragon factor 23 use 13 processor 25 channel model serial perform nest gmre factor 3 better direct solver parallel speedup vicin 4 use 13 processor thu 25 channel problem order magnitud reduct execut time achiev result indic use semiit solver parallel comput consider reduc comput load practic pwr transient calcul furthermor result indic distribut memori parallel comput help allevi constraint size problem exe cute final method develop scalabl suggest within reach model pwr core 193 flow channel explicitli repres 7 acknowledg author appreci work mr jeni wu gener transient result report paper r march algorithm ellipt boundari valu problem reduc effect global commun gmresm cg parallel distribut memori comput direct method spars matric effect order precondit gmre algorithm numer method engin scientist supercomput appli nuclear reactor assess advanc numer method twophas fluid flow flexibl innerout precondit gmre algorithm gmre gener minim residu algorithm solv nonsymmetr linear system comparison precondit nonsymmetr krylov method largescal mimd machin perform conjug gradientlik algorithm transient twophas subchannel analysi gmresr famili nest gmre method comput challeng develop effici parallel algorithm datadepend comput thermalhydraul supercomput applic tr