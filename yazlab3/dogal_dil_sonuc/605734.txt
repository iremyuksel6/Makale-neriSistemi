parallel two level block ilu precondit techniqu solv larg spars linear system discuss issu relat domain decomposit multilevel precondit techniqu often employ solv larg spars linear system parallel comput implement parallel precondition solv gener spars linear system base two level block ilu factor strategi give new data structur strategi construct local coeffici matrix local schur complement matrix processor precondition construct fast robust solv certain larg spars matric numer experi show domain base two level block ilu precondition robust effici publish ilu precondition base schur complement techniqu parallel spars matrix solut b introduct high perform comput techniqu includ parallel distribut computa tion undergon gradual matur process past two decad move experiment laboratori studi mani engin scientif appli cation although share memori parallel comput rel easi program commonli use architectur parallel comput practic distribut technic report 30500 depart comput scienc univers kentucki lexington ky 2000 research support part us nation scienc foundat grant ccr 9902022 ccr9988165 part univers kentucki center comput scienc univers kentucki colleg engin email cshencsukyedu z email jzhangcsukyedu url httpwwwcsukyedujzhang memori comput use mpi pvm messag pass 17 20 even share memori parallel comput use mpi code portabl made distribut program style preval result develop effici numer linear algebra algorithm specif aim high perform comput becom challeng issu 9 10 mani numer simul model problem cpu consum part comput solv larg spars linear system accept solv larg spars linear system iter method becom method choic due favor memori comput cost compar direct solut method base gaussian elimin one drawback mani iter method lack robust ie iter method may yield accept solut given problem common strategi enhanc robust iter method exploit precondit techniqu howev robust precondition deriv certain type incomplet lu factor coeffici matrix effici implement parallel comput nontrivi challeng recent trend parallel precondit techniqu gener spars linear system use idea domain decomposit concept processor assign certain number row linear system solv discuss relat point view comparison differ domain decomposit strategi see 3 19 34 refer therein simpl parallel precondition deriv use simpl parallel iter method commonli use parallel precondition engin comput point block jacobi precondition 4 36 precondition easi implement effici sens number precondit iter requir solv realist problem still larg 35 sophist approach parallel precondit use domain decomposit schur complement strategi construct parallel precondition 34 precondition construct approach may scalabl ie number precondit iter increas rapidli number processor increas techniqu class includ variou distribut schur complement method solv gener spars linear system develop 2 5 28 27 spars matric aris finit differ discret partial differenti equat pde level set techniqu usual employ extract inher parallel discret scheme ilu0 factor perform forward backward triangular solv associ precondit parallel within level set approach seem suitabl implement share memori machin small number processor 11 mani realist problem unstructur mesh parallel extract level set strategi inadequ furthermor ilu0 precondition may accur enough subsequ precondit iter may converg slowli may converg thu higher accuraci precondition advoc author increas robust 8 21 37 45 24 30 howev higher accuraci precondition usual mean fillin entri kept precondition coupl among node increas well 24 increas coupl reduc inher parallel new order techniqu must employ extract parallel higher accuraci precondition addit standard domain decomposit concept precondit techniqu design specif target parallel comput includ spars approxim invers multilevel treatment 1 7 14 39 40 although claim inher parallel precon dition effici spars approxim invers techniqu run respect distribut parallel comput scarc 9 recent class high accuraci precondition combin inher parallel domain decomposit robust ilu factor scalabl potenti multigrid method develop 30 31 multilevel block ilu precondition bilum bilutm test show promis converg rate scalabl solv certain problem construct precondition base block independ set order recurs block ilu factor schur complement although class precondition contain obviou parallel within level parallel implement yet report studi mainli address issu implement multilevel block ilu precondition distribut environ use distribut spars matrix templat 26 bilutm precondition saad zhang 31 modifi implement two level block ilu precondition distribut memori parallel architectur pbilu2 use saad psparslib librari 1 mpi basic commun routin pbilu2 precondition compar one favor schur complement base precondition 27 numer experi articl organ follow section 2 background block independ set order bilutm precondition given section 3 outlin distribut represent gener spars linear system section 4 discuss construct precondition pbilu2 base two level block ilu factor numer experi comparison two schur complement base precondition solv variou distribut linear system present section 5 demonstr merit two level block ilu precondition conclud remark comment futur work given section 6 independ set bilutm distribut spars matrix solver reli classic domain decomposit concept partit adjac graph coeffici matrix graph partit 1 psparslib librari avail onlin httpwwwcsumneduresearcharpap sparslibpspabshtml algorithm softwar packag avail 16 18 22 techniqu extract parallel incomplet lu factor bilum bilutm usual relay fact mani row spars matrix elimin simultan given stage gaussian elimin set consist row call independ set 13 larg scale matrix comput degre parallel extract tradit point independ set order inadequ concept block independ set propos 30 thu block independ set set group block unknown coupl unknown two differ group block 30 variou heurist strategi find point independ set may extend find block independ set differ properti 30 simpl usual effici strategi socal greedi algorithm group nearest node togeth consid gener spars linear system form unstructur realvalu matrix order n greedi algorithm graph partition use find block independ set adjac graph matrix initi candid node block includ node correspond row matrix given block size k greedi algorithm start first node group nearest k neighbor node drop node link group k node vertex cover set vertex cover set set node least one link least one node least one block block independ set process repeat time candid node gone either one independ block vertex cover set number remain candid node less k put vertex cover set mean vertex cover set gener cover case detail algorithm descript see 30 remark necessari independ block number node 33 chosen cardin sake load balanc parallel comput sake easi program parallel implement graph partition similar greedi algorithm describ first invok partit adjac graph base result partit matrix correspond right hand side unknown vector b x distribut individu processor suppos block independ set uniform block size k found matrix symmetr permut two two block matrix form p permut matrix diagon matrix dimens ks number uniform block size k block usual dens k small spars k larg implement bilum exact invers techniqu use comput b gamma1 invert small independ note 33 direct invers strategi usual produc dens invers matric even origin block highli spars larg size sever sparsif strategi propos maintain sparsiti b gamma1 33 addit spars approxim invers base multilevel block ilu precondition propos 43 articl employ ilu factor strategi comput spars incomplet lu factor b approach similar one use bilutm 31 construct bilutm precondition base restrict ilu factor 2 dual drop strategi ilut 31 multilevel block ilu precondition bilutm retain robust flexibl ilut also power ilut solv difficult problem offer inher parallel exploit parallel distribut architectur 3 distribut spars linear system slu precondition distribut spars linear system collect set equat assign differ processor parallel solut spars linear system begin partit adjac graph coeffici matrix base result partit data distribut processor pair equationsunknown assign processor type distribut matrix data structur base subdomain decomposit concept propos 29 26 also see 28 base concept matrix assign processor unknown processor divid three type 1 interior unknown coupl local equat 2 local interfac unknown coupl nonloc extern local equat 3 extern interfac unknown belong subdomain coupl local equat submatrix assign certain processor say processor split two part local matrix act local variabl interfac matrix x act extern variabl accordingli local equat given processor written local matrix reorder way interfac point list last interior point local system written block format n indic subdomain neighbor refer subdomain exactli set processor refer processor need commun receiv inform part product x iext reflect contribut local equat neighbor subdomain j sum contribut result multipli x extern interfac unknown ie precondition built upon distribut data structur origin matrix form approxim global schur complement explicitli domain decomposit base precondition exploit 27 simplest one addit schwarz procedur form block jacobi bj iter block refer submatric associ subdomain ie even though construct easili block jacobi precondit robust ineffici compar schur complement type precondition one best among schur complement base precondition slu distribut approxim schur lu precondition 27 precondit global matrix defin term block lu factor involv solv global schur complement system precondit step incomplet lu factor use slu approxim local schur complement numer result report 27 show schur ilu precondition demonstr superior scalabl perform block jacobi precondition effici latter term parallel run time 4 class two level block precondit techniqu pbilu2 two level block ilu precondition base bilutm techniqu describ 31 note bilutm offer good parallel robust due larg size block independ set graph partition bilutm greedi algorithm find block independ set 30 31 41 distribut matrix base block independ set implement block size block independ set must given search algorithm start choic block size k base problem size densiti coeffici matrix choic k may also depend upon number avail processor assum block independ set uniform block size k found coeffici matrix permut block form small independ block divid sever group accord number avail processor sake load balanc processor group hold approxim number independ block number independ block differ group may differ 1 time global vector unknown x split two subvector right hand side vector b also conform split subvector f g reorder lead block systemb bm fm um number processor use comput block diagon contain sever independ block note submatrix f row number block submatrix b submatric e c also divid part accord load balanc criterion order approxim amount load processor e c also row number submatric assign processor u local part unknown vector f g local part right hand side vector partit assign certain processor time matrix distribut processordata assign done processor hold sever row equat local system equat processor written u part unknown subvector act submatric anoth part unknown subvector act f c b act complet local vector u take u local unknown complet interior vector precondition base type block independ set order domain decomposit differ straightforward domain decomposit base rowwis strip partit 3 use 27 obviou differ partit 3 5 5 action f complet local local 3 howev sinc natur submatric differ two decomposit strategi easi say one better stage 42 deriv schur complement techniqu key idea domain decomposit techniqu develop precondition global system 1 exploit method approxim solv schur complement system parallel parallel construct pbilu2 precondition base block independ set domain decomposit comput approxim global schur complement describ deriv global schur complement anoth part coeffici matrix need partit sent certain processor rewrit reorder coeffici matrix system 2 bm fm thu two way partit submatrix e one partit e row column submatric also assign processor number column block diagon submatrix number row submatrix c remark 41 clear potenti confus two represent submatrix e row partit e 4 use repres local matrix form 5 differ local matrix 3 kept throughout comput process column partit e 6 conveni comput schur complement matrix parallel column partit e kept construct schur complement matrix case submatrix e small highli spars b larg consid block lu factor 2 form 0 global schur complement suppos invert b mean rewrit equat 8 fmc processor comput one compon sum independ partit row submatrix part row must conform submatrix c scatter processor global commun need scatter final local part schur complement matrix construct independ processor simplest implement approach construct distribut schur complement matrix incomplet lu factor use parallel block restrict ikj version gaussian elimin similar sequenti algorithm use bilutm 31 method decreas commun among processor offer flexibl control amount fillin ilu factor 43 parallel restrict gaussian elimin bilutm high accuraci precondition base incomplet lu factor util dual drop strategi ilut control comput storag memori cost 24 implement base restrict ikj version gaussian elimi nation discuss detail 31 remain part subsect outlin parallel implement restrict ikj version gaussian elimin use distribut data structur discuss previou subsect ith processor local submatrix form base submatric assign processor ilu factor local matrix perform 2 local matrix processor look like pbilu2 local matrix mean store local processor necessarili mean act interior unknown note submatrix c size submatrix c equat 7 submatrix element correspond nonzero entri submatrix may zero other zero element recal permut matrix 2 left hand side 7 let submatric c submatrix c obtain perform restrict gaussian elimin local matrix 10 slightli differ elimin procedur first perform ilu factor gaussian elimin upper part local matrix ie submatrix b f continu gaussian elimin lower part elimin perform respect nonzero accept fillin entri submatrix entri modifi accordingli perform oper lower part upper part matrix access modifi see figur 1 f process access process access access modifi access modifi access modifi figur 1 illustr restrict ikj version gaussian elimin submatric respect equat 10 done three kind submatric form use later iter 1 upper part matrix upper part gaussian elimin ub l gamma1 2 upper part matrix also perform ilu factor block diagon submatrix b b lb thu extract submatric lb ub upper part factor local matrix later use 3 restrict factor lower part obtain new reduc submatrix repres c form piec global schur complement matrix fact submatrix note b gamma1 f factor matrix l gamma1 f alreadi avail factor upper part comput processor first solv auxiliari matrix q ub follow matrixmatrix multipl q howev part comput done implicitli restrict ikj gaussian elimin process sens comput construct piec schur complement matrix processor done restrict ilu factor lower part local matrix word c form without explicit linear system solv matrixmatrix multipl detail comput procedur see 31 consid equat 9 schur complement comput rewritten form comput done parallel thank block diagon structur b gaussian elimin exact factor global schur complement matrix form sum submatric togeth submatrix part use partit origin submatrix c correspond part scatter relev processor receiv sum part submatric scatter differ processor local schur complement matrix form local mean row global schur complement held given processor note remark 42 restrict ikj gaussian elimin yield block ilu factor local matrix 10 form howev submatric l gamma1 longer need later comput discard strategi save consider storag space differ current implement slu psparslib librari 25 29 27 44 induc global precondition possibl develop precondition global system 1 exploit method approxim solv reduc system 8 techniqu base reorder global system two two block form 2 consid block lu factor equat 7 block factor matrix precondit approxim lu factor 0 approxim global schur complement matrix form 12 therefor global precondit oper induc schur complement solv equival solv lu f forward solv l backward substitut u comput procedur would consist follow three step g use auxiliari 1 comput schur complement right hand side 2 approxim solv reduc system 3 back substitut u variabl ie solv step comput parallel processor commun boundari inform exchang among processor matrix partit approach differ one use 27 need commun among processor comput global schur complement right hand side g processor easi see mc b mc gammab local schur complement right hand side comput way rewrit approxim reduc schur complement system submatrix x ij boundari matrix act extern variabl numer way solv reduc system one option consid 27 start replac approxim system form local approxim local schur complement matrix formul view block jacobi precondit version schur complement system 13 system solv iter acceler gmre requir solv step current implement ilut factor perform purpos block jacobi precondit third step schur complement precondit perform without problem sinc b block diagon solut comput parallel iter step processor actual solv lb factor lb avail need exchang boundari inform among processor sinc compon requir f processor 5 numer experi numer experi compar perform previous describ pbilu2 precondition distribut schur complement lu slu precondition 27 solv spars matric discret two dimension convect diffus problem applic problem comput fluid dynam comput carri 32 processor 200 mhz subcomplex 64 processor hp exemplar 2200 xclass supercomput univers kentucki 8 super node interconnect high speed low latenc network super node 8 processor attach supercomput total 16 gb share memori theoret oper speed 51 gflop use mpi librari interprocessor commun major part code mainli written fortran 77 program languag c routin handl dynam alloc memori mani commun subroutin slu precondition code taken psparslib librari 25 tabl contain numer result n denot dimens matrix nnz repres number nonzero spars matrix np number processor use iter number precondit fgmre iter outer erat ftime cpu time second precondit solut process fgmre ptime total cpu time second solv given spars matrix start initi distribut matrix data processor master processor processor 0 ptime includ graph partit time initi permut time associ partit done sequenti processor 0 thu ptime includ matrix distribut local reorder precondition construct iter process time ftime sratio stand sparsiti ratio ratio number nonzero precondition number nonzero origin matrix k block size use pbilu2 p number nonzero allow l u factor ilu factor drop toler p mean use saad ilut 24 precondition use flexibl variant restart gmre fgmre 23 solv origin linear system sinc acceler permit chang precondit oper step current case sinc use iter process approxim solv schur complement matrix outer fgmre iter size krylov subspac set 50 linear system form assum exact solut vector unit initi guess random vector compon 0 1 converg achiev 2norm residu approxim solut reduc 6 order magnitud use innerout iter process maximum number outer precondit fgmre iter 500 inner iter solv schur complement system use gmres5 without restart block jacobi type precondition inner iter stop 2norm residu inner iter reduc 100 number inner iter greater 5 51 5point 9point matric first compar parallel perform differ precondition solv 5point 9point matric 5point 9point matric gener discret follow convect diffus equat two dimension unit squar socal reynold number convect coeffici chosen px expgammaxi right hand side function use sinc gener artifici right hand side spars linear system state 5point matric gener use standard central differ discret scheme 9point matric gener use fourth order compact differ scheme 15 two type matric use test bilum ilu type precondition 30 31 41 comparison result parallel iter solver report cpu time result iter number howev gener difficult make fair comparison two differ precondit algorithm without list resourc cost achiev given result sinc accuraci precondition usual influenc fillin entri kept memori storag cost precondition import indic effici precondition precondition use memori space gener faster use less memori space good precondition use much memori space still achiev fast converg end report paper number precondit iter parallel cpu time precondit solut process parallel cpu time entir comput process sparsiti ratio first chose 0 5point matrix block size chosen 200 drop paramet chosen slu use one level overlap among subdomain suggest 27 test result list tabl 1 found pbilu2 precondition faster slu precondition 27 solv problem pbilu2 take smaller number iter converg slu converg rate pbilu2 slu strongli affect number processor employ indic good scalabl respect parallel system two precondition moreov pbilu2 took much less parallel cpu time slu need half memori space consum slu solv matrix see remark 42 explan differ storag space pbilu2 slu 3 also test matrix smaller valu case report two test case slu one level overlap subdomain nonoverlap subdomain test result list tabl 2 experi found 3 sparsiti ratio pbilu2 slu measur storag space use store precondition may case storag space slu could releas howev sparsiti ratio slu report articl base slu code distribut psparslib librari version 30 download httpwwwcsumneduresearcharpap sparslibpspabshtml novemb 1999 tabl 1 5point matrix one level overlap slu precondition np iter ftime ptime sratio pbilu2 slu 34 3422 5437 1202 pbilu2 tabl 2 5point matrix one level overlap nonoverlap result bracket slu precondition np iter ftime ptime sratio slu 44 50 3147 5159 915 925 overlap nonoverlap subdomain make much differ term parallel run time parallel cpu time overlap case report tabl 2 observ agreement made 27 howev overlap version slu converg faster nonoverlap version iron nonoverlap version slightli larger sparsiti ratio storag space precondition primarili determin dual drop paramet p overlap make local submatrix look larger thu reduc sparsiti ratio rel number nonzero coeffici matrix remark pbilu2 seen converg faster take less parallel run time slu overlap nonoverlap solv 5point matrix use given paramet sinc cost perform overlap nonoverlap slu close report result overlap version slu remain numer test compar result tabl 1 2 see higher accuraci pbilu2 precondition use larger p perform better lower accuraci pbilu2 term iter count parallel run time higher accuraci one cours take memori space store slu precondition overlap test 27 compar precondition bj block jacobi si pure schur complement iter sapinv distribut approxim block lu factor spars approxim etc numer experi 27 show slu retain superior perform bj si precondition compar schur complement precondit local schur complement invert sapinv ever parallel pbilu2 precondition shown effici slu explain commun cost pbilu2 experiment data correspond numer result tabl 2 exampl total parallel comput time ptime 1803 second pbilu2 commun time construct schur complement matrix 145 second commun construct pbilu2 case cost 804 total parallel comput time 4 total parallel comput time ptime 11425 second commun time 072 second commun time construct pbilu2 063 total parallel comput time cost commun construct pbilu2 precondition high also use larger n vari gener larger 5point 9point matric comparison result given tabl 3 4 result compar result list tabl 1 2 howev parallel run time ptime slu tabl 3 4 increas dramat tripl number processor increas 24 32 result 5point matrix given tabl 5 see pbilu2 perform much better slu furthermor scalabl slu degener test problem number iter 13 4 processor use increas 22 processor use pbilu2 precondition number iter almost constant 12 number processor increas 4 32 larg ptime result slu especi distribut larg amount data given parallel comput use partit strategi slu may present problem anoth set test run solv 9point matrix parallel iter time ftime respect differ number processor pbilu2 slu plot figur 2 pbilu2 solv 9point matrix faster slu figur 3 number precondit fgmre iter pbilu2 slu compar respect number tabl 3 9point matrix gamma4 one level overlap slu precondition np iter ftime ptime sratio slu 19 1676 4666 450 pbilu2 slu 19 2431 15124 451 tabl 4 5point matrix gamma4 one level overlap slu precondition np iter ftime ptime sratio tabl 5 5point matrix one level overlap slu precondition np iter stime ptime sratio k slu 19 4180 10168 711 slu 22 3522 163028 710 time number processor dash line slu iter solid line pbilu2 iter figur 2 comparison parallel iter time ftime pbilu2 slu precondition solv 9point matrix paramet use processor employ solv 9point matrix figur 3 indic converg rate pbilu2 improv number processor increas converg rate slu deterior number processor increas summar comparison result subsect 5point 9point matric finit differ discret convect diffus problem test seen pbilu2 need less half storag space requir slu paramet chosen compar storag space consum slu pbilu2 still outperform slu faster converg rate less parallel run time meanwhil see number processor increas parallel cpu time decreas number iter affect significantli pbilu2 52 fidap matric set test matric extract test problem provid fidap packag 12 4 mani matric small zero diagon difficult solv standard ilu precondition 42 test 31 fidap matric precondition found pbilu2 solv twice mani fidap matric slu test pbilu2 solv 20 fidap matric slu solv 9 test show parallel two level block ilu precondition 4 matric avail onlin matrixmarket nation institut standard technolog httpmathnistgovmatrixmarket number number processor dash line slu iter solid line pbilu2 iter figur 3 comparison number precondit fgmre iter pbilu2 slu precondition solv 9point matrix paramet use robust slu precondition approach also shown merit term smaller construct parallel solut cost smaller memori cost smaller number iter compar slu precondition sake breviti list result three repres larg test matric tabl 6 7 figur 4 note tabl 6 mean precondit iter method converg number iter greater 500 vari paramet fillin p drop toler tabl 6 precondition adjust size block independ set pbilu2 approach pbilu2 clearli shown robust slu solv fidap matrix fidap035 matrix larger fidapm29 test also adjust fillin drop toler paramet p 50 slu pbilu2 test result pbilu2 converg report tabl 7 note small valu requir ilu factor seem difficult slu converg test problem paramet pair list tabl 7 paramet pair test slu result list tabl 7 figur 4 show parallel iter time ftime respect number processor pbilu2 solv fidap019 matrix see parallel iter time decreas number processor increas demonstr good speedup solv unstructur gener spars matrix even fidap matric pbilu2 slu converg pbilu2 usual show superior perform slu term number iter tabl precondition np p iter ftime ptime sratio tabl 7 fidap035 matrix precondition np p k iter ftime ptime sratio 28 169 413 396 iter time number processor solid line pbilu2 iter figur 4 parallel iter time ftime pbilu2 fidap019 matrix paramet use pbilu2 900 sparsiti ratio approxim 345 tabl 8 flat10a matrix precondition np k iter ftime ptime sratio tabl 9 flat30a matrix precondition np k iter sratio sparsiti ratio 53 flat matric flat matric fulli coupl mix finit element discret three dimension navierstok equat 4 44 5 flat10a mean matrix first newton step nonlinear iter 10 element x coordin direct 1 element z coordin direct one element z coordin direct limit comput memori use gener matric explan hold flat30a matrix use 30 element x coordin direct matric gener keep variabl structur coupl navierstok equat may nonzero entri actual numer zero valu note two matric actual symmetr sinc first newton step veloc vector set zero howev symmetri inform util comput see tabl 8 9 pbilu2 abl solv two cfd matric small valu two matric difficult slu converg small sparsiti ratio reflect previou remark two flat matric mani numer zero entri ignor threshold base ilu factor count toward sparsiti ratio calcul matric fulli coupl mix finit element discret navierstok equat notori difficult solv precondit iter method 6 44 standard ilu type precondition tend fail produc unstabl factor unless 5 flat matric avail second author variabl order properli 44 suitabl order difficult implement sequenti environ 6 44 seem howev nontrivi task perform analog order parallel environ 6 conclud remark futur work implement parallel two level block ilu precondition base schur complement precondit discuss detail distribut small independ block form subdomain processor gave comput procedur construct distribut schur complement matrix parallel compar parallel precondition pbilu2 scalabl parallel two level schur lu precondition publish recent numer experi show pbilu2 demonstr good scalabl solv larg spars linear system parallel comput also found pbilu2 faster comput effici slu test case pbilu2 also effici term memori consumpt sinc use less memori space slu achiev better converg rate fidap flat matric test section 52 53 small zero main diagon entri poor converg perform pbilu2 slu mainli due instabl associ ilu factor matric diagon threshold strategi 32 38 employ pbilu2 exclud row small diagon submatrix b ilu factor stabl parallel implement diagon threshold pbilu2 investig futur studi plan extend parallel two level block ilu precondition truli parallel multilevel block ilu precondition futur research also plan test parallel precondition emerg high perform comput platform pc cluster r mpi implement spai precondition t3e parallel nonoverlap domain decomposit algorithm compress fluid flow problem triangul main comparison domain decomposit ilu precondit iter method nonsymmetr ellipt problem parallel finit element solut threedimension rayleigh benardmarangoni flow parpr parallel precondition packag refer manual version 20 precondit conjug gradient method incompress navierstok equat priori sparsiti pattern parallel spars approxim invers precondi tioner toward cost effect ilu precondition high level fill numer linear algebra highperform comput develop trend parallel solut linear system parallel ilu0 precondition cfd problem sharedmemori comput fidap exampl manual comput solut larg spars posit definit system parallel precondit approxim invers connect machin singl cell high order scheme convectiondiffus equat variabl coeffici chaco user guid scalabl parallel comput parallel multilevel kway partit scheme irregular graph comparison domain decomposit techniqu ellipt partial differenti equat parallel implement introduct parallel comput direct method spars matric partit spars matric eigenvector graph flexibl innerout precondit gmre algorithm ilut dual threshold incomplet lu precondition parallel spars matrix librari p sparslib iter solver modul iter method spars linear system distribut schur complement techniqu gener spars linear system domain decomposit multilevel type techniqu gener spars linear system design iter solut modul parallel spars matrix librari p sparslib bilum block version multielimin multilevel ilu precondition gener spars linear system bilutm domainbas multilevel block ilut precondition gener spars matric diagon threshold techniqu robust multilevel ilu precondition gener spars linear system enhanc multilevel block ilu precondit strategi gener spars linear system domain decomposit parallel multilevel method ellipt partial differenti equat high perform precondit parallel comput incompress flow materi process numer experi diagon precondit applic spars matrix solver effect precondition multilevel dual reorder strategi robust incomplet lu factor indefinit matric paralleliz precondition base factor spars approxim invers techniqu spars approxim invers parallel precondit spars matric precondit iter method finit differ scheme convectiondiffus precondit krylov subspac method solv nonsymmetr matric cfd applic spars approxim invers multilevel block ilu precondit techniqu gener spars matric perform studi incomplet lu precondition solv linear system fulli coupl mix finit element discret 3d navierstok equat use iter refin solut spars linear system tr comparison domain decomposit techniqu ellipt partial differenti equat parallel implement high perform precondit applic spars matrix solver effect precondition partit spars matric eigenvector graph introduct parallel comput flexibl innerout precondit gmre algorithm toward costeffect ilu precondition highlevel fill domain decomposit parallel comput incompress flow materi process parallel multilevel seri ikiway partit scheme irregular graph develop trend parallel solut linear system precondit iter method finit differ scheme convectiondiffus distribut schur complement techniqu gener spars linear system priori sparsiti pattern parallel spars approxim invers precondition spars approxim invers multilevel block ilu precondit techniqu gener spars matric enhanc multilevel block ilu precondit strategi gener spars linear system scalabl parallel comput numer linear algebra high perform comput comput solut larg spars posit definit multilevel dual reorder strategi robust incomplet lu factor indefinit matric iter method spars linear system ctr chi shen jun zhang fulli parallel block independ set algorithm distribut spars matric parallel comput v29 n1112 p16851699 novemberdecemb jun zhang tong xiao multilevel block incomplet choleski precondition solv normal equat linear least squar problem korean journal comput appli mathemat v11 n12 p5980 januari chi shen jun zhang kai wang distribut block independ set algorithm parallel multilevel ilu precondition journal parallel distribut comput v65 n3 p331346 march 2005