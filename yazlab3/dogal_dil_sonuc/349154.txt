upper lower bound learn curv gaussian process paper introduc illustr nontrivi upper lower bound learn curv onedimension guassian process analysi carri emphasis effect induc bound smooth random process describ modifi bessel squar exponenti covari function present explan earli linearlydecreas behavior learn curv bound well studi asymptot behavior curv effect nois level lengthscal tight bound also discuss b introduct fundament problem system learn exampl estim amount train sampl need guarante satisfactori generalis capabl new data theoret interest also vital practic import exampl algorithm learn data use safetycrit system reason understand generalis capabl obtain recent year sever author carri analysi issu result present depend theoret formalis learn problem approach analysi generalis includ base asymptot expans around optim paramet valu eg aic akaik 1974 nic murata et al 1994 probabl approxim correct converg approach eg vapnik 1995 bayesian method pac uniform converg method concern frequentist style confid interv deriv random introduc respect distribut input nois target function central concern result identifi flexibl hypothesi class f approxim function belong exampl vapnikchervonenki dimens f note bound independ input nois densiti assum train test sampl drawn distribut problem understand generalis capabl system also address bayesian framework fundament assumpt concern kind function system requir model word bayesian perspect need put prior target function context learn curv bound analys averag probabl distribut function paper use gaussian prior function advantag gener simpl linear regress prior analyt tractabl prior function obtain neural network neal 1996 shown fix hyperparamet larg class neural network model converg gaussian process prior function limit infinit number hidden unit hyperparamet bayesian neural network defin paramet correspond gaussian process gp william 1997 calcul covari function gp correspond neural network certain weight prior transfer function investig gp predictor motiv result rasmussen 1996 compar perform obtain gp obtain bayesian neural network rang task conclud gp least good neural network although present studi deal regress problem gp also appli classif problem eg barber william 1997 paper mainli concern analysi upper lower bound learn curv gp plot expect generalis error number train sampl n known learn curv mani result avail concern lean curv differ theoret scenario howev mani concern asymptot behaviour curv usual great practic import unlik enough data reach asymptot regim main goal explain earli behaviour learn curv gaussian process structur paper follow gp regress problem introduc section 2 shown whole theori gp base choic prior covari function c p x x 0 section 3 present covari function use studi section 4 learn curv gp introduc present properti learn curv gp well problem may aris evalu upper lower bound learn curv gp nonasymptot regim present section 5 bound deriv two differ approach one make use main properti generalis error wherea deriv eigenfunct decomposit covari function asymptot behaviour upper bound also discuss set experi run order assess upper lower bound learn curv section 6 present result obtain investig link tight bound smooth stochast process model gp summari result open question present last section gaussian process collect random variabl fy x jx 2 xg index set x defin stochast process gener domain x might r dimens although could even gener joint distribut characteris statist random variabl give complet descript stochast process gp stochast process whose joint distribut gaussian fulli defin give gaussian prior distribut everi finit subset variabl follow concentr regress problem assum valu target function x gener underli function x corrupt gaussian nois mean 0 varianc oe 2 given collect n train data observ output valu input point x would like determin posterior probabl distribut p yjx n order set statist model stochast process set n random variabl model function valu respect introduc similarli collect target valu denot set train input also denot vector whose compon test valu point x distribut p yjx n infer use bay theorem order need specifi prior function well evalu likelihood model evid data choic prior distribut stochast vector gaussian prior distribut gamma2 prior describ distribut true underli valu without refer target valu covari matrix sigma partit element k covari ith jth train point ie k compon vector k x covari test point train data covari test point gp fulli specifi mean e covari function set valid assumpt provid known offset trend data remov also deal x 6 0 introduc extra notat complex discuss possibl choic covari function c p x x 0 given section 3 moment note covari function assum depend upon input variabl x x 0 thu correl function valu depend upon spatial posit input vector usual chosen closer input vector higher correl function valu likelihood relat underli valu function target data assum gaussian nois corrupt data write likelihood likelihood refer stochast variabl repres data 2 r n andomega n theta n matrix given prior distribut valu function p bay rule specifi distribut p yjx n term likelihood model p tji evid data p n given assumpt standard result eg whittl 1963 deriv analyt form predict distribut marginalis predict distribut turn x n mean varianc gaussian function probabl valu x regard predict gp test point x k covari matrix target estim varianc oe 2 x posterior distribut consid error bar x follow alway omit subscript take understood sinc estim 1 linear combin train target gp regard linear smoother hasti tibshirani 1990 3 covari function choic covari function crucial one properti two gp differ choic covari function remark divers due role covari function incorpor statist model prior belief underli function word covari function analyt express prior knowledg function model misspecifi covari function affect model infer influenc evalu equat 1 2 formal everi function produc symmetr posit semidefinit covari matrix k set input space x chosen covari function applic point view interest function contain inform structur underli process model choic covari function link priori knowledg smooth function x connect differenti covari function meansquar differenti process relat smooth process covari function given follow theorem see eg adler exist finit x x stochast process x mean squar differenti ith cartesian direct x theorem relev link differenti properti covari function smooth random process justifi choic covari function depend upon prior belief degre smooth x work mainli concern stationari covari func tion stationari covari function translat invari ie c p x x depend upon distanc two data point follow covari function use present order simplifi notat consid case stationari covari function squar exponenti se defin lengthscal process paramet defin characterist length process estim distanc input space function x expect vari significantli larg valu indic function almost constant input space wherea small valu lengthscal design function vari rapidli graph covari function shown continu line figur 1 se function infinit mani deriv give rise smooth random process x poss meansquar differenti order 1 possibl tune differenti process introduc modifi bessel covari function order k mb k defin exp k delta modifi bessel function order see eg equat 8468 gradshteyn ryzhik 1993 set constant c p 1 factor k constant depend order bessel function matern 1980 show function mb k defin proper covari stein 1989 also note process covari function mb k differenti studi deal modifi bessel covari function order note mb 1 correspond ornsteinuhlenbeck covari function describ process mean squar differenti k 1 mb k behav like se covari function easili shown consid power spectra mb k se se exp sinc lim mb k behav like se larg k provid rescal accordingli modifi bessel covari function also interest describ markov process order k ihara 1991 defin x strict sens markov process order k differenti everi x 2 r p state gaussian process 1 note definit markov process discret continu time rather differ discret time markov process order k depend previou k time continu time depend deriv last time howev function valu previou time clearli allow approxim comput markov process order k strict sens autoregress model order k ark power spectrum fourier domain form power spectrum mb k form power spectrum ark model stochast process whose covari function mb k strict sens kple markov process characterist mb k covari function import ultim affect evalu generalis error shall see section 6 figur 2 show graph four discretis random function gener use mb k covari function se func tion note smooth random function specifi depend choic covari function particular roughest function gener ornsteinuhlenbeck covari function figur wherea smoothest one produc se figur 2d intermedi level regular characteris function figur 2b 2c correspond mb 2 mb 3 respect note number zerolevel upcross 0 1 denot n u weakli depend order process mb 2 mb 3 en u deriv eg via finit differ thu one would expect continuoustim situat previou k process valu contain inform need predict next time note ornsteinuhlenbeck process depend previou observ respect see papouli 1991 eqn 167 detail se process en u ornsteinuhlenbeck process nondifferenti formula given en u appli case learn curv gaussian process learn curv model function relat generalis error amount train data independ test point well locat train data depend upon amount data train set learn curv gp evalu estim generalis error averag distribut train test data regress problem measur generalis capabl gp squar differ e g target valu test point x predict made use equat 1 bayesian generalis error point x defin expect dn x actual distribut stochast process assumpt data set actual gener gp possibl read equat 2 bayesian generalis error x given train data n see let us consid n 1dimension distribut target valu x 1 x zeromean multivari gaussian predict test point x henc expect generalis error x given theta theta theta tt use theta tt k equat 5 ident oe 2 x given equat 2 addit nois varianc oe 2 sinc deal noisi data varianc also calcul vivarelli 1998 covari matrix pertin calcul true prior gp predictor differ incorrect covari function use express generalis error becom c indic c denot correct incorrect covari function respect shown vivarelli 1998 alway larger equat 5 anoth properti generalis error deriv follow observ ad data point never increas size error bar predict oe 2 n x prove use standard result condit multivari gaussian see vivarelli 1998 also understood inform theoret argument condit addit variabl never increas entropi random variabl consid x random variabl observ distribut gaussian varianc independ although mean depend entropi gaussian is2 log log monoton assert prove argument extens qazaz et al 1997 inequ deriv gener linear regress dn x similar inequ appli also bayesian generalis error henc remark appli section 5 evalu upper bound learn curv equat 5 calcul generalis error point x averag dn x densiti distribut test point p x expect generalis error e dn particular choic p x c p x comput express reduc n theta n matrix comput e x theta theta k x k x also note equat 7 independ test point x still depend upon choic train data n order obtain proper learn curv gp e g dn need averag 2 possibl choic train data n howev difficult obtain analyt form e g gp function n presenc k equat 5 matrix k vector k x depend locat train point calcul averag respect data point seem hard motiv look upper lower bound learn curv gp 5 bound learn curv noiseless case lower bound generalis error n observ due michelli wahba 1981 let order eigenvalu covari function domain input space x show e g n bound learn curv noisi case sinc bound use observ consist project random function onto first eigenfunct expect tight observ consist function evalu result awar pertain asymptot properti n ritter 1996 shown optim sampl input space asymptot generalis error hansen 1993 show linear regress model possibl averag distribut train set random process obey sacksylvisak 3 condit order see ritter et al 1995 detail sacksylvisak condit gener sacksylvisak order mb k covari function 1 exampl mb 1 process henc generalis error show n gamma12 asymptot decay case x ae r asymptot optim design input space uniform grid silverman 1985 prove similar result random design haussler opper 1997 develop gener asymptot bound expect loglikelihood test point see n train point follow introduc upper lower bound learn curv gp nonasymptot regim upper bound particularli use practic provid overestim number exampl need give certain level perform lower bound similarli import contribut fix limit outperform model bound present deriv two differ approach first approach make use particular form assum generalis error x e g x error bar gener one data point greater gener n data point former consid upper bound latter sinc observ hold varianc due one data point envelop surfac gener loos speak stochast process possess meansquar deriv said satisfi sacksylvisak condit order varianc due data point also upper bound oe 2 n x particular oe 2 dn x cf equat 5 envelop upper bound generalis error gp follow argument assert upper bound e g dn x one gener everi gp train subset n larger subset n tighter bound two upper bound present differ number train point consid evalu covari deriv onepoint upper bound e u 1 n twopoint upper bound e u 2 n present section 51 section 52 respect section 53 report asymptot expans e u 1 n term oe 2 second approach base expans stochast process term eigenfunct covari function within framework opper propos bound train generalis error opper vivarelli 1999 term eigenvalu c p x x 0 lower bound e l n obtain present section 54 order tractabl analyt express bound deriv introduc three assumpt input space x restrict interv 0 1 ii probabl densiti distribut input point uniform iii prior covari function c p x x 0 stationari 51 onepoint upper bound e u deriv onepoint upper bound let us consid error bar gener one data point x sinc c equat 2 becom x far away train point x oe 2 confid predict test point lie far apart data point x quit low error bar larg closer x x smaller error bar x irrespect valu c p 0 r vari 0 1 normal c p 0 ae oe 2 thu oe 2 far use hypothesi concern dimens variabl x thu observ hold regardless dimens input space effect one data point help introduc first upper bound interv 0 1 split n subinterv theta 2 b centr around ith data point x let us consid ith train point error bar oe 2 x x 2 theta 1 relat illustr figur 3 envelop surfac error due datapoint denot e g x upper bound overal generalis error sinc deal posit function upper bound expect generalis error interv theta written p x distribut test point sum contribut come train datapoint side equat 8 set interv contribut varianc due x contribut equat 8 also shown figur 3 assumpt stationar covari function integr right hand side equat 9 depend upon differ adjac train point ie x right hand side equat 9 rewritten dx equat 11 deriv chang variabl two integr equat equat 11 upper bound e g still depend upon choic train data n interv integr note argument integr delta equat 11 differ adjac train point denot differ model probabl densiti distribut use theori order statist david 1970 given uniform distribut n train data interv 0 1 densiti distribut differ adjac point p sinc true differ omit superscript thu expect integr equat 11 p n integr calcul follow similar procedur let us consid second line obtain integr part last line follow fact abl write upper bound learn curv calcul integr express straightforward though involv evalu hypergeometr function evalu function comput intens found prefer evalu equat 14 numer 52 twopoint upper bound e un second bound introduc natur extens previou idea use two data point rather one construct expect tighter one introduc section 51 let us consid two adjac data point x x i1 interv 0 1 argument present previou section follow inequ hold 2 x varianc predict x gener data point x x i1 similarli equat 9 sum contribut side equat 15 get upper bound generalis error defin calcul see appendix obtain 1 calcul integr respect e u 2 n complic determin delta denomin distribut n prefer evalu numer e u 53 asymptot upper bound equat 14 expans e u 1 n term oe 2 limit larg amount train data obtain expans depend upon covari function deal expand covari function around 0 asymptot form e u 1 n mb 1 n wherea function mb 2 mb 3 se asymptot valu e u depend neither lengthscal process order covari function mb k k 1 function ratio r lim point section 51 minimum generalis error achiev gp train one datapoint n 1 scenario correspond situat everi test point close datapoint mention begin section asymptot learn curv mb k se covari function delta respect although expans decay asymptot faster learn curv reach asymptot plateau oe 2 also note asymptot valu get closer true nois level r 1 ie unrealist case oe 2 smooth process enter asymptot factor factor affect rate approach asymptot valu oe 2 e u 1 n notic larger lengthscal nois level increas rate decay e u 1 n asymptot plateau asymptot form e u 2 n mb 1 mb 2 mb 3 se covari function vivarelli 1998 valu depend upon choic covari function 0 similarli expans e u 1 n decay rate 2 n faster asymptot decay actual learn curv reach asymptot plateau lim straightforward verifi asymptot plateau e u 2 n lower one e u 1 n correspond error bar estim gp two observ locat test point 54 lower bound e l n opper opper vivarelli 1999 propos bound learn curv train error base decomposit stochast process x term eigenfunct covari c p x x 0 denot k set function satisfi integr equat z bayesian generalis error e x true underli stochast function x gp predic tion written term eigenvalu c p x x 0 particular averag distribut input data e g n written e g n infinit dimens diagon matrix eigenvalu v matrix depend train data ie v use jensen inequ possibl show lower bound learn curv upper bound train error opper paper mean compar lower bound actual learn curv gp bound rather must add oe 2 express obtain equat 23 give actual lower bound 6 result point section 4 analyt calcul learn curv gp infeas sinc generalis error complic function train data insid element k x k gamma1 problemat perform integr distribut train point compar learn curv gp bound found need evalu expect integr equat 25 distribut data e edn theta dn estim e g n obtain use mont carlo approxim expect use 50 gener train data sampl uniformli input space 0 1 gener expect generalis error gp evalu use 1000 datapoint use 50 gener train data obtain estim learn curv e g n 95 confid interv sinc studi focus behaviour bound learn curv gp assum true valu paramet gp known chose valu constant covari function equat 4 c p allow lengthscal nois level oe 2 assum sever valu begin studi smooth process affect behaviour learn curv empir learn curv figur 4 obtain process whose covari function mb 1 01 notic learn curv exhibit initi linear decreas explain consid without train data generalis error maximum allow model c introduct train point x 1 creat hole error surfac volum hole proport valu lengthscal depend covari function addit new data point x 2 effect gener new hole surfac data point like two data lie far apart one give rise two distinct hole thu effect small dataset exert pull error surfac proport amount train point explain initi linear trend concern asymptot behaviour learn curv verifi agre theoret analysi carri ritter 1996 particular loglog plot learn curv mb k covari function show asymptot behaviour similar remark appli se covari function asymptot decay rate opper 1997 also note smoother process describ covari function smaller amount train data need reach asymptot regim behaviour learn curv affect also valu lengthscal process nois level illustr figur 7 learn curv shown figur 5a obtain mb 1 covari function set nois level oe 2 01 vari valu paramet intuit figur 5a suggest decreas lengthscal stretch earli behaviour learn curv approach asymptot plateau last longer due effect induc differ valu lengthscal stretch compress input space verifi rescal amount data n ratio two lengthscal two curv figur 5a lay top variat nois level shift learn curv prior valu c p 0 offset equal nois level cf equat 5 order see signific effect nois learn curv figur 5b show loglog graph e obtain stochast process mb 3 covari function set notic two main effect nois varianc affect actual valu generalis error sinc learn curv obtain high nois level alway one obtain low nois level second effect concern amount data necessari reach asymptot regim learn curv characteris high nois level need fewer datapoint attain asymptot regim stochast process differ covari function differ valu lengthscal nois varianc behav similar way follow discuss result two main subsect result bound e u 2 n present section 61 wherea lower bound section 54 shown section 62 result obtain experi show common characterist show bound learn curv obtain set 61 upper bound e un e un graph figur 6 show empir learn curv confid interv two upper bound e u n curv shown mb 1 mb 2 mb 3 se covari function limit amount train data possibl notic upper error bar associ edn e g n lie actual upper bound effect due variabl generalis error small data set suggest bound quit tight small n effect disappear larg n estim generalis error less sensit composit train set expect twopoint upper bound e u 2 n tighter onepoint upper bound e u note tight upper bound depend upon covari function tighter rougher process mb 1 get wors smoother process explain recal covari function mb k correspond markov process order k cf section 3 although markov process actual hidden presenc nois e g n still depend train data lie close test point x distant point sinc bound e u calcul use local inform name closest datapoint test point closest datapoint left right respect natur varianc x depend local data point tighter bound becom instanc let us consid mb 1 covari function first order markov process noisefre process knowledg datapoint lie beyond left right neighbour x reduc generalis error x 4 although noisi case distant datapoint 4 process valu train point test point form markov chain knowledg process valu left right test point block reduc generalis error term oe 2 covari matrix k like local inform still import bound learn curv comput mb 2 mb 3 confirm remark looser mb 1 se covari function effect still hold actual enlarg section 53 shown asymptot behaviour bound depend covari function plot upper bound confirm analysi carri section 53 show e u approach asymptot plateaux particular e u tend oe 2 tend qualiti bound process characteris differ length scale differ nois level compar one describ far tight e u still depend smooth process explain begin section variat lengthscal effect rescal number train data observ explicitli asymptot analysi equat 19 decay rate depend factor n fix covari function note bound tighter lower nois varianc due fact lower nois level better hidden markov process manifest smaller nois level influenc remot observ learn curv becom closer bound generalis error reli local behaviour process around test data contrari larger nois level hide underli markov process thu loosen bound 62 bound e l n also run experi comput lower bound obtain equat 24 process gener covari prior mb 1 mb 2 mb 3 se equat 24 show evalu e l n involv comput infinit sum term truncat seri consid term add signific contribut sum ie j k oe 2 machin precis sinc contribut seri posit quantiti comput still lower bound learn curv figur 7 show result experi set 01 graph lower bound lie empir learn curv tighter larg amount data particular smoothest process larg amount data 95 confid interv lay actual lower bound lower bound tend nois level oe 2 empir learn curv loglog plot e l n show asymptot decay zero gamma2kgamma12k delta mb k se covari function respect graph figur 7 show also tight bound depend smooth stochast process particular smooth process characteris tight lower bound learn curv e g n explain observ e l n lower bound learn curv upper bound train error valu smooth function larg variat train point thu model infer better test data reduc generalis error pull closer train error sinc two error sandwich bound equat 24 e l n becom tight smooth process also notic tight lower bound depend nois level becom tight high nois level loos small nois level consist gener characterist e l n monoton decreas function nois varianc opper vivarelli 1999 paper present nonasymptot upper lower bound learn curv gp theoret analysi carri onedimension gp characteris sever covari function support numer simul start observ increas amount train data never worsen bayesian generalis error upper bound learn curv estim generalis error gp train reduc dataset mean given train set envelop generalis error gener one two datapoint upper bound actual learn curv gp sinc expect generalis error distribut train data analyt tractabl introduc two upper bound e u 1 n 2 n amen averag distribut test train point studi evalu expect valu futur direct research also deal evalu varianc order highlight behaviour bound respect smooth stochast process investig bound modifi bessel covari function order k describ stochast process differenti squar exponenti function describ process mean squaredifferenti order 1 experiment result shown learn curv bound characteris earli linearli decreas behaviour due effect exert datapoint pull surfac prior generalis error also notic tight bound depend smooth stochast process due fact bound reli subset train data ie one two datapoint modifi bessel covari function describ markov process order k although simul markovian process hidden nois learn curv depend mainli local inform bound becom tighter rougher process also investig behaviour curv respect variat correl lengthscal process varianc nois corrupt stochast process notic lengthscal stretch behaviour curv effect rescal number train data nois level effect hide underli markov process upper bound becom tighter smaller nois varianc expans bound limit larg amount data highlight asymptot behaviour depend upon covari function approach asymptot plateau mb 1 covari smoother process rate decay plateau e u 2 n numer simul support analysi one limit analysi dimens input space bound made analyt tractabl use order statist result split one dimension input space gp higher dimension space partit input space replac voronoi tessel depend data n averag distribut appear difficult one suggest approxim evalu upper bound integr ball whose radiu depend upon number exampl volum input space bound hold case expect effect due larger input dimens loosen upper bound note recent work sol lich 1999 deriv good approxim learn curv method appli one dimens 5 also ran experi use lower bound propos opper base knowledg eigenvalu covari function process sinc bound e l n also upper bound train error observ bound tighter smooth process learn curv becom closer train error also nois vari tight e l n low nois level loosen lower bound unlik upper bound lower bound appli also multivari problem easili extend high dimens input space howev verifi opper vivarelli 1999 bound becom less tight input space higher dimens appendix twopoint upper bound e u appendix deriv equat 17 start equat 16 start calcul oe 2 x covari matrix gener two data point 2 theta 2 matrix straightforward evalu oe 2 consid two train data x x i1 covari matrix 5 refer sollich 1999 ad manuscript revis april 1999 gp evalu determin k covari vector test point x k varianc assum form chang variabl covari c p turn upper bound gener oe 2 2 x interv theta 6 0 n 1 1 2 notic similarli equat 11 also integr 1 delta 2 delta determin delta depend upon length interv integr evalu contribut upper bound interv theta 0 x 1 x n 1 integr varianc oe 2 gener x 1 x n theta 0 x 1 x n 1 respect henc right hand side equat 16 rewritten 1 delta defin equat 12 equat 26 still depend distribut train data function distanc adjac train point similarli equat 11 obtain upper bound independ train data integr equat 13 distribut differ gammac acknowledg research form part valid verif neural network system project fund jointli epsrc grk 51792 british aerospac thank dr manfr opper dr andi wright bae help discuss also thank anonym refere comment help improv paper f v support studentship british aerospac r geometri random field new look statist model identif gaussian process bayesian classif via hybrid mont carlo order statist tabl integr stochast linear learn exact test train error averag gener addit model mutual inform inform theori design problem optim surfac interpol network inform criteriondetermin number hidden unit artifici neural network model bayesian learn neural network lectur note statist 118 regress gaussian process averag case per formanc gener bound bay error regress gaussian process upper bound bayesian error bar gener linear regress evalu gaussian process method nonlinear regress almost optim differenti use noisi data multivari integr approxim random field satisfi sack ylvisak condit aspect spline smooth approach nonparametr regress curv filter learn curv gaussian process theori learnabl natur statist learn theori studi generalis gaussian process bayesian neural network predict regul linear least squar method comput infinit network figur 6a figur 7 figur 7a tr ctr peter sollich anason hale learn curv gaussian process regress approxim bound neural comput v14 n6 p13931428 june 2002