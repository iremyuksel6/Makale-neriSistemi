improv gener activ learn activ learn differ learn exampl learn algorithm assum least control part input domain receiv inform situat activ learn provabl power learn exampl alon give better gener fix number train examplesin articl consid problem learn binari concept absenc nois describ formal activ concept learn call select sampl show may approxim implement neural network select sampl learner receiv distribut inform environ queri oracl part domain consid use test implement call sgnetwork three domain observ signific improv gener b introduct vs activ learn neural network gener problem studi respect random sampl train exampl chosen random network simpli passiv learner approach gener refer learn exampl baum haussler 1989 examin problem analyt neural network cohn tesauro 1992 provid empir studi neural network gener learn exampl also number empir effort le cun et al 1990 aim improv neural network gener learn exampl learn exampl howev univers applic paradigm mani natur learn system simpli passiv instead make use least form activ learn examin problem domain activ learn mean form learn learn program control input train natur system human phenomenon exhibit high level eg activ examin object low subconsci level eg fernald kuhl 1987 work infant reaction motheres speech within broad definit activ learn restrict attent simpl intuit form concept learn via membership queri membership queri learner queri point input domain oracl return classif point much work formal learn theori direct studi queri see eg angluin 1986 valiant 1984 recent queri examin respect role improv gener behavior mani formal problem activ learn provabl power passiv learn randomli given exampl simpl exampl locat boundari unit line interv order achiev expect posit error less ffl one would need draw 1 train exampl publish machin learn 152201221 1994 preliminari version paper appear cohn et al 1990 one allow sequenti make membership queri binari search possibl assum uniform distribut posit error ffl may reach oln 1 queri one imagin number algorithm employ membership queri activ learn studi problem learn binari concept errorfre environ problem learner may proceed examin inform alreadi given determin region uncertainti area domain believ misclassif still possibl learner ask exampl exclus region paper discuss formal simpl approach call select sampl section 2 describ concept learn problem detail give formal definit select sampl describ condit necessari approach use section 3 describ sgnetwork neural network implement techniqu inspir versionspac search mitchel 1982 section 4 contain result test implement sever differ problem domain section 5 discuss limit select sampl approach section 6 7 contain refer relat work field conclud discuss paper concept learn select sampl given arbitrari domain x defin concept c subset point domain exampl x might twodimension space c might set point lie insid fix rectangl plane classifi point x 2 x membership concept c write otherwis popular use artifici neural network concept classifi x present input appropri train network activ design output node threshold x 2 c x instanc concept c formal concept class c set concept usual describ descript languag exampl class c may set twodimension axisparallel rectangl see figur 1 case neural network concept class usual set concept network may train classify10000011 figur 1 concept class defin set axisparallel rectangl two dimens sever posit neg exampl depict sever consist concept class 21 gener target concept train exampl pair x tx consist point x usual drawn distribut p point classif tx x 2 say x tx posit exampl otherwis neg exampl concept c consist exampl x tx cx tx concept produc classif point x target error c respect distribut p probabl c disagre random exampl drawn p write randomli accord p gener problem pose follow given concept class c unknown target arbitrari error rate ffl confid ffi mani exampl draw classifi arbitrari distribut p order find concept c 2 c consist exampl fflc p ffl confid least problem formal valiant 1984 studi neural network baum haussler 1989 haussler 19891100000011 figur 2 region uncertainti rs set point x domain two concept consist train exampl yet disagre classif x 22 region uncertainti consid concept class c set exampl classif region domain may implicitli determin figur 2 concept c consist instanc may agre part interest area determin avail inform defin region uncertainti consist arbitrari distribut p defin size region increment learn procedur classifi train exampl ff monoton nonincreas point fall outsid rs leav unchang point insid restrict region thu ff probabl new random point p reduc uncertainti rs serv envelop consist concept disagr concept must lie within rs rs also bound potenti error consist hypothesi choos error current hypothesi ffl ffl ff sinc basi chang current hypothesi without contradict point ff also bound probabl addit point reduc error 23 select sampl activ learn let us consid learn sequenti process draw exampl one anoth determin much inform success exampl give us draw random whole domain probabl individu sampl reduc error ff defin decreas zero draw exampl mean effici learn process also approach zero eventu exampl draw provid us inform concept tri learn consid happen recalcul rs region uncertainti new exampl draw exampl within rs exampl reduc rs reduc uncertainti decreas effici draw exampl call process select sampl distribut p known eg p uniform perform select sampl directli randomli queri point accord p lie strictli insid rs frequent howev sampl distribut well target concept unknown case choos point domain impugn risk assum distribut differ greatli actual underli p mani problem though still make use distribut inform without pay full cost draw classifi exampl rather assum draw classifi exampl atom oper valiant 1984 blumer et al 1988 may divid oper two step first draw unclassifi exampl distribut second queri classif point cost draw point distribut small compar cost find point proper classif filter point drawn distribut draw random select classifi train fall rs approach well suit problem speech recognit unlabel speech data plenti classifi label speech segment labori process train set size 50 100 150 200 random sampl pass sampl 3 pass sampl 4 pass sampl pass sampl pass sampl figur 3 batch size select sampl approach one process yield diminish improv ad comput cost figur plot error vs train set size select sampl use differ batch size learn axisparallel rectangl two dimens sinc calcul rs may comput expens may want perform select sampl batch first pass draw initi batch train exampl 0 p train determin initi rs defin new distribut p 0 sampl zero outsid maintain rel distribut p insid rs make second pass draw second batch train exampl ad first determin new smaller rs smaller batch size pass made effici algorithm draw train exampl see figur 3 howev sinc rs recalcul pass advantag must weigh ad comput cost incur calcul 24 approxim select sampl even simpl concept class set axisparallel rectangl two dimens may difficult comput expens exactli repres region uncertainti class rectangl neg exampl lie along corner region add complex caus nick outer corner rs figur 2 realist complic class repres exactli easili becom difficult imposs task use good approxim rs may howev suffici allow select sampl practic implement select sampl possibl number approxim process includ maintain close superset subset rs assum abl maintain superset r point rs also superset select sampl insid r assur exclud part domain interest penalti pay effici may also train point interest effici approach compar pure select sampl measur ratio p rx abl maintain subset r sampl train algorithm must take addit precaut given iter part rs exclud sampl need ensur success iter choos subset cover entir region uncertainti exampl techniqu discuss next section also need keep number exampl iter small prevent oversampl one part domain remaind paper denot arbitrari algorithm approxim true region uncertainti r 3 neural network select sampl select sampl approach hold promis improv gener mani trainabl classifi remaind paper concern demonstr approxim select sampl may implement use feedforward neural network train error backpropag backpropag algorithm rumelhart et al 1986 supervis neural network learn techniqu network present train set inputoutput pair x tx learn output tx given input x train neural network use standard backpropag take train exampl x tx copi x input node network figur 4 1 calcul individu neuron output layer layer begin first hidden layer proceed output layer output neuron j comput w ji connect weight neuron j neuron squash function produc neuron output rang 0 1 defin error output node n error valu propag back network see rumelhart et al 1986 detail neuron j error term x connect weight w ji adjust ad deltaw ji j constant learn rate adjust increment decreas error network exampl x tx present train exampl turn suffici larg network gener converg set weight assum input normal rang 0 1 first hidden layer second hidden layer output layer input layer network output network input connect weight figur 4 simpl feedforward neural network node comput weight sum input pass sum sigmoid squash function pass result output network accept small error train exampl concept learn model target valu train exampl 1 0 depend whether input instanc concept learn pattern train error less threshold point need draw attent distinct neural network architectur configur 2 architectur neural network refer paramet network chang train case network topolog transfer function configur network refer network paramet chang train case weight given connect neuron although network train algorithm involv chang network topolog train eg ash 1989 consid fix topolog train weight adjust theori method describ modif equal applic trainabl classifi neural network architectur singl output node concept class c specifi set configur network take configur implement map input x output 0 1 mani configur may implement map set threshold 05 output may say particular configur c repres concept c x 2 c cx 05 see figur 5 train train set say network configur c implement concept c consist train set use c denot concept c network c implement consid naiv algorithm select sampl neural network examin short come describ sgnet base versionspac paradigm mitchel 1982 overcom difficulti 31 naiv neural network queri algorithm observ neural network implement concept learner may produc realvalu output threshold suggest naiv algorithm defin region uncertainti network train toler divid point domain one three classif 1 09 greater 0 01 less uncertain 01 09 may say last categori correspond region network uncertain may thu defin r approxim region uncertainti figur 6 problem appli approach measur uncertainti particular configur uncertainti among configur possibl given architectur fact 2 terminolog judd 1988 figur 5 threshold output train neural network c serv classifi repres concept c hope similar unknown target concept part rs full region compris differ possibl consist network configur limit exacerb induct bia learn algorithm includ backpropa gation backpropag algorithm attempt classifi set point tend draw sharp distinct becom overli confid region still unknown result r chosen method gener small subset true region uncertainti patholog exampl behavior exhibit figur 7a 7b figur 7a initi random sampl fail yield posit exampl triangl right train backpropag exampl yield region uncertainti two contour concentr left half domain complet exclus right final result 10 iter queri learn shown figur 7b strategi relat one prone failur form whenev region detail target concept discov initi random sampl stage 32 version space mitchel 1982 describ learn procedur base partial order gener concept learn concept c 1 gener anoth concept c 2 c 2 ae c 1 c 1 6ae c 2 c 2 6ae c 1 two concept incompar concept class c set exampl version space subset consist g bound concept version space maintain two subset set specif consist concept cg similarli set gener concept consist concept c must case c g 2 g 2 g one may activ learn version space examin instanc fall differ g region delta symmetr differ oper instanc region prove posit gener accommod new inform prove neg g g modifi exclud either case version space space plausibl hypothes reduc everi queri 33 implement activ versionspac search sinc entir neural network configur repres singl concept complet version space directli repres singl neural network fact haussler 1987 point size figur naiv approach repres region uncertainti use network transit area 0 1 repres part domain network uncertain g set could grow exponenti size train set repres set complet would requir keep track manipul exponenti number network configur howev modifi versionspac search make problem tractabl done impos accord distribut p strict index order concept class defin concept c 1 gener concept c 2 random point x drawn definit gener concept class compar make sens speak order repres singl gener concept g singl specif concept may still mani concept gener impedi need know concept gener case greater gener concept g chosen maintain two concept window version space r sdeltag subset deltag thu point x guarante reduc size version space posit invalid leav us anoth either gener one equal specif one includ new point similarli new point classifi neg invalid g proceed fashion approxim stepbystep travers g set use fix represent size 34 sgnet neural network versionspac search algorithm sinc interest select exampl improv gener behavior given neural network architectur n defin concept class question set concept learnabl n learn algorithm manag obtain network configur repres g concept describ simpl matter implement modifi versionspac search follow two subsect first describ one may learn specif gener concept associ network describ two network may use select sampl r defin region disagre 341 implement specificgener network describ one may learn specif concept consist given data case learn g gener concept analog specif network set exampl accord distribut p one classifi posit exampl point fact posit classifi neg much possibl rest domain requir amount choos c consist minim p rx 2 c figur 7 patholog exampl naiv network queri left initi random sampl fail detect second disjoint region target concept b right 10 success iter naiv queri algorithm ignor region concentr region seen exampl dot line denot true boundari unknown target concept network may arriv employ induct bia induct bia predisposit learn algorithm solut other learn algorithm inher least form induct bia whether prefer simpl solut complex one tendenc choos solut absolut valu paramet remain small 3 explicitli add new induct bia backpropag algorithm penal network part domain classifi posit add bia prefer specif concept gener one weight penalti must care adjust larg enough outweigh train exampl network converg train data must howev larg enough outweigh induct bia learn algorithm forc find specif configur consist neg bia may implement draw unclassifi point p creat case p known arbitrarili label neg exampl add background exampl train set figur 8 creat background bia domain weight input distribut p network least error background pattern one specif accord p order allow network converg actual train exampl spite background exampl must balanc influenc background exampl train data network learn train exampl x error term equat 1 approach zero error term arbitrari background exampl may remain constant unless push random background exampl exert network weight deltaw ji decreas match normal train exampl deltaw ji x background exampl domin network converg solut achiev balanc use differ learn rate train exampl background exampl dynam decreas background learn rate function network error train set time present train exampl x calcul new background learn rate error network x constant train singl background 3 induct bias inher backpropag well studi appear tendenc fit data use smallest number unit possibl figur 8 train larg number background point addit regular train data forc network specif configur exampl use valu j 0 repeat formal algorithm follow 1 initi network random configur c 2 actual train exampl x 3 otherwis select next actual train exampl x tx 4 calcul output error network c input x backpropag network adjust weight accord deltaw ji 5 calcul new background learn rate 6 draw point p creat background exampl 0 7 calcul output error backpropag network adjust weight accord modifi equat deltaw ji 7 go step 2 optim fl set weight updat background pattern alway infinitesim smaller weight updat actual train pattern allow network anneal specif configur howev requir prohibit amount train time empir found set provid adequ bia still allow converg reason number iter similar procedur use produc gener network ad posit induct bia classifi background point drawn p posit 342 implement activ learn sgnet repres concept g simpl matter test point x membership r determin sx 6 gx select sampl may implement follow point drawn distribut sdeltag two network agre classif point discard point sdeltag true classif queri ad train set practic merg input g network illustr figur 9 train togeth import note techniqu somewhat robust failur mode degrad effici singl sampl iter rather caus overal failur learn process either typic network architectur split separ g network input merg g g figur 9 construct sgnetwork equival origin g network fail converg train data point fail converg contain sdeltag region elig addit sampl next iter case found addit exampl suffic push network local minimum network converg train set settl solut near specificgener network consist data exampl glean next iter still use sinc chosen virtu lie area two network disagre point settl discrep two may lead oversampl region caus techniqu fail effect two failur mode minim keep number exampl taken iter small increas effici learn process term number exampl classifi observ tradeoff comput resourc requir time new data ad train set network may complet readjust incorpor new inform found practic larg train set size often effici simpli retrain entir network scratch new exampl ad recent work pratt offer hope retrain may made effici use inform transfer strategi iter 4 experiment result experi use select sampl run three type problem solv simpl boundari recognit problem two dimens learn 25input realvalu threshold function recogn secur region small power system 41 triangl learner twoinput network two hidden layer 8 3 unit singl output train uniform distribut exampl posit insid pair triangl neg elsewher task chosen intuit visual appeal requir learn nonconnect concept task demand train algorithm sampl select scheme simpl convex shape baselin case consist 12 network train randomli drawn exampl train set size 10 150 point increment 10 exampl eight test case run architectur data select four run sgnetwork use 15 select sampl iter 10 exampl figur 10a 10b addit 12 run naiv queri algorithm describ section 31 run comparison network train select sampl data show mark consist improv randomli sampl network one train naiv queri figur 11 naiv queri algorithm display much errat perform two algorithm possibl due patholog natur failur mode figur 10 triangl learner problem learn 150 random exampl left learn 150 exampl drawn 15 pass select sampl b right dot line denot true boundari unknown target concept 42 realvalu threshold function use 25bit realvalu threshold problem quantit measur network perform simpl higherdimension problem six run select sampl use iter 10 exampl per iter train problem compar 12 ident network train randomli sampl data result figur 12 indic much steeper learn curv select sampl plot gener error number train exampl network train randomli sampl data exhibit roughli polynomi curv would expect follow blumer et al 1988 use simpl linear regress 1 ffl error data fit coeffici determin r 2 0987 network train select sampl data comparison fit indic fit polynomi good visual select sampl network exhibit steeper drop gener error would expect activ learn method use linear regress natur logarithm error select sampl network exhibit decreas gener error match error drop 15 indic good fit exponenti curv comparison randomli sampl network fit domain sgnetwork appear provid almost exponenti improv gener increas train set size much one would expect good activ learn algorithm suggest sgnetwork repres good approxim region uncertainti domain thu implement good approxim select sampl addit experi run use 2 3 4 20 iter indic error decreas sampl process broken smaller frequent iter observ consist increas effici sampl new inform incorpor earlier sampl process 43 power system secur analysi variou load paramet electr power system within certain rang system secur otherwis risk thermal overload brownout previou research aggoun et al 1989 determin problem amen neural network learn random sampl problem domain ineffici term exampl need rang paramet system run known distribut inform readili avail set paramet point domain one analyt determin whether system secur must done solv train set size 50 100 150 03 random sampl naiv queri select sampl figur gener error vs train set size random sampl naiv queri select sampl irregular naiv queri algorithm error may due intermitt failur find triangl intial random sampl timeconsum system equat thu sinc classif point much expens determin input distribut problem amen solut select sampl baselin case random sampl four dimens studi hwang et al 1990 use comparison experi ran six set network initi random train set 500 data point ad singl iter select sampl network train small second iter 300 point total 800 well larg second iter 2000 total 2500 point result compar baselin case 800 2500 point randomli sampl data estim network error test 14979 randomli drawn test point improv singl extra iter select sampl yield small set 107 total error 517 instead 547 larg set result improv 126 total 421 instead 482 differ signific greater 90 confid 5 limit select sampl approach number limit select sampl approach practic mention previou section discuss implement techniqu other theoret 51 practic limit discuss earlier paper exact implement select sampl practic rel simpl concept class class becom complex becom difficult comput maintain accur approxim rs case maintain superset increas concept complex seem lead case r effect contain entir domain reduc effici select sampl random sam pling exampl section 24 illustr nice bound box suffic approxim train set size sampl select sampl polynomi exponenti figur 12 gener error vs train set size random sampl select sampl standard deviat error averag 000265 random case 000116 select sampl case rectangl two dimens nick box bound 20dimension figur could conceiv requir approxim contain domain space case maintain subset increas concept complex lead extrem r contain small subset rs case oversampl region becom critic problem due induct bia train algorithm even train set size one may omit larg region domain 52 theoret limit select sampl draw power abil differenti region uncertainti bulk domain case represent complex concept larg neural network mani hidden unit howev rs extend whole domain concept alreadi welllearn even though maximum error may small due number place error may aris total uncertainti may remain larg thu depend desir final error rate select sampl may come effect longer need similarli input dimens larg bulk domain may uncertain even simpl concept one method avoid problem use bayesian probabl measur degre util queri variou part region uncertainti approach recent studi david mackay 1991 discuss briefli follow section 6 relat work work describ paper extens result publish cohn et al 1990 prior work sinc mani relat result activ learn larg bodi work studi effect queri strict learn theori viewpoint primarili respect learn formal concept boolean express finit state automata angluin 1986 show minim finit state automata polynomi learnabl valiant sens exampl alon could learn use polynomi number queri oracl provid counterexampl valiant 1984 consid variou class learnabl use varieti form activ learn work eisenberg rivest 1990 put bound degre membership queri exampl help gener underli distribut unknown addit given certain smooth constraint distribut describ queri may use learn class initi segment unit line actual implement queri system learn recent explor work done hwang et al 1990 implement queri neural network mean invert activ train network determin uncertain approach show promis concept learn case rel compact connect concept alreadi produc impress result power system static secur problem howev suscept patholog discuss section 31 algorithm due baum lang 1991 use queri reduc comput cost train singl hiddenlay neural network algorithm make queri allow network effici determin connect weight input layer hidden layer seung et al 1992 independ propos similar scheme select queri base lack consensu committe learner freund et al 1993 show size committe increas beyond two learner use select sampl accuraci one util estim increas sharpli work david mackay 1992 pursu relat approach data select use bayesian analysi assign prior probabl concept network configur one determin util queri variou part rs fact point lie within rs mean consist configur disagre classif point point edg rs though may configur disagre queri point decreas size rs infinitesim small amount use bayesian analysi one may effect determin number configur disagre given point thu determin part rs uncertain 7 conclus paper present theori select sampl describ neural network implement theori examin perform result system sever domain select sampl rudimentari form activ learn benefit formal ground learn theori neural network implement test demonstr signific improv passiv random sampl techniqu number simpl problem paradigm suit concept learn problem relev input distribut known cost obtain unlabel exampl input distribut small compar cost label exampl limit select sampl becom appar complex problem domain approach open door studi sophist techniqu queri learn natur intuit mean activ learn acknowledg work support nation scienc foundat grant number ccr9108314 washington technolog center ibm corpor major work done david cohn dept comput scienc engin univers washington remaind done david cohn ibm j watson research center yorktown height ny 10598 would like thank jai choi siri weerasooriya work run simul data power system problem would also like thank two anonym refere suggest earlier version paper r artifici neural network power system static secur assess learn regular set queri counterexampl dynam node creation backpropag network size net give valid gener construct hidden unit use exampl queri learnabl vapnikchervonenki dimens train 3node neural network npcomplet train connectionist network queri select sampl tight vapnikchervonenki bound neural comput volume4volumeissue2issuepages249269pag sampl complex paclearn use random chosen exampl acoust determin infant prefer motheres speech learn conjunct concept structur domain gener pac model neural net learn applic queri learn base boundari search gradient comput train multilay perceptron complex load shallow neural network optim brain damag gener search learn intern represent error propag theori learnabl tr ctr gari weiss ye tian maxim classifi util train data costli acm sigkdd explor newslett v8 n2 p3138 decemb 2006 patricia g foschi huan liu activ learn detect spectral variabl subject color infrar imageri pattern recognit letter v25 n13 p15091517 1 octob 2004 traci hammond randal davi interact learn structur shape descript automat gener nearmiss exampl proceed 11th intern confer intellig user interfac januari 29februari 01 2006 sydney australia geoff hulten pedro domingo mine complex model arbitrarili larg databas constant time proceed eighth acm sigkdd intern confer knowledg discoveri data mine juli 2326 2002 edmonton alberta canada p engelbrecht r brit supervis train use unsupervis approach activ learn neural process letter v15 n3 p247260 june 2002 prem melvil foster provost maytal saartsechanski raymond mooney econom activ featurevalu acquisit expect util estim proceed 1st intern workshop utilitybas data mine p1016 august 2121 2005 chicago illinoi rebecca hwa minim train corpu parser acquisit proceed 2001 workshop comput natur languag learn p16 juli 0607 2001 toulous franc brigham anderson andrew moor activ learn hidden markov model object function algorithm proceed 22nd intern confer machin learn p916 august 0711 2005 bonn germani rebecca hwa sampl select statist grammar induct proceed 2000 joint sigdat confer empir method natur languag process larg corpora held conjunct 38th annual meet associ comput linguist p4552 octob 0708 2000 hong kong sean p engelson ido dagan minim manual annot cost supervis train corpora proceed 34th annual meet associ comput linguist p319326 june 2427 1996 santa cruz california kiyonori ohtak analysi select strategi build dependencyanalyz corpu proceed colingacl main confer poster session p635642 juli 1718 2006 sydney australia georg k baah alexand gray mari jean harrold onlin anomali detect deploy softwar statist machin learn approach proceed 3rd intern workshop softwar qualiti assur novemb 0606 2006 portland oregon jason baldridg mile osborn activ learn hpsg pars select proceed seventh confer natur languag learn hltnaacl 2003 p1724 may 31 2003 edmonton canada prem melvil raymond j mooney divers ensembl activ learn proceed twentyfirst intern confer machin learn p74 juli 0408 2004 banff alberta canada mark steedman rebecca hwa stephen clark mile osborn anoop sarkar julia hockenmai paul ruhlen steven baker jeremiah crim exampl select bootstrap statist parser proceed confer north american chapter associ comput linguist human languag technolog p157164 may 27june 01 2003 edmonton canada hinrich schtze emr velipasaoglu jan pedersen perform threshold practic text classif proceed 15th acm intern confer inform knowledg manag novemb 0611 2006 arlington virginia usa rebecca hwa sampl select statist pars comput linguist v30 n3 p253276 septemb 2004 jame f bowr jame rehg mari jean harrold activ learn automat classif softwar behavior acm sigsoft softwar engin note v29 n4 juli 2004 p engelbrecht sensit analysi decis boundari neural process letter v10 n3 p253266 dec 1999 stephen soderland learn inform extract rule semistructur free text machin learn v34 n13 p233272 feb 1999 michael lindenbaum shaul markovitch dmitri rusakov select sampl nearest neighbor classifi machin learn v54 n2 p125152 februari 2004 steven wolfman tessa lau pedro domingo daniel weld mix initi interfac learn task smartedit talk back proceed 6th intern confer intellig user interfac p167174 januari 1417 2001 santa fe new mexico unit state gaurav pandey himanshu gupta pabitra mitra stochast schedul activ support vector learn algorithm proceed 2005 acm symposium appli comput march 1317 2005 santa fe new mexico qi su dmitri pavlov jyhherng chow wendel c baker internetscal collect humanreview data proceed 16th intern confer world wide web may 0812 2007 banff alberta canada atsushi fujii takenobu tokunaga kentaro inui hozumi tanaka select sampl examplebas word sens disambigu comput linguist v24 n4 p573597 decemb 1998 sunita sarawagi anuradha bhamidipati interact dedupl use activ learn proceed eighth acm sigkdd intern confer knowledg discoveri data mine juli 2326 2002 edmonton alberta canada leonardo franco sergio canna gener select exampl feedforward neural network neural comput v12 n10 p24052426 octob 2000 aleksand kocz joshua alspector asymmetr missingdata problem overcom lack neg data prefer rank inform retriev v5 n1 p540 januari 2002 jianqiang shen thoma g dietterich activ em reduc nois activ recognit proceed 12th intern confer intellig user interfac januari 2831 2007 honolulu hawaii usa francoi barbanon daniel p mirank sphinx schema integr exampl journal intellig inform system v29 n2 p145184 octob 2007 yevgeniy vorobeychik michael p wellman satind singh learn payoff function infinit game machin learn v67 n12 p145168 may 2007 kinh tieu paul viola boost imag retriev intern journal comput vision v56 n12 p1736 januaryfebruari 2004 dilek hakkanitr giusepp riccardi gokhan tur activ approach spoken languag process acm transact speech languag process tslp v3 n3 p131 octob 2006 zhihua zhou ming li tritrain exploit unlabel data use three classifi ieee transact knowledg data engin v17 n11 p15291541 novemb 2005 pabitra mitra b uma shankar sankar k pal segment multispectr remot sens imag use activ support vector machin pattern recognit letter v25 n9 p10671074 2 juli 2004 joel ratsabi learn multicategori classif sampl queri inform comput v185 n2 p298327 septemb 15 yoram baram ran elyaniv kobi luz onlin choic activ learn algorithm journal machin learn research 5 p255291 1212004 mariaflorina balcan alina beygelzim john langford agnost activ learn proceed 23rd intern confer machin learn p6572 june 2529 2006 pittsburgh pennsylvania maytal saartsechanski foster provost activ sampl class probabl estim rank machin learn v54 n2 p153178 februari 2004 david lewi william gale sequenti algorithm train text classifi proceed 17th annual intern acm sigir confer research develop inform retriev p312 juli 0306 1994 dublin ireland huan liu hiroshi motoda lei yu select sampl approach activ featur select artifici intellig v159 n12 p4974 novemb 2004 vijay iyengar chidanand apt tong zhang activ learn use adapt resampl proceed sixth acm sigkdd intern confer knowledg discoveri data mine p9198 august 2023 2000 boston massachusett unit state hema raghavan omid madani rosi jone activ learn feedback featur instanc journal machin learn research 7 p16551686 1212006 russel greiner adam j grove dan roth learn costsensit activ classifi artifici intellig v139 n2 p137174 august 2002 raymond j mooney lorien roy contentbas book recommend use learn text categor proceed fifth acm confer digit librari p195204 june 0207 2000 san antonio texa unit state henrik jacobsson crystal substochast sequenti machin extractor cryssmex neural comput v18 n9 p22112255 septemb 2006 huan liu hiroshi motoda issu instanc select data mine knowledg discoveri v6 n2 p115130 april 2002 xingquan zhu xindong wu costconstrain data acquisit intellig data prepar ieee transact knowledg data engin v17 n11 p15421556 novemb 2005 gedimina adomaviciu alexand tuzhilin toward next gener recommend system survey stateoftheart possibl extens ieee transact knowledg data engin v17 n6 p734749 june 2005 p engelbrecht sensit analysi select learn feedforward neural network fundamenta informatica v46 n3 p219252 august 2001 andri p engelbrecht sensit analysi select learn feedforward neural network fundamenta informatica v45 n4 p295328 decemb 2001 hasenjg h ritter activ learn neural network new learn paradigm soft comput physicaverlag gmbh heidelberg germani 2002