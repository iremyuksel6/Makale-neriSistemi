finit precis error analysi neural network hardwar implement parallel process low precis fix point hardwar use build high speed neural network comput engin low precis result drastic reduct system cost reduc silicon area requir implement singl process unit taken advantag implement multipl process unit singl piec silicon oper parallel import question aris much precis requir implement neural network algorithm low precis hardwar theoret analysi error due finit precis comput undertaken determin necessari precis success forward retriev backpropag learn multilay perceptron analysi easili extend provid gener finit precis analysi techniqu neural network algorithm set hardwar constraint may evalu b introduct high speed desir implement mani neural network algorithm backpropag learn multilay perceptron mlp may attain use finit precis hardwar finit precis hardwar howev prone error method theoret deriv statist evalu error present could use guid detail hardwar design algorithm implement paper devot deriv techniqu involv well detail backpropag exampl intent provid gener framework neural network algorithm set hardwar constraint may evalu section 2 demonstr sourc error due finit precis comput statist properti gener error model also deriv equat error output gener compound oper may written exampl error equat deriv section 3 oper requir forward retriev error backpropag step mlp statist analysi simul result result distribut error individu step mlp also includ section error equat integr section 4 predict influenc finit precis comput sever stage earli middl final stage backpropag learn final conclud remark given section 5 2 sourc error finit precis comput finit precis comput nonlinear oper multipl variabl sever sourc error exist exampl comput oewx two input variabl w x input error ffl w ffl x respect whose sourc prior finit precis data manipul error gener due finit precis comput involv oper specif finit precis multipl two variabl gener one error ffl 3 similarli finit precis nonlinear oper oe gener error ffl oe therefor result finit precis result equal assum error product ffl w ffl x neglig first order taylor seri approxim use input error propag oper exampl multipl two variabl finit precis error propag error ie wffl x propag error along gener finit precis multipl error ffl 3 propag nonlinear oper result total finit precis error total finit precis error ffl impos becom input finit precis error variabl futur oper 21 error gener propag success oper compound oper produc success oper oe shown figur 1 error input ffl x error gener oper ffl oe propag remain oper output approxim output error ffl term ffl x ffl oe oe 8 figur 1 figur 1 success oper gener propag error defin intermedi result first success oper 5 carri similar expans intermedi valu rewrit k 1 defin 1 product shown chain rule deriv approxim deriv without error note approxim equival approxim alreadi made first order taylor seri therefor x 22 error gener propag gener compound oper effect finit precis error output gener system compound oper multipl input variabl calcul extens previou analysi success oper singl variabl 8 follow step employ 1 break comput calcul graph see exampl given figur 2 gener calcul graph made n oper foe g system input g 2 number oper oe intermedi gener input fy k g oper oe lower indic oper output extend eq 8 multipl input total finit precis error ffl given 8 3 use calcul graph partial deriv evalu substitut eq 9 give equat ffl 4 statist method discuss use evalu error eq 9 method includ comput mean varianc variou function random variabl well approxim use central limit theorem 23 common techniqu finit precis comput three common techniqu use finit precis comput truncat jam round truncat oper simpli chop q lowest order bit number leav new lowest order bit 2 r th place unchang jam oper chop q lowest order bit number forc new lowest order bit 2 r th place 1 q remov bit 1 otherwis new lowest order bit retain valu oper equival replac 2 r th bit logic 2 r th bit q bit chop jam oper advantag gener error zero mean gener error higher varianc compar truncat one round oper also chop q lowest order bit number new lowest order bit 2 r th place q bit valu chop greater equal 2 r01 result valu increment 2 r otherwis remain unchang 1 error gener truncat jam round techniqu may consid discret random variabl distribut rang determin specif techniqu employ statist view error desir know mean varianc error gener three techniqu discret random variabl x mean given varianc usual assum uniformli distribut truncat truncat gener error uniformli distribut rang 02 r 2 q possibl error valu equal probabl therefor 1 mean varianc may comput jam error gener jam uniformli distribut probabl error zero twice probabl error hold possibl valu rang error x result 2 q1 0 1 possibl error valu mean varianc jam error round round gener error uniformli distribut rang 02 r01 2 q possibl error valu equal probabl therefor 1 mean varianc comput nonlinear function discret random variabl nonlinear function discret random variabl x mean varianc given 24 statist properti independ random variabl two independ random variabl x mean x varianc oe 2 constant follow properti mean varianc shown 7 1 multipl constant 2 sum two independ random variabl 3 product two independ random variabl 25 statist properti sum independ random variabl expect squar error expect squar error written term mean varianc error consid set error independ random variabl fffl g mean varianc oe 2 expect valu averag sum squar fffl g written n note oe expect valu averag sum squar error equal central limit theorem central limit theorem 7 state fx g independ random variabl densiti sum properli normal tend normal curv n 1 discret random variabl probabl tend sampl normal curv normal achiev coupl differ way discret random variabl mean varianc oe 2 sum x result invok central limit theorem probabl sum random variabl x equal discret valu x k approach sampl normal curv n larg oe x sum product independ random variabl central limit theorem extend cover case random variabl sum product random variabl say independ random variabl fx g fy g probabl densiti random variabl xy approach normal curv larg n mean varianc equal xy 28 3 applic neural network retriev learn shown oper retriev learn phase neural network model formul linear affin transform interleav simpl linear nonlinear scalar oper term hardwar implement formul call mac multipli accumul processor hardwar 4 5 1 without loss gener specif discuss multilay perceptron neural network model backpropag learn 10 9 31 forward retriev back propag mlp given train fixedweight mlp retriev phase receiv input test pattern fx 0i g propag forward network comput activ valu output layer fx lj g use indic classif regress purpos hand commonli use learn phase mlp input train pattern fx 0i g first propag forward network activ valu fx lj g comput accord forward oper use retriev phase output activ valu fx lj g compar target valu g valu output delta fffi lj g neuron output layer deriv error signal propag backward allow recurs comput hidden delta fffi lj g well updat valu weight f1w lij g layer mani method propos acceler learn estim local curvatur train error surfac use second order deriv inform discuss method beyond scope paper refer 3 oper forward retriev llayer perceptron formul forward affin transform interleav nonlinear scalar activ function x li denot activ valu th neuron l th layer w l1ij denot synapt weight interconnect th neuron l th layer j th neuron th layer nonlinear activ function f1 usual taken sigmoid learn mlp follow iter gradient descent approach follow updat present train data pair 10 9 comput backpropag error ffi lj formul backward affin transform interleav postmultipl deriv nonlinear activ function initi outputlay propag error 32 finit precis analysi forward retriev explicitli follow procedur discuss section 22 calcul graph forward retriev oper simplifi notat see eq 29 mlp shown figur r r09 0x n w nj f figur 2 calcul graph forward retriev mlp 3 denot truncat jam round oper carri analyt formula given eq 9 forward retriev mlp sever partial deriv need comput eq 33 w ij i3 i3 substitut valu partial deriv eq 34 37 gener propag error variabl oper eq 9 ffl i3 33 finit precis analysi output eq 32 calcul graph comput backpropag error output neuron simplifi notat shown figur 3 r 00 r 023 j0 figur 3 calcul graph output neuron carri analyt formula given eq 9 output delta comput mlp partial deriv eq 39 evalu j0 substitut partial deriv individu error term overal finit precis error output delta comput 34 finit precis analysi hidden eq 31 calcul graph comput backpropag error hidden layer neuron simplifi notat shown figur 4 follow similar partial deriv evalu use eq 45 comput finit precis error hidden delta see eq 9 ffl k3 ffl k r 023 figur 4 calcul graph hidden neuron 35 finit precis analysi weight updat eq 30 calcul graph comput weight updat without momentum term simplifi notat shown figur 5 r 023 j3 figur 5 calcul graph follow similar partial deriv evalu use eq 47 comput finit precis error weight updat see eq 9 36 statist evalu finit precis error given analyt express finit precis error associ forward retriev backpropag mlp statist evalu error undertaken evalu base mean varianc analysi use truncat jam round techniqu also base statist properti independ random variabl sum independ random variabl sum product independ random variabl discuss section 23 24 25 first step choos precis compon employ step problem practic limit precis implement mlp algorithm might use precis follow 2 1 neuron 8bit 8 bit right decim input output target rang 00 10 2 weight bias use 16 bit one sign bit 3 bit left 12 bit right decim rang 080 80 3 output ffi 8 bit one sign bit 7 bit right decim rang 005 05 4 hidden ffi use 16 bit one sign bit 3 bit left 12 bit right decim rang 080 80 finit precis error forward retriev expect forward retriev error calcul singl layer neuron multipl layer neuron propag upward finit precis error lower layer first simplif may made equat 38 multipli accumul step comput without gener error enough bit eg 24 bit use intermedi step i3 case ffl i3 practic sinc expens accumul precis small 3 oper reduc final 24bit sum 8bit one sign bit 3 bit left 4 bit right decim valu use input sigmoid lookup tabl equat 38 may rewritten invok central limit theorem sum necessari know distribut random variabl w ij ffl w ij tabl 1 show statist evalu valu contribut compon eq 49 base assumpt bit size given evalu start 16bit weight uniformli distribut across entir rang 08 8 weight error come truncat 24bit weight 16 bit input neuron x 8bit valu uniformli distribut 00 10 truncat 24bit valu therefor ffl x truncat error 16 distribut f 0 j approxim function normal distribut random variabl ffl 3 error gener accumul 24bit valu jam becom 8bit valu ffl f j error gener lookup tabl approxim round 2 bit place rv type q r oe 2 round 8 12 truncat jam round 2 5 tabl 1 mean varianc variabl forward retriev calcul base precis assumpt given tabl 1 variou size bitalloc weight figur 6 show statist evalu averag sum squar ffl defin eq 49 24 due finit precis comput singl step forward retriev evalu weight bit number say kbit weight alway contain one sign bit 3 leftofdecim bit k04 rightofdecim bit lower solid curv show statist evalu finit precis error introduc neuron first hidden layer upper solid curv show second hidden layer output layer 2layer perceptron note statist evalu error show divein around 8bit weight fact suggest implement finit precis hardwar forward retriev purpos train network use high precis comput constraint sign plu 3 bit left decim download well train finit precis portion 8 bit total 4 bit right decim weight hardwar perform degrad due finit precis convers almost neglig weight bit averag squar figur statist evalu simul evalu valu effl 2 introduc neuron first second hidden layer simul also conduct verifi statist evalu 2layer perceptron 100 input 100 hidden neuron 100 output neuron simul 100 set randomli gener 100dimension input data test averag weight network also randomli gener averag sum squar differ finit precis comput full precis 64 bit contribut compon comput thu obtain lower dash curv show simul evalu finit precis error introduc neuron first hidden layer upper dash curv show output layer curv match quit consist produc statist evalu 4 finit precis analysi iter learn discuss section 31 backpropag learn involv four consecut step comput forward retriev output delta comput hidden delta comput weight updat therefor weight updat iter present train pattern finit precis error ffl 1w introduc f1w lij g given eq 48 fact propag result error gener previou three step therefor final mathemat express finit precis error singl weight updat iter formul straightforward manner base exist deriv given eq 38 44 46 48 statist evalu valu averag sum squar weight updat error ffl 1w due finit precis comput singl learn iter thu comput backpropag learn discuss simpli nonlinear optim problem base simpl gradient descent search eleg way comput gradient use chain rule layer network gradient descent search updat weight base first deriv approxim error surfac updat individu weight independ other 6 therefor even approach comput effici behav unwis converg slowli complex error surfac due strong influenc introduc gradient descent search approxim real effect learn converg accuraci due finit precis comput difficult measur therefor statist evalu averag sum squar ffl 1w determin network propens learn 41 ratio finit precis error meaning measur ae potenti indic effect finit precis comput weight updat defin ratio statist averag sum squar finit precis weight updat error ffl 1w full precis weight updat magnitud 1w 2 ratio serv use indic addit impact caus finit precis comput top caus gradient descent approxim backpropag learn ratio depend number bit assign finit precis comput current stage learn progress specifi distribut differ desir actual output g specif assum ffl x lj 0 sinc abil learn depend abil learn finit precis valu base practic choic finit precis bit size given section 36 vs number bit say k bit assign weight fw ij g weight updat f1w ij g statist evalu ratio sever differ stage learn figur 7 show statist evalu valu finit precis ratio weight connect neuron hidden output layer 2layer mlp 4 differ valu use repres four differ stage learn earli stage middl stage converg stage converg stage figur 8 show statist evalu valu finit precis ratio weight connect neuron hidden input layer four differ valu fi note finit precis ratio curv gradual along variou stage learn dive around region number bit weight 1214 bit soft converg stage 1416 hard converg stage learn get almost steadi number bit increas dive point indic potenti strong disturb converg accuraci backpropag lean long run therefor give good guidelin number bit requir weight learn similar learn converg accuraci attain use high precis comput also interest note later stage learn impact finit precis error get larger due smaller valu fi network finetun weight weight bit finit precis figur 7 statist evalu valu finit precis ratio toplay weight 2layer mlp four differ stage learn evalu 42 simul result iter learn verifi theoret evalu impact caus finit precis comput simpl regress problem design map 2dimension input fx 1 1dimension output fyg mlp contain 2 input neuron 8 hidden neuron 1 output neuron adopt 256 pair randomli select data train finit precis learn simu weight bit finit precis figur 8 statist evalu valu finit precis ratio hiddenlay weight 2layer mlp four differ stage learn evalu lation perform base choic bit size compon given section 36 vs differ number bit assign fw ij g f1w ij g figur 9 show averag 256 train data squar differ desir actual output 2d regress problem network converg hard converg usual requir kind nonlinear regress problem note predict point around 1516 bit weight squar differ curv dive impli inabl converg desir map number bit weight less 16 bit similar support result also observ xor classif problem use mlp 2 input 3 hidden 1 output see figur 10 due classif natur xor problem soft converg good enough termin train therefor predict point 1213 bit weight squar differ curv dive anoth interest observ worthwhil mention total finit precis error singl iter weight updat mainli gener final jam oper comput output delta hidden delta weight updat therefor even though requir least 13 16 bit assign comput weight updat store total weight valu number weight bit comput forward retriev hidden delta step learn low 8 bit without excess degrad learn converg accuracy002006010140184 weight bit averag squar figur 9 averag squar differ desir actual output 2d regress problem network converg 010203 weight bit averag squar figur 10 averag squar differ desir actual output problem network converg conclud remark paper devot deriv finit precis error analysi techniqu neural network implement especi analysi backpropag learn mlp analysi techniqu propos versatil prepar ground wider varieti neural network algorithm recurr neural network competit learn network etc network share similar comput mechan use backpropag learn forward retriev oper shown 8bit weight suffici maintain perform use high precis comput hand network learn least 1416 bit precis must use weight avoid train process divert much trajectori high precis comput r vlsi architectur highperform implement limit artifici neural network nonlinear optim neural network learn parallel architectur artifici neural net unifi architectur artifici neural network recurs least squar learn algorithm neural net work pizer victor l learn intern represent error propag beyond regress new tool predict analysi behavior scienc tr unifi systol architectur artifici neural network learn intern represent error propag ctr ming zhang stamati vassiliadi jo g delgadofria sigmoid gener neural comput use piecewis approxim ieee transact comput v45 n9 p10451049 septemb 1996 cesar alippi luciano briozzo accuraci vs precis digit vlsi architectur signal process ieee transact comput v47 n4 p472477 april 1998 yongsoon lee seokbum ko fpgabas face detector use neural network scalabl float point unit proceed 5th wsea intern confer circuit system electron control signal process p315320 novemb 0103 2006 dalla texa cesar alippi random algorithm systemlevel polytim analysi robust comput ieee transact comput v51 n7 p740749 juli 2002