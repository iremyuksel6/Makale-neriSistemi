optim thread mpi execut smp cluster previou work shown use thread execut mpi program yield great perform gain multiprogram sharedmemori machin paper investig design implement threadbas mpi system smp cluster studi indic proper design thread mpi execut pointtopoint collect commun perform improv substanti compar processbas mpi implement cluster environ contribut includ hierarchyawar adapt commun scheme thread mpi execut threadsaf network devic abstract use eventdriven synchron provid separ collect pointtopoint commun channel paper describ implement design illustr perform advantag linux smp cluster b introduct commerci success smp architectur smp cluster commod compon wide deploy high perform comput due great econom advantag cluster 1 2 mpi messagepass standard 14 wide use highperform parallel applic implement larg array comput system paper studi fast execut mpi program dedic smp cluster mpi paradigm mpi node execut piec program separ address space mpi node uniqu rank allow mpi node identifi commun peer result global variabl declar mpi program privat mpi node natur map mpi node pro cess howev commun process go oper system kernel could costli previou studi 16 18 show processbas implement suer larg perform loss multiprogram sharedmemori machin smm map mpi node thread open possibl fast synchron address space share approach requir compil transform mpi program threadsaf form demonstr previou tmpi work 16 18 approach deliv signific perform gain larg class mpi c program multiprogram smm extend thread mpi implement singl smm support smp cluster straightforward smp cluster environ process thread within machin commun share memori commun process thread dierent machin go network normal sever order magnitud slower sharedmemori access thu addit map mpi node thread within singl machin import ecient mpi implement take advantag twolevel commun channel hierarchi common intuit cluster environ internod messag delay domin perform com munic thu advantag execut mpi node thread diminish shown later experi counter intuit multithread yield great perform gain mpi commun smp cluster environ use thread speed synchron thread singl smp node also greatli reduc buer orchestr overhead commun among thread dierent node paper present design implement threadbas mpi system linux smp cluster examin benefit multithread platform key optim propos hierarchyawar adapt commun scheme thread mpi execut threadsaf network devic abstract use eventdriven synchron provid separ collect pointtopoint commun channel eventdriven synchron among mpi node take advantag lightweight thread elimin spin overhead caus busi poll channel separ allow flexibl ecient design collect commun prim itiv experi conduct dedic smp cluster expect greater perform gain demonstr nonded cluster futur work rest paper organ follow section 2 introduc background overview threadbas mpi system tmpi cluster section 3 discuss design tmpi system section 4 report perform studi section 5 conclud paper 2 background overview design base mpich known portabl mpi implement achiev good perform across wide rang architectur 8 scheme target mpi program execut use thread thu first give type mpi program address paper briefli give overview mpich system give highlevel overview threadbas mpi system tmpi smp cluster 21 use thread execut mpi program use thread execut mpi program improv perform portabl mpi program minim impact multiprogram due fast context switch ecient synchron thread shown 16 experi sgi origin 2000 indic thread mpi execut outperform sgi nativ mpi implement order magnitud multiprogram smm mention need compiletim transform mpi program emerg process model use mpi paradigm major task procedur call variabl privat basic provid perthread copi global variabl insert statement origin program fetch thread privat copi variabl insid function global variabl referenc algorithm base gener mechan avail thread librari call threadspecif data tsd extend tsd make feasibl run multithread mpi program note tsdbase transform algorithm abl support multithread within singl mpi node howev current tmpi runtim system allow thread within singl mpi node call mpi function simultan mpithreadseri detail inform pleas refer 18 everi mpi program transform map mpi node thread one major restrict aect applic thread execut mpi program call lowlevel librari function threadsaf eg signal sinc scientif program involv type function particularli mpi specif discourag use signal techniqu applic larg class scientif applic two minor factor need mention aect much applic techniqu 1 total amount memori use mpi node run one smp node fit singl virtual address space problem consid 64bit os becom popular 2 fix perprocess file descriptor tabl size unix system sinc unix network librari use file descriptor repres stream connec tion applic might fail total number file open mpi node singl smp rel larg applic regular read computewrit pattern modifi separ file io program circumv problem 22 mpich smp cluster mpich follow sophist layer design welldocu discuss literatur design borrow idea mpich briefli summar architectur design mpich section goal mpich layer design make easi fast port system new architectur yet allow room tuneup replac rel small part softwar shown figur 1 mpich commun implement consist four layer bottom top 1 devic layer includ variou oper system facil softwar driver kind commun devic 2 adi layer layer encapsul dierenc variou commun devic provid uniform interfac upper layer adi layer export pointtopoint commun interfac 7 3 mpi pointtopoint primit built directli upon adi layer also manag highlevel mpi commun semant context commun 4 mpi collect primit built upon point topoint primit layer messag share channel pointtopoint commun collect commun mpich use special tag distinguish messag belong user pointtopoint commun intern messag collect oper mpi collect mpi pointtopoint adi chameleon interfac t3d sgi other shmem mpi collect mpi point point abstract devic interfac devic figur 1 mpich commun architectur port mpich dierent platform necessari wrap commun devic target system provid adi interfac design mainli target larg parallel system network cluster map mpi node individu process easi modifi current mpich system map mpi node lightweight thread lowlevel layer mpich threadsaf even though latest mpich releas support mpi2 standard mt level actual mpithreadsingl cluster node mpi node process mpich daemon process interprocess pipe share memori connect ws ws ws ws figur 2 mpich use combin tcp share memori shown figur 1 current support smp cluster mpich basic combin two devic sharedmemori devic network devic tcpip figur 2 show sampl setup configur exampl 8 mpi node evenli scatter cluster node also 4 mpi daemon process one cluster node fulli connect daemon process necessari drain messag tcp connect deliv messag across clusternod boundari mpi node commun daemon process standard interprocess commun mechan domain socket mpi node cluster node also commun share memori ws ws ws ws cluster node mpi node process mpich daemon process interprocess pipe connect figur 3 mpich use tcp one also configur mpich compil time make complet oblivi sharedmemori architectur insid cluster node use loopback devic commun among mpi node run cluster node case setup look like figur 3 exampl show number mpi node node distribut previou configur dierent previou one 8 daemon process one mpi node send messag mpi node cluster node go path send messag mpi node dierent cluster node possibl faster 23 thread mpi execut smp cluster cluster node mpi node thread connect direct mem access thread sync ws ws ws ws figur 4 commun structur thread mpi execut section provid overview thread mpi execut smp cluster describ potenti advantag tmpi facilit understand take sampl program use figur 2 illustr setup commun channel tmpi threadbas mpi system figur 4 see map mpi node cluster node thread insid process add addit daemon thread process despit appar similar figur 2 number dierenc design mpich 1 tmpi commun mpi node cluster node use direct memori access instead sharedmemori facil provid oper system 2 tmpi commun daemon mpi node also use direct memori access instead domain socket 3 unlik processbas design remot messag send receiv deleg daemon process threadbas mpi implemen tation everi mpi node send receiv remot mpi node directli shown later section dierenc impact softwar design provid potenti perform gain tmpi threadbas mpi system addit tmpi give us follow immedi advantag processbas mpi system compar mpich system use mix tcp share memori depict figur 2 1 mpich usual share memori limit system resourc osimpos limit maximum size singl piec share memori 1 also systemwid bound total size share memori fact one test benchmark come mpich fail limit sharedmemori resourc threadbas system doesnt suffer problem thread process access whole address space 2 easi way mpich aggreg dier ent type devic result mpi node use nonblock poll check pend messag either devic could wast cpu cycl sender readi yet synchron among thread flexibl lightweight combin eventdriven style daemon mpi node freeli choos busi spin block wait messag 3 share memori use mpich persist systemwid resourc automat way perform resourc clean program doesnt clean execut oper system could run sharedmemori descriptor buggi user program exit without call proper resourc cleanup function leav garbag share memori os thu reliabl user program run mpich base cluster sensit type error threadbas system tmpi problem share address space access complet userlevel oper compar threadbas mpi system pure tcpip base mpich implement depict figur 3 follow disadvantag 1 two data copi send messag two mpi node cluster node mpich synchron two mpi node also becom complic threadbas system tmpi 2 prolifer daemon process network connect situat get even wors fatter cluster node node processor run mpi node 24 relat work mpi network cluster also studi number project lammpi 13 4 mpi system base upon multicomput manag environ call trolliu 3 dierent mpich sens design specif network cluster lower level commun provid standalon servic uniqu request progress interfac address issu optim mpi perform cluster smp sun mpi implement 17 discuss optim mpi collect commun primit larg scale smp cluster focu work 1 version redhat linux 60 instal kernel version 2215 number 4mb optim collect oper singl fat smp node mpistart 9 made coupl optim smp cluster modifi mpich adi layer propos twolevel broadcast scheme take advantag hierarch commun channel collect commun design extend idea highli optim thread mpi execut magpi 10 optim mpi collect commun primit cluster connect widearea network mpifm 11 mpiam 19 12 attempt optim perform lower level commun devic mpi techniqu appli tmpi system mpilit 15 lpvm 20 tpvm 6 studi problem run messag pass program use thread singl sharedmemori machin knowledg research eort toward run mpi node use thread smp cluster research complement work focus take advantag execut mpi node use thread 3 system design implementa tion section detail design implement threadbas mpi system tmpi 31 system architectur mpi inter intra other mpi commun inter intramachin commun abstract network thread sync interfac os facil thread pthread thread impl figur 5 tmpi commun architectur system architectur mpi commun primit shown figur 5 2 four layer bottom top 1 oper system facil tcp socket interfac pthread 2 network thread synchron abstract layer potenti layer allow portabl tmpi system perform tuneup provid dierent implement network commun thread synchron thread synchron abstract almost direct map pthread api except thread launch batch style creat thread entiti start singl function call time function call return thread finish execut network devic abstract tailor thread mpi execut model dierent either tradit socket interfac mpich adi interfac talk detail section 32 section 33 2 synchron primit shown figur 5 compareandswap 3 low level abstract commun manag thread intra dierent cluster node intraclusternod commun manag commun among thread singl cluster node interclusternod commun layer wrap network abstract interfac manag logic commun multithread level specif thread local rank uniqu among thread cluster node global rank uniqu among thread cluster node given global rank need find cluster node thread resid local rank cluster node well revers lookup also need anoth function interclusternod commun modul resourc discov alloc api allow flexibl placement mpi node eg mani cluster node use rank mpi node place cluster node decis made anywher complet automat fulli control user suppli paramet 4 mpi commun primit implement includ notion commun messag tag context implement upon three build block intra interclusternod commun thread interfac particularli collect primit implement awar twolevel commun hierarchi despit similar mpich design shown figur 1 coupl notabl dierenc two system top layer tmpi awar dierent mechan commun thread dierent cluster node implement organ commun take advantag twolevel hierarch commun channel discuss section 33 section 34 tmpi collect commun primit built upon pointtopoint commun prim itiv instead implement independ top layer network devic abstract provid pointtopoint collect commun among process dierent cluster node 32 tmpi network devic abstract network devic abstract tmpi call netd abstract network applic group relev process dierent machin uniqu rank provid basic commun function applic thin layer contain 28 core function group three categori connect manag includ creation process number cluster node setuptear commun channel relev process fulli connect queri id total number process collect commun provid collect commun among relev process creat netd current implement use adapt algorithm choos among three dierent span tree base number process involv size small simpl scatter tree use hypercub use size larg balanc binari tree chosen size fall middl pointtopoint commun netd uniqu pointtopoint commun api messag contain messag header part option payload part content messag header interpret netd receiv messag caller must provid netd callback function call messag handl netd buer messag header invok handl sender id messag messag header messag handl respons examin header perform necessari action receiv payload part one interfac necessari ecient support mpi commun primit tmpi mpi node receiv messag unknown sourc mpianysourc situat complic tmpi normal network devic provid atom receiv oper thu multipl thread wait messag port singl logic messag could fragment receiv dierent thread get around prob lem tmpi use daemon thread receiv incom messag invok messag handl context daemon thread messag handl respons decod actual sourc destin buer incom data notifi destin thread 33 separ pointtopoint collect commun channel netd weve mention figur 4 everi thread process access commun port origin process featur inspir us idea separ collect pointtopoint commun channel allow tmpi take full advantag synchron natur collect commun elimin intermedi daemon overhead reason figur 4 thick line actual repres two tcp connec tion one dedic collect oper pointtopoint commun daemon thread respons serial incom messag pointtopoint commun separ pointtopoint commun channel collect commun channel base care observ mpi semant mpi receiv oper much complic collect oper besid wildcard receiv oper discuss outoford notion messag tag asynchron notion nonblock receiv thu daemon thread requir serial also buer purpos due limit buer network devic daemon thread activ drain incom messag deadlock situat permit mpi standard could happen figur 6 show exampl node 0 node 1 would block mpibsend statement without presenc daemon thread even enough buer space avail collect commun mpi oper never oford alway synchron sens collect oper complet mpi node involv reach rendezv point 3 structur span tree determin runtim oper mpich separ collect pointpoint commun channel highlevel mpi collect commun primit implement top point topoint commun layer result collect oper go pointtopoint commun daemon caus unnecessari overhead separ pointto point collect commun channel could benefit processbas mpi implement well howev may eectiv tmpi two process cluster node directli share network commun port eg socket 34 hierarchyawar collect commun design collect commun tmpi implement seri intra interclusternod collect communi cation exampl mpibcast interbcast follow intrabcast mpiallreduc intra reduc follow interreduc interbcast finish intrabcast intraclusternod collect commun take advantag address space share implement base ecient lockfre fifo queue algorithm way collect commun take full advantag twolevel commun hierarchi conceptu twophas collect commun build span tree two step idea first mention mpistart project 9 essenti design root mpi node cluster node form span tree connect cluster node mpi node connect local root cluster node form whole span tree span tree exactli n1 edg cross clusternod boundari call network edg n number cluster node involv commun note mpich use share memori set actual take twostep approach build span tree directli given number mpi node without know distribut mpi node cluster node result span tree collect commun mpich may network edg figur 7 compar span tree sampl mpi program size 9 run 3 cluster node see tmpi left part result 2 network edg mpich right part 5 network edg 3 possibl employ techniqu pipelin asynchron oper optim collect oper howev optim requir mpi standard eectiv emin real applic base experi span tree mpi program 9 node three cluster node three cluster node contain mpi node 02 35 68 respect thick edg network edg tmpi mpich figur 7 collect commun span tree tmpi mpich adapt buffer manag pointto point commun pointtopoint commun tmpi bear lot similar mpich design conceptu mpi node receiv queue unexpectedmessag queue sender come correspond receiv send request handl deposit receiv unexpect messag queue similarli receiv come correspond sender receiv request handl store receiv queue pair sender receiv dierent cluster node daemon thread receiv side act behalf remot sender one dicult problem face design temporari buer messag correspond receiv readi yet intraclusternod pointtopoint com munic alway block sender till receiv come intern buer space avail ever sender send messag remot receiv know whether sucient buer space hold messag messag size mpi could arbitrarili larg tradit conserv solut threephas asynchron messagepass protocol 5 tmpi address space share fast synchron among thread lead us ecient adapt buer solut combin optimist eager push protocol threephas protocol basic sender need make guess base messag size whether transfer actual messag data send request metadata eagerpush protocol send metadata first send data later receiv readi threephas protocol remot daemon receiv side acknowledg whether sender made correct guess figur 8 show three simplifi case tmpi inter clusternod pointtopoint commun protocol provid detail descript follow note figur accommod case synchron readi send oper figur 8 sender send request actual data receiv side either receiv alreadi arriv still intern buer space avail daemon accept data store send request inform actual data proper queue notifi sender data accept wake receiv necessari sendersid dae mpibsendbuf bigsiz type 1 mpirecvbuf bigsiz type 1 mpibsendbuf bigsiz type 0 mpirecvbuf bigsiz type 0 figur possibl deadlock without daemon thread success eagerpush reqdat r got dat b fail eagerpush degrad threephas protocol reqdat got req receiv readi dat r got dat r c threephas protocol got req receiv readi dat r got dat r sender daemon q msg queue r receiv network transact msg queue op receiv arrivalwakeup network boundari figur 8 pointtopoint commun node dierent cluster node mon receiv confirm free data behalf sender figur 8 b sender still send data request time receiversid daemon accept incom data daemon receiv discard data store request metadata unexpectedmessag queue later receiv arriv discov partial complet send request ask associ data sender sendersid daemon send data receiv side receiversid daemon receiv data save data receiversuppli user buer tell sender data receiv subsequ sendersid daemon dealloc buer upon receiv acknowledg note actual data transfer twice case figur 8 c sender decid send request part data part separ whole flow essenti figur 8 except actual data transfer design allow optim perform sender make correct guess function correctli degrad perform guess wrong remain issu decid sender switch eagerpush threephas protocol decis complic fact intern buer space tmpi share mpi node cluster node aggreg among seri request common intuit choos eagerpush protocol often possibl could still prefer buer request even though buer space avail abl amount buer space might use hold data multipl subsequ request like tradit nonpreempt schedul problem favor short request algorithm could suboptim due lack futur knowledg current implement tmpi sender send data request messag size static defin threshold receiv side receiv daemon greedili buer incom data whenev possibl current threshold set 100kbyte base empir studi final allow receiv daemon send messag could result deadlock condit design care tmpi netd layer support block nonblock send nonblock send mere put messag queue send daemon respons send messag dealloc memori necessari receiv daemon alway use nonblock send 36 discuss benefit address space share tmpi one benefit threadbas mpi implement potenti save data copi addressspac shar ing intraclusternod pointtopoint commun tmpi need one intermedi data copi mpich take two intermedi data copi share memori three intermedi data copi without share memori interclusternod pointto point commun sinc daemon either sender receiversid access sender receiverbu tmpi take zero two intermedi data copi mpich alway need three intermedi data copi ing addit data need move across process boundari tmpi data need transfer across process boundari three time mpich sinc two involv process must synchron transfer data one mpich perform sensit os schedul multiprogram environ tmpi scalabl presenc singl receiv daemon thread handl incom messag potenti bottleneck term scalabl tmpi possibl configur multipl daemon incom connect partit among multipl daemon howev current compiletim paramet plan studi adapt choos number daemon runtim fu ture also creat instanc nonblock send daemon make respons messag send oper initi mpi node could benefici sudden surg outgo data want block sender thread current small scale set none configur nece sari point tmpi design scalabl accommod larg cluster fat smp node 4 experi section evalu eectiv propos optim techniqu thread mpi implement versu processbas implement choos mpich refer processbas implement due wide accept emphas experi meant illustr whether flaw mpich design instead discuss previou sec tion want show potenti advantag thread mpi execut smp cluster environ fact optim techniqu tmpi possibl multithread mpi implement implement prototyp system linux smp cluster includ 45 mpi function mpi 11 standard list appendix yet support heterogen architectur 4 support userdefin data type layout mpich version use contain function mpi 11 standard provid partial support mpi2 standard howev ad function rel independ task aect experiment result experi conduct cluster six quad xeon 500mhz smp 1gb main memori cluster node two fast ethernet card connect lucent canjun switch oper system redhat linux 60 run linux kernel version 2215 channel bond enabl 5 41 microbenchmarkperform point topoint commun section use microbenchmark make finegrain analysi pointtopoint commun subsystem tmpi mpich pingpong test first use pingpong benchmark access perform pointtopoint commun vari data size 4 byte 1 megabyt common practic choos dierent metric small larg messag messag size smaller 1 4 note mpich detect whether underli system homogen startup time data convers overhead incur 5 channelbond allow singl tcp connect util multipl network card could improv network bandwidth help reduc network delay case achiev raw bandwidth 200mbp pair cluster node kilobyt report perform term roundtrip delay messag lager 1 kilobyt report transferr defin total amount byte sentreceiv divid roundtrip time round trip time pingpong short messag tmpi mpich transfer rate b pingpong long messag tmpi mpich figur 9 interclusternod pingp perform figur 9 show pingpong perform two mpi node dierent cluster node see messag size small tmpi perform slightli better mpich except messag small case tmpi save interprocess data transfer overhead system call context switch becom evid messag size becom larger tmpi constant 2mb bandwidth advantag mpich due save data copi round trip time pingpong short messag tmpi transfer rate b pingpong long messag tmpi figur 10 intraclusternod pingpong perform two version mpich use mpich share memori mpich1 mpich without share memori access impact share memori thread processbas mpi system figur 10 show pingpong perform two mpi node cluster node compar perform three mpi system tmpi mpich share memori mpich1 mpich without use share memori mpich2 evid ignor underli sharedmemori architectur yield much wors perform mpich2 compar two system tmpi advantag mpich1 mainli come save intermedi memori copi long messag fast synchron among thread short messag result tmpi perform nearli doubl mpich share memori note tmpi mpich1 perform drop reach peak transfer rate like caus underli memori content oneway messag pipelin second benchmark simpl oneway sendrecv test sender keep send messag receiv bench mark examin impact synchron among mpi node compar averag time take sender complet send oper short messag still use transfer rate metric larg messag messag size byte singl op time oneway sendrecv short messag tmpi mpich messag size kb transfer rate b oneway sendrecv long messag tmpi mpich figur oneway sendrecv perform figur 11 show oneway sendrecv perform two mpi node dierent cluster node theoret speak ing thread processbas mpi implement yield similar perform short messag averag time send oper equal singl trip time divid number outstand messag fli basic limit network bandwidth howev mention processbas mpi implement interclusternod messag pass need travel local daemon remot daemon time data need copi daemon local buer unless sender receiv daemon precis synchron either buer could full empti caus stall pipelin even though might still bandwidth avail seen figur 11 mpich perform unstabl messag size small due diculti synchron among process messag size becom larg fewer outstand messag perform less sensit synchron addit also see tmpi 30 advantag mpich short messag 2mb bandwidth advantag larg messag due save extra data copi need processbas mpi system comparison repeat oneway sendrecv test two mpi node cluster node result shown figur 12 compar among three mpi system notat figur 10 expect mpich1 perform almost ident tmpi small messag slightli poorer tmpi larg messag intermedi daemon process along data path either system cost extra data copi mpich1 amort among multipl outstand request hand mpich2 show irregular behavior certain rang messag size note figur 12 use dierent scale accommod perform curv mpich2 messag size 800 byte singl send oper take 34m complet messag size goe beyond op time tmpi messag size byte oneway sendrecv short messag tmpi op time transfer rate b oneway sendrecv long messag tmpi figur 12 intraclusternod oneway sendrecv perform two version mpich use mpich share memori mpich1 mpich without share memori mpich2 fall normal rang 50 150 abl identifi exact sourc problem think might someth resourc content os level regardless glitch perform mpich2 much wors two mpi system due ignor underli share memori 42 microbenchmark perform collect commun compar perform collect commun primit run three microbenchmark call mpireduc mpibcast mpiallreduc number time respect compar averag time oper data volum involv benchmark small cost mainli come synchron run benchmark three differ set 4 mpi node cluster node 41 node scatter 4 dierent cluster node 14 node scatter 4 mpi node 44 mpibcast mpireduc use three dierent variat regard root node dierent iter test alway stay rotat among mpi node rotat repeat use root coupl time root shift combo number conclus drawn experi result shown figur 13 1 case mpich2 perform worst among three mpi system except 14 case mpich1 mpich2 perform almost signifi import take advantag share memori mpi system smp cluster environ 2 tmpi 71 time faster mpibcast 77 time faster mpireduc mpich1 exploit share memori within smp among factor includ address space share hierarchi awar algorithm perform gain mainli come separ collect pointtopoint commun channel theoret speak mpibcast mpireduc oper alway faster mpiallreduc oper howev unit mpi reduc mpi bcast mpi allreduc node distr root tmpi mpich1 mpich2 tmpi mpich1 mpich2 tmpi mpich1 mpich2 rotat rotat rotat combo16125322242856656622204102462054 736 1412 19914 figur 13 collect commun perform number shown tabl averag time oper run benchmark three mpi system tmpi mpich share memori mpich1 mpich without share memori mpich2 mpi reduc mpi bcast test case root alway root rotat among mpi node rotat fix root number time rotat combo notat b use node distribut mean use b cluster node mpi node experi show case mpich mpibcast mpireduc oper perform wors mpiallreduc oper anomali caus mpich design collect commun top pointtopoint commun messag collect commun still go daemon get store messag queue match travers queue mani outstand request cost queue oper becom expens due content daemon could becom bottleneck mpiallreduc test suer problem mpi node synchron one outstand request hand separ commun channel pointtopoint collect com munic tmpi show anomali 3 tmpi root test perform much better rotat root test 14 44 case mean tmpi take better advantag messag pipelin due address space share elimin intermedi daemon 4 figur also evid advantag hierarchi awar commun design exampl mpibcast test 44 perform roughli equal summat 41 case 14 case sinc tmpi broadcast 44 case 14 broadcast follow 41 broadcast individu cluster node hand without use twolevel span tree mpich1 44 perform 1060 wors summat 41 case 14 case similar conclus also hold mpireduc mpiallreduc 43 macrobenchmark perform section use two applic kernel benchmark access eectiv tmpi optim kernel benchmark use matrixmultipl mm gaussianelimin ge perform computationintens linear algebra comput mm consist mostli mpibsend mpirecv ge mpibcast run mm 1 16 mpi node ge 1 24 mpi node detail node distribut shown figur 14 mm ge 9 33 4 22 figur 14 distribut mpi node b mean use b cluster node mpi node ensur number cluster node number mpi node cluster node decreas increas total number mpi node compar tmpi mpich use share memori smp perform result depict figur 15 see number mpi node small 4 tmpi mpich perform similarli howev number mpi node becom larg cluster node involv tmpi show better scalabl mpich mm tmpi constant 150mflop advantag mpich mainli come save intermedi copi ge neither system keep linear speedup mpich perform even degrad reach peak 12 mpi node tmpi outperform mpich 100 4 6 cluster node involv inde verifi advantag tmpi smp cluster environ reason tmpi gain much advantag ge case compar mm case ge call mpi broadcast function mm use point topoint commun demonstr previou section tmpi outperform mpich substanti collect commun 5 conclud remark paper present design implement threadbas mpi system smp cluster contribut includ novel network devic abstract interfac tailor thread mpi execut smp cluster hierarchyawar commun propos number optim techniqu includ separ number mpi node mflop matrix multipl tmpi mpich 25100300500700900number mpi node mflop b gaussian elimin tmpi mpich figur 15 macrobenchmark perform pointtopoint collect commun channel adapt buer eventdriven synchron take advantag multithread micro macro benchmark test experi show tmpi outperform mpich substanti note tmpi optim target class c program mpich design gener mpi program experi confirm even cluster environ internod network latenc rel high exploit threadbas mpi execut smp deliv substanti perform gain global commun fast lightweight syn chroniz experi focu dedic cluster futur work studi perform multiprogram environ threadbas synchron may achiev perform gain 6 acknowledg work support part nsf acir0082666 acir0086061 ccr9702640 would like thank lingkun chu help cluster administr would also like thank kai shen anonym refere valuabl comment 7 r case network workstat beowulf parallel workstat scientif comput lam open cluster environ mpi parallel comput architectur hardwaresoftwar approach tpvm distribut concurr comput lightweight process abstract devic definit support implement highlevel pointtopoint messag pass interfac deliv network perform numer applic mpi collect commun oper cluster wide area system mpifm higher perform mpi workstat cluster httpwwwlscndedulam forum year1999year httpwww mpisim use parallel simul evalu mpi program adapt twolevel thread manag fast mpi execut share memori machin optim mpi collect cluster largescal smp proceed acmiee supercomput 99 program transform runtim support thread mpi execut share memori machin httpnowcsberkeleyedufastcommmpi lpvm step toward multithread pvm tr highperform portabl implement mpi messag pass interfac standard mpifm mpisim magpi optim mpi collect cluster largescal smp adapt twolevel thread manag fast mpi execut share memori machin program transform runtim support thread mpi execut sharedmemori machin mpistart multiprotocol activ messag cluster smp parallel comput architectur ctr jian ke martin burtscher evan speight runtim compress mpi messan improv perform scalabl parallel applic proceed 2004 acmiee confer supercomput p59 novemb 0612 2004 rohit fernand keshav pingali paul stodghil mobil mpi program comput grid proceed eleventh acm sigplan symposium principl practic parallel program march 2931 2006 new york new york usa lingkun chu hong tang tao yang kai shen optim data aggreg clusterbas internet servic acm sigplan notic v38 n10 octob weirong zhu yanwei niu guang r gao perform portabl earth case studi across sever parallel architectur cluster comput v10 n2 p115126 june 2007