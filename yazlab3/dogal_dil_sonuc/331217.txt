effici algorithm blockcycl array redistribut processor set abstractruntim array redistribut necessari enhanc perform parallel program distribut memori supercomput paper present effici algorithm array redistribut cyclicx p processor cyclickx q processor algorithm reduc overal time commun consid data transfer commun schedul index comput cost propos algorithm base gener circul matrix formal algorithm gener schedul minim number commun step elimin node content commun step network bandwidth fulli util ensur equals messag transfer commun step furthermor time comput schedul index set significantli smaller take omaxpq time less 1 percent data transfer time comparison schedul comput time use stateoftheart scheme base bipartit match scheme 10 50 percent data transfer time similar problem size therefor propos algorithm suitabl runtim array redistribut evalu perform scheme implement algorithm use c mpi ibm sp2 result show algorithm perform better previou algorithm respect total redistribut time includ time data transfer schedul index comput b introduct mani high perform comput hpc applic includ scientif comput signal process consist sever stage 2 10 22 exampl applic includ multidimension fast fourier transform altern direct implicit adi method solv twodimension diffus equat linear algebra solver execut applic distribut memori supercomput data distribut need stage reduc perform degrad due remot memori access program execut proce one stage anoth data access pattern number processor requir exploit parallel applic may chang chang usual caus data distribut stage unsuit subsequ stage data redistribut reloc data distribut memori reduc remot access overhead sinc paramet redistribut gener unknown compil time runtim data redistribut necessari howev cost redistribut offset perform benefit achiev redistribut therefor runtim redistribut must implement effici ensur overal perform improv array data typic distribut blockcycl pattern onto given set processor blockcycl distribut block size x denot cyclicx block contain x consecut array element block assign processor roundrobin fashion distribut pattern cyclic block distribut special case blockcycl distribut gen eral blockcycl array redistribut problem reorgan array one blockcycl distribut anoth ie cyclicx cyclici import case problem redistribut cyclicx cyclickx aris mani scientif signal process applic type data redistribut occur within processor set differ processor set data redistribut given initi layout final layout consist four major step index comput commun schedul messag pack unpack final data transfer four step contribut total array redistribut cost briefli explain four cost associ data redistribut array element belong comput local memori locat local index array element local index use pack array element messag similarli destin processor determin sourc processor indic receiv messag comput local indic find locat receiv messag store total time comput indic denot index comput cost schedul comput cost commun schedul specifi collect senderreceiv pair commun step sinc commun step processor send one messag processor receiv one messag care schedul requir avoid content minim number commun step time comput commun schedul signific reduc cost import criteria perform runtim redistribut messag packingunpack cost sender messag consist word differ memori locat need gather buffer send node typic requir memori read memori write oper gather data form compact messag buffer time perform data gather sender messag pack cost similarli receiv side messag unpack data word need store appropri memori locat data transfer cost data transfer cost commun step consist startup cost transmiss cost startup cost incur softwar overhead commun oper total startup cost reduc minim number messag transfer step transmiss cost incur transfer bit network depend network bandwidth tabl 1 summar key featur well known data distribut algorithm literatur 4 13 14 15 21 known algorithm ignor one cost scheme focu effici index set comput complet ignor schedul commun event base index block scheme focu find destin processor gener messag destin commun schedul consid lead node content perform commun inturn lead higher data transfer cost node incur addit delay scheme elimin node content explicitli schedul commun event 3 17 23 although scheme 3 17 23 effici schedul algorithm design data redistribut processor set redistribut differ processor set caterpillar algorithm propos 11 use simpl round robin schedul avoid node content key featur schedul index comput commun pitfal 14 commun schedul index comput use line segment node content occur minim transmiss cost commun schedul effici index comput sourc destin set node content occur minim transmiss cost caterpillar simpl schedul algorithm index comput scan array segment node content minim transmiss cost number commun step bipartit match scheme larg schedul comput overhead schedul comput time node content stepwis strategi minim number commun step greedi strategi minim transmiss cost scheme fast schedul index comput schedul comput time node content minim number commun step data transfer cost tabl 1 comparison variou scheme array redistribut howev algorithm fulli util network bandwidth ie size data sent node commun step vari node node lead increas data transfer cost scheme 5 reduc data transfer cost howev schedul comput cost signific bipartit graph match use 5 take op state oftheart workstat time rang 100 msec p q interest problem interest schedul comput cost larger data transfer cost algorithm 5 optim data transfer cost number commun step non alltoal commun case one three case occur perform redistribut consid algorithm 5 optim data transfer cost alltoal commun case differ messag size optim data transfer cost necessari transfer messag equal size commun step paper propos novel effici algorithm data redistribut cyclicx p processor cyclickx q processor algorithm use optim number commun step fulli util network bandwidth step commun schedul determin use gener circul matrix framework schedul comput cost q implement show schedul comput time rang 100 sec p q rang 50100 processor comput index set commun schedul use set equat deriv gener circul matrix formul experiment result show schedul comput time neglig compar data transfer cost array size interest messag packingunpack cost scheme gener optim commun schedul thu scheme minim total time data redistribut make scheme attract runtim well compiletim data redistribut techniqu use implement scalabl redistribut librari implement direct hpf 1 develop parallel algorithm supercomput applic particular techniqu lead effici distribut corner turn oper commun kernel need parallel signal process applic 26 27 redistribut scheme implement use mpi c easili port variou hpc platform perform sever experi illustr improv perform compar stateoftheart experi perform determin data transfer schedul index comput cost one experi use 64 processor ibm sp2 partit 28 sourc processor 36 destin processor expans factor set 14 array size vari 226 mbyte 564 mbyte compar caterpillar algorithm data transfer time lower ratio data transfer time algorithm caterpillar algorithm 492 537 schedul comput time propos algorithm much less bipartit match scheme 5 p q 64 schedul comput time bipartit match scheme 100 msec algorithm 100 sec exampl schedul comput time use bipartit match scheme 1332 msec time use algorithm 1786 sec rest paper organ follow section 2 explain tablebas framework also discuss gener circul matrix formal deriv conflict free commun schedul section 3 explain redistribut algorithm index comput section 4 report experiment result ibm sp2 conclud remark made section 5 array n48 c cyclic4 q6 processor b cyclic2 p3 processor figur 1 blockcycl redistribut array point view array element b p processor c cyclicx p processor cyclickx q processor exampl 2 approach redistribut section present approach blockcycl redistribut problem subsect 21 discuss two view redistribut illustr concept superblock follow subsect explain tablebas framework redistribut use destin processor tabl column row reorgan subsect 23 discuss gener circul matrix formal allow us comput commun schedul effici 21 array processor point view blockcycl distribut cyclicx array defin follow given array n element p processor block size x array element partit contigu block x element th block b consist array element whose indic vari ix x block distribut onto processor roundrobin fashion block b assign processor j p j paper studi problem redistribut cyclicx p processor cyclickx q processor denot x array point view element array shown along singl horizont axi processor indic mark block redistribut x q period found p processor cyclic4 q processor c initi distribut tabl f final distribut tabl f20 b 22 b 9 b 22 b 9 initi layout b final layout figur 2 blockcycl redistribut cyclicx p processor cyclickx q processor processor point view exampl 2 block movement pattern exampl figur 1 b 0 b 3 b 6 b 9 initi assign p 0 move q respect b 12 p 0 move q 0 find commun pattern b 0 b 11 repeat block collect block call superblock period block movement pattern kq size superblock figur 1 superblock size lcm3 2 delta next superblock block b 12 b 23 move fashion processor point view blockcycl distribut repres 2 dimension tabl column correspond processor row index local block index entri tabl global block index therefor element j tabl repres th local block j th processor figur 2 show exampl 2 3 2 6 global block index destin processor index figur 3 exampl destin processor tabl processor point view block distribut tabl roundrobin fashion tabl correspond sourc processor denot initi layout repres block initi assign sourc processor similarli final layout repres block assign destin processor problem redistribut block initi layout final layout layout shown figur 2a respect initi layout partit collect row size l similarli final layout partit disjoint collect row collect row note collect correspond superblock block locat rel posit within superblock move way redistribut block transfer singl commun step mpi deriv data type handl block singl block without loss gener consid first superblock follow illustr algorithm refer tabl repres indic block within first superblock initi final layout initi distribut tabl final distribut tabl f shown figur 2c f respect cyclic redistribut problem essenti involv reorgan block within superblock initi distribut tabl final distribut tabl f 22 tablebas framework redistribut given redistribut paramet p k q block locat f determin redistribut block move initi locat final locat f thu processor ownership local memori locat block chang redistribut redistribut conceptu consid tabl convers process f decompos independ column row reorgan column row reorgan reorgan column reorgan 9 7 11 figur 4 tabl convers process redistribut column reorgan block rearrang within column tabl therefor local oper within processor memori row reorgan block within row rearrang oper therefor lead chang ownership block requir interprocessor commun destin processor block initi distribut tabl determin redistribut paramet global block index send commun event tabl construct replac block index initi distribut tabl destin processor index shown figur 3 denot destin processor tabl dpt th entri destin processor index th local block sourc processor j 0 consid one superblock l theta p matrix row correspond commun step algorithm commun step processor send data atmost one destin processor q p atmost p processor destin processor set receiv data destin processor remain idl commun step therefor commun step p commun pair hand q destin processor receiv data time maximum number commun pair commun step minp q without loss gener follow discuss assum q p figur 4 show tablebas framework redistribut convert initi distribut tabl final distribut tabl f dpt use use commun schedul effici lead node content sinc sever processor tri send data destin processor commun step exampl figur 4 step 0 sourc processor 0 1 tri commun destin processor 0 howev everi row consist p distinct destin processor indic among node content avoid commun step motiv column reorgan elimin node content dpt reorgan column reorgan reorgan tabl call send commun schedul tabl section 3 discuss reorgan perform l theta p matrix well entri destin processor index row correspond contentionfre commun step maintain correspond set column reorgan appli result distribut tabl 0 correspond commun step block row 0 transfer destin processor specifi correspond entri refer figur 4 first commun step sourc processor 0 1 2 transfer block 0 4 2 destin processor 0 2 1 respect specifi step call row reorgan distribut tabl 0 f correspond receiv block destin processor reorgan final distribut tabl f anoth set column reorgan exampl need oper receiv block store memori locat destin processor key idea choos requir row reorganizationscommun event perform effici support easytocomput contentionfre commun schedul far discuss redistribut problem cyclicx p processor cyclickx q processor dual relationship exist problem cyclicx p processor cyclickx q processor problem cyclickx p processor cyclicx processor redistribut cyclickx p processor cyclicx q processor redistribut revers direct redistribut x q send receiv commun schedul tabl receiv send commun schedul tabl q therefor scheme x extend redistribut problem cyclickx p processor cyclicx q processor 23 commun schedul use gener circul matrix framework commun schedul perform local rearrang data within processor well interprocessor commun local rearrang data call column reorgan result send commun schedul tabl show p k q send commun schedul inde gener circul matrix avoid node content matrix circul matrix satisfi follow properti 1 n row row 0 circularli right shift k time 0 k 2 shift l time note definit extend block circul matric chang row row block matrix gener circul matrix matrix partit block size theta n result block matrix form circul matrix block either circul matrix gener circul matrix figur 5 illustr gener circul matrix two observ gener circul matrix block along block diagon ident ii element row 0 distinct row element distinct show approach destin processor tabl transform gener circul matrix distinct element row 3 effici redistribut algorithm discuss commun schedul algorithm redistribut classifi commun pattern 3 class redistribut problem x altern formul cyclicx cyclici problem accord follow lemma let g denot kq gener circul matrix circul matrix submatrix figur 5 gener circul matrix lemma 1 commun pattern induc x p k q requir non alltoal commun commun fix messag size k ffg ff integ greater 0 iii alltoal commun differ messag size g k k 6 ffg among three case case alltoal processor commun messag size optim schedul use trivial roundrobin schedul howev non trivial achiev messag size pair node commun step alltoal case differ messag size therefor focu two case redistribut requir schedul non alltoal commun alltoal commun differ messag size 31 non alltoal commun given redistribut paramet p q k get l theta p initi distribut tabl dpt let g dpt everi k th 1 row similar pattern differ destin processor indic shuffl row row similar pattern adjac result shuffl dpt 1 shuffl dpt 1 divid q 1 slice row direct divid p 1 slice column direct dpt consid k 1 theta p 1 block matrix made q 1 theta g 1 submatric block matrix convert gener circul matrix reorgan block column block matrix reorgan individu column within submatrix appropri amount result gener circul matrix commun schedul matrix b i1 c figur step column reorgan procedur k ident valu row 0 dpt distribut k distinct row henc row 0 distinct valu sinc gener circul matrix element row distinct achiev conflictfre schedul rigor proof fact dpt transform gener circul matrix use column reorgan found 25 schedul commun step p sourc processor transfer equals messag p distinct destin processor ensur network bandwidth fulli util number commun step also minim therefor data transfer cost minim reorgan element move within column incur interprocessor commun figur 6 show exampl dpt x 6 4 convert gener circul matrix form column reorgan exampl 3 figur 6a show initi distribut tabl figur 6d show correspond dpt row shuffl shown figur 6b e partit shuffl tabl submatric size 3 theta 2 diagon submatric diagon element submatrix shown figur 6c f figur 6f gener circul matrix give commun schedul dpt convert send commun schedul tabl set reorgan appli initi data distribut tabl convert 0 shown figur 6 expens reorgan larg amount data within local memori instead reorgan done maintain pointer element array sourc processor tabl point data block pack commun step denot send data locat tabl entri local block index correspond entri 0 entri si j point destin processor correspond entri j scheme comput schedul data index set time algebra manipul procedur give follow two equat directli comput individu entri equat denot quotient integ divis remaind integ divis similarli n solut nk proof correct mathemat formul found 25 formula comput commun schedul index set redistribut extrem effici compar method present 5 use bipartit match algorithm furthermor use formula processor comput entri need send commun schedul tabl henc schedul index set comput perform distribut way total cost comput schedul index set q amort cost comput step commun schedul index set comput o1 scheme minim number commun step avoid node content commun step equals messag transfer therefor scheme minim total data transfer cost 32 alltoal commun differ messag size alltoal commun case aris g g 1 g state lemma 1 g q first superblock dpt construct dpt therefor column entri q destin processor column sever block transfer 31354131020355241357destin processor tabl send commun schedul tabl transform dpt messag messag figur 7 exampl illustr alltoal case differ messag size x 4 3 6 destin column reorgan state section 31 appli dpt result gener circul matrix k 1 theta p 1 circul block matrix block q 1 theta g 1 submatrix also circul matrix block matrix first g 2 block column distinct block everi g th row entri differ circularshift pattern block fold onto block first row therefor first g 2 row block matrix use determin send commun schedul tabl q theta p gener circul matrix sinc block everi g th 2 row fold onto block first row alltoal commun case differ messag size block first k 1 mod g 2 row size k 1 e whole block remain row size b k 1 c figur 7 show exampl send commun schedul tabl x 4 3 6 gener alltoal case differ messag size exampl processor entri 6 destin processor correspond dpt l theta p matrix l appli column reorgan result gener circul matrix consid k 1 theta p 1 block matrix k block 1 first g use tabl 3 rd row fold onto st row henc messag size 1 st row 2 2 nd row 1 k 1 multipl g 2 messag size everi row therefor network bandwidth fulli util send equal size messag commun step 33 data transfer cost distribut memori model commun cost two paramet startup time transfer time startup time incur commun event independ messag size gener startup time consist transfer request acknowledg latenc context switch latenc latenc initi messag header unit transmiss time cost transfer messag unit length network total transmiss time messag proport size thu total commun time send messag size unit one processor anoth model model reorgan data element among processor processor unit data anoth processor also take time model assum node content ensur commun schedul redistribut use distribut memori model perform algorithm analyz follow assum array n element initi distribut cyclicx p processor redistribut cyclickx q processor use algorithm commun cost perform x l case non alltoal commun ii case alltoal commun proof analysi found 25 4 experiment result experi conduct ibm sp2 algorithm written c mpi tabl 2 show comparison propos algorithm caterpillar algorithm11 bipartit match scheme5 respect data transfer cost schedul index comput cost alltoal commun case equals messag data transfer cost commun step three algorithm also schedul comput perform simpl way henc consid tabl 2 tabl 2 size array assign sourc processor non alltoal commun case p algorithm well bipartit match scheme perform less number commun step compar caterpillar algorithm alltoal commun case differ messag size messag transmit commun step size bipartit match scheme well algorithm therefor network bandwidth fulli util total transmiss cost caterpillar algorithm transmiss cost commun step domin largest messag transfer step let denot size largest messag sent commun step note total startup cost algorithm qt sinc number commun step hand total transmiss cost tabl 2 comparison data transfer cost schedul index comput cost caterpillar algorithm bipartit match scheme algorithm note l q non alltoal commun case maximum transfer data size commun step non alltoal commun alltoal commun differ messag size data transfer cost schedul index comput cost data transfer cost schedul index comput cost caterpillar algorithm bipartit match scheme 5 algorithm p q q bipartit match scheme algorithm less caterpillar algorithm caterpillar algorithm well algorithm perform schedul index comput oq time howev schedul index comput cost bipartit match scheme op evalu total redistribut cost data transfer cost consid 3 differ scenario correspond rel size p q scenario 1 p q scenario scenario experi choos 3 array consist singl precis integ size element 4 byte array size chosen multipl size superblock avoid pad use dummi data rest section organ follow subsect 41 report experiment result overal redistribut time algorithm caterpillar algorithm subsect 42 show experiment result data transfer time algorithm caterpillar algorithm subsect 43 compar algorithm bipartit match scheme respect schedul comput time j0 jn1 j ts redistribut routin comput schedul index set i0 in2 sourc processor sourc processor pack messag send messag destin processor els destin processor receiv messag sourc processor unpack messag ts comput tavg nodetim node comput figur 8 step measur redistribut time 41 total redistribut time subsect report experiment result total redistribut time algorithm caterpillar algorithm total redistribut time consist schedul comput time index comput time packingunpack time data transfer time experi sourc destin processor set disjoint commun step sender pack messag send receiv unpack messag receiv pack oper sourc processor unpack oper destin processor overlap ie send messag commun step sender start pack messag commun step receiv start unpack messag receiv step methodolog measur total redistribut time shown figur 8 time measur use mpiwtim call n1 number run run execut redistribut number commun step processor measur nodetimej j th run gener sourc destin processor perform interprocessor commun last step complet redistribut earlier processor receiv messag unpack barrier synchron mpibarri perform end redistribut measur nodetim averag nodetim p processor total array size5001500250035004500tot redistribut time msec ibm sp2 algorithm caterpillar algorithm total array size5001500250035004500tot reditribut time msec ibm sp2 algorithm caterpillar algorithm total array size5001500250035004500tot redistribut time msec ibm sp2 algorithm caterpillar algorithm total array size10003000500070009000tot redistribut time msec ibm sp2 algorithm caterpillar algorithm maximum time b averag time c median time minimum time figur 9 maximum averag median minimum total redistribut time comput save tavg measur valu store array shown figur 8 redistribut perform n1 time maximum minimum median averag total redistribut time comput n1 run experi n1 set 20 figur 9 total redistribut time algorithm caterpillar algorithm compar ibm sp2 experi 64 node use 28 sourc processor 36 destin processor total number array element singl precis vari 564480 226 mbyte 14112000 564 mbyte k set 14 figur 9a show maximum time tmax figur 8 observ larg varianc measur valu figur 9b show result averag time tavg figur 8 figur 9c show result use median time tmed figur 8 still varianc measur valu howev smaller varianc found averag maximum time figur 9d show minimum time redistribut tmin figur 8 plot accur observ redistribut time sinc minimum time smallest compon due os interfer effect relat environ remain plot section show tmin redistribut 2 28 14 36 non alltoal commun case non alltoal commun case messag commun step size total number commun step algorithm 36 caterpillar algorithm therefor redistribut time algorithm theoret 50 caterpillar algorithm experiment result shown figur 9d redistribut time algorithm 518 551 caterpillar algorithm figur show sever experiment result non alltoal commun case figur 10a b c show result use number commun step use algorithm 26 39 52 respect number commun step use caterpillar algorithm 78 therefor redistribut time algorithm expect reduc 67 50 33 compar caterpillar algorithm experiment result confirm similar reduct time achiev experiment result shown figur 10 figur 11 compar overal redistribut time alltoal commun case differ messag size figur 11a report experiment result 4 28 6 36 array size vari 677376 271 mbyte 16934400 677 mbyte case total redistribut time msec ibm sp2 algorithm caterpillar algorithm total array size5001500250035004500 total redistribut time msec ibm sp2 algorithm caterpillar algorithm total array size500150025003500 total redistribut time msec ibm sp2 algorithm caterpillar algorithm total array size500150025003500 total redistribut time msec ibm sp2 algorithm caterpillar algorithm total array size50015002500tot redistribut time msec ibm sp2 algorithm caterpillar algorithm total array size50015002500 total redistribut time msec ibm sp2 algorithm caterpillar algorithm c figur 10 total redistribut time non alltoal commun case total redistribut time msec ibm sp2 algorithm caterpillar algorithm total array size50015002500tot redistribut time msec ibm sp2 algorithm caterpillar algorithm total array size50015002500 total redistribut time msec ibm sp2 algorithm caterpillar algorithm total array size50015002500 total redistribut time msec ibm sp2 algorithm caterpillar algorithm c figur 11 total redistribut time alltoal commun case differ messag size j0 jn1 j redistribut routin comput schedul index set i0 in2 sourc processor sourc processor pack messag ts send messag destin processor ts els destin processor ts receiv messag sourc processor ts unpack messag comput tavg nodetr node comput figur 12 step measur data transfer time algorithm number step 36 within superblock half messag two block half one block algorithm equals messag transfer commun step therefor half step two block messag sent half one block messag sent caterpillar algorithm attempt schedul commun oper send equals messag therefor redistribut time step determin time transfer largest messag theoret total redistribut time algorithm reduc 25 compar caterpillar algorithm experi achiev 179 reduct redistribut time array size small algorithm approxim perform sinc startup cost domin overal data transfer cost array size increas reduct time perform distribut use algorithm improv scenario obtain similar result see figur 11b c 42 data transfer time subsect report experiment result data transfer time algorithm caterpillar algorithm experi perform manner discuss subsect 41 data set use experi use previou subsect data transfer time commun step first measur total data transfer time comput sum measur time commun step methodolog measur time shown figur 12 figur 13 data transfer time algorithm caterpillar algorithm report experi perform ibm sp2 figur 13a report maximum data transfer time tmax figur 12 larg variat measur valu observ figur 13b c show averag time tavg figur 12 median time tmed figur 12 data transfer time respect valu comput use maximum timetmax figur 13d show minimum data transfer timetmin plot accur observ data transfer time sinc minimum time smallest compon due os interfer effect relat environ therefor accur comparison rel perform redistribut algorithm remaind section show plot correspond tmin redistribut 2 28 14 36 non alltoal commun case messag commun step size total number commun step use algorithm total number step 36 use caterpillar algorithm therefor data transfer time algorithm theoret 50 caterpillar algorithm experiment result see figur 13d redistribut time algorithm 492 537 caterpillar algorithm figur 14 show sever experiment result non alltoal commun case similar reduct time achiev experi figur 15 report experiment result alltoal commun case differ messag size data transfer time alltoal commun case sensit network content sinc everi sourc processor commun everi destin processor algorithm number step 36 within superblock half messag two block half one block algorithm equals messag transfer commun step therefor half step two block messag sent one block messag sent half caterpillar algorithm attempt send equals messag commun step therefor data transfer time step determin time transfer largest messag theoret data transfer time algorithm reduc 25 compar caterpillar 25003500data transfer time msec ibm sp2 algorithm caterpillar algorithm total array size500data transfer time msec ibm sp2 algorithm caterpillar algorithm total array size500data transfer time msec ibm sp2 algorithm caterpillar algorithm total array size500 data transfer time msec ibm sp2 algorithm caterpillar algorithm maximum time b averag time c median time minimum time figur 13 maximum averag median minimum data transfer time data transfer time msec ibm sp2 algorithm caterpillar algorithm total array size5001500 data transfer time msec ibm sp2 algorithm caterpillar algorithm total array size5001500 data transfer time msec ibm sp2 algorithm caterpillar algorithm total array size5001500 data transfer time msec ibm sp2 algorithm caterpillar algorithm total array size500data transfer time msec ibm sp2 algorithm caterpillar algorithm total array size500 data transfer time msec ibm sp2 algorithm caterpillar algorithm c figur 14 data transfer time non alltoal commun case data transfer time msec ibm sp2 algorithm caterpillar algorithm total array size500data transfer time msec ibm sp2 algorithm caterpillar algorithm total array size500data transfer time msec ibm sp2 algorithm caterpillar algorithm total array size500data transfer time msec ibm sp2 algorithm caterpillar algorithm c figur 15 data transfer time alltoal commun case differ messag size tabl 3 comparison schedul comput time sec procedur 23 use bipartit match p q k scheme bipartit match scheme 5 48 276587 207762816 algorithm experi larg messag size achiev 155 reduct small messag algorithm approxim perform sinc startup time domin data transfer time experiment result report figur 15b c 43 schedul comput time time comput schedul caterpillar algorithm well algorithm neglig compar total redistribut time even though schedul caterpillar algorithm simpler caterpillar algorithm need time index comput identifi block pack commun step time approxim schedul comput time schedul comput time bipartit match scheme 5 much higher caterpillar algorithm algorithm rang hundr msec quit signific schedul comput cost bipartit match scheme increas rapidli number processor increas hand algorithm comput commun schedul effici processor comput entri send commun schedul tabl thu schedul comput distribut way schedul comput time rang 100 sec comparison scheme bipartit match scheme respect schedul comput time shown tabl 3 time scheme includ index comput time bipartit match scheme time shown schedul comput time conclus paper show effici algorithm perform redistribut cyclicx p processor cyclickx q processor propos algorithm repres use gener circul matrix formal algorithm minim number commun step avoid destin node content commun step network bandwidth fulli util ensur messag size transfer commun step therefor total data transfer cost minim schedul index comput cost also import perform runtim redistri bution algorithm schedul index set comput omaxp q time comput extrem fast compar bipartit match scheme 5 take op schedul index comput time small enough neglig compar data transfer time make algorithm suitabl runtim data redistribut acknowledg would like thank staff mhpcc assist evalu algorithm ibm sp2 also would like thank manash kirtania assist prepar manuscript r redistribut blockcycl data distribut use mpi processor map techniqu toward effici data redistribut schedul blockcycl array redistribut parallel matrix transpos algorithm distribut memori concurr comput parallel implement synthet apertur radar high perform comput platform fast runtim block cyclic data redistribut multipro cessor messag pass interfac forum runtim array redistribut hpf program effici algorithm array redistribut automat gener effici array redistribut routin distribut memori multicomput compil techniqu blockcycl distribut multiphas array redi tribut model evalu multiphas array redi tribut model evalu approach commun effici data redistribut commun issu heterogen embed system basiccycl calcul techniqu effici dynam data redistribut scalabl portabl implement spacetim adapt process effici algorithm blockcycl redistribut array shortest augment path algorithm dens spars linear assign problem tr ctr stavro souravla mano roumelioti pipelin techniqu dynam data transfer multiprocessor grid intern journal parallel program v32 n5 p361388 octob 2004 wang minyi guo dame wei divideandconqu algorithm irregular redistribut parallel compil journal supercomput v29 n2 p157170 august 2004 jihwoei huang chihp chu effici commun schedul method processor map techniqu appli data redistribut journal supercomput v37 n3 p297318 septemb 2006 ian n dunn gerard g l meyer qr factor share memori messag pass parallel comput v28 n11 p15071530 novemb 2002 chinghsien hsu shihchang chen chaoyang lan schedul contentionfre irregular redistribut parallel compil journal supercomput v40 n3 p229247 june 2007 chinghsien hsu spars matrix blockcycl realign distribut memori machin journal supercomput v33 n3 p175196 septemb 2005 emmanuel jeannot frdric wagner schedul messag data redistribut experiment studi intern journal high perform comput applic v20 n4 p443454 novemb 2006 minyi guo yi pan improv commun schedul array redistribut journal parallel distribut comput v65 n5 p553563 may 2005 chinghsien hsu kunm yu compress diagon remap techniqu dynam data redistribut band spars matrix journal supercomput v29 n2 p125143 august 2004 minyi guo ikuo nakata framework effici data redistribut distribut memori multicomput journal supercomput v20 n3 p243265 novemb 2001