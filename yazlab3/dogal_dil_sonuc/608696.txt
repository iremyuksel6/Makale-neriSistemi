price agent economi use multiag qlearn paper investig adapt softwar agent may util reinforc learn algorithm qlearn make econom decis set price competit marketplac singl adapt agent face fixedstrategi oppon ordinari qlearn guarante find optim polici howev popul agent tri adapt presenc adapt agent problem becom nonstationari histori depend known whether global converg obtain whether solut optim paper studi simultan qlearn two compet seller agent three moder realist econom model simplest case interest multiag phenomena occur state space small enough lookup tabl use repres qfunction find despit lack theoret guarante simultan converg selfconsist optim solut obtain model least small valu discount paramet case exact approxim converg also found even larg discount paramet show qderiv polici increas profit damp elimin cyclic price war compar simpler polici base zero lookahead shortterm lookahead one model shopbot model seller profit function symmetr find qlearn produc either symmetr brokensymmetri polici depend discount paramet initi condit b introduct reinforc learn rl procedur establish power practic method solv markov decis problem one signific activ investig rl algorithm qlearn watkin 1989 qlearn algorithm learn estim longterm expect reward given stateact pair nice properti need model environ use onlin learn strong converg qlearn exact optim valu function polici proven lookup tabl represent qfunction use watkin dayan 1992 feasibl small state space larg state space lookup tabl infeas rl method combin function approxim give good practic perform despit lack theoret guarante converg optim polici realworld problem fulli markov natur often nonstationari historydepend andor fulli observ order rl method gener use solv problem need extend handl nonmarkovian properti one import applic domain nonmarkovian aspect paramount area multiag system area expect increasingli import futur due potenti rapid emerg agent economi consist larg popul interact softwar agent engag variou form econom activ problem multipl agent simultan adapt gener nonmarkov agent provid effect nonstationari environ agent henc exist converg guarante hold gener known whether global converg obtain whether solut optim progress made analyz certain special case multiag problem exampl problem team agent share common object function studi exampl crite barto 1996 likewis pure competit case zerosum object function studi littman 1994 algorithm call minimaxq propos twoplay zerosum game shown converg optim valu function polici player sandholm crite studi simultan qlearn two player iter prison dilemma game sandholm crite 1995 found learn procedur gener converg stationari solut howev extent solut optim unclear recent hu wellman propos algorithm calcul optim qfunction twoplay arbitrarysum game hu wellman 1998 algorithm import first step howev yet appear useabl practic problem assum polici follow player nash equilibrium polici address equilibrium coordin problem ie multipl nash equilibria agent decid equilibrium choos suspect may seriou problem sinc accord folk theorem iter game krep 1990 prolifer nash equilibria suffici high emphasi futur reward ie larg valu discount paramet fl furthermor may inconsist assum nash polici polici impli qfunction calcul algorithm paper studi simultan qlearn econom motiv twoplay game player assum two seller similar ident product compet basi price time step seller altern take turn set price take account seller current price price set consum respond instantan determinist choos either seller 1s product seller 2s product product base current price pair p 1 lead instantan reward profit r 1 given seller 1 2 respect assum initi seller full knowledg expect consum demand given price pair fact full knowledg profit function work build prior research report tesauro kephart 1998 tesauro kephart 1999 paper examin effect includ fore sight ie abil anticip longerterm consequ agent current action two differ algorithm agent foresight present gener minimax search procedur twoplay zerosum game ii gener polici iter method dynam program player polici simultan improv selfconsist polici pair obtain optim expect reward two time step found includ foresight agent price algorithm gener improv overal agent profit usual damp elimin patholog behavior unend cyclic price war long episod repeat undercut amongst seller altern larg jump price price war found rampant prior studi agent economi model kephart hanson sairamesh 1998 sairamesh kephart 1998 agent use myopic optim myoptim price algorithm optim immedi reward anticip longerterm consequ agent current price set motiv studi simultan qlearn paper three fold first qfunction learn simultan selfconsist player polici impli qfunction selfconsist optim word agent abl correctli anticip longerterm consequ action agent action correctli model agent equival capabl henc classic problem infinit recurs oppon model avoid contrast approach adapt multiag system issu problemat exampl vidal durfe 1998 propos recurs oppon model scheme level0 agent oppon model level1 agent model oppon level0 level2 agent model oppon level1 etc approach effect way agent model agent equival level depth complex second advantag qlearn solut correspond deep lookahead principl qfunction repres expect reward look infint far ahead time exponenti weight discount paramet contrast prior work tesauro kephart 1999 base shallow finit lookahead final comparison directli model agent polici qfunction approach seem extens situat larg economi mani compet seller intuit approxim qfunction nonlinear function approxim neural network feasibl approxim correspond polici fur thermor qfunction approach agent need maintain singl qfunction wherea polici model approach agent need maintain polici model everi agent latter seem infeas number seller larg remaind paper organ follow section 2 describ structur dynam model twosel economi present three economicallybas model seller profit pricequ informationfilt shopbot known prone price war agent myopic optim shortterm payoff deliber choos paramet place system pricewar regim section 3 describ detail implement qlearn model economi first step examin simpl case ordinari qlearn one two seller use qlearn seller use fix price polici myopic opti mal myoptim polici examin interest novel situat simultan qlearn seller final section 5 summar main conclus discuss promis direct challeng futur work model agent economi real agent economi like contain larg number agent complex detail agent behav interact multipl time scale approach toward model understand complex begin make number simplifi assumpt first consid simplest possibl case two compet seller agent offer similar ident product larg popul consum agent seller compet basi price assum price discret lie minimumand maximum price number possibl price hundr render state space small enough feasibl use lookup tabl repres agent price polici expect profit time simul also discret time step assum consum compar current price two seller instantan determinist choos purchas one seller henc time step possibl pair seller price determinist reward profit given seller simul iter forev may may discount factor present valu futur reward worth note consum regard player model consum strateg role behav accord extrem simpl fix shortterm greedi rule buy lowest price product time step regard mere provid stationari environ two seller compet twoplay game clearli simplifi first step studi multiag phenomena futur work model extend includ strateg adapt behavior part consum well chang notion desir system behavior present model desir behavior would resembl collus two seller charg high price could obtain high profit obvious desir consum viewpoint regard dynam seller price adjust assum seller altern take turn adjust price rather simultan set price ie game extensiveform rather normalform choic alternatingturn dynam motiv two consider number seller becom larg model becom realist seem reason assum seller adjust price differ time rather time although probabl take turn welldefin order b alternatingturn dynam stay within normal qlearn framework qfunction impli determinist optim polici known twoplay altern turn game alway exist determinist polici good nondeterminist polici littman 1994 contrast game simultan move exampl rockpaperscissor possibl determinist polici optim exist qlearn formal mdp would modifi extend could yield nondeterminist optim polici studi qlearn three differ econom model describ detail elsewher sairamesh kephart 1998 kephart hanson sairamesh 1998 greenwald kephart 1999 first model call pricequ model sairamesh kephart 1998 model seller product distinguish differ valu scalar qualiti paramet higherqu product perceiv valuabl consum consum model tri obtain lowestpr product time step subject thresholdtyp constraint qualiti price ie consum maximum allow price minimum allow qual iti similar substitut seller product lead potenti direct price competit howev vertic differenti due differen ing qualiti valu lead asymmetri seller profit function believ asymmetri respons unend cyclic price war emerg seller employ myoptim price strategi second model informationfilt model describ detail kephart hanson sairamesh 1998 model two compet seller news articl somewhat overlap categori contrast vertic differenti pricequ model model contain horizont differenti differ articl categori extent categori overlap direct price competit extent differ asymmetri introduc lead potenti cyclic price war third model socal shopbot model describ greenwald kephart 1999 intend model situat internet consum may use shopbot compar price seller offer given product select seller lowest price model seller product exactli ident profit function symmetr myoptim price lead seller undercut minimum price point reach point new price war cycl launch due buyer asymmetri rather seller asymmetri fact buyer use shopbot buyer instead choos seller random mean profit seller abandon lowpric competit bargain hunter instead maxim exploit random buyer charg maximum possibl price exampl profit function studi taken pricequ model follow let p 1 p 2 repres price charg seller 1 seller 2 respect let q 1 q 2 repres respect qualiti paramet cost seller produc item qualiti q assum particular model consum behavior describ sairamesh kephart 1998 one show analyt limit infinit mani consum instantan profit per consum r 1 r 2 obtain seller 1 seller 2 respect given ae 1 ae 2 plot profit landscap seller 1 function price p 1 p 2 given figur 1 follow paramet set q q specif paramet set chosen known gener harm price war agent use myopic optim price see figur myopic optim price seller 1 function seller 2s price p p 2 obtain valu p 2 sweep across valu p 1 choos valu give highest profit see small valu p 2 peak profit obtain wherea larger valu p 2 eventu discontinu shift peak follow along parabolicshap ridg landscap analyt express myopic optim price seller 1 function p 2 follow defin x similarli myopic optim price seller 2 function price set seller 1 p p 1 given follow formula assum price discret ffl price discret interv also note pass similar profit landscap seller informationfilt model shopbot model three fig 1 sampl profit landscap seller 1 pricequ model function seller 1 price p1 seller 2 price p2 model exist multipl disconnect peak landscap rel height chang depend seller price lead price war seller behav myopic regard inform set made avail seller made simplifi assumpt first step player essenti perfect inform model consum behavior perfectli also perfect knowledg other cost profit function henc model thu twoplay perfectinform determinist game similar game like chess main differ profit model strictli zerosum termin absorb node model state space also model payoff given player everi time step wherea game chess payoff given termin node mention previous constrain price set two seller lie rang minimum maximum allow price price discret one creat lookup tabl seller profit function furthermor optim price polici seller function seller price p p 2 p p 1 also repres form tabl lookup 3 singleag qlearn first consid ordinari singleag qlearn twosel econom model procedur qlearn follow let qs repres discount longterm expect reward agent take action state discount futur reward accomplish discount paramet fl valu reward expect n time step futur discount fl n assum qs function repres lookup tabl contain valu everi possibl stateact pair assum tabl entri initi arbitrari valu procedur solv qs infinit repeat follow twostep loop 1 select particular state particular action observ immedi reward r stateact pair observ result state 0 2 adjust qs accord follow equat deltaq ff learn rate paramet max oper repres choos optim action b among possibl action taken successor state 0 lead greatest qvalu wide varieti method may use select stateact pair step 1 provid everi stateact pair visit infinit often stationari markov decis problem qlearn procedur guarante converg correct valu provid ff decreas time appropri schedul first consid use qlearn one two seller econom model seller maintain fix price polici simul describ fix polici fact myoptim polici p repres exampl pricequ model equat 3 4 price applic distinct state action somewhat blur assum state seller suffici describ seller last price action current price decis suffici state descript histori need either determin immedi reward calcul myoptim price fixedstrategi player also modifi concept immedi reward r nextstat 0 twoagent case defin 0 state obtain start one action qlearner respons action fixedstrategi oppon likewis immedi reward defin sum two reward obtain two action modif introduc state 0 would player move state possibl altern investig includ sidetomov addit inform statespac descript simul report sequenc stateact pair select qtabl updat gener uniform random select amongst possibl tabl entri initi valu qtabl gener set immedi reward valu consequ initi qderiv polici correspond myoptim polici learn rate vari time accord initi learn rate ff0 usual set 01 constant simul time measur unit n 2 size qtabl n number possibl price could select either player number differ valu discount paramet fl studi rang result singleag qlearn three model indic qlearn work well expect case model valu discount paramet exact converg qtabl stationari optim solut found converg time rang hundr sweep tabl element smaller valu fl thousand updat largest valu fl addit qlearn converg measur expect cumul profit polici deriv qfunction ran qpolici player myopic polici 100 random start state 200 time step averag result cumul profit player found case seller achiev greater profit myopic oppon use qderiv polici use myopic polici true even due redefinit q updat sum two time step case effect correspond twostep optim rather onestep optim myopic polici furthermor cumul profit obtain qderiv polici monoton increas increas fl expect also interest note mani case expect profit myopic oppon also increas play qlearner also improv monoton increas fl explan rather better exploit myopic oppon would expect zerosum game qlearner instead reduc region would particip mutual undercut price war typic find model myopic vs myopic play largeamplitud price war gener start high price persist way low price q learner compet myopic oppon still price war start high price howev qlearner abandon price war quickli price decreas effect pricewar regim smaller confin higher averag price lead closer approxim cooper collus behavior greater expect utilit player illustr exampl result singleag qlearn shown figur 2 figur 2a plot averag profit seller shopbot model one seller myopic qlearner model symmetr doesnt matter seller qlearner figur 2b plot myopic price curv seller 2 qderiv price curv seller 1 see curv maximum price 1 minimum price approxim 058 portion curv lie vs q shopbot model myopic vs myopic averag profit pfig 2 result singleag qlearn shopbot model averag profit per time step qlearner seller 1 fill circl myopic seller seller 2 open circl vs discount paramet fl dash line indic baselin expect profit seller myopic b crossplot qderiv price curv seller 1 vs myopic price curv seller 2 dash line arrow indic tempor pricepair trajectori use polici start fill circl along diagon indic undercut behavior case seller respond oppon price undercut ffl price discret interv system dynam state p 1 figur 2b obtain altern appli two price polici done simpl iter graphic construct given start point one first hold move horizont p 1 p 2 curv one hold move vertic p 2 p 1 curv see figur iter graphic construct lead unend cyclic price war whose trajectori indic dash line note pricewar behavior begin price pair 1 1 persist price approxim 083 point seller 1 abandon price war reset price 1 lead anoth round undercut amplitud price war diminish compar situat player use myopic polici case seller 1s curv would mirror imag seller 2s curv price war would persist way minimum price point lead lower expect profit seller multiag qlearn examin interest challeng case simultan train qfunction polici seller approach use formal present previou section altern adjust random entri seller 1s qfunction follow random entri seller 2s qfunction seller qfunction evolv seller price polici correspondingli updat optim agent current qfunction model twostep payoff r seller equat 5 use oppon current polici impli current qfunction paramet experi gener set valu previou section experi qfunction initi instantan payoff valu polici correspond myopic polici although initi condit explor experi vs q pq model myopic vs myopic 1 myopic vs myopic 2 averag profit vs q pq model g fig 3 result simultan qlearn pricequ model averag profit per time step seller 1 solid diamond seller 2 open diamond vs discount paramet fl dash line indic baselin myopic vs myopic expect profit note seller 2s profit higher seller 1s even though seller 2 lower qualiti paramet b crossplot qderiv price curv fl dash line arrow indic sampl price dynam trajectori start fill circl price war elimin dynam evolv fix point indic open circl simultan qlearn pricequ model find robust converg uniqu pair price polici independ valu fl illustr figur 3b solut also correspond solut found gener minimax gener dp tesauro kephart 1999 note repeat applic pair price curv lead dynam trajectori eventu converg fixedpoint locat p 04 detail analysi price polici fixedpoint solut present tesauro kephart 1999 brief suffici low price seller 2 pay seller 1 abandon price war charg high price 09 valu correspond highest price seller 2 charg without provok undercut seller 1 base twostep lookahead calcul seller 1 undercut seller 2 repli undercut note fix point correspond nash equilibrium sinc player incent deviat base onestep lookahead calcul conjectur tesauro kephart 1999 solut observ figur 3b correspond subgameperfect equilibrium fudenberg tirol 1991 rather nash equilibrium cumul profit obtain pair price polici plot figur 3a interest seller 2 lowerqu seller actual obtain significantli higher profit seller 1 higherqu seller contrast myopic vs myopic price seller 2 wors seller 1 vs q shopbot model myopic vs myopic averag profit fig 4 result simultan qlearn shopbot model averag profit per time step seller 1 solid diamond seller 2 open diamond vs discount paramet fl dash line indic baselin myopic vs myopic expect profit b crossplot qderiv price curv solut symmetr dash line arrow indic sampl price dynam trajectori c crossplot qderiv price curv 09 solut asymmetr shopbot model find exact converg qfunction valu fl howev case exact converg found find good approxim converg qfunction polici converg stationari solut within small random fluctua tion differ solut obtain valu fl gener find symmetr solut shape iden tical obtain small fl wherea broken symmetri solut similar pricequ solut obtain larg fl also found rang fl valu 01 02 either symmetr asymmetr solut could obtain depend initi condit asymmetr solut counterintuit us expect symmetri two sell er profit function would lead symmetr solut hindsight appli type reason pricequ model explain asymmetr solut plot expect profit seller function fl shown figur 4a plot symmetr asymmetr solut obtain respect shown figur 4b 4c myopic vs myopic 1 myopic vs myopic 2 averag profit vs q model g05 pfig 5 result multiag qlearn informationfilt model averag profit per time step seller 1 solid diamond seller 2 open diamond vs discount paramet fl data point qfunction polici dash line indic baselin expect profit seller myopic b crossplot qderiv price curv final informationfilt model found simultan qlearn produc exact good approxim converg small valu 05 larg valu fl converg obtain simultan qlearn solut yield reducedamplitud price war monton increas profit seller function fl least 05 data point examin fl 05 even though converg qpolici still yield greater profit seller myopic vs myopic case plot qderiv polici system dynam shown figur 5b expect profit player function fl plot figur 5a conclus examin singleag multiag qlearn three model twosel economi seller altern take turn set price instantan profit given seller base current price pair model fall categori twoplay alternatingturn arbitrarysum markov game reward statespac transit determinist game markov state space fulli observ reward histori depend three model pricequ informationfilt shopbot largeamplitud cyclic price war obtain seller myopic optim instantan profit without regard longerterm impact price polici find three model use qlearn one seller myopic oppon invari result exact converg optim qfunction optim polici oppon allow valu discount paramet fl use qderiv polici yield greater expect profit qlearner monoton increas profit fl increas mani case side benefit also enhanc welfar myopic oppon come reduc amplitud undercut pricewar regim case elimin complet also studi interest challeng situat simultan train qfunction seller difficult seller qfunction polici chang provid nonstationari environ adapt seller converg proof exist simultan qlearn multipl agent nevertheless despit absenc theoret guarante find gener good behavior algorithm model economi two model shopbot pricequ find exact good approxim converg simultan selfconsist qfunction optim polici valu fl wherea informationfilt model simultan converg found fl 05 informationfilt shopbot model monoton increas expect profit seller also found small valu fl pricequ model simultan qlearn yield asymmetr solut correspond solut found tesauro kephart 1999 highli advantag lesserqu seller slightli disadvantag higherqu seller compar myopic vs myopic price similar asymmetr solut also found shopbot model larg fl even though profit function player symmetr model exist rang discount paramet valu solut obtain simultan qlearn selfconsist optim outperform solut obtain tesauro kephart 1999 presum previous publish method base limit lookahead wherea qfunction principl look ahead infinit far appropri discount intruig simultan qlearn work well model despit lack theoret converg proof sandholm crite also found simultan qlearn gener converg iter prison dilemma game empir find suggest deeper theoret analysi simultan qlearn may worth investig may underli theoret principl explain simultan qlearn work least certain class arbitrarysum profit function sever import challeng also face extend approach largerscal realist simul econom situat real world two domin seller gener number seller much greater situat forese agent economi number compet seller larg case seller profit price function high input dimension infeas use lookup tabl statespac represent like sort compact represent combin function approxim scheme necessari furthermor mani seller concept seller take turn adjust price welldefin order becom problemat could lead addit combinatori explos mechan calcul expect reward anticip possibl order oppon respons furthermor econom model moder degre realism profit function unrealist assumpt knowledg dynam work report state space fulli observ infinit frequent zero cost zero propag delay expect consum demand given price pair instantan determinist fulli known player inde player exact profit function fulli known player also assum player would altern take turn equal often welldefin order adjust price assumpt knowledg dynam one could hope develop algorithm could calcul advanc someth like gametheoret optim price algorithm agent howev realist agent economi like agent much less full knowledg state economi agent may know detail agent profit function inde agent may know profit function extent buyer behavior unpredict dynam buyer seller may also complex random unpredict assum may also inform delay buyer seller part econom game may involv pay cost order obtain inform state economi faster frequent greater detail final expect buyer behavior nonstationari complex coevolut buyer seller strategi realworld complex daunt reason believ learn approach qlearn may play role practic solu tion advantag qlearn one need model either instantan payoff statespac transit environ one simpli observ actual reward transit base learn theori qlearn requir exhaust explor state space guarante converg may necessari function approxim use case train function approxim rel small number observ state may gener well enough unobserv state give decent practic perform sever recent empir studi provid evid tesauro 1995 crite barto 1996 zhang dietterich 1996 acknowledg author thank ami greenwald help discuss regard shopbot model r improv elev perform use reinforc learn game theori shopbot pricebot multiag reinforc learn theoret framework algorithm pricewar dynam freemarket economi softwar agent cours microeconom theori markov game framework multiag reinforc learn ing dynam price qualiti differenti inform comput market multiag qlearn semicompetit domain tempor differ learn tdgammon foresightbas price algorithm economi softwar agent foresightbas price algorithm agent economi learn nest agent model inform economi learn delay reward qlearn highperform jobshop schedul timedelay td network tr ctr prithviraj raj dasgupta yoshitsugu hashimoto multiattribut dynam price onlin market use intellig agent proceed third intern joint confer autonom agent multiag system p277284 juli 1923 2004 new york new york simon parson michael wooldridg game theori decis theori multiag system autonom agent multiag system v5 n3 p243254 septemb 2002 leigh tesfats agentbas comput econom grow economi bottom artifici life v8 n1 p5582 march 2002 cooper multiag learn state art autonom agent multiag system v11 n3 p387434 novemb 2005