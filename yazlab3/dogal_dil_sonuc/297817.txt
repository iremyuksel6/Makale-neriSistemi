bayesian classif gaussian process abstractw consid problem assign input vector one class predict pcschmi x twoclass problem probabl class one given schmi x estim yschmi x gaussian process prior place yschmi x combin train data obtain predict new schmi x point provid bayesian treatment integr uncertainti paramet control gaussian process prior necessari integr carri use laplac approxim method gener multiclass problem 2 use softmax function demonstr effect method number dataset b introduct consid problem assign input vector x one class predict p cjx classic exampl method logist regress twoclass problem probabl class 1 given x estim oew x b gammay howev method flexibl ie discrimin surfac simpli hyperplan xspace problem overcom extent expand input x set basi function foexg exampl quadrat function compon x highdimension input space larg number basi function one associ paramet one risk overfit train data motiv bayesian treatment problem prior paramet encourag smooth model put prior paramet basi function indirectli induc prior function produc model howev possibl would argu perhap natur put prior directli function one advantag functionspac prior impos gener smooth constraint without tie limit number basi function regress case task predict realvalu output possibl carri nonparametr regress use gaussian process gp see eg 25 28 solut regress problem gp prior gaussian nois model place kernel function train data point coeffici determin solv linear system paramet describ gaussian process unknown bayesian infer carri describ 28 gaussian process method extend classif problem defin gp input sigmoid function idea use number author although previou treatment typic take fulli bayesian approach ignor uncertainti posterior distribut given data uncertainti paramet paper attempt fulli bayesian treatment problem also introduc particular form covari function gaussian process prior believ use model point view structur remaind paper follow section 2 discuss use gaussian process regress problem essenti background classif case section 3 describ applic gaussian process twoclass classif problem extend multipleclass problem section 4 experiment result present section 5 follow discuss section 6 paper revis expand version 1 gaussian process regress use first consid regress problem ie predict real valu output new input valu x given set train data ng relev strategi transform classif problem regress problem deal input valu logist transfer function stochast process prior function allow us specifi given set input x distribut correspond output denot prior function p similarli p joint distribut includ also specifi p tji probabl observ particular valu actual valu ie nois model z z z henc predict distribut found margin product prior nois model note order make predict necessari deal directli prior function space n n 1dimension joint densiti howev still easi carri calcul unless densiti involv special form p tji p gaussian p jt gaussian whose mean varianc calcul use matrix comput involv matric size n theta n specifi p multidimension gaussian valu n placement point x mean prior function gaussian process formal stochast process collect random variabl fy xjx 2 xg index set x case x input space dimens number input gp stochast process fulli specifi mean function covari function cx x finit set variabl joint multivari gaussian distribut consid gp x j 0 assum nois model p tji gaussian mean zero varianc oe 2 predict mean varianc x given eg 25 21 parameter covari function mani reason choic covari function formal requir specifi function gener nonneg definit covari matrix set point model point view wish specifi covari point nearbi input give rise similar predict find follow covari function work well l x l lth compon x vector paramet need defin covari function note analog hyperparamet neural network defin paramet log variabl equat 4 sinc posit scaleparamet covari function obtain network gaussian radial basi function limit infinit number hidden unit 27 w l paramet equat 4 allow differ length scale input dimens irrelev input correspond w l becom small model ignor input close relat automat relev determin ard idea mackay 10 neal 15 v 0 variabl specifi overal scale prior v 1 specifi varianc zeromean offset gaussian distribut gaussian process framework allow quit wide varieti prior function exampl ornsteinuhlenbeck process covari function cx x rough sampl path meansquar differenti hand squar exponenti covari function equat 4 give rise infinit ms differenti process gener believ gp method quit generalpurpos rout impos prior belief desir amount smooth reason highdimension problem need combin model assumpt ard anoth model assumpt may use build covari function sum covari function one may depend input variabl see section 33 detail 22 deal paramet given covari function straightforward make predict new test point howev practic situat unlik know covari function use one option choos parametr famili covari function paramet vector either estim paramet exampl use method maximum likelihood use bayesian approach posterior distribut paramet obtain calcul facilit fact log likelihood l log p dj calcul analyt log 2 5 kj denot determin k also possibl express analyt partial deriv log likelihood respect paramet l see eg 11 given l deriv respect straightforward feed inform optim packag order obtain local maximum likelihood gener one may concern make point estim number paramet larg rel number data point paramet may poorli determin may local maxima likelihood surfac reason bayesian approach defin prior figur 1 x obtain yx squash sigmoid function oe distribut paramet obtain posterior distribut data seen attract make predict new test point x one simpli averag posterior distribut z gp possibl integr analyt gener numer method may use suffici low dimens techniqu involv grid space use highdimension difficult locat region parameterspac high posterior densiti grid techniqu import sampl case markov chain mont carlo method may use work construct markov chain whose equilibrium distribut desir distribut p jd integr equat 7 approxim use sampl markov chain two standard method construct mcmc method gibb sampler metropolishast algorithm see eg 5 howev condit paramet distribut amen gibb sampl covari function form given equat 4 metropolishast algorithm util deriv inform avail mean tend ineffici randomwalk behaviour parameterspac follow work neal 15 bayesian treatment neural network william rasmussen 28 rasmussen 17 use hybrid mont carlo hmc method duan et al 4 obtain sampl p jd hmc algorithm describ detail appendix 3 gaussian process twoclass classif simplic exposit first present method appli twoclass problem extens multipl class cover section 4 use logist transfer function produc output interpret x probabl input x belong class 1 job specifi prior function transform specifi prior input transfer function shall call activ denot see figur 1 twoclass problem use logist function gammay denot probabl activ correspond input x respect fundament gp approach classif regress problem similar except error model ny oe 2 regress case replac bernoey choic v 0 equat 4 affect hard classif ie x hover around 05 take extrem valu 0 1 previou relat work approach discuss section 33 regress case two problem address make predict fix paramet b deal paramet shall discuss issu turn 31 make predict fix paramet make predict use fix paramet would like comput r requir us find p new input x done find distribut activ use appropri jacobian transform distribut formal equat obtain p jt ident equat 1 2 3 howev even use gp prior p gaussian usual express classif data ts take valu 0 1 mean margin obtain p jt longer analyt tractabl face problem two rout follow use analyt approxim integr equat 13 ii use mont carlo method specif mcmc method approxim consid analyt approxim base laplac approxim approxim discuss section 33 laplac approxim integrand p yjt approxim gaussian distribut center maximum function respect invers covari matrix given gammarr log p yjt find maximum carri use newtonraphson iter method allow approxim distribut calcul detail maxim procedur found appendix 32 integr paramet make predict integr predict probabl posterior p jt p tjp saw 22 regress problem p tj calcul exactli use p r p tjyp yjdi integr analyt tractabl classif problem let use log log 2 8 use laplac approxim maximum find log log 2 denot righthand side equat log p tj stand approxim integr space also done analyt employ markov chain mont carlo method follow neal 15 william rasmussen 28 use hybrid mont carlo hmc method duan et al 4 describ appendix use log p tj approxim log p tj use broad gaussian prior paramet 33 previou relat work work gaussian process regress classif develop observ 15 larg class neural network model converg gp limit infinit number hidden unit comput bayesian treatment gp easier neural network regress case infinit number weight effect integr one end deal hyperparamet result 17 show gaussian process regress compar perform stateoftheart method nonparametr method classif problem seen aris combin two differ strand work start linear regress mccullagh nelder 12 develop gener linear model glm twoclass classif context give rise logist regress strand work develop nonparametr smooth regress problem view gaussian process prior function trace back least far work kolmogorov wiener 1940 gaussian process predict well known geostatist field see eg 3 known krige altern consid rough penalti function one obtain spline method recent overview see 25 8 close connect gp rough penalti view explor 9 combin glm nonparametr regress one obtain shall call nonparametr glm method classif earli refer method includ 21 16 discuss also found text 8 25 two differ nonparametr glm method usual describ bayesian treatment firstli fix paramet nonparametr glm method ignor uncertainti henc need integr describ section 31 second differ relat treatment paramet discuss section 22 given paramet one either attempt obtain point estim paramet carri integr posterior point estim may obtain maximum likelihood estim crossvalid gener crossvalid gcv method see eg 25 8 one problem cvtype method dimens larg comput intens search regiongrid parameterspac look paramet maxim criterion sens hmc method describ similar search use gradient inform 1 carri averag posterior distribut paramet defenc gcv method note wahba comment eg 26 refer back 24 method may robust unrealist prior one differ kind nonparametr glm model usual consid method exact natur prior use often rough penalti use express term penalti kth deriv yx give rise power law power spectrum prior yx also differ parameter covari function exampl unusu find paramet like ard introduc equat 4 nonparametr glm model hand wahba et al 26 consid smooth spline analysi varianc ssanova decomposit gaussian process term build prior sum prior function decomposit ff ff import point function involv order interact univari function give rise addit model includ sum full interact term one use bayesian point view question kind prior appropri interest model issu also recent work relat method present paper section 31 mention necessari approxim integr equat 13 describ use laplac approxim follow preliminari version paper present 1 gibb mackay 7 develop altern analyt approxim use variat method find approxim gaussian distribut bound margin likelihood p tj approxim distribut use predict p jt thu x paramet gibb mackay estim maxim lower bound p tj also possibl use fulli mcmc treatment classif problem discuss recent paper neal 14 method carri integr posterior distribut simultan work gener sampl p jd two stage process firstli fix n individu updat sequenti use gibb sampl sweep take time 2 matrix k gamma1 comput time 3 actual make sens perform quit gibb sampl scan updat paramet probabl make markov chain mix faster secondli paramet updat use hybrid mont carlo method make predict one averag predict made would possibl obtain deriv cvscore respect knowledg use practic 4 gp multipleclass classif extens preced framework multipl class essenti straightforward although notat complex throughout employ oneofm class code scheme 2 use multiclass analogu logist functionth softmax functionto describ class probabl probabl instanc label class c denot c upper index denot exampl number lower index class label similarli activ associ probabl denot c formal link function relat activ probabl c automat enforc constraint 1 target similarli repres c specifi use oneofm code log likelihood take form c softmax link function give c two class case shall assum gp prior oper activ space specifi correl activ c one import assumpt make prior knowledg restrict correl activ particular class whilst difficulti extend framework includ interclass correl yet encount situat felt abl specifi correl formal activ correl take form hy c 12 k ii 0 c element covari matrix cth class individu correl matrix k c form given equat 4 twoclass case shall use separ set paramet class use independ process perform classif redund forc activ one process say zero would introduc arbitrari asymmetri prior simplic introduc augment vector notat twoclass case c denot activ correspond input x class c notat also use defin similar manner defin exclud valu correspond test point x let definit augment vector gp prior take form ae oe equat 12 covari matrix k block diagon matric k individu matrix k c express correl activ within class c twoclass case use laplac approxim need find mode p jt procedur describ appendix c twoclass case make predict x averag softmax function gaussian approxim posterior distribut present simpli estim integr use 1000 draw gaussian random vector gener class repres vector length zero entri everywher except correct compon contain 1 5 experiment result use newtonraphson algorithm initi time entri 1m iter mean rel differ element w consecut iter less 10 gamma4 hmc algorithm step size use paramet larg possibl keep reject rate low use trajectori made leapfrog step gave low correl success state prior paramet set gaussian mean gamma3 standard deviat 3 simul step size produc low reject rate 5 paramet correspond w l initi gamma2 v 0 0 sampl procedur run 200 iter first third run discard burnin intend give paramet time come close equilibrium distribut test carri use rcoda packag 3 exampl section 51 suggest inde effect remov transient although note wide recogn see eg 2 determin equilibrium distribut reach difficult problem although number iter use much less typic use mcmc method rememb iter involv leapfrog step ii use hmc aim reduc random walk behaviour seen method metropoli algorithm autocorrel analysi paramet indic gener low correl obtain lag iter matlab code use run experi avail ftpcsastonacukneuralwillickigpclass 51 two class tri method two well known two class classif problem leptograpsu crab pima indian diabet dataset 4 first rescal input mean zero unit varianc train set matlab implement hmc simul task take sever hour sgi challeng machin 200mhz r10000 although good result obtain much less time also tri standard metropoli mcmc algorithm crab problem found similar result although sampl method somewhat slower hmc result crab pima task togeth comparison method 20 18 given tabl 1 2 respect tabl also includ result obtain gaussian process use estim paramet maxim penalis likelihood found use 20 iter scale conjug gradient optimis b neal mcmc method detail setup use neal method given appendix e leptograpsu crab problem attempt classifi sex crab basi five anatom attribut option addit colour attribut 50 exampl avail crab sex colour make total 200 label exampl split train set 20 crab sex colour make 80 train exampl 120 exampl use test set perform gp method equal best method report 20 name 2 hidden unit neural network direct input output connect logist output unit train maximum likelihood network1 tabl 1 neal method gave similar level perform also found estim paramet use maximum penalis likelihood mpl gave similar perform less minut comput time pima indian diabet problem use data made avail prof ripley trainingtest split 200 332 exampl respect 18 baselin error obtain simpli classifi record come diabet give rise error 33 neal gp method compar best altern perform error around 20 encourag result obtain use laplac approxim neal method similar 5 also estim paramet use maximum penalis likelihood rather mont carlo integr perform case littl wors 217 error 2 minut comput time 3 avail comprehens r archiv network httpwwwcituwienacat 4 avail httpmarkovstatsoxacukpubprnn 5 perform obtain gibb mackay 7 similar method made 4 error crab task colour given 70 error pima dataset method colour given colour given neural network1 3 3 neural network2 5 3 linear discrimin 8 8 logist regress 4 4 pp regress 4 ridg gaussian process laplac 3 3 approxim hmc gaussian process laplac 4 3 approxim mpl gaussian process neal method 4 3 tabl 1 number test error leptograpsu crab task comparison taken ripley 1996 ripley 1994 respect network2 use two hidden unit predict approach ripley 1993 use laplac approxim weight network local minimum method pima indian diabet neural network 75 linear discrimin 67 logist regress 66 pp regress 4 ridg function 75 gaussian mixtur 64 gaussian process laplac 68 approxim hmc gaussian process laplac 69 approxim mpl gaussian process neal method 68 tabl 2 number test error pima indian diabet task comparison taken ripley 1996 ripley 1994 respect neural network one hidden unit train maximum likelihood result wors net two hidden unit ripley 1996 analysi posterior distribut w paramet covari function equat inform figur 51 plot posterior margin mean 1 standard deviat error bar seven input dimens recal variabl scale zero mean unit varianc would appear variabl 1 3 shortest lengthscal therefor variabl associ 52 multipl class due rather long time taken run code abl test rel small problem mean hundr data point sever class furthermor found full bayesian integr possibl paramet set beyond comput mean therefor satisfi maximum penalis likelihood approach rather use potenti gradient hmc routin simpli use input scale conjug gradient optimis base 13 instead attempt find mode class posterior rather averag posterior distribut test multipl class method forens glass dataset describ 18 dataset 214 exampl 9 input 6 output class dataset small perform figur 2 plot log w paramet pima dataset circl indic posterior margin mean obtain hmc run burnin one standard deviat error bar squar symbol show log wparamet valu found maxim penal likelihood variabl 1 number pregnanc 2 plasma glucos concentr 3 diastol blood pressur 4 tricep skin fold thick 5 bodi mass index 6 diabet pedigre function 7 age comparison wahba et al 1995 use gener linear regress found variabl 1 2 5 6 import estim use 10fold cross valid comput penalis maximum likelihood estim multipl gp method took approxim 24 hour sgi challeng gave classif error rate 233 see tabl 3 compar best method perform neal method surprisingli poor may due fact allow separ paramet process constrain equal neal code also small perhap signific differ specif prior see appendix e detail 6 discuss paper extend work william rasmussen 28 classif problem demonstr perform well dataset tri believ kind gaussian method forens glass neural network 4hu 238 linear discrimin 36 pp regress 5 ridg function 35 gaussian mixtur 308 decis tree 322 gaussian process la mpl 233 gaussian process neal method 318 tabl 3 percentag test error forens glass problem see ripley 1996 detail method process prior use easili interpret model neural network prior parameter function space exampl posterior distribut ard paramet illustr figur 51 pima indian diabet problem indic rel import variou input interpret also facilit incorpor prior knowledg new problem quit strong similar gp classifi supportvector machin svm 23 svm use covari kernel differ gp approach use differ data fit term maximum margin optim found use quadrat program comparison two algorithm interest direct futur research problem method base gp requir comput trace determin linear solut involv n theta n matric n number train exampl henc run problem larg dataset look method use bayesian numer techniqu calcul trace determin 22 6 although found techniqu work well rel small size problem test method comput method use speed quadrat program problem svm may also use gp classifi problem also investig use differ covari function improv approxim employ acknowledg thank prof b ripley make avail leptograpsu crab pima indian diabet forens glass dataset work partial support epsrc grant grj75425 novel develop learn theori neural network much work carri aston univers author grate acknowledg hospit provid isaac newton institut mathemat scienc cambridg uk paper written thank mark gibb david mackay radford neal help discuss anonym refere comment help improv paper appendix maxim case describ find iter vector p jt maxim materi also cover 8 x533 25 x92 provid complet term equat 9 welldefin complet set activ bay theorem log log p depend normal factor maximum p jt found maxim psi respect use log log 2 14 k covari matrix gp evalu x psi defin similarli equat 8 k partit term n theta n matrix k n theta 1 vector k scalar k viz enter equat 14 quadrat prior term data point associ maxim respect achiev first maxim psi respect quadrat optim determin find maximum psi use newtonraphson iter new differenti equat 8 respect find nois matrix given result iter equat avoid unnecessari invers usual conveni rewrit form note gammarrpsi alway posit definit optim problem convex given converg solut easili found use w zero append n diagon posit given mean varianc easi find r mean distribut p jt order calcul gaussian integr logist sigmoid function employ approxim base expans sigmoid function term error function gaussian integr error function anoth error function approxim fast comput specif use basi set five scale error function interpol logist sigmoid chosen point 6 give accur approxim desir integr small comput cost justif laplac approxim case somewhat differ argument usual put forward eg asymptot normal maximum likelihood estim model finit number paramet dimens problem grow number data point howev consid infil asymptot see eg 3 number data point bound region increas local averag train data point x provid tightli local estim x henc yx reason parallel formal argument found 29 thu would expect distribut p becom gaussian increas data appendix b deriv log p tj wrt hmc mpl method requir deriv l log p tj respect compon exampl k deriv involv two term one due explicit depend l log 2 k also chang caus chang howev chosen rpsiyj l gamma2 log jk depend jk gamma1 w j aris depend w henc differenti one obtain henc requir deriv calcul appendix maxim multipleclass case gp prior likelihood defin equat 13 11 defin posterior distribut activ jt appendix interest laplac approxim posterior therefor need find mode respect drop unnecessari constant multiclass analogu equat 14 c exp 6 detail use basi function erfx use interpol oex principl appendix defin psi analog equat 8 first optim psi respect afterward perform quadrat optim psi respect order optim psi respect make use hessian given k mn theta mn blockdiagon matrix block k c although form two class case equat 17 slight chang definit nois matrix w conveni way defin w introduc matrix pi mn theta n matrix form use notat write nois matrix form diagon matrix outer product twoclass case note gammarrpsi posit definit optim problem convex updat equat iter optim psi respect activ follow form given equat 18 advantag represent nois matrix equat 23 invert matric find determin use ident blockdiagon invert blockwis thu rather requir determin invers mn theta mn matrix need carri expens matrix comput n theta n matric result updat equat form given equat 18 nois matrix covari matric multipl class form essenti result need gener method multipleclass problem although mention time complex problem scale 3 rather due ident equat 24 25 calcul function gradient still rather expens experi sever method mode find laplac approxim advantag newton iter method fast quadrat converg integr part newton step calcul invers matrix act upon vector ie gamma1 b order speed particular step use conjug gradient cg method solv iter correspond linear system b repeatedli need solv system w chang updat save time run cg method converg time call experi cg algorithm termin 1n calcul deriv log p tj wrt multipleclass case analog twoclass case describ appendix b appendix hybrid mont carlo hmc work creat fictiti dynam system paramet regard posit variabl augment momentum variabl p purpos dynam system give paramet inertia randomwalk behaviour space avoid total energi h system sum kinet energi potenti energi e potenti energi defin pjd expgamma ie sampl joint distribut p given p p expgamma gamma k margin distribut requir posterior sampl paramet posterior therefor obtain simpli ignor momenta sampl joint distribut achiev two step find new point phase space nearident energi h simul dynam system use discretis approxim hamiltonian dynam ii chang energi h gibb sampl momentum variabl hamilton first order differenti equat h approxim use leapfrog method requir deriv e respect given gaussian prior log p straightforward differenti deriv log p tj also straightforward although implicit depend henc must taken account describ appendix b calcul energi quit expens new need perform maxim requir laplac approxim equat 9 propos state accept reject use metropoli rule depend final energi h necessarili equal initi energi h discret appendix e simul setup neal code use fbm softwar avail httpwwwcsutorontocaradfordfbmsoftwarehtml exampl command use run pima exampl modelspec pima1log binari gpgen pima1log fix 05 1 mcspec pima1log repeat 4 scanvalu 200 heatbath hybrid 6 05 gpmc pima1log 500 follow close exampl given neal document gpspec command specifi form gaussian process particular prior paramet v 0 ws see equat 4 express 00505 specifi gammadistribut prior v 0 x02051 specifi hierarch gamma prior ws note jitter 01 also specifi prior covari function improv condit covari matrix mcspec command give detail mcmc updat procedur specifi 4 repetit 200 scan valu follow 6 hmc updat paramet use stepsiz adjust factor 05 gpmc specifi sequenc carri 500 time aim reject rate around 5 exceed stepsiz reduct factor reduc simul run r statist spatial data hybrid mont carlo bayesian data analysi effici implement gaussian process variat gaussian process classifi nonparametr regress gener linear model correspond bayesian estim stochast process smooth spline bayesian method backpropag network maximum likelihood estim model residu covari spatial regress gener linear model scale conjug gradient algorithm fast supervis learn mont carlo implement gaussian process model bayesian regress classif bayesian learn neural network automat smooth regress function gener linear model evalu gaussian process method nonlinear regr sion pattern recognit neural network statist aspect neural network flexibl nonlinear approach classif densiti ratio bayesian numer analysi natur statist learn theori comparison gcv gml choos smooth paramet gener spline smooth problem spline model observ data classif comput infinit network gaussian process regress comparison krige nonparametr regress method tr ctr christoph k william connect kernel pca metric multidimension scale machin learn v46 n13 p1119 2002 keerthi k b duan k shevad n poo fast dual algorithm kernel logist regress machin learn v61 n13 p151165 novemb 2005 mrio figueiredo adapt spars supervis learn ieee transact pattern analysi machin intellig v25 n9 p11501159 septemb koji tsuda propag distribut hypergraph dual inform regular proceed 22nd intern confer machin learn p920927 august 0711 2005 bonn germani w addison r h glendin robust imag classif signal process v86 n7 p14881501 juli 2006 hyunchul kim daijin kim zoubin ghahramani sung yang bang appearancebas gender classif gaussian process pattern recognit letter v27 n6 p618626 15 april 2006 yasemin altun alex j smola thoma hofmann exponenti famili condit random field proceed 20th confer uncertainti artifici intellig p29 juli 0711 2004 banff canada wei chu zoubin ghahramani prefer learn gaussian process proceed 22nd intern confer machin learn p137144 august 0711 2005 bonn germani balaji krishnapuram alexand j hartemink lawrenc carin mario figueiredo bayesian approach joint featur select classifi design ieee transact pattern analysi machin intellig v26 n9 p11051111 septemb 2004 wei chu sathiya keerthi chong jin ong bayesian trigonometr support vector classifi neural comput v15 n9 p22272254 septemb yasemin altun thoma hofmann alexand j smola gaussian process classif segment annot sequenc proceed twentyfirst intern confer machin learn p4 juli 0408 2004 banff alberta canada hyunchul kim jaewook lee cluster base gaussian process neural comput v19 n11 p30883107 novemb 2007 bart bakker tom hesk task cluster gate bayesian multitask learn journal machin learn research 4 p8399 1212003 mark girolami simon roger variat bayesian multinomi probit regress gaussian process prior neural comput v18 n8 p17901817 august 2006 liefeng bo ling wang licheng jiao featur scale kernel fisher discrimin analysi use leaveoneout cross valid neural comput v18 n4 p961978 april 2006 volker tresp gener bayesian committe machin proceed sixth acm sigkdd intern confer knowledg discoveri data mine p130139 august 2023 2000 boston massachusett unit state malt kuss carl edward rasmussen assess approxim infer binari gaussian process classif journal machin learn research 6 p16791704 1212005 lehel csat manfr opper spars onlin gaussian process neural comput v14 n3 p641668 march 2002 manfr opper ole winther gaussian process classif meanfield algorithm neural comput v12 n11 p26552684 novemb 2000 michael lindenbaum shaul markovitch dmitri rusakov select sampl nearest neighbor classifi machin learn v54 n2 p125152 februari 2004 volker tresp scale kernelbas system larg data set data mine knowledg discoveri v5 n3 p197211 juli 2001 charl micchelli massimiliano pontil learn vectorvalu function neural comput v17 n1 p177204 januari 2005 michael e tip spars bayesian learn relev vector machin journal machin learn research 1 p211244 912001 balaji krishnapuram lawrenc carin mario figueiredo alexand j hartemink spars multinomi logist regress fast algorithm gener bound ieee transact pattern analysi machin intellig v27 n6 p957968 june 2005 van gestel j k suyken g lanckriet lambrecht b de moor j vandewal bayesian framework leastsquar support vector machin classifi gaussian process kernel fisher discrimin analysi neural comput v14 n5 p11151147 may 2002 zhihua zhang jame kwok dityan yeung modelbas transduct learn kernel matrix machin learn v63 n1 p69101 april 2006 gavin c cawley nicola l c talbot prevent overfit model select via bayesian regularis hyperparamet journal machin learn research 8 p841861 512007 matthia seeger pacbayesian generalis error bound gaussian process classif journal machin learn research 3 p233269 312003 arnulf b graf felix wichmann heinrich h blthoff bernhard h schlkopf classif face man machin neural comput v18 n1 p143165 januari 2006 ralf herbrich thore graepel colin campbel bay point machin journal machin learn research 1 p245279 912001 liam paninski jonathan w pillow eero p simoncelli maximum likelihood estim stochast integrateandfir neural encod model neural comput v16 n12 p25332561 decemb 2004 alexand j smola bernhard schlkopf bayesian kernel method advanc lectur machin learn springerverlag new york inc new york ny