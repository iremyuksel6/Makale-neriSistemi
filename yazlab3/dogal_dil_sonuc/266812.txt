tune compil optim simultan multithread compil optim often driven specif assumpt underli architectur implement target machin exampl target sharedmemori multiprocessor parallel program compil minim share order decreas highcost interprocessor commun paper reexamin sever compil optim context simultan multithread smt processor architectur issu instruct multipl thread function unit cycl unlik sharedmemori multiprocessor smt provid benefit finegrain share processor memori system resourc unlik current multiprocessor smt expos benefit interthread instructionlevel parallel hide latenc therefor optim appropri convent machin may inappropri smt revisit three optim light loopiter schedul softwar specul execut loop tile result show three optim appli differ context smt architectur thread parallel cyclic rather block algorithm nonloop program softwar specul compil longer need concern precis size tile match cach size follow new guidelin compil gener code improv perform program execut smt machin b introduct compil optim typic driven specif assumpt underli architectur implement target machin exampl compil schedul longlat oper earli minim critic path order instruct base processor issu slot restrict maxim function unit util alloc frequent use variabl regist benefit fast access time new process paradigm chang architectur assumpt howev must reevalu machinedepend compil optim order maxim perform new machin simultan multithread smt 323121 13 multithread processor design alter sever architectur assumpt compil tradit reli smt processor instruct multipl thread issu function unit cycl take advantag simultan threadissu capabl processor resourc memori subsystem resourc dynam share among thread singl featur respons perform gain almost 2x wideissu superscalar roughli 60 singlechip share memori multiprocessor multiprogram spec92 specint95 parallel splash2 specfp95 workload smt achiev improv limit slowdown singl execut thread 2 13 simultan multithread present compil differ model hide oper latenc share code data oper latenc hidden instruct execut thread thread longlat oper addit multithread instruct issu increas instructionlevel parallel ilp level much higher sustain singl thread factor suggest reconsid uniprocessor optim copyright 1997 ieee publish proceed micro30 decemb 13 1997 research triangl park north carolina person use materi permit howev permiss reprintrepublish materi advertis promot purpos creat new collect work resal redistribut server list reus copyright compon work work must obtain ieee contact manag copyright permiss ieee servic center 445 hoe lane po box 1331 piscataway nj hide latenc expos ilp expens increas dynam instruct count smt latencyhid benefit may need extra instruct may consum resourc could better util instruct concurr thread multipl thread resid within singl smt processor cheapli share common data incur penalti fals share fact benefit crossthread spatial local call question compilerdriven parallel techniqu origin develop distributedmemori multiprocessor partit data physic distribut thread avoid commun coher cost smt may benefici parallel program process contigu data paper investig extent simultan multithread affect use sever compil optim particular examin one parallel techniqu loopiter schedul compilerparallel applic two optim hide memori latenc expos instructionlevel parallel softwar specul execut loop tile result prescrib differ usag three optim compil smt processor found block loop schedul may use distribut data distributedmemori multiprocessor cyclic iter schedul appropri smt architectur reduc tlb footprint parallel applic sinc smt thread run singl processor share memori hierarchi data share among thread improv local memori page softwar specul execut may incur addit instruct overhead convent wideissu superscalar instruct throughput usual low enough addit instruct simpli consum resourc would otherwis go unus howev smt processor simultan multithread instruct issu increas throughput roughli 62 8wide processor softwar specul execut degrad perform particularli nonloopbas applic simultan multithread also impact loop tile techniqu tile size select smt processor far less sensit variat tile size convent processor must find appropri balanc larg tile low instruct overhead small tile better cach reus higher hit rate processor elimin perform sweet spot hide extra miss larger tile addit threadlevel parallel provid multithread tile loop smt decompos thread comput tile rather creat separ tile thread done multiprocessor tile way rais perform smt processor moderatelys memori subsystem aggress design remaind paper organ follow section 2 provid brief descript smt processor section 3 discuss detail two architectur assumpt affect simultan multithread ramif compilerdirect loop distribut softwar specul execut loop tile section 4 present experiment methodolog section 5 7 examin compil optim provid experiment result analysi section 8 briefli discuss compil issu rais smt relat work appear section 9 conclud section 10 2 microarchitectur simultan multithread processor smt design eightwid outoford processor hardwar context eight thread everi cycl instruct fetch unit fetch four instruct two thread fetch unit favor high throughput thread fetch two thread fewest instruct wait execut fetch instruct decod regist renam insert either integ float point instruct queue operand becom avail instruct thread issu function unit execut final instruct retir perthread program order littl microarchitectur need redesign enabl optim simultan multithread compon integr part convent dynamicallyschedul superscalar major except larger regist file 32 architectur regist per thread plu 100 renam regist two addit pipelin stage access regist one read write instruct fetch scheme mention sever perthread mechan program counter return stack retir trap logic identifi tlb branch target buffer notabl miss list special perthread hardwar schedul instruct onto function unit instruct schedul done convent outoford superscalar instruct issu operand calcul load memori without regard thread renam hardwar elimin interthread regist name conflict map threadspecif architectur regist onto processor physic regist see 31 detail larg hardwar data structur cach tlb branch predict tabl share among thread addit crossthread conflict cach branch predict hardwar absorb smt enhanc latencyhid capabl 21 tlb interfer address techniqu describ section 5 rethink compil optim explain simultan multithread reli novel featur attain greater processor perform coupl multithread wide instruct issu schedul instruct differ thread cycl new design prompt us revisit compil optim automat parallel loop enhanc memori perform andor increas ilp section discuss two factor affect smt uniqu design data share among thread avail instruct issu slot light three compil optim affect interthread data share convent parallel techniqu target multiprocessor thread physic distribut differ processor minim cach coher interprocessor commun overhead data loop distribut techniqu partit distribut data match physic topolog multiprocessor parallel compil attempt decompos applic minim synchron commun loop typic achiev alloc disjoint set data processor work independ 34107 contrast smt multipl thread execut processor affect perform two way first real fals interthread data share entail local memori access incur coher overhead smt share l1 cach consequ share even fals share benefici second share data among thread memori footprint parallel applic reduc result better cach tlb behavior factor suggest loop distribut polici cluster rather separ data multipl thread latencyhid capabl avail instruct issu slot workload wideissu processor typic sustain high instruct throughput low instructionlevel parallel singl execut thread compil optim softwar specul execut loop tile block tri increas ilp hide reduc instruct latenc respect often side effect increas dynam instruct count despit addit instruct optim often profit instruct overhead accommod otherwis idl function unit issu instruct multipl thread smt processor fewer empti issu slot fact sustain instruct throughput rather high roughli 2 time greater convent superscalar 13 furthermor smt better job hide latenc singlethread processor use instruct one thread mask delay anoth environ aforement optim may less use even detriment overhead instruct compet use instruct hardwar resourc smt simultan multithread capabl natur toler high latenc without addit instruct overhead examin compil optim describ methodolog use experi chose applic spec 92 12 spec 95 30 splash2 35 benchmark suit tabl 2 program compil multiflow trace schedul compil 22 gener dec alpha object file multiflow chosen gener highqual code use aggress static schedul wide issu loop unrol ilpexpos optim implicitlyparallel applic spec suit first parallel suif compil 15 suif c output fed multiflow block loop distribut polici commonli use multiprocessor execut implement suif use applic compil latest version suif 5 access sourc implement altern algorithm describ section 5 hand suif also find tileabl loop determin appropri multiprocessorori tile size particular data set cach gener tile code experi tile size manual code specul execut enableddis modifi multiflow compil machin descript file specifi instruct move specul trace schedul experi static gener profiledriven trace latter profil inform gener instrument applic execut train input data set differ set use simul object file gener multiflow link version anl 4 suif runtim librari creat execut smt simul process unmodifi alpha execut use emulationbas instructionlevel simul model detail processor pipelin hardwar support oford execut entir memori hierarchi includ tlb usag memori hierarchi processor consist two level cach size latenc bandwidth characterist shown applic data set instruc tion simul f applu 33x33x33 array 2 iter 272 x x mgrid su2cor 16x16x16x16 vector len 4k 2 iter 54 b x x tomcatv 513x513 array 5 iter fft 64k data point lu 512x512 matrix 431 x water nsquar 512 molecul 3 timestep 870 x water spatial 512 molecul 3 timestep 784 x compress train input set 64 x go train input set 2stone9 700 x li train input set 258 x test input set dhryston 164 x perl train input set scrabbl 56 x mxm matrix multipli 256x128 128x64 array gmt 500x500 gaussian elimin 354 x adi integr stencil comput solv partial differenti equat tabl 1 benchmark last three column identifi studi applic use specul execut l1 icach l1 dcach l2 cach cach size byte 128k 32k 128k line size byte 64 64 64 bank 8 8 1 transfer timebank 1 cycl 1 cycl 4 cycl cach fill time cycl latenc next level 10 tabl 2 memori hierarchi paramet choic valu first aggress repres forecast smt implement roughli three year futur use experi second set typic today memori subsystem use emul larger data set size 29 use tile studi tabl 2 model cach behavior well bank bu content two tlb size use loop distribut experi 48 128 entri illustr perform loop distribut polici sensit tlb size larger tlb repres probabl configur futur generalpurpos smt smaller appropri less aggress design smt multimedia co processor page size typic rang 28mb tlb size miss requir two full memori access incur 160 cycl penalti branch predict use mcfarlingstyl hybrid predictor 256entri 4way setassoci branch target buffer 8k entri selector choos global histori predictor 13 histori bit local predictor 2kentri local histori tabl index 4kentri 2bit local predict tabl 24 length simul limit detail simul result parallel comput portion applic norm simul parallel applic initi phase applic use fast simul mode simul cach warm main comput phase reach turn detail simul model 5 loop distribut reduc commun coher overhead distributedmemori multiprocessor parallel compil employ block loop parallel polici distribut iter across processor block distribut assign thread processor continu array data iter manipul figur 1 figur present smt speedup applic parallel use block distribut two tlb size good speedup obtain mani applic number thread increas smaller tlb perform sever program hydro2d swim tomcatv degrad 6 8 thread 8 thread case particularli import applic parallel exploit 8 hardwar context smt analysi simul bottleneck metric indic slowdown result thrash data tlb indic tlb miss rate tabl 3 tlb thrash direct result block partit increas total work set applic thread work disjoint data set sever case 8 thread requir mani tlb entri loop stride sever larg array sinc primari data set usual larger typic 8kb page size least one tlb entri requir array swim benchmark specfp95 illustr extrem exampl one loop 9 larg array access iter loop loop parallel use block distribut data tlb footprint 9 array 8 exclud entri requir data size less 72 signific thrash occur parallel profit lesson tlb share resourc need manag effici smt least three approach consid 1 use fewer 8 thread parallel 2 increas data tlb size 3 parallel loop differ first altern unnecessarili limit use thread hardwar context neither exploit smt parallel applic fullest potenti second choic incur cost access time hardwar although increas chip densiti futur processor may abl accommod 1 even larger tlb 1 found 64 entri solv problem howev 128 entri data tlb avoid tlb thrash figur 2b indic achiev speedup least specfp95 data set applic number thread applu 07 09 10 09 10 hydro2d 01 01 01 07 63 mgrid 00 00 00 00 01 su2cor 01 52 77 62 55 tomcatv 01 01 01 20 107 tabl 3 tlb miss rate miss rate shown block distribut 48entri data tlb bold entri correspond decreas perform see figur 2 number thread increas howev desir reduc tlb footprint smt true smt workload would multiprogram exampl multipl parallel applic could execut togeth compris thread hardwar context thread schedul could schedul 8 thread first parallel applic context switch run second later switch back first type environ would performancewis minim data tlb footprint requir applic exampl tlb footprint multiprogram workload consist swim hydro2d would greater 128 entri third desir solut reli compil reduc data tlb footprint rather distribut loop iter block organ could use cyclic distribut cluster access multipl thread onto fewer page cyclic partit swim would consum 9 rather 72 tlb entri cyclic partit also requir less instruct overhead calcul array partit bound non neglig although much less import factor compar block cyclic loop distribut code data figur 1 figur 3 illustr speedup attain cyclic distribut block tabl 4 contain correspond chang data tlb miss rate 48entri tlb applic better cyclic distribut case signific decreas data tlb miss coupl long 160 cycl tlb miss penalti major factor cyclic increas tlb conflict tomcatv 2 4 thread number miss low overal program perform suffer 6 8 thread tomcatv origin loop block parallel c cyclic parallel figur 1 block cyclic loop distribut exampl code exampl loop nest shown use block distribut code structur b cyclic version shown c right e illustr portion array access thread two polici clariti assum 4 thread assum row array 2kb 512 doubl precis element block distribut thread access differ 8kb page memori cyclic e howev loop decompos manner allow four thread access singl 8kb page time thu reduc tlb footprint dimens dimens thread 0 thread 1 thread 2 thread 3 block 48entri data tlb26 figur 2 speedup one thread block parallel number thread applu hydro2d mgrid su2cor swim tomcatv average1030 b 128entri data tlb applu hydro2d mgrid su2cor swim tomcatv average1030speedup block data tlb miss rate jump 2 11 caus correspond hike speedup cyclic absolut miss rate larger data tlb low enough usual 02 except applu su2cor reach 09 chang produc littl benefit cyclic contrast su2cor saw degrad cyclic schedul increas loop unrol instruct overhead perform degrad seen smaller tlb size cyclic improv tlb hit rate offset overhead mgrid saw larg perform improv tlb size reduct dynam instruct count figur 1b 1c illustr cyclic parallel requir fewer comput longlat divid summari result suggest use cyclic loop distribut smt rather tradit block distribut parallel applic larg data footprint cyclic distribut increas program speedup saw speedup high 41 even smallish specfp95 refer data set applic smaller data footprint cyclic broke even one applic odd interact loop unrol factor cyclic worsen perform multiprocessor smt processor cyclic distribut would still appropri within node applic 48entri tlb 128entri tlb number thread number thread applu 0 50 58 53 15 0 91 98 85 69 hydro2d 0 0 14 91 99 0 0 0 0 14 mgrid 0 0 0 0 50 0 0 0 0 0 su2cor 14 99 99 99 97 0 0 98 91 94 tomcatv 0 60 60 96 99 0 60 60 60 60 tabl 4 improv decreas tlb miss rate cyclic distribut block 35 41 applu hydro2d mgrid su2cor swim tomcatv mean10speedup versu block b 128entri data tlb applu hydro2d mgrid su2cor swim tomcatv mean10speedup versu block 48entri data tlb thread 4 thread 6 thread 8 thread figur 3 speedup attain cyclic block parallel applic execut time block normal 10 number thread thu bar compar speedup cyclic block number thread hybrid parallel polici might desir though block distribut across processor minim interprocessor commun 6 softwar specul execut today optim compil reli aggress code schedul hide instruct latenc global schedul techniqu trace schedul 22 hyperblock schedul 23 instruct predict branch path may move condit branch execut becom specul runtim branch path taken specul instruct useless potenti wast processor resourc inord superscalar vliw machin softwar specul necessari hardwar provid schedul assist smt processor whose execut core outoford superscalar instruct dynam schedul specul execut hardwar multithread also use hide latenc number smt thread increas instruct throughput also increas therefor latencyhid benefit softwar specul execut may need less even unnecessari addit instruct overhead introduc incorrect specul may degrad perform experi design evalu appropri softwar specul execut smt processor result highlight two factor determin effect smt static branch predict accuraci instruct throughput correctlyspecul instruct instruct overhead incorrectlyspecul instruct howev add dynam instruct count therefor specul execut benefici applic high specul accuraci eg loopbas program either profiledriven stateoftheart static branch predict tabl 5 compar dynam instruct count profiledriven 2 specul nonspecul version applic small increas dynam instruct count indic compil assist profil inform abl accur predict path execut 3 consequ specul may incur penalti higher increas dynam instruct count hand mean wrongpath specul probabl loss smt perform instruct overhead influenc effect specul factor level instruct throughput program without specul also import determin easili specul overhead absorb suffici instruct issu bandwidth low ipc incorrect specul may caus harm higher 2 use profiledriven specul provid bestcas comparison smt without profil mispredict would occur overhead instruct would gener consequ softwar specul would wors perform report make absenc appear even benefici smt 3 specfp95 program radix splash2 compress specint95 loopbas small increas dynam instruct count specul increas specint95 increas splash2 increas applu 21 compress 29 fft 137 hydro2d 19 go 126 lu 125 mgrid 05 li 73 radix 00 su2cor 01 m88ksim 40 waternsquar 30 tomcatv 12 tabl 5 percentag increas dynam instruct count due profiledriven softwar specul execut data shown 8 thread one thread number ident close applic bold high specul instruct overhead high ipc without specul ital former spec specint95 spec spec splash2 spec spec applu 49 47 compress 41 40 fft 60 64 hydro2d 56 54 go 24 23 lu 67 68 mgrid 72 71 li 45 46 radix 54 54 su2cor 61 60 m88ksim 42 41 water nsquar 64 61 tomcatv 62 59 spatial tabl throughput instruct per cycl without profiledriven softwar specul 8 thread program bold high ipc without specul plu high specul overhead ital former perthread ilp thread softwar specul less profit incorrectlyspecul instruct like compet use instruct processor resourc particular fetch bandwidth function unit issu tabl 6 contain instruct throughput applic program ipc higher softwar specul indic degre absorpt specul overhead other lower addit hardwar resourc conflict notabl l1 cach miss specul instruct overhead relat static branch predict accuraci instruct throughput togeth explain speedup lack thereof illustr figur 4 factor high nonloop base fft li lu speedup without softwar specul greatest rang 22 4 one factor low moder speedup minim nonexist specfp95 applic radix waternsquar high ipc go m88ksim perl specul overhead 5 without either factor softwar specul help perform reason benefit architectur hid latenc execut specul instruct 4 applic other well thread use advantag turn specul gener becom even larger addit thread provid parallel therefor specul instruct like compet use instruct processor resourc applu hydro2d tomcatv mgrid su2cor swim05speedup specul specfp95 thread 4 thread 6 thread 8 thread lu go fft li perl specul b splash2 spec95 int nsquar spatial figur 4 speedup applic execut without softwar specul specul specul execut cycl specul execut cycl bar greater 10 indic specul better otherwis idl function unit bottom line loopbas applic compil softwar specul execut nonloop applic compil without either improv smt program perform maintain current level perform never hurt 6 7 loop tile order improv cach behavior loop tile take advantag data reus section examin two tile issu tile size select distribut tile thread tile size chosen appropri reduct averag memori access time compens tile overhead instruct 20116 code figur 6b 6c illustr sourc overhead smt howev tile may less benefici first smt enhanc latencyhid capabl may render tile unnecessari second addit tile instruct may increas execut time given smt higher multithread throughput factor influenc whether softwar specul address issu examin tileabl loop nest differ memori access characterist execut smt processor benefit tile vari size cach chang smaller cach requir smaller tile natur introduc instruct overhead hand smaller tile also produc lower averag memori latenc ie fewer conflict miss latenc reduc benefit tile better therefor vari tile size measur perform impact rang tile overhead also simul two memori hierarchi gaug interact cach size memori latenc tile size larger memori configur repres probabl smt memori subsystem machin product approxim 3 year futur see section 4 configur smaller model today memori hierarchi design provid appropri ratio data set cach size model loop larger ie realist data set benchmark experi thread given separ tile tile norm figur 5 present perform total execut cycl averag memori access time dynam instruct count rang tile size larger memori configur 8thread smt execut applic compar singlethread run 5 even though float point comput waterspati high ipc without specul 65 therefor specul instruct bottleneck integ unit execut without specul profit approxim execut superscalar 13 result indic tile profit smt convent processor mxm may seem except sinc tile bring improv except show harm appli optim program execut smt appear insensit tile size almost tile size examin smt abl hide memori latenc indic flat amat curv still absorb tile overhead therefor smt less depend static algorithm determin optim tile size particular cach work set contrast convent processor like tile size sweet spot even outoford execut modern processor well altern singledi processor architectur lack suffici latencyhid abil consequ requir exact tile size calcul compil tile size also perform determin less aggress memori subsystem result shown indic tile smt robust across 6 keep mind specul without runtim support pro file rel benefit specul versu specul would higher exampl 8 thread waternsquar break even profiledriven specul howev reli multiflow static branch predict give specul slight edg speedup 11 nevertheless gener conclus still hold good branch predict low multithread ipc need softwar specul benefit applic execut smt figur 5 tile result larger memori subsystem separ tilesthread horizont axe tile size tile size 0 mean tile size greater 0 one dimens tile measur array element vertic axe metric evalu tile dynam instruct count total execut cycl amat mxm dynam instruct count million total execut cycl million averag memori access time cycl amat total execut cycl million averag memori access time cycl amat adi 8 thread gmt 8 thread memori hierarchi altern rang data set size execut time cours higher perform depend amat paramet rather tile overhead adi becam slightli less toler tile size chang largest tile size measur 32x32 amat increas sharpli interthread interfer small cach loop nest either tile size fit cach altern tile techniqu describ use second loop tile issu distribut tile thread parallel loop multiprocessor differ tile alloc processor thread maxim reus reduc interprocessor commun smt howev tile manner could detriment privat perthread tile discourag interthread tile share increas total thread tile footprint singleprocessor smt factor make block loop iter schedul inappropri smt rather give thread tile call block tile singl tile share thread loop iter distribut cyclic across thread cyclic tile see figur 6 code explan block cyclic tile figur 7 effect perthread data layout tile share cyclic tile optim increas tile size reduc overhead 7c larger tile cyclic tile drop execut time applic execut small memori smt closer smt aggress memori hierarchi put anoth way perform program larg data set origin loop block tile jtlb jt ub jtjtsize it1 itits jjt j minnjtjtsize1j kmax1 kt iit minmititsize1i c cyclic tile kmax1 kt figur code block cyclic version tile loop nest approach smaller exampl figur 8c illustr larger tile size greater 8 array element per dimens cyclic tile reduc mxm amat enough decreas averag execut time smaller cach hierarchi 51 compar block figur 8b within 35 block tile memori subsystem sever time size figur 8a smallest tile size increas tile overhead overwhelm smt abil hide memori latenc cyclic tile still appropri multiprocessor smt hierarch 8 hybrid tile approach might effect cyclic tile could use maxim local processor block tile could distribut tile across processor minim interprocessor thread 1 thread 2 thread 3 dimens c optim cyclic0011 blocked223344 dimens b cyclic21i dimens dimens figur 7 comparison block cyclic tile techniqu multipl thread block tile shown tile 4x4 array element number repres order tile access thread cyclic tile tile still 4x4 array tile share thread exampl thread get one row tile shown b cyclic tile thread work smaller chunk data time tile overhead greater c tile size increas 8x8 reduc overhead within tile thread respons 16 element origin block exampl total execut time million cycl averag memori access time cycl amat dynam instruct count million b c figur 8 tile perform 8thread mxm tile size along xaxi result shown block tile larger memori subsystem b block tile smaller memori subsystem c cyclic tile also smaller memori subsystem 8 compil optim addit optim studi paper compilerdirect prefetch predic execut softwar pipelin also reevalu context smt processor convent processor compilerdirect prefetch 26 use toler memori latenc long prefetch overhead due prefetch instruct addit memori bandwidth andor cach interfer minim smt overhead detriment interfer thread prefetch also compet thread predic execut 231628 architectur model instruct execut guard boolean predic determin whether instruct execut nullifi compil use ifconvers 2 transform control depend data depend therebi expos ilp like softwar specul execut aggress predic incur addit instruct overhead execut instruct either nullifi produc result never use softwar pipelin 927181 improv instruct schedul overlap execut multipl loop iter rather pipelin loop smt execut parallel separ hardwar context allevi increas regist pressur normal associ softwar pipelin multithread could also combin softwar pipelin necessari optim discuss paper origin design increas singlethread ilp intrathread parallel still import smt processor simultan multithread reli multipl thread provid use parallel throughput often becom import perform metric smt rais issu compil throughput singlethread exampl perspect singl run thread optim tradit appli may desir reduc thread run time global perspect greater throughput therefor use work achiev limit amount specul work 9 relat work three compil optim discuss paper wide investig nonsmt architectur loop iter schedul sharedmemori multiprocessor evalu wolf lam 34 carr mckinley tseng 7 anderson amarasingh lam 3 cierniak li 10 among other studi focu schedul minim commun synchron overhead restructur loop data layout improv access local processor particular anderson et al discuss block cyclic map scheme present heurist choos global schedul optim like trace schedul 22 superblock 25 hyperblock 23 allow code motion includ specul motion across basic block therebi expos ilp staticallyschedul vliw wideissu superscalar studi ilp limit lam wilson 19 found specul provid greater speedup loopbas numer applic nonnumer code studi includ effect wrongpath instruct previou work code transform improv local propos variou framework algorithm select appli rang loop transform 1433617347 studi illustr effect tile also propos loop transform enabl better tile lam rothberg wolf 20 coleman mckinley 11 carr et al 6 show applic perform sensit tile size present techniqu select tile size base problems cach paramet rather target fixeds fixedcach occup conclus paper examin compil optim context simultan multithread architectur smt architectur differ previou parallel architectur sever signific way first smt thread share processor memori system resourc singl processor finegrain basi even within singl cycl optim smt therefor seek benefit finegrain share rather avoid done convent sharedmemori multiprocessor second smt hide intrathread latenc use instruct activ thread optim expos ilp may need third instruct throughput smt high therefor optim increas instruct count may degrad perform effect compil strategi simultan multithread processor must recogn uniqu characterist result show specif case smt processor benefit chang compil optim strategi particular show 1 cyclic iter schedul oppos block schedul appropri smt abil reduc tlb footprint 2 softwar specul execut bad smt decreas use instruct throughput 3 loop tile algorithm less concern determin exact tile size smt perform less sensit tile size 4 loop tile increas rather reduc interthread tile share appropri smt increas benefit share memori system resourc acknowledg would like thank john odonnel equat technolog inc tryggv fossum digit equip corp sourc alpha axp version multiflow compil jennif anderson dec western research laboratori provid us suifparallel copi benchmark also would like thank jeffrey dean dec wrl refere whose comment help improv paper research support washington technolog center nsf grant mip9632977 ccr9200832 ccr9632769 darpa grant f306029720226 onr grant n00014 92j1395 n000149411136 dec wrl fellowship intel corpor r optim loop parallel convers control depend data depend data comput transform multiprocessor portabl program parallel processor compil blockabl numer algo rithm compil optim improv data local hierarch tile improv superscalar perform approach scientif array process architectur design ap120bfps164 famili unifi data control transform distribut sharedmemori machin tile size select use cach organ data layout new cpu benchmark suit spec simultan multithread platform nextgener processor strategi cach local memori manag global program transform maxim multiprocessor perform suif compil highli concurr scalar process maxim loop parallel improv data local via loop fusion distribut softwar pipelin effect schedul techniqu vliw machin limit control flow parallel cach perform optim block algorithm convert threadlevel parallel instructionlevel parallel via simultan multithread multiflow trace schedul compil effect compil support predic execut use hyperblock combin branch predictor superblock effect techniqu vliw superscalar compil design evalu compil algorithm prefetch schedul techniqu easili schedul horizont architectur high perform scientif comput cydra 5 department supercomput scale parallel program multiprocessor methodolog exampl exploit choic instruct fetch issu implement simultan multithread processor simultan multi thread maxim onchip parallel data local optim algorithm loop transform theori algorithm maxim parallel splash2 program character methodolog consider tr highli concurr scalar process strategi cach local memori manag global program transform optim loop parallel softwar pipelin effect schedul techniqu vliw machin cydra 5 department supercomput cach perform optim block algorithm data local optim algorithm new cpu benchmark suit spec limit control flow parallel design evalu compil algorithm prefetch effect compil support predic execut use hyperblock compil blockabl numer algorithm multiflow trace schedul compil superblock compil optim improv data local unifi data control transform distribut sharedmemori machin tile size select use cach organ data layout splash2 program simultan multithread exploit choic compilerdirect page color multiprocessor convert threadlevel parallel instructionlevel parallel via simultan multithread convers control depend data depend portabl program parallel processor scale parallel program multiprocessor maxim multiprocessor perform suif compil simultan multithread loop transform theori algorithm maxim parallel hierarch tile improv superscalar perform maxim loop parallel improv data local via loop fusion distribut schedul techniqu easili schedul horizont architectur high perform scientif comput ctr mark n yankelevski constantin polychronopoulo coral multigrain multithread processor architectur proceed 15th intern confer supercomput p358367 june 2001 sorrento itali nichola mitchel larri carter jeann ferrant dean tullsen ilp versu tlp smt proceed 1999 acmiee confer supercomput cdrom p37e novemb 1419 1999 portland oregon unit state jack l lo luiz andr barroso susan j egger kourosh gharachorloo henri levi sujay parekh analysi databas workload perform simultan multithread processor acm sigarch comput architectur news v26 n3 p3950 june 1998 alex settl joshua kihm andrew janiszewski dan connor architectur support enhanc smt job schedul proceed 13th intern confer parallel architectur compil techniqu p6373 septemb 29octob 03 2004 evangelia athanasaki niko anastopoulo kornilio kourti nectario koziri explor perform limit simultan multithread memori intens applic journal supercomput v44 n1 p6497 april 2008 gari zoppetti gagan agraw lori pollock jose nelson amar xinan tang guang gao automat compil techniqu thread coarsen multithread architectur proceed 14th intern confer supercomput p306315 may 0811 2000 santa fe new mexico unit state steven swanson luke k mcdowel michael swift susan j egger henri levi evalu specul instruct execut simultan multithread processor acm transact comput system toc v21 n3 p314340 august calin cacav david padua estim cach miss local use stack distanc proceed 17th annual intern confer supercomput june 2326 2003 san francisco ca usa jame burn jeanluc gaudiot smt layout overhead scalabl ieee transact parallel distribut system v13 n2 p142155 februari 2002 joshua redston susan j egger henri levi analysi oper system behavior simultan multithread architectur acm sigplan notic v35 n11 p245256 nov 2000 joshua redston susan j egger henri levi analysi oper system behavior simultan multithread architectur acm sigarch comput architectur news v28 n5 p245256 dec 2000 luke k mcdowel susan j egger steven gribbl improv server softwar support simultan multithread processor acm sigplan notic v38 n10 octob