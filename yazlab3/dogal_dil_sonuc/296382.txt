track best expert gener recent rel loss bound onlin algorithm addit loss algorithm whole sequenc exampl loss best expert bound gener allow sequenc partit segment goal bound addit loss algorithm sum loss best expert segment model situat exampl chang differ expert best certain segment sequenc exampl singl segment case addit loss proport log n n number expert constant proportion depend loss function algorithm produc best partit howev loss bound show predict close best partit number segment k1 sequenc length ell bound addit loss algorithm best partit ok log nk logellk case loss per trial bound one obtain algorithm whose addit loss loss best partit independ length sequenc addit loss becom oklog n k loglk l loss best partitionwith k1 segment algorithm track predict best expert aresimpl adapt vovk origin algorithm singl best expert case origin algorithm keep one weight per expert spend o1 time per weight trial b introduct consid follow onlin learn model learn occur seri trial label trial goal predict outcom 2 0 1 receiv end trial begin trial algorithm receiv ntupl x element x ti 2 0 1 ntupl x repres predict expert e valu outcom trial algorithm produc predict base current expert predict tupl x past predict outcom end trial algorithm receiv outcom algorithm incur loss measur discrep predict outcom similarli expert incur loss well possibl goal minim total loss algorithm trial arbitrari sequenc instanc outcom pair pair call exampl sinc make assumpt relationship predict expert outcom alway sequenc author support nsf grant ccr9700201 extend abstract appear herbster warmuth 1995 herbster k warmuth far away predict particular algorithm thu minim total loss arbitrari sequenc exampl unreason goal refin relativ goal minim addit loss algorithm loss best expert whole sequenc expert larg loss goal might actual easi achiev sinc algorithm addit loss loss best expert may small howev least one expert predict well algorithm must learn quickli produc predict close predict best expert sens addit loss algorithm loss best expert bound expert framework might use variou set exampl expert might predict chanc rain likelihood stock market rise fall anoth set expert might variou subalgorithm recogn particular pattern master algorithm combin expert predict need know particular problem domain simpli keep one weight per expert repres belief expert predict decreas weight function loss expert previou work vovk vovk 1998 other littleston warmuth 1994 haussler kivinen warmuth 1998 produc algorithm upper bound addit loss algorithm loss best expert algorithm compar loss best expert call staticexpert algorithm paper addit loss bound algorithm form c ln n larg class loss function c constant depend loss function l n number expert class loss function contain essenti common loss function except absolut loss discret loss 1 count predict mistak treat special case littleston warmuth 1994 vovk 1995 cesabianchi freund haussler helmbold schapir warmuth 1997 exampl loss function squar rel entropi loss respect see section 2 definit loss function paper consid modif goal introduc littleston warmuth littleston warmuth 1994 sequenc exampl subdivid k segment arbitrari length distribut segment associ expert sequenc segment associ sequenc expert call partit loss partit sum total loss expert associ segment best partit size k partit k segment smallest loss modifi goal perform well rel best partit size k goal model real life situat natur exampl might chang differ expert produc better predict exampl pattern might chang differ subalgorithm may predict better differ segment onlin sequenc pattern seek design master algorithm track perform best sequenc expert sens incur small addit loss best partit size k whole sequenc exampl given ahead time one could comput best partit certain size associ expert use dynam program algorithm get exampl onlin never produc best partit even abl bound addit loss best offlin partit arbitrari sequenc exampl trial k expert distinct partit immedi get good bound problem expand set n expert expert partitionexpert repres singl partit trial sequenc predict trial expert associ segment contain current trial thu use staticexpert algorithm obtain bound c ln addit loss algorithm loss best partit two problem first algorithm ineffici sinc number partitionexpert exponenti number partit second bound addit loss grow sequenc length abl overcom problem instead keep one weight exponenti mani partit get away keep one weight per expert done staticexpert algorithm track predict best partit essenti free n subalgorithm expert whose predict want combin staticexpert algorithm new master algorithm take addit time per trial time requir simul n subalgorithm develop two main algorithm fixedshar algorithm variabl share algorithm base staticexpert algorithm maintain weight form e gammajt expert cf littleston warmuth 1994 vovk 1995 total past loss expert past trial trial master algorithm combin expert predict use current weight expert outcom trial receiv multipli weight everi expert e gammajl l loss expert current trial call updat weight loss updat modifi staticexpert algorithm ad addit updat obtain algorithm sinc model best expert may shift seri trial simpli use weight form e gammajt expert optim segment loss prior segment may arbitrarili larg thu weight may becom arbitrarili small need modifi staticexpert algorithm small weight recov quickli reason expert share portion weight expert loss updat call share updat fixedshar variableshar algorithm first loss updat follow share updat differ algorithm share updat fraction expert weight ad weight expert fixedshar algorithm expert share fix fraction weight guarante ratio weight expert total weight expert may bound differ form lower bound weight use wml algorithm companion paper learn shift disjunct auer warmuth 1998 appear journal issu latter two method appli learn problem loss discret 4 herbster k warmuth loss ie count mistak contrast method work gener class continu loss function staticexpert algorithm handl vovk 1998 haussler et al 1998 class includ common loss function squar loss rel entropi loss helling loss class tight bound addit loss haussler et al 1998 algorithm loss best expert ie nonshift case fix share algorithm obtain addit loss ock1 log nk log k k essenti sketch algorithm use staticexpert algorithm exponenti mani partitionexpert salient featur fixedshar algorithm still use o1 time per expert per trial howev algorithm addit loss still depend length sequenc lower bound give partial evid seem unavoid loss function loss singl trial unbound rel entropi loss case loss particular trial one squar loss develop second algorithm call variabl share algorithm algorithm obtain bound addit loss independ length sequenc also share weight loss updat howev amount expert share commensur loss expert current trial particular expert loss share weight version share updat trivial implement cost constant amount time n weight although algorithm easi describ prove addit loss bound take care believ techniqu constitut practic method track predict best expert provabl worstcas addit loss bound essenti ingredi success nonstationari set seem algorithm stationari set multipl weight updat whose loss bound grow logarithm dimens problem besid vovk aggreg algorithm vovk 1998 weight major algorithm littleston warmuth 1994 use loss updat basi work number algorithm develop exampl algorithm learn linear threshold function littleston 1988 littleston 1989 algorithm whose addit loss bound loss best linear combin expert sigmoid linear combin expert bound kivinen warmuth 1997 helmbold kivinen warmuth 1995 signific progress recent achiev nonstationari set build techniqu develop paper see discuss conclus section paper outlin follow preliminari section 2 present algorithm section 3 give basic proof techniqu section 4 section 5 6 contain detail proof fixedshar variabl share algorithm respect absolut loss treat special case section 7 section 8 discuss subtl power gener variabl share algorithm call proximityvariableshar algorithm gener lead improv bound case best expert next segment alway like close previou expert preliminari lower bound given section 9 simul result artifici data exemplifi method given section 10 final section 11 conclud discuss recent work casual reader might interest detail proof recommend read section contain preliminari section 2 algorithm section 3 simul section 10 2 preliminari let denot number trial n denot number expert label conveni simpli refer expert index thu expert refer expert e predict n expert trial refer predict tupl x predict expert trial denot x ti expert may view oracl extern algorithm thu may repres predict neural net decis tree physic sensor perhap even human expert outcom trial predict algorithm trial instanceoutcom pair call tth exampl paper outcom expert predict predict algorithm 0 1 throughout paper alway denot arbitrari sequenc exampl ie sequenc element 0 1 n theta 0 1 length loss function lp q function consid four loss function paper squar rel entropi helling absolut loss ent p hel p trial loss algorithm ly similarli loss expert trial ly x ti call subsequ contigu trial segment notat nonneg integ 0 denot segment start trial number end trial 0 round paren use end trial includ segment current sequenc abbrevi loss expert segment tt 0 ltt 0 loss algorithm whole trial sequenc defin ls readi give main definit paper use scenario best expert chang time inform kpartit slice sequenc k segment expert associ segment formal kpartit denot p nkte consist three posit integ n k two tupl e posit integ number length trial sequenc n size expert pool k number target shift tupl k element refer one trial convent use 1 tupl divid trial sequenc 6 herbster k warmuth paramet initi initi weight w ti predict loss updat receiv tth outcom share updat three algorithm staticexpert ti share updat fixedshar 4 variableshar 5 figur 1 staticexpert fixedshar variableshar algorithm call ith segment 0th segment also refer initi segment tupl e k1 element element e denot expert associ ith segment i1 loss given kpartit loss function l trial sequenc 3 algorithm four algorithm consid paper staticexpert fix share variableshar proximityvariableshar first three summar figur 1 proximityvariableshar algorithm gener variableshar algorithm algorithm given figur 3 discuss gener defer section 8 algorithm learn process proce trial 1 denot trial number algo rithm maintain one posit weight per expert weight w ti normal version v thought measur algorithm belief qualiti ith expert predict start trial weight expert initi 1n algorithm follow three paramet j c ff paramet j learn rate quantifi drastic first updat paramet c set 1j loss function absolut loss except treat separ section 7 paramet ff quantifi rate shift expect occur fixedshar algorithm design potenti unbound loss function rel entropi loss variableshar algorithm assum loss per trial lie 0 1 fixedshar al gorithm ff rate shift per trial thu five shift expect 1000 trial sequenc 1200 variableshar algorithm ff approxim rate shift per unit loss best partit five shift expect occur partit total loss 80 ff 116 tune paramet j c consid greater depth section 4 ff section 5 6 final staticexpert algorithm use paramet ff sinc assum shift occur trial algorithm receiv instanc summar predict n expert x algorithm plug current instanc x normal weight v predict function predv x order produc predict simplest case algorithm predict weight mean expert predict ie predv sophist predict function introduc vovk vovk 1998 discuss section 4 predict algorithm perform two updat step first updat loss updat second share updat loss updat weight expert multipli e gammajl l loss ith expert current trial thu updat occur l learn rate j intensifi effect updat use w ti denot weight middl two updat weight refer intermedi weight share updat staticexpert algorithm vacuou howev algorithm share updat crucial briefli argu necess share updat nonstationari set give intuit descript function move predict well best expert predict well sequenc expert loss updat longer appropri sole updat assum two expert two segment first segment expert 1 small loss expert 2 larg loss role revers second segment end first segment loss updat caus weight expert 2 almost zero howev second segment predict expert 2 import weight need recov quickli share updat make sure possibl simul section 10 further intuit share updat need two share updat summar 8 herbster k warmuth straightforward implement cost time per expert per trial fixedshar w ff variableshar w contrast implement figur 1 use intermedi variabl pool cost o1 time per expert per trial loss updat everi expert share fraction weight equal everi expert receiv weight enabl expert recov weight quickli rel expert fixedshar updat 6 expert share fraction ff weight trial one expert perfect long segment type share optim sinc perfect expert keep share weight possibl nonperfect expert variableshar updat 7 sophist roughli expert share weight loss larg perfect expert doesnt share expert high loss eventu collect weight howev perfect expert start incur high loss rapidli begin share weight expert allow good expert previous small rel weight recov quickli discuss paramet ff shift rate introduct discuss algorithm use exponenti mani static expert one partit goal achiev bound close ineffici algorithm use n weight bound obtain share algorithm slightli weaker partitionexpert algorithm grace degrad neither length sequenc number shift k known advanc 4 predict function proof techniqu consid two choic predict function simplest predict weight mean warmuth 1997 pred wmean v sophist predict function give slightli better bound introduc vovk vovk 1998 haussler et al 1998 defin l 0 z function must monoton let l gamma1 1 z denot invers l 0 z l 1 z vovk predict defin two step pred vovk v loss c valu j 1c function pred wmean v x pred vovk v x ent p hel p q 1 1 figur 2 c 1crealiz c valu loss predict function pair follow definit technic condit relat predict function predv x loss function l constant c j et al 1998 vovk 1998 loss function l predict function pred c jrealiz constant c j total weight 1 consid four loss function paper squar rel entropi helling absolut loss see section 2 howev algorithm limit loss function techniqu vovk 1998 haussler et al 1998 warmuth 1997 determin constant c j wide class loss function algorithm also easi adapt classif use major vote littleston warmuth 1994 predict function count mistak loss practic applic worstcas loss bound may provabl given loss function howev share updat may still use interest applic predict disk idl time see work helmbold et al helmbold long sherrod 1996 squar rel entropi helling loss c jrealiz pred wmean pred vovk j 1c valu c henc two predict function summar figur 2 sinc absolut loss complex bound treat section smaller valu c lead smaller loss bound see lemma 1 c valu pred vovk cf column two figur 2 optim larg class loss function haussler et al 1998 proof loss bound algorithm base follow lemma lemma embodi key featur algorithm predict done loss incur algorithm temper correspond chang total weight lemma give inequ lemma use vovk 1998 haussler et al 1998 proof essenti sinc share updat chang total weight w ti haussler et al 1998 sequenc exampl expert total loss master algorithm figur 1 may herbster k warmuth bound loss function l predict function pred c jrealiz cf definit 1 figur 2 proof sinc l pred c jrealiz definit 1 sinc share updat chang total weight ti w t1 impli henc sinc w far use basic techniqu littleston warmuth 1994 vovk 1995 cesabianchi et al 1997 haussler et al 1998 ie c ln w becom potenti function amort analysi static expert case 1c final weight form w n thu lemma lead bound relat loss algorithm loss static expert share updat make much difficult lower bound final weight intuit suffici share weight recov quickli howev much share final weight low follow section bound final weight individu expert term loss partit loss partit lp nkte sum sequenc loss defin sequenc expert partit expert accumul loss segment bound weight use lemma 2 fixedshar algorithm lemma 7 variabl share algorithm sinc partit compos distinct segment must also quantifi weight transfer expert associ segment expert associ follow segment done lemma 3 fixedshar algorithm lemma 8 variableshar algorithm lower bound weight combin lemma 1 bound total loss fixedshar algorithm theorem 1 variableshar algorithm theorem 2 5 fixedshar analysi algorithm work unbound loss function total addit loss grow length sequenc lemma 2 sequenc exampl intermedi weight expert trial 0 least e gammajltt 0 time weight expert start trial 0 formal proof combin loss fixedshar updat equat 6 rewritten drop addit term produc share updat appli iter trial tt 0 sinc bound w weight trial 0 loss updat weight trial 0 reduc factor e gammajli 0 x 0 therefor rt simpl algebra definit lab bound lemma follow lemma 3 sequenc exampl weight expert start trial 1 least ff time intermedi weight expert j trial proof expand fixedshar updat 4 thu w ff done bound addit loss herbster k warmuth theorem 1 let sequenc exampl let l pred c j realiz kshift sequenc partit p nkte total loss fixedshar algorithm paramet ff satisfi proof recal e k expert last segment lemma 1 bound w 1ek note follow weight arbitrari partit express follow telescop product thu appli lemma 3 2 final term w equal one sinc appli share updat final trial therefor definit lp nkte e gammajlp nkte substitut bound w 1ek 16 simplifi obtain 15 bound theorem 1 hold k tradeoff term ck ln n cjlp nkte ie k small ck ln n term small cjlp nkte term larg viceversa optim choic ff obtain differenti bound theorem 1 ff follow corollari rewrit bound theorem 1 term optim paramet choic ff corollari give interpret theorem bound term code length introduc follow notat let 1gammap binari entropi measur nat 1gammaq binari rel entropi nat 2 corollari 1 let sequenc exampl let l pred c j realiz kshift sequenc partit p nkte total loss fixedshar algorithm paramet ff satisfi ck ff bound becom interpret bound ignor constant c j differ nat bit term ln n k lnn gamma 1 account encod expert partit log n bit initi expert logn gamma 1 bit expert thereaft final need encod k shift occur inner boundari partit ff interpret probabl shift occur gamma1 trial term gamma1 hff dff kff correspond expect optim code length see chapter 5 cover thoma 1991 code shift estim ff instead true probabl ff bound thu exampl close similar predict code brought mani paper eg feder merhav gutman 1992 note ff minim bound theorem 1 depend k unknown learner practic good choic ff may determin experiment howev upper bound lower bound k may tune ff term bound corollari 2 let sequenc exampl k posit integ 1 set 1 loss fix share algorithm bound p nkte partit k k proof recal loss bound given theorem 1 set separ term appli inequ last inequ follow condit obtain bound corollari replac equat 20 upper bound k 3 14 herbster k warmuth 6 variableshar analysi variableshar algorithm assum loss expert per trial lie 0 1 henc variableshar algorithm work combin squar helling absolut loss function rel entropi loss function variableshar algorithm upper bound addit loss algorithm independ length trial sequenc abbrevi w ti w ti sinc section need refer weight expert middl trial first give two technic lemma follow convex r fi r c appli first inequ lemma 4 rh c db b 1gammac thu lemma 6 begin trial 1 may lower bound weight expert either express express b j expert differ ae w ti e gammajli x ti proof expand loss updat variableshar updat trial cf 7 express obtain drop summat term express b drop one summand second term w appli lemma 4 obtain b lemma 7 weight expert start trial start trial 0 reduc factor e gammaj theta proof lemma 6a trial weight expert reduc follow w t1i appli iter rt theta lemma 6b lower bound weight transfer expert p expert q singl trial next lemma show weight transfer sequenc trial lemma 8 distinct expert p q ltt 0 2 trial may lower bound weight expert q theta e gammaj proof expert p accumul loss trial tt 0 transfer part weight specif expert q via variableshar updat let 0 denot weight transfer expert p expert q trial denot total weight transfer expert p expert q trial tt 0 transfer weight howev still reduc function loss expert q success trial lemma 7 weight ad trial reduc factor e gammaj theta lower bound factor e gammaj e gammaj thu theta e gammaj complet proof lemma still need lower bound total transfer weight w tp ff l loss expert p trial ie assumpt 1 2 direct applic lemma 6b weight transfer expert p expert q first trial segment least w tp ff l e gammajl likewis appli lemma 7 trial ti expert p appli lemma 6b trial give us lower bound transfer weight total transfer weight ff ff l herbster k warmuth split last sum two term ff l ff upper bound expon 1 gamma ff one also replac sum first expon upper bound substitut 1 lead applic lemma 5 thu rewrit inequ ff theta ff appli lemma 5 give us ff proof loss bound variableshar algorithm proce analog proof fixedshar algorithm loss bound case follow weight sequenc expert along sequenc segment within segment bound weight reduct expert lemma 2 fixedshar analysi lemma 7 variableshar analysi pass one segment next bound weight expert correspond new segment weight expert former segment lemma 3 8 respect former lemma use fix share algorithm simpl sinc trial expert alway share fix fraction weight howev sinc weight share everi trial produc bound depend sequenc length variableshar algorithm produc bound independ length accomplish expert share weight accord loss howev expert accumul signific loss use lemma 8 bound weight follow expert term previou expert nevertheless former expert make signific loss current segment impli may bound current segment former expert collaps segment togeth word collaps two consecut segment creat singl segment associ expert first segment origin two consecut segment segment thu determin bound term relat collaps partit whose loss much wors lemma 9 partit p nkte exist collaps partit p nk 0 segment except initi segment expert associ prior segment incur least one unit loss loss whole sequenc collaps partit exce loss origin partit follow properti hold proof recal e expert associ ith segment compris trial i1 segment loss expert e associ prior segment gamma 1 less one merg segment segment combin segment new partit associ expert e igamma1 formal iter decrement k one delet e tupl e continu 24 hold bound loss collaps partit p nk 0 note loss new expert subsum segment one thu per applic transform loss increas one thu sinc applic done theorem 2 4 let sequenc exampl let l pred c jrealiz let l 01 rang partit p nkte total loss variableshar algorithm paramet ff satisfi proof lemma 1 let p nkte arbitrari partit proof need properti loss segment except initi segment regard expert associ prior segment least one cf 24 properti hold use lemma 9 replac p nkte collaps partit p nk 0 properti hold properti hold alreadi p nkte notat conveni refer p nkte p nk 0 recal loss exce loss p nkte sinc 24 hold exist trial q ith segment 1 lt 0 1 express w 1e 0 telescop product appli lemma 7 8 ii ff herbster k warmuth simplifi follow bound ff last inequ follow 25 thu substitut bound simplifi obtain bound theorem optim upper bound function ff sinc k lp nkte known learn algorithm tune ff base upper bound lp nkte approach use corollari 2 5 corollari 3 let sequenc exampl l k posit real set l loss variableshar algorithm bound follow ck p nkte partit lp nkte l addit l partit p nkte lp nkte l obtain upper bound ck proof proceed upper bound three term contain ff bound theorem 2 use rewrit appli ident ln1 x bound lp nkte l give follow upper bound previou express l l therefor upper bound use express upper bound equat 29 obtain equat 27 l upper bound equat 29 first term bound 1 k second term 2 k k ln 9 2 region thu upper bound use express upper bound equat 29 give us equat 28 done 7 absolut loss analysi absolut loss function l ab p jrealiz predict function pred vovk pred wmean howev cj 1 thu tune complex sake simplic use weight mean predict littleston warmuth 1994 section theorem 3 littleston warmuth 1994 1 absolut loss function l ab p jrealiz predict function pred wmean v x obtain slightli tighter bound could also use vee algorithm absolut loss 2 jrealiz haussler et al 1998 algorithm take log n time produc predict weight mean vee predict allow outcom lie 0 1 binari outcom absolut loss time predict function exist realiz criterion vee predict vovk 1998 cesabianchi et al 1997 unlik c 1crealiz loss function discuss earlier cf figur 2 absolut valu loss constant paramet thu must tune practic tune j may produc numer minim upper bound howev use tune j produc freund schapir theorem 4 lemma 4 p q q herbster k warmuth use tune bound variableshar algorithm theorem 2 theorem 5 let loss function absolut loss let sequenc exampl l k posit real k k lp nkte l k l set two paramet variableshar algorithm ff j k respect k k loss algorithm weight mean predict bound follow altern let l k posit real k k lp nkte l k l set two paramet variableshar algorithm ff j k respect k k loss algorithm weight mean predict bound follow 8 proximityvariableshar analysi section discuss proximityvariableshar algorithm see figur 3 recal variableshar algorithm expert share fraction weight depend loss trial fraction share uniformli among remain expert proximityvariableshar algorithm enabl expert share nonuniformli expert proximityvariableshar updat cost per expert per trial instead o1 see figur 3 algorithm allow us model situat prior knowledg like pair consecut expert let us consid paramet algorithm ntupl contain initi weight algorithm ie w paramet initi initi weight w n ti predict loss updat receiv tth outcom proximityvariableshar updat figur 3 proximityvariableshar algorithm second addit paramet besid j c complet direct graph size n without loop edg weight jk fraction weight share expert j expert k natur vertex outgo edg must nonneg sum one 0 probabl distribut prior initi expert probabl distribut prior expert follow expert j upper bound proximityvariableshar algorithm fix share algorithm could gener similarli take proxim account theorem 6 let sequenc exampl let l pred c jrealiz let l 01 rang partit p nkte total loss proximityvariableshar algorithm paramet ff satisfi proof omit proof bound sinc similar correspond proof theorem 2 variableshar algorithm chang 1 fraction replac correspond paramet note set give previou bound variableshar algorithm theorem 2 case last sum ok ln n account code length name best expert except first one use proximityvariableshar algorithm get last sum ok case 22 herbster k warmuth simpl exampl assum processor circular list two processor distanc processor iid mod 1d 2 next best expert alway constant away previou one last sum becom ok cours notion close choic paramet might suitabl note price decreas last sum updat time 2 per trial howev expert arrow end label valu share updat proximityvariableshar algorithm still 9 lower bound upper bound fixedshar algorithm grow length sequenc addit loss algorithm loss best kpartit approxim hold unbound loss function rel entropi loss restrict loss lie 0 1 variableshar algorithm give addit loss bound approxim loss best kpartit k l one natur question whether similar reduct possibl unbound loss function word whether unbound loss function bound form possibl replac minf lg give evid contrari give adversari argument forc algorithm make loss best onepartit adversari set section limit give construct easili extend adversari forc lnnln gamma log 2 n addit loss best onepartit n expert iter adversari may forc addit loss best kpartit assum log 2 n gamma 1 posit integ theorem 7 rel entropi loss exist exampl sequenc length two expert lp 21te partit singl shift loss 0 furthermor algorithm proof adversari strategi describ figur 4 use denot predict arbitrari learn algorithm l loss trial conveni number trial two expert one alway predict 0 alway predict 1 adversari return sequenc 0 outcom follow sequenc 1 outcom neither sequenc empti thu singl shift best partit partit loss 0 2 trial 2 1 otherwis assum without loss gener 2 thu 3 new trial 4 gammat 5 els go step 7 go step 3 7 let remain trial exit figur 4 adversari strategi prove ls thu prove lemma clearli loss gener assum note threshold 1 gammat furthermor l ent 0 1 l ent 1 1 thu condit 4i 5i follow condit 4ii hold simpl induct shift occur condit 5ii hold sinc condit 4ii gammat therefor add l least ln gamma condit 5i obtain condit 5ii done step 5 never execut shift occur last trial step 6 skip thu step 5 never execut trial condit 4ii bound lemma first reason lower bound tight show upper bound algorithm discuss paper close lower bound number partit 1 thu may expand set expert partitionexpert discuss introduct use staticexpert algorithm weight mean predict give upper bound total loss algorithm loss best partit zero match lower bound second bound fixedshar algorithm cf corollari 1 larger lower bound gamma2 addit term may upper bound 1 herbster k warmuth total loss algorithm trial loss variabl share algorithm loss static algorithm vovk loss fix share algorithm loss typic expert loss best partit k3 variabl share loss bound fix share loss bound figur 5 loss variableshar algorithm vs staticexpert algorithm scale weight figur 6 rel weight variableshar algorithm 10 simul result section discuss simul artifici data simul mainli meant provid visual algorithm track predict best expert seen empir evid practic use algorithm believ merit algorithm clearli reflect strong upper bound prove theorem 8000103050709scale weight trial vovk rel weight figur 7 rel weight staticexpert algorithm earlier section simul show loss algorithm typic sequenc exampl bound paper worstcas bound hold even adversariallygener sequenc exampl surprisingli loss algorithm simul random sequenc close correspond worstcas bound proven paper thu simul show loss bound tight sequenc compar perform staticexpert algorithm two share algorithm follow set chose use squar loss loss function widespread use task tune learn rate loss function simpl use vovk predict function cf equat 9 chose accord figur 2 consid sequenc 800 trial four distinct segment begin trial 1 201 401 601 trial outcom 0 predict tupl contain predict 64 expert gener predict 64 expert chose differ expert best one segment best expert alway expect loss 1120 per trial 63 expert expect loss 112 per trial end segment new best expert chosen sinc outcom alway 0 gener expect loss sampl predict uniform random distribut 0 1 typic best expert respect thu expect loss best 6 partit denot segment boundari 800 varianc oe 2 044 actual loss best partit particular simul use plot 647 fixedshar algorithm tune f base valu use ff f 26 herbster k warmuth tune suggest corollari 1 variableshar algorithm tune ff v base valu use ff v tune suggest corollari 3 use theorem 1 2 calcul worst case upper bound loss fixedshar algorithm variableshar algorithm 2489 2150 respect see theta mark figur 5 simul artifici data show worstcas bound rather tight even simpl artifici data mani heurist find suitabl tune use tune prescrib theorem notic type simul result rel insensit tune ff exampl calcul ff v variableshar algorithm overestim 10 standard deviat loss bound algorithm increas 002 actual loss algorithm simul increas 017 figur 5 plot loss staticexpert algorithm versu loss two share algorithm examin figur show first segment staticexpert algorithm perform compar share algo rithm howev remain three segment staticexpert algorithm perform poorli loss essenti bad loss typic expert slope total loss typic expert staticexpert algorithm essenti later segment share algorithm perform poorli begin new segment howev quickli learn new best expert current segment share algorithm loss plateau almost slope slope total loss best expert two share algorithm qualit behavior even though fixedshar algorithm incur approxim 10 addit loss variableshar algorithm simul tri learn rate j slightli smaller two verifi even choic learn rate total loss staticexpert algorithm improv significantli figur 6 7 plot weight normal weight vector w maintain variableshar algorithm staticexpert algorithm trial sequenc figur 6 see variableshar algorithm shift rel weight rapidli latter part segment rel weight best expert almost one correspond plot fixedshar algorithm similar hand see figur 7 staticexpert algorithm also learn best expert segment 1 howev staticexpert algorithm unabl shift rel weight suffici quickli ie take length second segment partial unlearn best expert first segment rel weight best expert segment one two essenti perform random walk third segment final segment rel weight best expert segment three also perform random walk summari see simul evid fixedshar variableshar updat necessari track shift expert 11 conclus paper essenti gave reduct multipl updat algorithm work well compar best expert arbitrari segment exampl algorithm work well compar best partit ie concaten segment two type share updat analyz fix share algorithm work well loss function unbound variableshar algorithm suitabl case rang loss lie 01 first method essenti one use wml algorithm littleston warmuth 1994 recent altern develop auer warmuth 1998 learn shift disjunct loss discret loss classif problem method simpl effect algorithm updat mistak occur ie conserv updat second method variableshar updat sophist particular one expert predict perfectli collect weight howev expert start incur larg loss share weight expert help next best expert recov weight zero method present littleston warmuth 1994 inspir number recent paper auer warmuth 1998 adapt winnow algorithm learn shift disjunct compar best shift disjunct complic compar best expert howev sinc classif problem simpl share updat similar fixedshar updat suffici focu paper track predict best expert class loss function origin staticexpert algorithm vovk develop vovk 1998 haussler et al 1998 share updat appli experiment predict disk idl time helmbold et al 1996 onlin manag invest portfolio singer 1997 addit reduct shown expert metric task system algorithm blum burch 1997 share updat use success new domain metric task system natur probabilist interpret share algorithm recent given vovk 1997 particular applic share algorithm necessari consid choos paramet ff theoret techniqu exist fixedshar algorithm elimin need choos valu ff ahead time one method tune paramet among thing specialist framework freund schapir singer warmuth 1997 even though bound produc way alway optim anoth method incorpor prior distribut possibl valu ff sake simplic discuss method herbster 1997 vovk 1997 singer 1997 paper 28 herbster k warmuth acknowledg would like thank peter auer phillip long robert schapir volodya vovk valuabl discuss also thank anonym refere help comment note 1 discret loss defin ae 2 note lent p q use dpkq notat customari inform theori 3 replac assumpt k k 2 k obtain bound final term c k replac 2c k 4 vovk recent prove sharper bound algorithm vovk 1997 ff 5 unlik corollari 2 need lower bound k 6 call partit describ segment boundari 1 201 401 601 best partit respect tradeoff k lp nkte express implicitli theorem 2 r track best disjunct use expert advic element inform theori univers predict individu sequenc ieee transact inform theori decisiontheoret gener onlin learn applic boost use combin predictor special sequenti predict individu sequenc gener loss function dynam disk spindown techniqu mobil comput track best expert ii addit versu exponenti gradient updat linear predict learn irrelev attribut abound new linearthreshold algorithm mistak bound logarithm linearthreshold learn algorithm phd thesi weight major algorithm toward realist competit portfolio select algorithm game predict expert advic derandom stochast predict strategi predict dotproduct expert framework tr ctr atsuyoshi nakamura learn specialist decis list proceed twelfth annual confer comput learn theori p215225 juli 0709 1999 santa cruz california unit state jeremi z kolter marcu maloof use addit expert ensembl cope concept drift proceed 22nd intern confer machin learn p449456 august 0711 2005 bonn germani v vovk probabl theori brier game theoret comput scienc v261 n1 p5779 06172001 peter auer manfr k warmuth track best disjunct machin learn v32 n2 p127150 aug 1998 olivi bousquet manfr k warmuth track small set expert mix past posterior journal machin learn research 3 312003 avrim blum carl burch onlin learn metric task system problem machin learn v39 n1 p3558 april 2000 chri mesterharm track linearthreshold concept winnow journal machin learn research 4 1212003 peter auer use confid bound exploitationexplor tradeoff journal machin learn research 3 312003 giovanni cavallanti nicol cesabianchi claudio gentil track best hyperplan simpl budget perceptron machin learn v69 n23 p143167 decemb 2007 marco barreno blain nelson russel sear anthoni joseph j tygar machin learn secur proceed 2006 acm symposium inform comput commun secur march 2124 2006 taipei taiwan wei yan christoph clack divers committe vote depend profit proceed 9th annual confer genet evolutionari comput juli 0711 2007 london england mark herbster manfr k warmuth track best regressor proceed eleventh annual confer comput learn theori p2431 juli 2426 1998 madison wisconsin unit state claudio gentil robust pnorm algorithm machin learn v53 n3 p265299 decemb mark herbster manfr k warmuth track best linear predictor journal machin learn research 1 p281309 912001 amol deshpand zachari ive vijayshankar raman adapt queri process foundat trend databas v1 n1 p1140 januari 2007