simpl strategi encod tree automata sigmoid recurs neural network abstractrec number author explor use recurs neural net rnn adapt process tree treelik structur one import languagetheoret formal process treestructur data determinist finitest tree automata dfsta may easili realiz rnn use discretest unit threshold linear unit recent result sima neural network world7 1997 pp 679686 show threshold linear unit oper binari input implement analog unit use continu activ function bound real input construct proof find scale factor weight reestim bia accordingli paper explor applic result simul dfsta sigmoid rnn analog rnn use monoton grow activ function also present altern scheme onehot encod input yield smaller weight valu therefor work lower satur level b introduct last decad number author explor use analog recurs neural net rnn adapt process data laid tree treelik structur direct acycl graph arena frasconi gori sperduti 5 recent establish rather gener formul adapt process structur data focus direct order acycl graph includ tree sperduti starita 18 studi classic structur direct order graph includ cyclic graph sperduti 18 studi comput power recurs neural net structur processor one import languagetheoret formal process treestructur data determinist nitest tree automata dfsta also call determinist frontiertoroot ascend tree automata19 7 may easili realiz rnn use discretest unit threshold linear unit tlu sperduti fact 17 recent shown elmanstyl 3 rnn use tlu may simul dfsta provid intuit explan similar express kremer 10 special case determinist nite automata also work sigmoid network increment gain sigmoid function lead arbitrarili precis simul step function howev unawar attempt establish nite valu gain exact simul may inde perform analog rnn recent result sma 16 show tlu oper binari input simul analog unit use continu activ function bound real input tlu neuron comput output appli threshold step activ function bias linear combin binari input correspond analog neuron work activ function gx two dierent nite limit b x 1 given input output toler construct proof nd scale factor weight basic valu gain analog activ function use scale factor shift valu bia paper dene three possibl way encod dfsta discretest rnn use tlu explor applic sma result turn discretest rnn sigmoid rnn simul origin sigmoid mean analog activ function monoton grow addit present altern scheme analog simul yield smaller weight valu sma scheme discretest case therefor work lower satur level goal nd smallest possibl scale factor guarante correct behavior last approach assum onehot encod input gener approach use 2 stabl encod famili nite state machin fsm varieti sigmoid discretetim recurr neural network similar spirit previou work omlin gile 13 14 determinist nite automata class fsm particular discretetim recurr neural network dtrnn architectur secondord dtrnn use gile et al 6 follow section tree automata recurs network intro duce section 3 describ three dierent scheme encod recurs neural network discretest rnn use tlu main result sma16 present section 4 togeth similar construct case exclus also call onehot encod input section 5 describ convers discretest rnn sigmoid counterpart dierent scheme evalu compar magnitud result weight valu final present conclus last section tree automata recurs neural network explor neural network simul tree automata need specifi notat tree describ architectur recurs neural network 21 tree nitest machin denot rank alphabet nite set symbol associ function give rank symbol 1 subset symbol rank denot set tree dene set string made symbol augment parenthesi comma repres order label tree recurs 1 0 symbol rank 0 singlenod tree 2 root node label f rank children valid tree belong 1 rank may dene gener relat r n formul equival symbol one possibl rank split determinist nitest tree automaton dfsta vetupl nite set state alphabet label rank function r f q subset accept state nite collect transit function form maximum rank valenc dfsta tree 2 result 2 q oper dfsta tree 2 dene unden otherwis 2 word state associ given tree depend label root node f also state ndfsta associ children 1 convent unden transit lead unaccept tree maximum number children node tree la usual languag la recogn dfsta subset dene one may gener denit dfsta produc output label node visit act like structurepreserv nitest tree transduc two gener possibl correspond class nitest string transduc known meali moor machines9 15 meali tree transduc obtain replac subset accept state f denit dfsta collect output function g one possibl rank moor tree transduc obtain replac f singl output function whose argument new state convers dfsta regard particular case meali moor machin oper tree whose output function return two valu 22 neural architectur dene two recurs neural architectur similar use relat work frasconi gori sperduti 5 sperduti 17 sperduti starita 18 nd conveni talk meali moor neural network dene way network comput output use analog correspond nitest tree transduc rst architectur highord meali rnn second one rstorder moor rnn 2 221 highord meali recurs neural network highord meali recurs neural network consist two set singlelay neural network rst one comput next state play role collect transit function nitest tree transduc second one comput output play role collect output function meali nitest tree transduc nextstat function realiz collect singlelay network one possibl rank nx neuron m1 input port input subtre state vector 2 remain two combin highord moor rnn may easili shown comput power meali counterpart rstorder meali machin need extra layer comput arbitrari output function see dimension nx one input node label repres vector dimension n u node label input port take input vector equal dimension number input symbol n particular node tree label l u input vector associ node compon u k equal 1 input symbol node k 0 input symbol onehot exclus encod node label next state x comput correspond 1th order singlelay neural net follow repres bia network rank leaf ie l 2 0 express compon x reduc ik set jj weight type w 0 nxk play role initi state recurr network 4 output function realiz collect network n unit input structur output function node rank evalu 222 rstorder moor recurs neural network rstorder moor recurs neural network collect nextstat function one rank form ik structur input port highord counterpart singl output function form take nx input produc n output 3 encod tree automata discretest recurs neural network present three dierent way encod nitest recurs transduc discretest rnn use tlu activ function rst two use discretest version highord meali rnn describ section 221 third one use discretest version rst order moor rnn describ section 222 rst two encod straightforward third one explain detail encod base exclus onehot encod state nitest transduc n rnn said state compon x nx dimension state vector x take high valu compon take low valu addit exclus encod input n output n use one discretest rnn encod convert section 5 sigmoid rnn encod use two strategi describ section 4 31 highord meali encod use bias assum meali nitest tree transduc discretest highord meali rnn weight weight bia v behav stabl simul nitest tree transduc note uppercas letter use design weight discretest neural net would also case bias set valu 0 1 valu 12 happen best valu convers sigmoid rnn 32 highord meali encod use bias second possibl encod treetransduc counterpart string transduc encod describ 12 2 use bia nextstat weight bias w output weight bias v case bias construct encod also work bias set valu 1 1 0 optim valu convers sigmoid rnn 33 rstorder moor encod consid moor nitest tree transduc form encod dfsta rstorder rnn need split state rst statesplit found necessari implement arbitrari nite state machin rstorder discretetim recurr neural network 8 1 2 also recent describ sperduti 17 encod rnn easili shown equival easili construct use method describ sperduti 17 follow new set state q 0 subset dene nextstat function new set dene follow shorthand notat use final new output function dene follow split encod discretest rnn eq 7 8 choos paramet follow 3 exist q 0 jm 0 zero otherwis exist q 0 l zero otherwis zero otherwis dicult show oper discretest rnn equival correspond therefor case previou construct dierent valu bias also possibl one shown happen optim convers sigmoid rnn stabl simul discretest unit analog unit 41 use sma theorem follow restat theorem sma 16 includ conveni notat slightli chang order adapt present studi 3 rememb uppercas letter use denot weight discretest rnn threshold linear unit tlu neuron comput output g h threshold activ function w j j realvalu weight binari input vector consid analog neuron weight w activ function g two dierent limit lim x1 gx magnitud max call maximum input toler final let also map dene unden otherwis map classi output analog neuron three categori low 0 high 1 forbidden unden let r map r kg sma theorem state input toler 0 max output toler 0 exist analog neuron activ function g weight w r x 2 f0 1g n accord construct proof theorem 16 set sucient condit equat hold b b jgx aj x jgx bj x jj jj small possibl sma prescript simpli scale weight tlu get analog network bia shift conveni avoid zero valu argument activ function note input analog unit allow within 0 1 wherea output allow within b construct recurs network output one analog unit normal use input anoth analog unit therefor natur choic choic compat instanc use logist function g l whose limit exactli 0 activ function hyperbol tangent tanhx particular case eq 23 24 reduc 25 becom follow reder simpl sucient condit stabl simul tlu analog unit suitabl strictli grow activ function restrict exclus encod input wherea sma construct valid binari input vector simplic prescript allow altern straightforward worstcas analysi lead weight common situat smaller obtain direct applic sma theorem 42 use simpl scheme exclus encod input condit stabl simul nitest machin fsm dtrnn studi follow approach relat sma16 carrasco et al 2 see also 12 11 condit assum special usual case onehot exclus encod input strictli grow activ function assumpt togeth worstcas analysi allow one obtain prescript choic suitabl weight stabl simul work lower satur level gener scheme eq 2325 usual prescript realiz singleparamet scale weight tlu includ bia scale equival nding nite valu gain sigmoid function ensur correct behavior note case exclus encod one use section 3 n possibl input binari vector b vector whose ith compon one rest zero therefor argument may take n dierent valu w 0 w binari input howev analog neuron input toler may receiv input vector r b ig properti exclus encod make possibl formul condit 22 hold possibl input two case distinguish 1 case 22 hold g strictli grow may also write w obvious minimum valu w jg therefor sucient condit analog neuron simul correspond tlu input b 2 similar argument lead sucient condit instanc choos w eq 29 30 fulll either order compar eq 2325 last two condit may written singl restrict pair condit simpl choic w adequ case w 0 case appear encod propos section 3 5 encod tree automata sigmoid recurs neural network mention theorem section 4 lead natur choic addit appli neuron recurs neural network due widespread use consid compar section variou possibl encod use logist function g although result dierent activ function may also obtain inde monoton growth function along real line enough follow deriv case eq 29 30 case want simul sigmoid rnn consid rst highord meali rnn architectur input x j 22 case rnn describ 4 product output x jm one rang 0 1 product alway rang 0 1 word forbidden region 1 dicult show equal hold therefor condit max suce purpos 1 want use scale factor weight possibl rank use consid rstorder moor rnn architectur case product condit max sucient previou section describ two dierent scheme simul discretest neuron take exclus input vector sigmoid neuron section describ applic two scheme three recurs neural network architectur describ section 22 51 use sima prescript sma construct section 41 give bias highord meali rnn section 31 follow therefor w condit 35 appli 4 togeth 1 condit ensur posit valu h shown 37 minimum valu allow h depend nx given architectur nx maximum valu xed chang exist least one valu allow one choos minimum valu h need stabl simul sens minim h choos appropri perform 2 lead valu shown tabl 1 minimum requir h function nx weight obtain grow slower logmn x seen inordin larg lead therefor satur analog rnn appli sma construct biasless highord meali rnn sec tion 32 get 4 use equat 4 6 nu nx term due exclus encod input nu 1n term ident zero uncertainti togeth 1 posit valu h weight obtain search minimum h satisfi condit shown tabl 2 seen weight show asymptot behavior one previou construct smaller still larg avoid satur final appli sma construct 5 rstorder moor rnn section 33 nextstat function rank accordingli weight exist q 0 jm 0 zero otherwis exist q 0 l zero otherwis 36 use togeth 1 output function accordingli weight zero otherwis sma construct appli provid consid possibl w ik u k nextstat function dierent bia w 0 u 2 f0 1g n choos safest prescript valid possibl valu bia present case bia alway valu 1 number addit term provid 1 use want singl valu h assign weight nextstat function output function use weight obtain search minimum h satisfi condit shown tabl 3 grow slower logmnx seen equal smaller one bias highord construct larger bias construct howev fact state split lead larger valu nx automata transit function taken account 52 use encod exclus input choos w includ bias obtain bias highord meali encod section 31 substitut eq 31 32 togeth 1 happen express one obtain previou section use sma construct biasless encod section 32 result shown tabl 2 result obvious ident report secondord discretetim recurr neural network use bias construct 2 instead appli altern encod biasless highord meali construct section 32 get togeth 1 suitabl minim h lead best possibl weight encod weight grow nx slower logmn result shown tabl 4 previou case result obvious ident report secondord discretetim recurr neural network use bias 2 final appli altern encod scheme rstorder moor construct section 33 particular form 33 case valid combin w 0 take valu w take valu minimum valu jw 0 exclus valu state vector equal 1 therefor togeth 1 suitabl minim lead weight grow nx slower logmnx valu shown tabl 5 valu smaller one obtain sma construct rstorder network still larg especi one consid split lead larg valu nx 6 conclus studi four strategi encod determinist nitest tree automata dfsta highord sigmoid recurs neural network rnn two strategi encod rstorder sigmoid rnn six strategi deriv three dierent strategi encod dfsta discretest rnn rnn use threshold linear unit appli two dierent weight map scheme convert one sigmoid rnn rst map scheme one describ sma16 second one altern scheme devis us strategi yield analog rnn simpl weight alphabet contain three weight proport singl paramet h best result ie smallest possibl valu h would desir derivativebas learn set obtain appl altern scheme biasless discretest highord rnn mention sma map yield larger weight case gener would also work distribut encod allow construct smaller rnn construct valu h suggest even though principl rnn nite weight abl simul exactli behavior dfsta practic dicult learn exact nitest behavior exampl small gradient present weight reach adequ larg valu smaller weight obtain cost enlarg size rnn due exclus encod state input sma result also work distribut encod acknowledg work support spanish comis interministeri de ciencia tecnologa grant tic970941 r find structur time learn initi state secondord recurr neural network regularlanguag infer gener framework adapt data structur process learn extract syntact pattern recognit introduct automata theori comput power elmanstyl recurr net work construct determinist stabl encod larg formal languag comput power neural network struc ture supervis neural network classi tree automata inform survey tr ctr barbara hammer alessio mich alessandro sperduti marc strickert recurs selforgan network model neural network v17 n89 p10611085 octobernovemb 2004 barbara hammer peter tio recurr neural network small weight implement definit memori machin neural comput v15 n8 p18971929 august henrik jacobsson rule extract recurr neural network taxonomi review neural comput v17 n6 p12231263 june 2005