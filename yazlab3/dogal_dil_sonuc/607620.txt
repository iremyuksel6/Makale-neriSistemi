latent semant kernel kernel method like support vector machin success use text categor standard choic kernel function inner product vectorspac represent two document analog classic inform retriev ir approacheslat semant index lsi success use ir purpos techniqu captur semant relat term insert similar measur two document one main drawback ir comput costin paper describ lsi approach implement kerneldefin featur spacew provid experiment result demonstr approach significantli improv perform impair b introduct kernelbas learn method km stateoftheart class learn algo rithm whose best known exampl support vector machin svm 3 approach data item map highdimension space inform mutual posit inner product use construct classif regress cluster rule modular system form gener purpos learn modul eg classif cluster dataspecif ele ment call kernel act interfac data learn machin defin map featur space kernelbas algorithm exploit inform encod innerproduct pair data item somewhat surprisingli inform suffici run mani standard machin learn algorithm perceptron converg algorithm princip compon analysi pca ridg regress nearest neighbour advantag adopt altern represent often effici method comput inner product complex case even infinit dimension vector sinc explicit represent featur vector correspond data item necessari km advantag access featur space would otherwis either expens complic repres strong model select techniqu base statist learn theori 26 develop system order avoid overfit high dimension space surpris one area system work natur text categor standard represent document highdimension vector standard retriev techniqu base precis innerproduct vector combin two method pioneer joachim 10 success explor sever other 6 11 approach document represent known bag word base map document larg vector indic word occur text vector mani dimens term corpu usual sever thousand correspond entri zero term occur document hand posit otherwis two document henc consid similar use approxim term despit high dimension space much higher train set size support vector machin shown perform well 10 paper investig one possibl avenu extend joachim work incorpor inform kernel use inform retriev ir represent known suffer drawback particular fact semant relat term taken account document talk relat topic use differ term map distant region featur space map captur semant inform would use particularli could achiev semant kernel comput similar document also consid relat differ term use kernel somehow take fact consider would enabl system extract much inform document one possibl approach one adopt 23 semant network use explicitli comput similar level term inform encod kernel defin new metric featur space equival map document anoth featur space paper propos use techniqu known inform retriev latent semant index lsi 4 approach document implicitli map semant space document share term still close term semant relat semant similar two term infer analysi cooccurr pat tern term cooccur often document consid relat statist cooccurr inform extract mean singular valu decomposit term document matrix way describ section 3 show step perform implicitli kernelinduc featur space amount kernel adapt semant kernel learn step fix dimens new featur space comput equival solv convex optim problem eigenvalu decomposit one global maximum found effici sinc eigenvalu decomposit becom expens larg dataset develop approxim techniqu base gramschmidt orthogonalis procedur practic method actual perform better lsi method provid experiment result text nontext data show techniqu deliv signific improv dataset certainli never reduc perform discuss advantag limit relationship method 2 kernel method text kernel method new approach solv machin learn problem develop algorithm make use inner product imag differ input featur space applic becom possibl rich featur space provid inner product comput way avoid need explicitli comput featur vector given input one key advantag approach modular decoupl algorithm design statist analysi problem creat appropri functionfeatur space particular applic furthermor design kernel perform modular fashion simpl rule exist combin adapt basic kernel order construct complex one way guarante kernel correspond inner product featur space main result paper also regard one kernel adapt procedur though idea use kernel defin featur space new 1 recent full potenti begun realis first problem consid classif label exampl socal support vector machin 2 3 correspond statist learn analysi describ 20 howev turn begin develop portfolio algorithm cluster 17 use princip compon analysi pca featur space regress 24 novelti detect 19 ordin learn 7 time link made statist learn approach bayesian approach known gaussian process 13 classic krieg known ridg regress 16 henc first time provid direct link distinct paradigm view develop clear defin appropri kernel function allow one use rang differ algorithm analys data concern potenti answer mani practic predict problem particular applic choos kernel correspond implicitli choos featur space sinc kernel function defin featur map oe given train set g inform avail kernel base algorithm contain entir matrix inner product known gram kernel matrix matrix repres sort bottleneck inform exploit oper matrix one fact virtual recod data suitabl manner solut sought linear function featur space weight vector w 0 denot transpos vector matrix kernel trick appli whenev weight vector express linear combin train point impli express f follow given explicit featur map oe use equat 1 comput correspond kernel often howev method sought provid directli valu kernel without explicitli comput oe show mani standard inform retriev featur space give rise particularli natur set kernel perhap best known method type refer polynomi kernel given kernel k polynomi construct creat kernel k appli polynomi posit coeffici k exampl consid fix valu integ p suppos featur space k f featur space k index ttupl featur f henc rel small addit comput cost time inner product comput one addit exponenti requir algorithm appli featur space vastli expand express power even extrem exampl consid gaussian kernel k transform kernel k follow whose featur space infinit mani dimens 3 vector space represent given document possibl associ bag term bag word simpli consid number occurr term contain typic word stem mean inflect inform contain last letter remov bag word natur represent vector follow way number dimens number differ term corpu entri vector index specif term compon vector form integ number repres frequenc term given document typic vector map space word frequenc inform merg inform eg word import uninform word given low weight way document repres column vector entri record mani time particular word stem use document typic ten thousand entri often number document furthermor particular document represent typic extrem spars rel nonzero entri basic vectorspac model bvsm document repres vertic vector index element dictionari corpu matrix whose column index document whose row index also call data matrix term document matrix defin document document matrix term term matrix consid featur space defin basic vectorspac model correspond kernel given inner product featur vector case gram matrix document document matrix gen eral consid transform document vector map oe simplest case involv linear transform type p appropri shape matrix case kernel form call represent vector space model vsm gram matrix case given 0 p 0 pd definit symmetr posit definit class model obtain vari matrix p natur one correspond differ linear map standard vector space model henc give differ scale project note jiang littman 9 use framework present collect differ method although without view kernel throughout rest paper use p refer matrix defin vsm describ number differ model case show appropri choic p realis vsm basic vector space model basic vector space model bvsm introduc 1975 salton et al 15 use kernel joachim 10 use vector represent map word vsm matrix case perform retriev system base simpl represent surprisingli good sinc represent document vector spars special techniqu deploy facilit storag comput dot product vector common map p obtain consid import term given corpu vsm matrix henc diagon whose entri p weight term sever method propos known strong influenc gener 11 often p function invers document frequenc idf total number document corpu divid number document contain given term exampl word appear document would regard inform one distanc uniform distribut good estim import better method obtain studi typic term distribut within document corpora simplest method given p measur obtain inform theoret quantiti empir model term frequenc sinc measur use label inform could also estim extern larger unlabel corpu provid background knowledg system describ previou section soon defin kernel appli polynomi gaussian construct increas express power joachim 10 dumai et al 5 appli techniqu basic vector space model classif task impress result particular use polynomi kernel seen includ featur tupl word degre chosen polynomi one problem represent treat term uncorrel assign orthogon direct featur space mean cluster document share mani term realiti word correl sometim even synonym document common term potenti close relat topic similar detect bvsm rais question incorpor inform semant featur map link document share relat term one idea would perform kind document expans ad expand version synonym close relat word exist term anoth somehow similar method would replac term concept inform could potenti glean extern knowledg correl exampl semant network howev way address problem also possibl use statist inform termterm correl 7deriv corpu extern refer corpu approach form basi latent semant index next subsect look two differ method case show implement directli kernel matrix without need work explicitli featur space allow combin kernel techniqu polynomi gaussian construct describ generalis vector space model earli attempt overcom limit bvsm propos wong et al 27 name generalis vsm gvsm document characteris relat document corpu measur bvsm method aim captur termterm correl look cooccurr inform two term becom semant relat cooccur often document effect two document seen similar even share term gvsm techniqu provid one metric easi see also constitut kernel function given term document data matrix gvsm kernel given matrix dd 0 term term matrix nonzero ij entri document corpu contain ith jth term two term cooccur document consid relat new metric take cooccurr inform account document map featur space index document corpu document repres relat document corpu reason also known dual space method 22 common case less document term method act bottleneck map forc dimension reduct gvsm vsm matrix p chosen 0 document term matrix method combin polynomi gaussian kernel construct techniqu exampl degre p polynomi kernel would featur ptupl document nonzero featur document share term document tupl knowledg combin previous consid either polynomi gaussian construct semant smooth vector space model perhap natur method incorpor semant inform directli use extern sourc like semant network section briefli describ one approach siola dalchebuc 23 use semant network wordnet 12 way obtain termsimilar inform network encod word dictionari relat word hierarch fashion eg synonym hypernym etc exampl word husband wife special case hypernym spous way distanc two term hierarch tree provid wordnet give estim semant proxim use modifi metric vector space document map bagofword approach siola dalchebuc 23 includ knowledg kernel handcraft entri squar vsm matrix p entri semant proxim term j semant proxim defin invers topolog distanc graph length shortest path connect case deserv special attent modifi metric give rise follow kernel follow distanc siola dalchebuc use distanc order appli gaussian kernel construct describ though polynomi construct could equal well appli kernel siola dalchebuc use termterm similar matrix incorpor semant inform result squar matrix p would also possibl use conceptterm relat matrix row would index concept rather term exampl one might consid husband wife exampl concept spous matrix p would case longer squar symmetr notic gvsm regard special case concept correspond document corpu term belong ith concept occur document 4 latent semant kernel latent semant index lsi 4 techniqu incorpor semant inform measur similar two document use construct kernel function conceptu lsi measur semant inform cooccurr analysi corpu techniqu use extract inform reli singular valu decomposit svd term document matrix document featur vector project subspac span first singular vector featur space henc dimens featur space reduc k control dimens vari k defin kernel featur space particular choic vsm matrix p see p comput directli origin kernel matrix without direct comput svd featur space order deriv suitabl matrix p first consid termdocu matrix svd decomposit sigma diagon matrix dimens u v orthogon ie u column u singular vector featur space order decreas singular valu henc project oper onto first k dimens given k ident matrix first k diagon element nonzero u k matrix consist first k column u new kernel express motiv particular map identifi highli correl dimens ie term cooccur often document corpu merg singl dimens new space creat new similar metric base context inform case lsi also possibl isometr reemb subspac back origin featur space defin p squar symmetr u k u 0 give rise kernel sinc view p termterm similar matrix make lsi special case semant smooth describ solia dalchebuc 23 need explicitli work entri termbyterm similar matrix help semant network howev infer semant similar directli corpu use cooccurr analysi interest kernel method map instead act termterm matric obtain implicitli work smaller documentdocu gram matrix origin term document matrix give rise kernel matrix sinc featur vector document j jth column svd decomposit relat eigenvalu decomposit k follow ith column v eigenvector k correspond eigenvalu featur space creat choos first k singular valu lsi approach correspond map featur vector vector ui k u 0 give rise follow kernel matrix k matrix diagon entri beyond kth set zero henc new kernel matrix obtain directli k appli eigenvalu decomposit k remultipli compon matric set first k eigenvalu zero henc obtain kernel correspond lsi featur space without actual ever comput featur relat comput kernel pca 18 immedi similar analysi possibl verifi also evalu new kernel novel input without refer explicit featur space order evalu learn function novel exampl must show evalu new kernel k new input train exampl kd function wish evalu form express still howev involv featur vector would like avoid evalu explicitli consid vector inner product new featur vector train exampl origin space inner product evalu use origin kernel show evalu fd follow henc evalu f new exampl first creat vector inner product origin featur space take inner product precomput row vector ff 0 v k v 0 none comput involv work directli featur space combin lsk techniqu polynomi gaussian construct open possibl perform lsi high dimension featur space exampl index tupl term experi appli approach report experiment section paper think polynomi map take conjunct term view lsk step soft disjunct sinc project link sever differ conjunct singl concept henc combin polynomi map follow lsk step produc function form reminisc disjunct normal form altern one could perform lsk step polynomi map appli polynomi map entri gram matrix obtain lsk step obtain space index tupl concept function obtain reminisc conjunct normal form appli approach ionospher data obtain improv perform conjectur result obtain depend strongli fit style function particular data main drawback approach comput complex perform eigenvalu decomposit kernel matrix although matrix smaller term document matrix usual longer spars make difficult process train set much larger thousand exampl present next section techniqu get round problem evalu approxim lsk approach 5 algorithm techniqu experi perform use eigenvalu decomposit routin provid numer recip c 14 complet eigendecomposit kernel matrix expens step possibl one tri avoid work real world data effici method develop obtain approxim lsk solut view lsk techniqu one method obtain low rank approxim kernel matrix inde project onto first k eigenvalu rank k approxim minimis norm result error matrix project onto eigensubspac one method obtain lowrank approxim also develop approxim strategi base gramschmidt decomposit similar approach unsupervis learn describ smola et al 25 project built span subset project set k train exampl select perform gramschmidt orthogonalis train vector featur space henc vector select remain train point transform becom orthogon next vector select one largest residu norm whole transform perform featur space use kernel map repres vector obtain refer method gk algorithm tabl 1 give complet pseudocod extract featur kernel defin featur space lsk method parametris number dimens select tabl 1 gsk algorithm given kernel k train set return feati j jth featur input classifi new exampl x return newfeatj jth featur exampl 51 implicit dimension reduct interest solut problem approxim latent semant solut possibl case directli interest lowrank matrix unlik inform retriev case plan use kernel conjunct optim problem type h hessian obtain pre postmultipli gram matrix diagon matrix contain f1 gamma1g label note h k eigenvalu sinc possibl easili cheapli modifi gram matrix obtain nearli solut one would obtain use much expens low rank approxim minimum error function occur point ff satisfi qhff 0 matrix h replac h minimum move new point e ff satisfi q us consid expans h eigenbasi expans ff e ff basi substitut formula equat coeffici ith eigenvalu give ff impli e ff fraction equat squash function approach zero valu approach 1 ae first case e second case e ff ff overal effect map paramet chosen care region spectrum eigenvalu decreas rapidli effect project solut onto space span eigenvector larger eigenvalu algorithm point view much effici explicitli perform lowrank approxim comput eigenvector deriv provid cheap approxim algorithm latent semant kernel also highlight interest connect algorithm 2norm soft margin algorithm nois toler also obtain ad diagon kernel matrix 21 note sever approxim view sinc exampl svm solut constrain optimis ff constrain posit case effect may differ support vector nearli orthogon eigenvector correspond larg eigenvalu fact procedur distinct standard soft margin approach born experi describ next section 6 experiment result empir test propos method text nontext data order demonstr gener applic method test effect differ condit result gener posit case improv signific worth addit comput case signific advantag use latent semant gramschmidt kernel certainli use never hurt perform 61 experi text data section describ seri systemat experi perform text data select two text collect name reuter medlin describ dataset reuters21578 conduct experi set document contain stori reuter news agenc name reuter dataset use reuter 21578 newer version corpu compil david lewi 1987 publicli avail httpwwwresearchattcomlewi obtain train set test set exist differ split corpu use modifi apt modeapt split modeapt split compris 9603 train 3299 test document reuter categori contain 1 mani 2877 document train set similarli test set categori 1 mani 1066 relev document medline1033 medline1033 second dataset use experi ment dataset compris 1033 medic document queri obtain nation librari medicin focus query23 query20 two queri contain 39 relev document select randomli 90 data train classifi 10 evalu alway 24 relev document train set 15 relev document test set perform 100 random split data experi reuter document preprocess remov punctuat word occur stop list also appli porter stemmer word weight term accord variant tfidf scheme given tf repres term frequenc df use document frequenc total number document document unit length featur space preprocess medlin document remov stop word punctuat weight word accord variant tfidf describ preced paragraph normalis document bia occur length document evalu use f1 perform measur given 2prp precis r recal first set experi conduct subset 3000 document reuters21578 data set select randomli 2000 document train remain 1000 document use test set focus top 097dimens baselin figur 1 generalis perform svm gsk lsk linear kernel earn 5 reuter categori earn acq moneyfx grain crude train binari classifi categori evalu perform new document repeat process 10 time categori use svm linear kernel baselin experi paramet c control trade error maximis margin tune conduct preliminari experi chose optim valu conduct experi ten split one categori ran svm reduc featur space also featur space full dimens valu c show best result full space select use experi medline1033 text corpu select valu c conduct experi one split data ran svm featur space full dimens optim valu c show best result select note use split experi choic seem perfect basi experiment observ reuter conclud method give optim valu c result experi reuter shown figur 1 4 note result averag 10 run algorithm start small dimension featur space increas dimension featur space interv extract featur figur demonstr perform lsk method compar baselin method generalis perform svm classifi vari vari dimension semant space increas valu k f1 number rise reach maximum fall number equival baselin method howev maximum substanti differ baselin method word sometim obtain modest gain incorpor inform kernel matrix figur 6 figur 7 illustr result experi conduct two medline1033 queri result averag 100 random run algorithm experi start small number dimens dimension increas interv extract featur result dimesns baselin figur 2 generalis perform svm gsk lsk linear kernel acq baselin figur 3 generalis perform svm gsk lsk linear kernel moneyfx dimens figur 4 generalis perform svm gsk lsk linear lkernel grain dimens baselin figur 5 generalistaion perform svm gsk lsk linear kernel crude baselin figur 6 generalis perform svm gsk lsk linear kernel query23 baselin figur 7 generalis perform svm gsk lsk linear kernel query20 tabl 2 f1 number vari dimens featur space svm classifi lsk svm classifi linear kernel baselin ten reuter categori categori k baselin 100 200 300 moneyfx 062 0673 0635 06 grain 0664 0661 067 0727 crude 0431 0558 0576 0575 trade 0568 0683 066 0657 interest 0478 0497 05 0517 ship 0422 0544 0565 0565 wheat 0514 051 0556 0624 microavg 0786 0815 0815 0819 query23 encourag show lsk potenti show substanti improv baselin method thu result reuter show case improv perform other signific improv result reuter medline1033 dataset demonstr gsk effect approxim strategi lsk case result approxim lsk howev worth note case figur 6 gsk may show substanti improv baselin method also lsk henc result demonstr gsk good approxim strategi lsk improv generalis perform lsk evid result medlin data extract inform featur use classif gsk achiev maximum high dimens situat phenomenon may caus practic limit larg data set address issu develop generalis gsk algorithm text classif furthermor conduct anoth set experi studi behaviour svm classifi semant kernel svm classifi linear kernel scenario classifi learnt use small train set select randomli 5 train data 9603 document focus top 10 categori earn 144 acq 85 moneyfx 29 grain 18 crude 16 trade 28 interest 19 ship 12 wheat 8 corn 6 note number relev document shown name categori binari classifi learnt categori evalu full test set 3299 document c tune one categori f1 number obtain result experi report tabl 2 microaverag f1 number also given set valu note gain categori loss perform other worth note svm classifi train semant kernel perform approxim baselin method even 200 dimens result demonstr propos method capabl perform reason well environ label document 62 experi nontext data figur 8 gener error polynomi kernel degre 23 4 ionospher data aver age 100 random split function dimens featur space present experi conduct nontext ionospher data set uci repositori ionospher contain 34 featur 315 point measur gain lsk compar perform svm polynomi kernel paramet c set conduct preliminari experi one split data keep dimension space full tri optim valu demonstr minimum error chosen valu use split reduc featur space note split data use tune paramet c use experi result shown figur 8 result averag 100 run begin experi set k small valu increas dimension space interv result show test error greatli reduc dimens featur space reduc curv also demonstr classif error svm classifi semant kernel reach minimum make peak valley show result equival baselin method result demonstr propos method gener appli domain text potenti improv perform svm classifi reduc dimens howev case show gain may success reduc dimens 7 generalis version gsk algorithm text classif section present generalis version gsk algorithm algorithm aros result experi report section 6 preliminari experi also contribut develop algorithm gsk algorithm present previou section extract featur rel document irrespect relev categori word featur comput respect label document gener categori distribut skew text corpora establish need bia featur comput toward relev document word introduc bia featur extract process comput featur use inform text classif main goal develop generalis version gsk algorithm extract inform featur fed classifi show high effect low number dimens achiev goal describ preced paragraph propos algorithm shown figur 9 gsk iter procedur greedili select document iter extract featur iter criterion select document maximum residu norm generalis version gsk algorithm focus relev document place weight norm relev document algorithm transform document new reduc featur space take set document input underli kernel function number bia b also fed algorithm number specifi dimens reduc featur space b give degre featur extract bias toward relev document algorithm start measur norm document concentr relev document place weight norm document next step document maximum norm chosen featur extract rel document process repeat time final document transform new dimension space dimens new space much smaller origin featur space note enough posit data avail train equal weight given relev irrelev document generalis version gsk algorithm provid practic solut problem may occur gskalgorithm algorithm may show good requir kernel k train set fd 1 number n end n 1 els end n end end return feati j jth featur input classifi new exampl end return newfeatj jth featur exampl figur 9 generalis version gsk algorithm generalis high dimens enough train data scenario generalis version gskalgorithm show similar perform lower dimens complet pseudocod algorithm given figur 9 8 experi generalis gskalgorithm employ generalis gsk algorithm transform reuter document new reduc featur space evalu propos method conduct experi full reuter data set use modeapt version perform experi 90 categori contain least one relev document train set test set order transform document new space two free paramet dimens reduc space b bia need tune analys generalist perform svm classifi respect b conduct set experi 3 reuter categori result experi shown tabl 3 set experi set dimension space 500 vari b result demonstr extract featur bias environ inform use insuffici train data basi experi select optim valu b next set experi note select optim valu c conduct preliminari experi one reuter categori set valu 1000 result set experi given tabl 4 given f1 valu 500 1000 dimension space microaverag f1 valu also shown tabl order learn svm classifi use sv light 8 experi describ section result show generalis gsk algorithm view substanti dimension reduct techniqu observ propos method show result compar baselin method dimension 500 note baselin method employ svm linear kernel note 500 dimension slow improv generalis perform svm microaverag f1 valu svm generalis gsk 0822 500 dimens wherea microaverag f1 number svm linear kernel 0854 result show perform propos techniqu compar baselin method result show generalis gsk algorithm practic approxim lsk learn algorithm provid enough posit train data need bia featur extract process howev learn algorithm enough posit train data svm may show good perform high dimension lead practic limit howev introduct bia toward relev document overcom problem henc make techniqu appli larg data set tabl 3 f1 number acq money fx wheat differ valu b 10 0922 0569 0707 11 0864 0695 0855 12 0864 0756 0846 20 0864 0748 0846 22 0864 0752 0855 24 0864 0756 0846 26 0864 0748 0846 28 0864 0752 0846 60 0864 0752 0857 10 tabl 4 f1 number topten reuter categori categori baselin 500 1000 acq 0923 0934 0948 moneyfx 0755 0754 0775 grain 0894 0902 093 crude 0872 0883 0880 trade 0733 0763 0761 interest 0627 0654 0691 ship 0743 0747 0797 wheat 0864 0851 087 corn 0857 0869 0895 microavg 9 conclus paper studi problem introduc semant inform kernel base learn method techniqu inspir approach known latent semant index borrow inform retriev lsi project data subspac determin choos first singular vector singular valu decomposit shown obtain inner product deriv project perform equival project onto first eigenvector kernel matrix henc possibl appli techniqu kernel defin featur space whatev origin dimension refer deriv kernel latent semant kernel lsk experiment demonstr efficaci approach text nontext data dataset substanti improv perform obtain use method other littl effect observ eigenvalu decomposit matrix rel expens comput also consid iter approxim method equival project onto first dimens deriv gramschmidt othogonalis data perform project effici kernel defin featur space experi show dataset socal gramschmidt kernel gsk effect lsk method despit success larg imbalanc dataset encount text classif task number dimens requir obtain good perform grow quit larg relev featur drawn small number posit document problem address bias gsk featur select procedur favour posit document henc greatli reduc number dimens requir creat effect featur space method describ paper similar flavour demonstr impress perform dataset question dataset make differ semant focus method effect fulli understood remain subject ongo research acknowledg author would like thank thorsten joachim chri watkin use discuss work support epsrc grant number grn08575 european commiss esprit work group neural comput learn neurocolt2 nr 27150 kermit 1st project kernel method imag text kermit nr 1st200025431 r theoret foundat potenti function method pattern recognit learn train algorithm optim margin classifi introduct support vector machin index latent semant analysi induct learn algorithm represent text categor automat crosslanguag retriev use latent semant index larg margin rank boundari ordin regress make largescal svm learn practic approxim dimens equal vectorbas inform retriev text categor support vector machin five paper wordnet gaussian process svm mean field leaveoneout numer recip c art scientif comput vector space model inform retriev ridg regress learn algorithm dual variabl kernel pca pattern reconstruct via approxim preimag kernel princip compon analysi sv estim distri bution support structur risk minim datadepend hierarchi margin distribut soft margin experi multilingu inform retriev use spider system support vector machin base semant kernel text categor tutori support vector regress spars kernel featur analysi statist learn theori gener vector space model inform retriev tr train algorithm optim margin classifi natur statist learn theori experi multilingu inform retriev use spider system gener vector space model inform retriev induct learn algorithm represent text categor make largescal support vector machin learn practic kernel princip compon analysi introduct support vector machin vector space model automat index text categor support vector machin repres text input space text categor suport vector machin ridg regress learn algorithm dual variabl approxim dimens equal vectorbas inform retriev support vector machin base semant kernel text categor ctr qiang sun gerald dejong explanationaug svm approach incorpor domain knowledg svm learn proceed 22nd intern confer machin learn p864871 august 0711 2005 bonn germani yaoyong li john shawetaylor use kcca japaneseenglish crosslanguag inform retriev document classif journal intellig inform system v27 n2 p117133 septemb 2006 yaoyong li john shawetaylor advanc learn algorithm crosslanguag patent retriev classif inform process manag intern journal v43 n5 p11831199 septemb 2007 yonghong tian tiejun huang wen gao latent linkag semant kernel collect classif link data journal intellig inform system v26 n3 p269301 may 2006 mehran sahami timothi heilman webbas kernel function measur similar short text snippet proceed 15th intern confer world wide web may 2326 2006 edinburgh scotland haixian wang zilan hu yue zhao effici algorithm gener discrimin analysi use incomplet choleski decomposit pattern recognit letter v28 n2 p254259 januari 2007 kevyn collinsthompson jami callan queri expans use random walk model proceed 14th acm intern confer inform knowledg manag octob 31novemb 05 2005 bremen germani serhiy kosinov stephan marchandmaillet igor kozintsev carol dulong thierri pun dual diffus model spread activ contentbas imag retriev proceed 8th acm intern workshop multimedia inform retriev octob 2627 2006 santa barbara california usa kristen grauman trevor darrel pyramid match kernel effici learn set featur journal machin learn research 8 p725760 512007 vikramjit mitra chiajiu wang satarupa banerje text classif least squar support vector machin approach appli soft comput v7 n3 p908914 june 2007 franci r bach michael jordan kernel independ compon analysi journal machin learn research 3 p148 312003