rel loss bound temporaldiffer learn foster vovk prove rel loss bound linear regress total loss onlin algorithm minu total loss best linear predictor chosen hindsight grow logarithm number trial give similar bound temporaldiffer learn learn take place sequenc trial learner tri predict discount sum futur reinforc signal qualiti predict measur squar loss bound total loss onlin algorithm minu total loss best linear predictor whole sequenc trial differ loss logarithm number trial bound hold arbitrari worstcas sequenc exampl also give bound expect differ case instanc chosen unknown distribut linear regress correspond lower bound show expect bound improv substanti b introduct consid follow model temporaldiffer learn learn proce sequenc trial trial gamma learner receiv instanc vector x 2 r n gamma learner make predict gamma learner receiv reinforc signal r 2 r pair call exampl learner tri predict outcom 2 r fix discount rate paramet fl 2 0 1 discount sum st futur reinforc signal 1 exampl r profit compani month interpret approxim compani worth time discount sum take account profit distant futur less import short term profit note outcom defin 11 welldefin reinforc signal bound episod set one also defin outcom finit discount sum discuss briefli section 7 strategi choos predict learner call onlin learn algorithm qualiti predict measur squar loss loss learner trial loss learner trial 1 want compar loss learner loss linear function linear function repres weight vector loss w trial ideal want bound addit loss learner loss best linear predictor arbitrari sequenc exampl ie want bound arbitrari arbitrari sequenc exampl first sum 12 total loss learner trial 1 argument infimum total loss linear function altern could defin 1 gamma fl st rs make convex combin reinforc signal r r t1 altern definit amount simpl rescal outcom predict rel loss bound temporaldiffer learn 3 w trial 1 thu 12 addit total loss learner total loss best linear function follow vovk 1997 also examin gener problem bound fix constant 0 akwk 2 measur complex w ie infimum 13 includ charg complex linear function larger valu obvious easier show bound 13 bound 12 13 hold arbitrari sequenc exampl call rel loss bound rel loss bound temporaldiffer learn set first shown schapir warmuth 1996 overview result given section 4 also show algorithm minim 12 use valu function approxim markov process import problem reinforc learn also call polici evalu markov process continu mani state repres real vector one want predict state valu instanc x correspond state environ action valu predict instanc correspond state environ action agent introduct reinforc learn see sutton barto 1998 paper organ follow discuss previous known rel loss bound linear regress temporaldiffer learn section 3 4 section 5 propos new second order learn algorithm temporaldiffer learn tl algorithm prove rel loss bound algorithm section 6 section 7 adapt tl algorithm episod case trial divid episod outcom discount sum futur reinforc signal discuss previou second order algorithm temporaldiffer learn section 8 give lower bound rel loss section 9 2 notat preliminari set ndimension real vector mn 2 n r mthetan set real matric row n column paper vector x 2 r n column vector x 0 denot transpos 4 j forster k warmuth x scalar product two vector wx 2 r n w euclidean norm vector x 2 r n recal basic fact posit semidefinit matric nthetan call posit definit x hold vector x 2 r n n f0g nthetan call posit semidefinit gamma sum two posit semidefinit matric posit semidefinit sum semidefinit matrix posit definit matrix posit definit everi posit definit matrix invert gamma matric b 2 r nthetan write b b gamma posit semidefinit case x 0 ax x 0 bx vector x 2 r n gamma shermanmorrison formula see press flanneri teukolski hold everi posit definit matrix 2 r nthetan everi vector exampl unit matrix 2 r nthetan posit definit everi vector x 2 r n matrix xx 0 2 r nthetan posit semidefinit find vector w 2 r n minim term appear rel loss 13 defin ks 22 convex w minim gradient zero ie 0 invert uniqu vector minim 22 rel loss bound temporaldiffer learn 5 might invert equat might uniqu solut solut smallest euclidean norm pseudoinvers definit pseudoinvers matrix see eg rektori 1994 also shown pseudoinvers matrix comput singular valu decomposit give number properti pseudoinvers appendix 0 appli shermanmorrison formula 21 show 3 known rel loss bound linear regress first note linear regress special case setup sinc standard algorithm linear regress ridg regress algorithm predict x trial rel loss bound algorithm similar bound given two theorem proven foster 1991 vovk 1997 azouri warmuth 1999 bound obtain ridg regress weaker one proven new algorithm develop vovk give simpl motiv algorithm discuss rel loss bound proven seen section 2 best linear function trial 1 ie linear function minim 22 would make predict b 0 x trial note via b predict depend exampl howev exampl instanc x known learner make predict trial set unknown outcom zero get predict b 0 predict introduc vovk 1997 use differ motiv motiv follow azouri warmuth 1999 forster 1999 give altern game theoret motiv vovk prove follow bound 13 predict algorithm theorem 31 consid linear regress sequenc exampl r n predict 6 j forster k warmuth x ti ith compon vector x vovk version theorem 31 term x 2 n replac larger bound supremum norm instanc last inequ theorem 31 follow z first inequ hold geometr mean alway smaller arithmet mean azouri warmuth 1999 forster 1999 give follow refin bound vovk linear regress algorithm case 0 consid show case 0 theorem 32 follow case 0 let go zero theorem 32 consid linear regress sequenc exampl r n theta r 0 predict ii 0 vector x rel loss bound temporaldiffer learn 7 proof show equal also hold case 0 show side equal continu 2 0 1 lemma a2 check 2 tg term continu 2 0 1 x 2 x tgamma1 follow lemma a2 otherwis x lemma a3 express 31 zero 0 show 31 converg zero 0 0 rewrit 31 appli 25 b 0 factor b 0 converg lemma a2 lemma a4 term x 0 goe zero 0 togeth show 31 inde goe zero learn algorithm theorem 31 theorem 32 second order algorithm use second deriv simpler first order algorithm call widrowhoff least mean squar algorithm widrow stearn 1985 algorithm maintain weight vector w 2 r n predict weight vector updat gradient descent w learn rate method set learn rate purpos obtain good rel loss bound given cesabianchi long warmuth 1996 kivinen warmuth 1997 method learner need know upper bound x euclidean norm instanc need know paramet w k vector w 2 r n norm kwk w loss vector w bound hold note vovk 1997 bound incompar bound theorem 31 see also next section bound 33 also hold ridg regress algorithm hassibi kivinen war muth 1995 paramet set depend w k believ proper tune bound also hold vovk 8 j forster k warmuth 4 known rel loss bound temporaldiffer learn case discount rate paramet fl assum zero schapir warmuth 1996 given number differ rel loss bound learn algorithm td first order algorithm td essenti gener widrowhoff algorithm slight modif learn algorithm td propos sutton 1988 schapir warmuth show loss td 1 specif set learn rate c loss algorithm td 0 specif set learn rate everi vector w 2 r n kwk w set learn rate depend upper bound x euclidean norm instanc w k learner need know paramet advanc loss best linear function often grow linearli eg exampl corrupt gaussian nois case rel loss bound 33 41 42 grow like second order learn algorithm propos temporaldiffer learn advantag rel loss bound prove grow logarithm also algorithm need know paramet like k w howev need know upper bound absolut valu outcom td sensit choic anoth advantag algorithm choos paramet like td algorithm 5 new second order algorithm temporaldiffer learn section propos new algorithm temporaldiffer learn set call algorithm tempor least squar algorithm shorter tl algorithm rel loss bound temporaldiffer learn 9 assum absolut valu outcom bound constant ie assum bound discount rate paramet fl paramet known learner know learner clip real number 2 r use function 51 motiv tl algorithm new secondord algorithm temporaldiffer learn given tabl call algorithm tempor least squar tl algorithm motiv tl algorithm motiv azouri warmuth 1999 gave vovk predict linear regress section 3 use equal ks hold best linear function trial 1 minim 22 would make predict ks trial set unknown outcom zero get predict e ks tl algorithm predict clip function assur predict lie bound rang follow show rel loss 13 tl j forster k warmuth tabl tempor least squar tl algorithm trial learner know ffl paramet fl ffl instanc ffl reinforc signal tl predict ks rk cy given 52 clip predict interv gammay invert invers gamma1 must replac pseudoinvers use result get worst averag case rel loss bound easier interpret 52 implement tl algorithm case 0 straightforward implement tl algorithm would need 3 arithmet oper trial comput invers matrix tabl ii give implement need 2 arithmet oper per trial achiev comput invers iter use shermanmorrison formula 25 implement make correct predict end forloop follow shermanmorrison formula 25 equal rel loss bound temporaldiffer learn 11 tabl ii implement tl 0 inv 1 2 r nthetan z receiv instanc vector x 2 r n inv inv gamma inv x predict receiv reinforc signal r 2 r z 6 rel loss bound tl algorithm temporaldiffer learn set know outcom need predict trial run vovk linear regress algorithm use outcom pre diction tl algorithm approxim vovk predict set futur reinforc signal zero also clip predict rang gammay show loss tl algorithm much wors loss vovk algorithm good rel loss bound known start show two lemma first technic lemma use prove second lemma bound absolut valu differ vovk predict b 0 unclip predict e tl algorithm lemma 61 0 vector x proof follow 0 pseudoinvers invers shermanmorrison formula 25 show j forster k warmuth thu gamma1 x 0 prove lemma 0 case 0 use lemma a2 let go 0 2 lemma 62 proof note thu lemma 61 yt z z show main result theorem 61 consid temporaldiffer learn 0 let sequenc exampl r n theta r outcom lie real interv gammay predict tl algorithm rel loss bound temporaldiffer learn 13 proof let know ie rel loss bound theorem 32 also hold clip predict c p thu suffic show hold 4y lemma 62 next two subsect appli theorem 61 show rel loss bound worst case averag case 61 worst case rel loss bound corollari 61 consid temporaldiffer learn 0 1 0 let sequenc exampl r n theta r outcom lie real interv gammay predict tl algorithm 14 j forster k warmuth proof first inequ follow theorem 61 theorem 32 ii second follow theorem 31 2 62 averag case rel loss bound assum outcom lie gammay instanc iid unknown distribut r n show upper bound expect rel loss 12 trial 1 depend n fl particular need term akwk 2 measur complex vector w rel loss 13 need assum instanc bound show result use theorem 61 bound sum term x 0 follow theorem tra trace squar matrix dimx dimension vector space x theorem 62 vector x linear span proof first look case choos orthonorm basi e em x also written mean interpret matrix nthetan linear function r n r n ident function x assert case 0 follow 1ijm 1ijm 1ijm rel loss bound temporaldiffer learn 15 1jm 1jm 1jm 1jm prove theorem case 0 choos arbitrari orthonorm basi e appli result 0 vector x 1gamman first note s1gamman x x 0 case s1gamman x 0 sinc vector fx 1gamman rank n equal 0 follow s1gamman x 0 first inequ case 0 follow atra gamma1 corollari 62 consid temporaldiffer learn set assum instanc x iid unknown distribut r n outcom given 11 lie real interv gammay predict tl algorithm expect 12 proof theorem 61 expect rel loss iid theorem 62 prove first inequ corollari 62 second follow j forster k warmuth tabl iii tempor least squar tl algorithm episod learn trial tl predict rk cy given 52 clip predict interv gammay startk first trial episod trial k belong invert invers gamma1 must replac pseudoinvers 7 episod learn studi set outcom st discount sum futur reinforc signal use algorithm polici evalu reinforc learn correspond look continu task see sutton barto 1998 episod task trial partit episod finit length outcom depend reinforc signal belong episod let trial first trial episod trial denot startt last endt notat discount rate paramet fl 2 0 1 outcom episod set defin st replac definit given 11 continu set definit rel loss 12 13 remain un chang note continu set essenti episod set one episod infinit length motiv section 5 get tl algorithm episod learn present tabl iii implement algorithm given tabl iv check correct implement verifi rel loss bound temporaldiffer learn 17 tabl iv implement tl episod learn inv 1 2 r nthetan new episod start trial set z 0 2 r n receiv instanc vector x 2 r n inv inv gamma inv x predict receiv reinforc signal r 2 r z hold everi iter forloop follow 25 equal note practition 0 small matrix might invert algorithm use pseudoinvers practic suggest use 0 tune paramet alway invert calcul pseudoinvers avoid also conjectur clip need practic data show rel loss bound episod learn assum bound outcom known learner advanc proof follow theorem similar proof theorem 61 theorem 71 consid temporaldiffer learn episod length let 0 let sequenc exampl r n theta r outcom given 71 lie real interv gammay predict tl algorithm tabl iii j forster k warmuth 0 72 bound instanc x iid unknown distribut r n expect 72 lower bound correspond 73 shown theorem 92 note theorem 71 exploit fact differ episod vari length relat theorem depend actual length episod easili develop 8 second order algorithm second order algorithm temporaldiffer learn propos bradtk barto 1996 boyan 1999 compar algorithm episod set algorithm maintain weight vector w predict trial bradtk barto leastsquar td shorter lstd algorithm use weight vector r x boyan lstd algorithm consid case paramet 2 0 1 like td algorithm use weight vector z contrast tl algorithm use weight vector rel loss bound temporaldiffer learn 19 predict tl algorithm trial formula invers must replac pseudoinvers matrix invert note tl algorithm paramet like td lstd case algorithm lstd ident import differ tl algorithm td lstd use differ definit covari matrix bradtk barto boyan experiment compar algorithm td compar strong assumpt bradtk barto also show w algorithm converg asymptot tl algorithm propos paper design minim rel loss 13 rel loss bound show tl well know whether similar rel loss bound hold lstd lstd algorithm experiment comparison would use 9 lower bound section give lower bound linear regress episod temporaldiffer learn first consid case linear regress ie case outcom equal reinforc signal outcom lie gammay corollari 62 give upper bound expect rel loss 12 exampl iid respect arbitrari distribut next theorem show bound 91 improv substanti proof similar proof theorem 2 vovk 1997 howev dimens n instanc greater one exampl proof gener stochast strategi iid theorem 91 consid linear regress constant c distribut set exampl r n theta gammay everi learn algorithm expect rel loss 12 exampl iid distribut j forster k warmuth proof fix paramet ff 1 gener distribut exampl follow stochast strategi vector 2 0 chosen prior distribut betaff ff n ie compon iid distribut betaff ff distribut exampl n exampl n e unit vector r n trial exampl gener iid calcul bay optim learn algorithm expect loss trial 1 minim expect rel loss algorithm give lower bound theorem 91 detail proof given appendix 2 consid set discuss section 7 trial partit episod outcom given st follow lower bound proven reduct previou lower bound linear regress theorem 92 consid episod temporaldiffer learn episod fix length let gammay rang outcom everi 0 constant c stochast strategi gener instanc x 1 x reinforc signal r 1 outcom lie gammay divis everi learn algorithm expect stochast choic exampl rel loss 12 proof modifi stochast strategi use proof theorem 91 strategi gener instanc x outcom trial gener whole episod trial instanc reinforc signal outcom episod fl consid qth trial episod fore 1 q trial learner essenti process scale exampl lower bound theorem 91 appli factor fl 2gammaq choic q lead factor 1 lower bound 2 rel loss bound temporaldiffer learn 21 10 conclus open problem propos new algorithm temporaldiffer learn tl algorithm contrari previou second order algorithm new algorithm use differ definit covari matrix see discuss section 8 main question whether differ realli help prove worst averag case rel loss bound tl algorithm would interest know tight bound practic data bound class linear function serv comparison class use second order algorithm addit loss loss best compar logarithm number trial conjectur even linear regress first order algorithm adapt learn rate addit loss logarithm number trial algorithm analyz appli case instanc expand featur vector dot product two featur vector given kernel function see saunder gam merman vovk 1998 also fourier wavelet transform use extract inform instanc see walker 1996 grap 1995 linear transform one reduc dimension comparison class lead smaller rel loss bound far compar total loss onlin algorithm total loss best linear predictor whole sequenc exampl suppos compar produc partit data sequenc k segment pick best linear predictor segment aim bound total loss onlin algorithm minu total loss best compar form bound obtain herbster warmuth 1998 case linear regress use firstord algorithm would like know whether simpl secondord algorithm linear regress requir 2 updat time per trial addit loss grow sum log section length paper focus continu learn outcom infinit discount sum futur reinforc signal section 7 discuss tl algorithm adapt set outcom depend reinforc signal episod st applic might make sens let outcom convex combin futur reinforc signal 22 j forster k warmuth episod defin st case outcom would averag futur reinforc signal know rel loss bound case defin 101 technic level would like know realli necessari clip predict temporaldiffer algorithm propos proof reduct previou proof linear regress direct proof might avoid clip anoth open technic question discuss end section 3 conjectur paramet vovk linear regress algorithm tune obtain bound form 33 proven first order widrowhoff algorithm similarli believ paramet new second order learn algorithm paper tune obtain bound 41 proven first order td algorithm schapir warmuth final note lower bound continu set fl 0 possibl show lower bound expect rel loss 12 see theorem 92 correspond lower bound episod case acknowledg jurgen forster support daad doktorandenstipendium im rahmen de gemeinsamen hochschulsonderprogramm iii von bund und landern manfr warmuth support nsf grant ccr9821087 thank nigel duffi valuabl comment r rel loss bound onlin densiti estim exponenti famili distribut linear analysi introductori cours rel loss bound gener linear regress predict worst case introduct wavelet unpublish manuscript depart comput scienc track best regressor addit versu exponenti gradient updat linear predict numer recip pascal survey applic mathemat ridg regress learn algorithm dual variabl worstcas analysi temporaldiffer learn algorithm learn predict method tempor differ reinforc learn introduct competit onlin linear regress fast fourier transform adapt signal process tr