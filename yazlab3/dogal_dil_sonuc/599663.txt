choos multipl paramet support vector machin problem automat tune multipl paramet pattern recognit support vector machin svm consid done minim estim gener error svm use gradient descent algorithm set paramet usual method choos paramet base exhaust search becom intract soon number paramet exce two experiment result assess feasibl approach larg number paramet 100 demonstr improv gener perform b introduct problem supervis learn one take set inputoutput pair attempt construct classier function f map input vector x 2 x onto label 2 interest pattern recognit classic case set label simpli 1g goal nd f 2 f minim error fx 6 futur exampl learn algorithm usual depend paramet control size class f way search conduct f sever techniqu exist perform select paramet idea nd paramet minim gener error algorithm hand error estim either via test data use learn hold test crossvalid techniqu via bound given theoret analysi tune multipl paramet usual multipl paramet tune time moreov estim error explicit function paramet applic strategi exhaust search paramet space becom intract sinc would correspond run algorithm everi possibl valu paramet vector discret propos methodolog automat tune multipl paramet support vector machin svm take advantag specic algorithm svm algorithm support vector machin svm realiz follow idea map ndimension input vector x 2 r n 1 high dimension possibl innit dimension featur space h construct optim separ hyperplan space dierent map construct dierent svm train data separ optim hyperplan one maxim distanc h space hyperplan closest imag vector x train data nonsepar train data gener concept use suppos maxim distanc equal imag train vector x within sphere radiu r follow theorem hold true 19 theorem 1 given train set size featur space h hyperplan w b margin mw b z radiu rz dene kwk maximum margin algorithm take input train set size return hyperplan featur space margin mw b z maxim note suppos train separ 1 rest articl refer vector matric use bold notat mean 0 assumpt probabl measur p underli data z expect misclass probabl bound expect taken random draw train set z size left hand side size right hand side theorem justi idea construct hyperplan separ data larg margin larger margin better perform construct hyperplan note howev accord theorem averag perform depend ratio efr 2 2 g simpli larg margin multipl paramet svm algorithm usual depend sever paramet one denot c control tradeo margin maxim error minim paramet appear nonlinear map featur space call kernel param ter simplic use classic trick allow consid c kernel paramet paramet treat uni framework wide acknowledg key factor svm perform choic kernel howev practic dierent type kernel use due diculti appropri tune paramet present techniqu allow deal larg number paramet thu allow use complex kernel anoth potenti advantag abl tune larg number paramet possibl rescal attribut inde priori knowledg avail mean attribut choic use spheric kernel ie give weight attribut one may expect better choic shape kernel sinc mani realworld databas contain attribut dierent natur may thu exist appropri scale factor give right weight right featur exampl see use radial basi function kernel rbf mani dierent scale factor input dimens usual approach consid tri pick best valu howev use propos method choos automat good valu scale factor inde factor precis paramet kernel moreov demonstr problem featur select address framework sinc correspond nding attribut rescal zero factor without harm gener thu see tune kernel paramet someth extrem use procedur allow would versatil tool variou task nding right shape kernel featur select nding right tradeo error margin etc give rational develop techniqu approach thu see goal nd hyperplan maxim margin also valu map paramet yield best gener error propos minimax ap maxim margin hyperplan coecient minim estim gener error set kernel paramet last step perform use standard gradient descent approach kind error estim consid sever way assess gener error valid error procedur requir reduc amount data use learn order save valid moreov estim smooth proper gradient descent leaveoneout error estim procedur give estim expect gener analyt function paramet examin accuraci estim uenc whole procedur nding optim paramet particular show realli matter variat estim relat variat test error rather valu relat outlin paper organ follow next section introduc basic svm dierent possibl estim gener error describ section 3 section 4 explain smooth these estim introduc section 5 framework minim estim gradient descent section 6 deal comput gradient error estim respect kernel paramet final section 7 8 present experiment result method appli varieti databas dierent context section 7 deal nding right penal along right radiu kernel nding right shape kernel section 8 present result appli method featur select learn introduc standard notat svm complet descript see 18 let fx set train exampl x 2 r n belong class label 1g svm methodolog map vector featur space use kernel function kx dene inner product featur space consid kernel k depend set paramet decis function given coecient 0 obtain maxim follow function i2 constraint coecient 0 dene maxim margin hyperplan highdimension featur space data map nonlinear function formul svm optim problem call hard margin formul sinc train error allow everi train point satis inequ correspond equal satis point call support vector notic one may requir separ hyperplan pass origin choos xed variant call hard margin svm without threshold case optim problem remain except constraint disappear deal nonsepar nonsepar case one need allow train error result call soft margin svm algorithm 4 shown soft margin svm quadrat penal error consid special case hard margin version modi kernel 4 16 ident matrix c constant penal train error focu remaind hard margin svm use 3 whenev deal nonsepar data thu c consid anoth paramet kernel function 3 estim perform svm ideal would like choos valu kernel paramet minim true risk svm classier unfortun sinc quantiti access one build estim bound section present sever measur expect error rate svm 31 singl valid estim one enough data avail possibl estim true error valid set estim unbias varianc get smaller size valid set increas valid set fx 0 estim step function 32 leaveoneout bound leaveoneout procedur consist remov train data one element construct decis rule basi remain train data test remov element fashion one test element train data use dierent decis rule let us denot number error leaveoneout procedur lx known 10 leaveoneout procedur give almost unbias estim expect gener error err probabl test error machin train sampl size 1 expect taken random choic sampl although lemma make leaveoneout estim good choic estim gener error nevertheless costli actual comput sinc requir run train algorithm time strategi thu upper bound approxim estim easi comput quantiti possibl analyt express denot f 0 classier obtain train exampl present f one obtain exampl remov also written thu u p upper bound p f 0 get follow upper bound leaveoneout error sinc hard margin svm monoton increas 321 support vector count sinc remov nonsupport vector train set chang solut comput machin ie u nonsupport vector restrict preced sum support vector upper bound term sum 1 give follow bound number error made leaveoneout procedur 17 n sv denot number support vector 322 jaakkolahaussl bound svm without threshold analyz optim perform svm algorithm comput leaveoneout error jaakkola haussler 8 prove inequ lead follow upper bound note wahba et al 20 propos estim number error made leaveoneout procedur hard margin svm case turn seen upper bound jaakkolahaussl one sinc 323 opperwinth bound hard margin svm without threshold opper winther 12 use method inspir linear respons theori prove follow assumpt set support vector chang remov exampl p k sv matrix dot product support vector lead follow estim 324 radiusmargin bound svm without threshold train error vapnik 18 propos follow upper bound number error leaveoneout procedur r radiu margin dene theorem 1 325 span bound vapnik chapel 19 3 deriv estim use concept span support vector assumpt set support vector remain leaveoneout procedur follow equal true distanc point set p give exact number error made leaveoneout procedur previou assumpt span estim relat approxim link jaakkolahaussl bound consid svm without threshold constraint remov denit span easili upper bound valu span 2 thu recov jaakkolahaussl bound link r 2 2 support vector number error made leaveoneout procedur bound x shown 19 span p bound diamet smallest sphere enclos train point sinc number error made leaveoneout procedur bound link opperwinth support vector chang hard margin case without threshold give valu opperwinth bound name 4 smooth test error estim estim perform svm valid error 4 leaveoneout error 5 requir use step function howev would like use gradient descent approach minim estim test error unfortun step function dierenti alreadi mention section 325 possibl bound x 1 x x 0 bound r 2 2 deriv leaveoneout error nevertheless larg error count one therefor might advantag instead use contract function form howev choic constant b dicult small estim accur larg result estim smooth instead tri pick good constant b one tri get directli smooth approxim test error estim posterior probabl recent platt propos follow estim posterior distribut p svm output fx 13 40 30 20 10 00 10 2348 0306 figur 1 valid error dierent valu width rbf kernel top left step function otherwis note bottom pictur minimum right place fx output svm constant b found minim kullbackleibl diverg p empir approxim p built valid set optim carri use second order gradient descent algorithm 13 accord estim best threshold svm classier f pa b x 05 note b 6 0 obtain correct compar usual svm threshold denit gener error classier z z error empir estim min note label valid set use directli last step indirectli estim constant b appear parametr form pa b better understand estim let us consid extrem case error valid set maximum likelihood algorithm go yield pa b x take binari valu consequ estim error probabl zero 5 optim kernel paramet let go back svm algorithm assum kernel k depend one sever paramet encod vector thu consid class decis function parametr b want choos valu paramet w see equat 2 maxim maximum margin algorithm model select criterion minim best kernel paramet precis xed want choos 0 one dimension paramet one typic tri nite number valu pick one give lowest valu criterion note abbrevi pa b x svm solut continu respect better approach propos cristianini et al 5 use increment optim algorithm one train svm littl eort chang small amount howev soon one compon comput everi possibl valu becom intract one rather look way optim along trajectori kernel paramet space use gradient model select criterion optim model paramet propos 2 demonstr case linear regress timeseri predict also propos 9 optim regular paramet neural network propos algorithm altern svm optim gradient step direct gradient paramet space achiev follow iter procedur 1 initi valu 2 use standard svm algorithm find maximum quadrat form w 3 updat paramet minim typic achiev gradient step see 4 go step 2 stop minimum reach solv step 3 requir estim vari thu restrict case k dierenti respect moreov consid case gradient respect comput approxim note 0 depend implicitli sinc 0 dene maximum w n kernel paramet deriv respect p 0 xed comput gradient r way perform step 3 make gradient step small eventu decreas converg improv use second order deriv newton method laplacian oper dene formul addit constraint impos project gradient 6 comput gradient section describ comput gradient respect kernel paramet dierent estim gener error first bound r 2 2 see theorem 1 obtain formul deriv margin section 61 radiu section 62 valid error see equat 4 show calcul deriv hyperplan paramet 0 b see section 63 final comput deriv span bound 7 present section 64 rst begin use lemma suppos given n1 vector v nn matrix smoothli depend paramet consid function let x vector x maximum l attain x word possibl dierenti l respect x depend note also true one constraint denit f remov proof rst need express equal constraint lagrang multipli inequ constraint lagrang multipli maximum follow condit veri v p x last term written follow v p b x use deriv optim condit name fact either henc v p result follow 61 comput deriv margin note featur space separ hyperplan follow expans normal min follow denit margin theorem 1 latter 1kwk thu write bound r 2 2 r 2 kwk 2 previou lemma enabl us comput deriv kwk 2 deed shown 18 lemma appli standard svm optim problem 2 give 62 comput deriv radiu comput radiu smallest sphere enclos train point achiev solv follow quadrat problem 18 constraint use previou lemma comput deriv radiu 0 maxim previou quadrat form 63 comput deriv hyperplan paramet let us rst comput deriv 0 respect paramet kernel purpos need analyt formul 0 first suppos point support vector remov train set assumpt done without loss gener sinc remov point support vector aect solut fact point lie margin z k n support vector h n matrix paramet svm written abl comput deriv paramet respect kernel paramet p inde sinc deriv invers matrix depend paramet p written 3 follow nalli easili use result calcul recov comput p inde denot h turn 3 inequ easili prove dierenti mm 64 comput deriv spanrul let us consid span valu recal span support vector x p dene distanc point set dene 6 valu span written note introduc lagrang multipli enforc constraint introduc extend vector extend matrix dot product support vector valu span written h submatrix k sv row column p remov v pth column k sv fact optim valu h 1 v follow last equal come follow block matrix ident known woodburi formula 11 2 close form obtain particularli attract sinc comput valu span support vector invert matrix k sv combin equat 12 11 get deriv span pp thu complex comput deriv spanrul respect paramet p kernel requir comput invers matrix k sv complex oper larger quadrat optim problem howev problem approach valu given span rule continu chang smoothli valu paramet coecient p chang continu span 2 actual discontinu support vector set support vector chang easili understood equat 6 suppos chang valu paramet point xm support vector anymor support vector set p go smaller discontinu like appear valu situat explain gure 2 plot valu span support vector x p versu width rbf kernel almost everywher span decreas henc neg deriv jump appear correspond chang set support vector moreov span global increas valu deriv give us good indic global evolut span one way solv problem tri smooth behavior span done impos follow addit constraint denit p equat constant given constraint point xm leav enter set support vector larg uenc span support vector sinc 0 small eect constraint make set p becom continu set support vector chang howev new constraint prevent us comput span ecient equat 12 possibl solut replac constraint 74e376e378e380e3 figur 2 valu p sum span train point dierent valu width rbf kernel vari small vicin regular term comput span new denit span equat 12 becom diagon matrix element shown gure 3 span much smoother minimum still right place experi took note comput deriv new express dicult previou span express interest look leaveoneout error svm without threshold case valu span regular write alreadi point section 325 valu span 20 21 22 23 24 25 26 27 28 29 3077e385e393e3 figur 3 left minima span regular dash line without regular solid line close right detail behavior span dierent valu regular recov opperwinth bound hand case span bound ident jaakkolahaussl one way span bound regular bound opperwinth jaakkolahaussl 7 experi experi dierent natur carri assess perform feasibl method rst set experi consist nding automat optim valu two paramet width rbf kernel constant c equat 3 second set experi correspond optim larg number scale factor case handwritten digit recognit show optim scale factor lead natur featur select demonstr applic method select relev featur sever databas 71 optim detail core techniqu present gradient descent algorithm use optim toolbox matlab perform includ second order updat improv converg speed crossvalid r 2 2 spanbound breast cancer 2604 474 2684 471 2559 418 diabeti 2353 173 2325 17 2319 167 heart 1595 326 1592 318 1613 311 thyroid 480 219 462 203 456 197 tabl 1 test error found dierent algorithm select svm paramet c rst column report result 14 second last column paramet found minim r 2 2 spanbound use gradient descent algorithm 72 benchmark databas rst set experi tri select automat width rbf kernel along constant c penal train error appear equat 3 order avoid ad posit constraint optim problem constant c width rbf kernel use parameter turn give stabl optim use benchmark databas describ 14 databas long 100 split train test set avail httpidafirstgmdderaetschdatabenchmarkshtm follow experiment setup 14 rst 5 train set kernel paramet estim use either 5fold crossvalid minim r 2 2 spanbound final kernel paramet comput median 5 estim result shown tabl 1 turn minim r 2 2 span estim yield approxim perform pickingup paramet minim crossvalid error surpris sinc crossvalid known accur method choos hyperparamet learn algorithm interest compar comput cost meth crossvalid r 2 2 spanbound breast cancer 500 142 7 diabeti 500 122 98 heart 500 9 62 thyroid 500 3 116 titan 500 68 34 tabl 2 averag number svm train one train set need select paramet c use standard crossvalid minim spanbound od tabl show mani svm train averag need select kernel paramet split result crossvalid one report 14 tri 10 dierent valu c perform 5fold crossvalid number svm train 5 train set need method gain complex impress averag 100 time less svm train requir nd kernel paramet main reason gain two paramet optim comput reason exhaust search crossvalid handl select 2 paramet wherea method highlight next section discuss explain section 32 r 2 2 seem rough upper bound spanbound accur estim test error 3 howev process choos kernel paramet matter bound whose minimum close optim kernel paramet even r 2 2 use estim test error previou experi show minim yield quit good result gener error obtain minim spanbound cf gure slightli better sinc minim latter dicult implement control local minima recommend practic minim r 2 2 experi follow section relat experi bound similar result obtain spanbound 73 automat select scale factor experi tri choos scale factor rbf polynomi kernel degre 2 precis consid kernel follow experi carri usp handwritten digit recognit databas databas consist 7291 train exampl 2007 test exampl digit imag size 16x16 pixel tri classifi digit 0 4 5 9 train set split 23 subset 317 exampl subset use experi assess feasibl gradient descent approach nding kernel paramet rst use 16 paramet one correspond scale factor squar tile 16 pixel shown gure 4 figur 4 16 tile scale factor 16 pixel ident scale paramet initi 1 evolut test error bound r 2 2 plot versu number iter gradient descent procedur gure 5 polynomi kernel 6 rbf note polynomi kernel test error went 9 wherea best test error one scale paramet 99 thu figur 5 evolut test error left bound r 2 2 right gradient descent optim polynomi kernel figur evolut test error left bound r 2 2 right gradient descent optim rbf kernel take sever scale paramet manag make test error decreas might interest see scale coecient found purpos took 256 scale paramet one per pixel minim polynomi kernel map scale coecient shown gure 7 result quit consist one could expect situat coecient near border pictur smaller middl pictur coecient directli interpret measur relev correspond featur figur 7 scale factor found optim procedur darker mean smaller scale factor discuss experi consid saniti check experi ment inde prove feasibl choos multipl kernel paramet svm lead overt howev gain test error main motiv sinc expect signi cant improv problem featur play similar role take scale factor equal databas seem reason choic howev highlight gure 7 method power tool perform featur select 8 featur select motiv featur select threefold 1 improv gener error 2 determin relev featur explanatori purpos 3 reduc dimension input space realtim applic find optim scale paramet lead featur select algo rithm inde one input compon useless classica tion problem scale factor like becom small scale becom small enough mean possibl remov without aect classic algorithm lead follow idea featur select keep featur whose scale factor largest also perform princip compon space scale princip compon scale factor consid two dierent parametr kernel rst one correspond rescal data input space 2 r n second one correspond rescal princip compon space matrix princip compon comput use follow iter procedur 1 initi 2 case princip compon scale perform princip compon analysi comput matrix 3 solv svm optim problem 4 minim estim error respect gradient step 5 discard dimens correspond small element return step 2 demonstr idea two toy problem show featur select reduc gener error appli featur select algorithm dna microarray data import nd gene relev perform classic also seem type algorithm featur select improv perform lastli appli algorithm face detect show greatli reduc input dimens without sacric perform 81 toy data compar sever algorithm standard svm algorithm featur select featur select algorithm estim r 2 2 span estim standard svm appli featur select via lter method three lter method use choos largest featur accord pearson correl coecient fisher criterion score 4 kolmogorovsmirnov test 5 note pearson coecient fisher criterion model nonlinear depend two follow artici dataset object assess abil algorithm select small number target featur presenc irrelev redund featur 21 rst exampl six dimens 202 relev probabl equal rst three featur drawn second three featur fx 4 drawn probabl 07 otherwis rst three drawn second three x 1 remain featur nois x second exampl two dimens 52 relev probabl equal data drawn follow drawn n abil drawn two normal distribut equal probabl rest featur nois x linear problem rst six featur redund rest featur irrelev nonlinear problem rst two featur irrelev use linear kernel linear problem second order polynomi kernel nonlinear problem impos featur select algorithm keep best two featur result shown gure 8 variou train set size take averag test error 500 sampl run train set size fisher score shown graph due space constraint perform almost ident correl coecient problem clearli see method outperform classic method featur select nonlinear problem among r r mean valu rth featur posit neg class ri standard deviat 5 ks tst fr denot rth featur train exampl p correspond empir distribut lter method kolmogorovsmirnov test improv perform standard svm rwbound gradient standard svm correl coeffici kolmogorovsmirnov test rwbound gradient standard svm correl coeffici kolmogorovsmirnov test b figur 8 comparison featur select method linear problem b nonlinear problem mani irrelev featur xaxi number train point yaxi test error fraction test point 82 dna microarray data next test idea two leukemia discrimin problem 6 problem predict treatment outcom medulloblastoma 1 rst problem classifi myeloid versu lymphoblast leukemia base express 7129 gene train set consist 38 exampl test set 34 exampl standard linear svm achiev 1 error test set use gradient descent r 2 achiev error 0 use error 1 use 1 gene use fisher score select featur result 1 error 1 gene second leukemia classic problem discrimin b versu cell lymphoblast cell 6 standard linear svm make 1 error problem use either span bound gradient descent r 2 result 0 error made use 5 gene use fisher score result 2 error made use 5 gene nal problem one predict treatment outcom patient medulloblastoma 60 exampl 7129 express valu dataset use leaveoneout measur error rate standard svm gaussian kernel make 24 error select gene use gradient descent r 2 achiev error 15 83 face detect trainabl system detect frontal nearfront view face gray imag present 7 gave good result term detect rate system use gray valu 1919 imag input seconddegre polynomi kernel svm choic kernel lead 40000 featur featur space search imag face dierent scale took sever minut pc make system realtim reduc dimension input space featur space requir featur select princip compon space use reduc dimension input space 15 method evalu larg cmu test set 1 consist 479 face 57000000 nonfac pattern figur 9 compar roc curv obtain dierent number select compon result show use 60 compon improv perform system 15 figur 9 roc curv dierent number pca gray featur 9 conclus propos approach automat tune kernel paramet svm base possibl comput gradient variou bound gener error respect param ter dierent techniqu propos smooth bound preserv accuraci predict locat minimum test error use smooth gradient abl perform gradient descent search kernel paramet space lead improv perform reduct complex solut featur select use method chose separ case appropri scale factor non separ case method allow us choos simultan scale factor paramet c see equat 3 benet techniqu mani first allow actual optim larg number paramet previou approach could deal 2 paramet even case small number paramet improv run time larg amount moreov experiment result demonstr accur estim error requir simpl estim like r 2 2 good behaviour term allow nd right paramet way render techniqu even applic sinc estim simpl comput deriv final approach avoid hold data valid thu make full use train set optim paramet contrari crossvalid method approach fact proven success variou situat open new direct research theori practic support vector machin practic side approach make possibl use highli complex tunabl kernel tune scale factor adapt shape kernel problem select relev featur theoret side demonstr even larg number paramet simultan tune overt eect remain low cours lot work remain done order properli understand reason anoth interest phenomenon fact quantit accuraci estim use gradient descent margin relev rais question design good estim paramet tune rather accur estim futur investig focu tri understand phenomena obtain bound gener error overal algorithm along look new problem approach could appli well new applic acknowledg author would like thank jason weston elodi nedelec help comment discuss r medulloblastoma diagnosi outcom predict gene express pro model select support vector chine support vector network dynam adapt kernel support vector machin face detect still gray imag probabilist kernel regress model adapt regular neural network model estim charact obtain statist procedur recognit gaussian process svm mean probabl support vector machin featur select face detect robust bound gener margin distribut natur statist learn theori statist learn theori bound error expect support vector machin gener approxim crossvalid support vector machin anoth way look margin like quantiti featur select support vector machin tr ctr alex holub max well pietro perona hybrid generativediscrimin visual categor intern journal comput vision v77 n13 p239258 may 2008 dityan yeung hong chang guang dai learn kernel matrix maxim kfdbase class separ criterion pattern recognit v40 n7 p20212028 juli 2007 tobia glasmach christian igel gradientbas adapt gener gaussian kernel neural comput v17 n10 p20992105 octob 2005 kristin p bennett michinari momma mark j embrecht mark boost algorithm heterogen kernel model proceed eighth acm sigkdd intern confer knowledg discoveri data mine juli 2326 2002 edmonton alberta canada yoram baram learn kernel polar neural comput v17 n6 p12641275 june 2005 carl gold alex holub peter sollich bayesian approach featur select paramet tune support vector machin classifi neural network v18 n56 p693701 june 2005 koji tsuda shinsuk uda taishin kin kiyoshi asai minim cross valid error mix kernel matric heterogen biolog data neural process letter v19 n1 p6372 februari 2004 carlo soar pavel b brazdil select paramet svm use metalearn kernel matrixbas metafeatur proceed 2006 acm symposium appli comput april 2327 2006 dijon franc tristrom cook two variat fisher linear discrimin pattern recognit ieee transact pattern analysi machin intellig v24 n2 p268273 februari 2002 carlo soar pavel b brazdil petr kuba metalearn method select kernel width support vector regress machin learn v54 n3 p195209 march 2004 sayan mukherje qiang wu estim gradient coordin covari classif journal machin learn research 7 p24812514 1212006 alain rakotomamonji variabl select use svm base criteria journal machin learn research 3 312003 malt kuss carl edward rasmussen assess approxim infer binari gaussian process classif journal machin learn research 6 p16791704 1212005 keith sullivan sean luke evolv kernel support vector machin classif proceed 9th annual confer genet evolutionari comput juli 0711 2007 london england mingrui wu bernhard schlkopf gkhan bakir build spars larg margin classifi proceed 22nd intern confer machin learn p9961003 august 0711 2005 bonn germani andrea argyri raphael hauser charl micchelli massimiliano pontil dcprogram algorithm kernel select proceed 23rd intern confer machin learn p4148 june 2529 2006 pittsburgh pennsylvania huseyin inc theodor b trafali hybrid model exchang rate predict decis support system v42 n2 p10541062 novemb 2006 sayan mukherje dingxuan zhou learn coordin covari via gradient journal machin learn research 7 p519549 1212006 mingrui wu bernhard schlkopf gkhan bakr direct method build spars kernel learn algorithm journal machin learn research 7 p603624 1212006 liefeng bo ling wang licheng jiao featur scale kernel fisher discrimin analysi use leaveoneout cross valid neural comput v18 n4 p961978 april 2006 alain rakotomamonji franci bach stphane canu yve grandvalet effici multipl kernel learn proceed 24th intern confer machin learn p775782 june 2024 2007 corvali oregon xuewen chen jong cheol jeong minimum refer set base featur select small sampl classif proceed 24th intern confer machin learn p153160 june 2024 2007 corvali oregon train algorithm fuzzi support vector machin noisi data pattern recognit letter v25 n14 p16471656 15 octob 2004 franci r bach gert r g lanckriet michael jordan multipl kernel learn conic dualiti smo algorithm proceed twentyfirst intern confer machin learn p6 juli 0408 2004 banff alberta canada r kumar kulkarni v k jayaraman b kulkarni symbol assist svm classifi noisi data pattern recognit letter v25 n4 p495504 march 2004 sren sonnenburg gunnar rtsch christin schfer bernhard schlkopf larg scale multipl kernel learn journal machin learn research 7 p15311565 1212006 kaiquan shen chongjin ong xiaop li einar p wildersmith featur select via sensit analysi svm probabilist output machin learn v70 n1 p120 januari 2008 zhihua zhang jame kwok dityan yeung modelbas transduct learn kernel matrix machin learn v63 n1 p69101 april 2006 kaimin chung weichun kao chialiang sun lilun wang chihjen lin radiu margin bound support vector machin rbf kernel neural comput v15 n11 p26432681 novemb mingwei chang chihjen lin leaveoneout bound support vector regress model select neural comput v17 n5 p11881222 may 2005 keem siah yap izham z abidin abdul rahim ahmad zahrul faizi hussien hooi loong pok fariq izwan ismail abdul malik mohamad abnorm fraud electr meter detect use hybrid support vector machin genet algorithm proceed third confer iast intern confer advanc comput scienc technolog p388392 april 0204 2007 phuket thailand olivi chapel train support vector machin primal neural comput v19 n5 p11551178 may 2007 guang dai dityan yeung kernel select forl semisupervis kernel machin proceed 24th intern confer machin learn p185192 june 2024 2007 corvali oregon yime ying dingxuan zhou learnabl gaussian flexibl varianc journal machin learn research 8 p249276 512007 christian igel tobia glasmach britta mersch nico pfeifer peter meinick gradientbas optim kerneltarget align sequenc kernel appli bacteri gene start detect ieeeacm transact comput biolog bioinformat tcbb v4 n2 p216226 april 2007 lior wolf amnon shashua featur select unsupervis supervis infer emerg sparsiti weightbas approach journal machin learn research 6 p18551887 1212005 baback moghaddam minghsuan yang learn gender support face ieee transact pattern analysi machin intellig v24 n5 p707711 may 2002 rakotomamonji analysi svm regress bound variabl rank neurocomput v70 n79 p14891501 march 2007 yi wu edward chang distancefunct design fusion sequenc data proceed thirteenth acm intern confer inform knowledg manag novemb 0813 2004 washington dc usa abdul majid asifullah khan anwar mirza combin support vector machin use genet program intern journal hybrid intellig system v3 n2 p109125 januari 2006 qiang wu yime ying dingxuan zhou multikernel regular classifi journal complex v23 n1 p108134 februari 2007 shin ando hitoshi iba classif gene express profil use combinatori method evolutionari comput machin learn genet program evolv machin v5 n2 p145156 june 2004 giorgio valentini thoma g dietterich biasvari analysi support vector machin develop svmbase ensembl method journal machin learn research 5 p725775 1212004 fabien lauer ching suen grard bloch trainabl featur extractor handwritten digit recognit pattern recognit v40 n6 p18161824 june 2007 r brunelli verif finger match comparison support vector machin gaussian basi function classifi pattern recognit letter v27 n16 p19051915 decemb 2006 r brunelli verif finger match comparison support vector machin gaussian basi function classifi pattern recognit letter v27 n16 p19051915 decemb 2006 gavin c cawley nicola l c talbot construct bayesian formul spars kernel learn method neural network v18 n56 p674683 june 2005 gavin c cawley nicola l c talbot fast exact leaveoneout crossvalid spars leastsquar support vector machin neural network v17 n10 p14671475 decemb 2004 piyush kumar joseph b mitchel e alper yildirim approxim minimum enclos ball high dimens use coreset journal experiment algorithm jea 8 saher esmeir shaul markovitch anytim learn decis tree journal machin learn research 8 p891933 512007 sbastien gadat laurent youn stochast algorithm featur select pattern recognit journal machin learn research 8 p509547 512007 k pelckman j suyken b moor addit regular tradeoff fusion train valid level kernel method machin learn v62 n3 p217252 march 2006 gavin c cawley nicola l c talbot prevent overfit model select via bayesian regularis hyperparamet journal machin learn research 8 p841861 512007 mathia adankon moham cheriet optim resourc model select support vector machin pattern recognit v40 n3 p953963 march 2007 mingkun li ishwar k sethi confidencebas classifi design pattern recognit v39 n7 p12301240 juli 2006 ataollah abrahamzadeh sey alireza seyedin mehdi dehghan digitalsignaltyp identif use effici identifi eurasip journal appli signal process v2007 n1 p6363 1 januari 2007 ron meir gunnar rtsch introduct boost leverag advanc lectur machin learn springerverlag new york inc new york ny zexuan zhu yewsoon ong manoranjan dash markov blanketembed genet algorithm gene select pattern recognit v40 n11 p32363248 novemb 2007