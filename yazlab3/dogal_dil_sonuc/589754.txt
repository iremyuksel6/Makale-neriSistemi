compileroptim simul largescal applic high perform architectur paper propos evalu practic automat techniqu exploit compil analysi facilit simul larg messagepass system use compil techniqu compilersynthes static task graph model identifi subset comput whose valu signific effect perform program gener symbol estim execut time comput program regular comput commun pattern inform allow us avoid execut simul larg portion comput code simul also allow us avoid perform messag data transfer still simul messag perform detail use techniqu integr mpisim parallel simul ucla rice dhpf compil infrastructur evalu accuraci benefit techniqu three standard messagepass benchmark wide rang problem system size optim simul error less 16 compar direct program measur case studi typic much smaller error furthermor requir factor 5 2000 less memori factor 10 less time execut origin simul dramat save allow us simul regular messagepass program system problem size 10 100 time larger possibl origin simul current stateoftheart simul b introduct predict parallel applic perform essenti step develop larg applic highli scalabl parallel architectur size system configur necessari larg problem size analyz altern architectur system consider research done analyt simul model perform predict complex scalabl system analyt method typic requir custom solut problem may tractabl complex interconnect network detail model scenario simul model like primari choic generalpurpos perform predict well known howev detail simul larg system computationintens long execut time signific deterr widespread use current gener parallel program simul use two techniqu reduc model execut time direct execut parallel simul direct execut simul use avail system resourc directli execut portion program parallel simul distribut comput workload among multipl processor use appropri synchron algorithm ensur execut model produc result event model execut causal order howev current state art even use direct execut parallel simul simul larg applic design architectur thousand processor run mani order magnitud slower physic counterpart paper propos implement evalu practic automat optim exploit compil support enabl effici simul larg messagepass parallel program goal enabl simul target system thousand processor realist problem size expect larg platform key idea underli work use compil analysi isol fragment local comput messag data whose valu affect perform program exampl comput determin loop bound control flow messag pattern volum effect perform wherea comput mani array valu signific effect perform comput abstract away simul rest program detail predict perform characterist applic similarli also possibl avoid perform data transfer mani messag whose valu affect perform simul perform messag detail two major aspect compil analysi requir accomplish optim identifi valu within program could affect program perform isol comput commun determin valu perform first step use compilersynthes static task graph model 4 5 abstract represent identifi sequenti comput task parallel structur program task schedul preced explicit commun controlflow determin parallel structur symbol express task graph control flow condit commun pattern volum scale express sequenti task execut time directli captur valu ie refer within express impact program perform second step use compil techniqu call program slice 21 identifi portion comput determin valu compil emit simplifi mpi code contain exactli comput must actual execut simul addit commun remain code fragment abstract away compil also need estim execut time abstract code use parameter direct measur addit reduc simul time optim dramat reduc memori requir simul major program array referenc redund comput alloc simul memori save potenti allow much larger problem size architectur studi would otherwis feasibl order demonstr impact optim combin mpisim parallel simul 6 2527 dhpf compil infrastructur 2 develop program simul framework incorpor new techniqu describ origin mpisim simul use direct execut parallel simul achiev substanti reduct simul time parallel program dhpf normal usag compil hpf program mpi varieti share memori system provid extens parallel program analysi capabl integr tool allow us evalu impact preced optim exist mpi hpf program without requir chang sourc code previou work modifi dhpf compil automat synthes static task graph model symbol task time estim mpi program compil hpf sourc program 1 work use static task graph plu program slice perform simul optim describ also extend mpisim exploit inform compil avoid execut signific portion comput code hypothesi significantli reduc memori time requir simul therefor enabl us simul much larger system problem size previous possibl use number wide use benchmark evalu util integr framework sweep3d 1 benchmark sp npb benchmark suit 8 tomcatv spec92 benchmark simul model applic valid measur rang problem size number processor error predict execut time compar direct measur 1 futur plan synthes inform exist mpi code well dhpf infrastructur support gener comput partit commun analysi symbol analysi capabl make feasibl wide class mpi program 16 case studi often substanti less valid done distribut memori ibm sp architectur well share memori sgi origin 2000 note mpisim simul mpi commun commun via share memori optim signific impact perform simul total memori usag simul use compil synthes model factor 5 2000 less origin simul simul time typic lower factor 510 dramat save allow us simul system problem size 10100 time larger possibl origin simul without signific reduct accuraci simul exampl success simul execut configur sweep3d target system 10000 processor mani case simul time faster origin program remaind paper proce follow section 2 first describ state art parallel program simul set stage work section 3 provid brief overview mpisim static task graph model section 4 describ optim strategi compil simul extens requir implement strategi section 5 describ experiment result section 6 present main conclus relat work analyt perform predict intract complex applic program simul commonli use studi well known simul larg system tend slow improv simul directexecut use 20 26 28 direct execut simul make use avail system resourc directli execut portion applic code simul architectur featur specif interest unavail exampl simul use studi variou architectur compon memori subsystem interconnect network specif one interest determin faster commun fabric network workstat valu given set applic one run applic current avail machin simul project network behavior benefit directexecut simul obviou first one estim valu new hardwar without expens purchas second one simul fastther need simul workstat behavior exampl level memori refer sinc part hardwar readili avail mani earli simul design sequenti execut 9 13 14 howev even use abstract model direct execut sequenti program simul tend slow slowdown factor rang 2 35 process simul program 9 sever recent effort explor use parallel execut 10 16 17 23 24 27 28 reduc model execut time vari degre success order multipl simul process maintain accuraci simul use protocol synchron process one wide use protocol quantum protocol let process comput given quantum synchron gener synchron simul use quantum protocol must tradeoff simul accuraci speedfrequ synchron slowdown simul synchron less frequent introduc error possibl execut statement outoford laps 16 17 parallel proteu use form program analysi increas simul window beyond fix quantum mpisim use parallel discret event simul conserv protocol 24 27 support protocol includ null messag protocol nmp 11 condit event protocol cep 12 new protocol combin two 22 discuss next section mpisim exploit determin present commun pattern applic reduc mani case complet elimin synchron overhead although simul protocol optim reduc simul time result improv still inadequ simul larg problem interest highend user instanc sweep3d kernel applic asci benchmark suit releas us depart energi largest configur requir comput grid one billion element memori requir execut time configur make impract simul even run simul high perform comput hundr processor overcom comput intract research use abstract simul avoid execut comput code entir 18 19 howev lead major limit make approach inapplic mani real world applic main problem abstract away code model essenti independ program control flow even though control flow may affect commun pattern well sequenti task time also preced solut requir signific user modif sourc program form special input languag order express requir inform abstract sequenti task commun pattern make difficult appli tool exist program written wide use standard messag pass interfac mpi high perform fortran hpf 3 background goal 31 mpisim parallel simul mpi program use direct execut start point work mpisim directexecut parallel simul perform predict mpi program mpisim simul mpi applic run parallel system refer target program system respect machin simul execut host machin may either sequenti parallel machin gener number processor host machin less number processor target architectur simul simul must support multithread simul kernel processor schedul thread ensur event host processor execut correct timestamp order target thread simul follow local code simul directli execut host processor commun command trap simul use appropri model predict execut time correspond commun activ target architectur support commonli use mpi commun routin pointtopoint collect commun simul collect commun function implement term pointto point commun function pointtopoint commun function implement use set core nonblock mpi function gener host architectur fewer processor target machin sequenti simul host machin one processor requir simul provid capabl multithread execut sinc mpi program execut collect singl thread process necessari provid capabl multithread execut mpi program mpisim memori execut time constraint sequenti simul led develop parallel implement mpisim mpisim port multipl parallel architectur includ distribut memori ibm sp2 well sharedmemori sgi origin 2000 simul kernel provid support sequenti parallel execut simul parallel execut support via set conserv parallel simul protocol 26 typic work follow 2 futur plan synthes inform exist mpi code well dhpf infrastructur support gener comput partit commun analysi symbol analysi capabl make feasibl wide class mpi program applic process simul model logic process lp 3 lp execut independ without synchron lp execut wait oper mpirecv mpibarri etc synchron protocol use decid lp proceed briefli describ default protocol use mpisim lp model comput local quantiti call earliest output time eot earliest input time eit 7 eot repres earliest futur time lp send messag lp model similarli eit repres lower bound receiv timestamp futur messag lp may receiv upon execut wait statement lp safe select match messag input buffer receiv timestamp less eit differ asynchron protocol differ method comput eit implement support varieti protocol mention previous primari overhead implement parallel conserv protocol due commun comput eit block suffer lp abl advanc eit suggest implement number optim significantli reduc frequenc strength synchron parallel simul thu reduc unnecessari block execut 26 27 optim gear toward exploit determin applic instanc consid lp block receiv statement input buffer contain singl messag gener lp proceed remov messag buffer might possibl anoth messag destin lp transit messag lower timestamp howev receiv statement known process determinist follow must exist uniqu messag match receiv statement soon lp receiv messag proceed without need synchron lp model best case everi receiv statement model known determinist synchron messag gener model parallel simul extrem effici preced optim two limit first work commun statement priori known determinist second use direct execut simul impli memori comput requir simul least larg target applic restrict target system applic problem size studi even use parallel host machin compilerdirect optim discuss next section primarili aim allevi restrict 32 static task graph represent seen next section compil analysi perform greatli facilit exploit appropri abstract represent parallel behavior program part poem project 3 15 develop abstract program represent call static task graph stg captur extens static inform parallel program 5 stg design comput automat parallel compil compact symbol represent parallel structur program independ specif program input valu number processor node stg repres set possibl parallel task typic one per process identifi symbol set integ process identifi illustr stg exampl mpi program shown figur 1 comput node loop nest repres set task one per process denot symbol set process id 0 p node also includ marker describ correspond region sourc code origin program node must repres contigu region code edg graph repres set edg connect pair parallel task describ symbol integ map exampl commun edg figur label map indic process p 1 send process node fall one three categori controlflow comput commun comput node includ symbol scale function captur number loop iter task 3 gener lp use simul multipl applic process scale function arbitrari program variabl commun node includ addit symbol inform describ pattern volum commun overal stg serv gener languag architectureindepend represent messagepass program previou work extend dhpf compil synthes static dynam task graph mpi program gener dhpf compil hpf sourc program 4 futur extract task graph directli exist mpi code compil support extrem valuabl enabl techniqu develop paper appli fulli automat ie without user intervent effici simul parallel program compilersupport techniqu effici largescal simul section begin motiv overal strategi use address key restrict simul scalabl identifi name time cost requir simul detail comput target program describ specif strategi accomplish 41 optim strategi challeng parallel program simul use perform evalu execut simul actual comput target program two purpos determin execut time comput b determin impact comput result perform program due artifact like commun pattern loop bound controlflow mani parallel program howev sophist compil extract extens inform target program static particular identifi two type relev inform often avail compiletim 1 parallel structur program includ sequenti portion comput task map task thread commun synchron pattern thread 2 symbol estim execut time isol sequenti portion comput inform provid simul directli may possibl avoid execut substanti portion comput code simul therefor reduc execut time memori requir simul illustr goal consid simpl exampl mpi code fragment figur 1 code perform shift commun oper array everi processor send boundari valu left neighbor code execut simpl comput loop nest simpl exampl commun pattern number iter loop nest depend valu block size per processor b array size n number processor p local processor identifi myid therefor comput valu must execut simul simul howev commun pattern loop iter count depend valu store array comput use comput loop nest earlier refer latter valu redund comput point view perform estim estim perform comput loop nest analyt could avoid simul code loop nest still simul commun behavior detail could achiev optim use compil gener simplifi code shown right figur code replac loop nest call special simulatorprovid delay function extend mpisim provid function simpli forward simul clock doubl precis anmax 1 doubl precis call mpicommsizempicommworld p ierr call mpicommrankmpicommworld myid ierr read n myid gt send d2n1 myidb1 processor myid1 endif myid lt p recv d2n1 myid1b1 processor myid1 endif endif integ allocat dummybuf call mpicommsizempicommworld p ierr call mpicommrankmpicommworld myid ierr call readandbroadcastw1 read n alloc dummybufn22 myid gt send dummybuf processor myid1 endif myid lt p recv dummybuf processor myid1 endif call delayn2 minnmyidbb figur 1 exampl illustr simpl mpi program b taskgraph mpi program c simplifi mpi program effici simul origin mpi code c simplifi mpi code b task graph origin mpi code task pair p q comput controlflow edg c commun edg comput task simul thread specifi amount compil estim cost loop nest form simpl scale function shown argument delay call function describ comput cost vari retain variabl b n p myid plu paramet w 1 repres cost singl loop iter current obtain valu w 1 direct measur one select problem size number processor use scale function comput requir delay valu problem size number processor note exampl compil avoid alloc array significantli reduc memori requir simul program addit optim compil prove data transfer messag also redund simul also avoid perform actual data transfer although simul messag oper detail also avoid alloc memori messag buffer messag optim lead save simul time memori usag paper develop automat compilerbas techniqu perform optim describ evalu potenti benefit techniqu particular goal use compilergener static task graph plu addit compil analysi avoid simul execut substanti portion comput code target program send unnecessari data use task graph identifi comput task candid elimin comput scale express delay function importantli identifi valu comput program impact perform use addit compil analysi distinguish comput comput valu ie redund defin specif four major challeng must address achiev goal first three address previou system known us must transform origin parallel program simplifi legal mpi program simul mpisim simplifi program must includ comput commun code need execut simul must yield perform estim origin program total execut time individu process total commun comput time well detail metric commun behavior b must abl abstract away much local comput within task feasibl elimin mani data structur origin program possibl isol redund comput program c must identifi messag whose content directli affect comput receiv exploit inform reduc simul time memori usag must estim execut time abstract comput task given program size number processor accur perform predict sequenti code challeng problem wide studi literatur use fairli straightforward approach describ section 45 refin approach part ongo work poem project follow subsect describ techniqu use address challeng implement dhpf mpisim first describ basic process use task graph gener simplifi mpi program describ compil analysi need identifi redund comput final discuss approach use estim perform elimin code 42 translat static task graph simplifi mpi program stg directli identifi local sequenti comput task control flow commun task pattern parallel program use compilergener stg basi analysi avoid perform complex ad hoc analysi identifi compon given inform first step identifi contigu region comput task andor controlflow stg collaps singl condens collaps task loop nest figur 1 note simpli transform stg simplifi analysi directli impli chang parallel program refer task graph result transform condens task graph later analysi consid singl comput task singl collaps task time decid simplifi code refer either singl sequenti task criteria collaps task depend goal perform studi first gener rule collaps region must includ branch exit region ie singl exit end region second current work collaps region must contain commun task aim simul commun precis final decid whether collaps condit branch involv difficult tradeoff import elimin controlflow refer larg array order achiev save memori time desir difficult estim perform code contain controlflow found howev typic branch involv larg array signific impact program perform exampl one minor condit branch loop nest sweep3d depend intermedi valu larg 3d array impact branch execut time rel neglig detect fact gener difficult within compil may depend expect problem size comput time therefor two possibl approach take precis approach allow user specifi direct specif branch elimin treat analyt program simul simpler approxim approach elimin condit branch insid collaps loop nest reli statist averag execut time iter provid good basi estim total execut time loop nest either approach use profil estim branch probabl elimin branch current taken second approach first one difficult implement could provid precis perform estim condens task graph also comput scale express collaps task describ number comput oper scale function program variabl introduc time variabl repres execut time sequenc statement singl loop iter denot w task approach use estim overal execut time sequenti task describ section 45 base condens task graph assum compil analysi section 43 need gener simplifi mpi program follow retain controlflow loop branch origin mpi code retain condens task graph ie controlflow collaps second retain commun code origin program particular call underli messagepass librari program array otherwis unus referenc commun call replac array refer refer singl dummi buffer use commun note without messag optim describ later section simul must still perform actual data transfer process simul messag messag optim attempt elimin data transfer use buffer size maximum messag size commun call program alloc buffer static dynam potenti multipl time depend requir messag size known third replac code sequenc sequenti task task graph call mpisim delay function pass argument describ estim execut time task insert sequenc call runtim function one per w paramet start program read valu paramet file broadcast processor final elimin data variabl referenc simplifi program 43 program slice identifi redund comput data major challeng perform transform mention earlier correctli effect identifi redund comput ie one safe elimin solut propos use program slice retain part comput code associ data structur affect program execut time given variabl referenc statement program slice find isol subset program comput data affect valu variabl 21 subset conserv limit precis static program analysi therefor may minim key requir appli program slice identifi variabl valu affect execut time program compilergener static task graph captur inform directli precis allow us avoid complic ad hoc analysi entir sourc code particular valu affect perform exactli variabl refer appear retain controlflow condens graph scale function sequenti task commun event sourc destin express commun descriptor commun call valu identifi program slice use isol comput data affect variabl valu program slice essenti reachabl analysi depend graph program includ data control depend particular given particular target refer use reachabl analysi identifi statement program affect valu refer chain depend ie feasibl path depend graph wellknown compil techniqu omit detail stateoftheart algorithm program slice describ 21 use basi implement appli techniqu howev requir target refer part program appear program depend graph comput compil express static task graph directli deriv correspond express program therefor use start point program slice express introduc dummi procedur call statement appropri point target program pass express argument rebuild program depend graph express use start point slice dummi procedur call later elimin obtain memori time save desir requir full interprocedur program slice complet elimin use mani larg array possibl gener interprocedur slice challeng feasibl compil techniqu current avail dhpf infrastructur take limit interprocedur side effect account order correctli handl call runtim librari routin includ commun call runtim routin dhpf compil runtim librari particular assum routin modifi argument pass refer modifi global ie common block variabl mpi program necessari suffici support singleprocedur benchmark expect incorpor full interprocedur slice futur support continu work poem final output slice analysi set comput must retain simplifi mpi code remain comput program except io statement commun call consid redund code gener simplifi mpi program describ previou section modifi slightli use inform sequenti task nonredund comput retain gener program rest task replac singl call simul delay function precis perform predict simul delay call includ time retain comput sinc simul time account explicitli execut time estim comput howev appli entir task practic found amount nonredund code small task therefor adjust execut time estim account retain code 44 messag optim simul redund messag note previou section data transfer messag may also redund point view perform case identifi avoid perform data transfer simul potenti lead addit time memori save although conceptu similar redund comput discuss messag optim separ mechan achiev optim somewhat differ explain first compil identifi redund messag direct result program slice analysi describ particular techniqu describ account interprocedur sideeffect slice directli identifi messag receiv call receiv redund valu correspond messag send call alreadi known compil compil provid inform simul flag mpi call redund buffer use messag alloc result simplifi mpi program actual messag optim follow call flag mpisim simul call detail send necessari protocol messag predict endtoend latenc messag send data receiv simul thread actual data avail simul applic howev call flag compil redund mpisim still simul call detail respect mpi commun protocol send empti messag receiv simul thread sinc redund receiv also flag receiv copi data buffer messag need present simul applic provid inform synchron program although optim reduc number messag sent size messag reduc memori use messag need alloc result lower latenc incur messag sent processor well smaller commun overhead due copi data enclos messag intofrom commun buffer also result lower memori usag simul 45 estim task execut time main approxim approach estim sequenti task execut time without direct execut analyt predict sequenti execut time extrem challeng problem particularli modern superscalar processor cach hierarchi varieti possibl approach differ tradeoff cost complex accuraci simplest approach one use paper measur task time specif w one select problem size number processor use symbol scale function deriv compil estim delay valu problem size number processor current scale function symbol function number loop iter incorpor depend cach work set problem size believ extens scale function approach captur nonlinear behavior caus memori hierarchi possibl perform estim measur task time simplifi mpi code mpi code timer parallel program dhpf parallel system figur 2 compil paramet measur simul parallel program two altern direct measur task time paramet use compil support estim sequenti task execut time analyt b use separ offlin simul sequenti task execut time 15 case need scale function remain includ issu mention import amort cost estim paramet mani predict experi scale function task depend intermedi comput result addit program input even case may appear compil exampl na benchmark sp grid size processor comput store array use loop bound use array make forward propag symbol express infeas therefor complet obscur relationship loop bound program input variabl simpli retain execut scale express includ refer array simplifi code evalu execut time abl autom fulli model process given hpf applic compil mpi modifi dhpf compil automat gener two version mpi program one simplifi mpi code delay call describ previous second full mpi code timer call insert perform measur w paramet output timer version directli provid input delay version code complet process illustr figur 2 perform detail experiment evalu compilerbas simul approach studi three issu experi 1 accuraci optim simul use compilergener inform compar origin simul direct measur target program 2 reduct memori usag achiev optim simul compar origin result improv overal scalabl simul term system size problem size simul 3 perform optim simul compar origin term absolut simul time term rel speedup compar sequenti model execut simul larg number target processor result categori present type optim consid paper elimin local comput elimin data content larg messag begin descript experiment methodolog describ result issu turn 51 experiment methodolog use three realworld benchmark tomcatv sweep3d na sp one synthet commun kernel sampl studi tomcatv spec92 floatingpoint benchmark studi hpf version benchmark compil mpi dhpf compil sweep3d depart energi asci benchmark 1 sp na parallel benchmark npb23b2 benchmark suit 8 mpi benchmark written fortran 77 final design synthet kernel benchmark sampl evalu impact compilerdirect optim program vari comput granular messag commun pattern commonli use parallel applic tomcatv dhpf compil automat gener three version output mpi code normal mpi code gener dhpf benchmark key array hpf code distribut across processor contigu block second dimens ie use hpf distribut block b simplifi mpi code call mpisim delay function make full use techniqu describ section 4 c normal mpi code timer call insert measur task time paramet describ section 45 sinc dhpf pars emit fortran mpisim support c use f2c translat version gener code c run mpisim two benchmark sweep3d na sp manual modifi exist mpi code gener simplifi mpi mpi code timer case sinc task graph synthesi mpi code implement yet code serv show compil techniqu develop appli larg rang code good result applic measur task time valu w 16 processor measur valu use experi problem size differ number processor except na sp measur task singl problem size 16 processor use task time problem size well recal scale function use current account cach work set cach perform chang either problem size number processor affect work set size per process therefor cach perform applic nevertheless measur approach provid accur predict optim simul shown next subsect benchmark except sampl evalu distribut memori ibm sp 128 processor sampl experi conduct share memori sgi origin 2000 8 processor 52 valid origin mpisim success valid number benchmark architectur 6 26 27 new techniqu describ section 4 howev introduc addit approxim model process key new approxim estim sequenti execut time portion comput code task abstract away aim section evalu accuraci mpisim appli techniqu applic optim simul henceforth denot mpisimtg valid direct measur applic execut time also compar predict origin simul studi multipl configur problem size number processor applic case mpisimtg valid measur system 4 begin tomcatv handl fulli automat step compil task measur simul shown figur 2 size tomcatv use valid 20482048 figur 3 show result 4 64 processor even though mpisim analyt model mpisim tg accur mpisim direct execut mpisimd error perform predict mpisimtg 16 averag error 113 measur system 4 messag optim introduc modifi underli commun model thu affect valid valid mpisim tomcatv2060100140 number processor runtim sec measur figur 3 valid mpisim 20482048 tomcatv ibm sp figur 4 show execut time model sweep3d total problem size 150150150 grid cell predict use mpisimtg mpisimd well measur valu 64 processor predict measur valu close differ 98 averag mpisimd differ measur valu 37 mpisimtg 72 number processor runtim sec measur figur 4 valid sweep3d ibm sp fix total problem size final valid mpisimtg na sp benchmark task time obtain 16 processor run class smallest three builtin size b c benchmark use experi problem size figur 5 6 show valid class largest size class c valid class good error less 7 valid class c also good averag error 4 even though task time obtain class result particularli interest program size class c averag run 166 time longer class demonstr compileroptim simul capabl accur project across wide rang scale factor furthermor cach effect appear play great role code two applic examin illustr fact error increas notic task time obtain small number processor use larger number processor valid sp class a100300500700 number processor runtim sec measur figur 5 valid na sp class ibm sp valid sp class c5001500250016 36 64 100 number processor runtim second measur figur valid na sp class c ibm sp figur 7 summar error mpisimtg incur simul three applic error within 16 figur emphas compilersupport approach combin analyt model simul accur rang benchmark system size problem size hard explor error without detail analysi applic therefor better quantifi error expect optim simul use sampl benchmark allow us vari comput commun ratio well commun pattern error mpisimtg predict measur number processor tomcatv sweep3d150cub figur 7 percent error incur mpisimtg predict applic perform valid origin 2000 two common commun pattern select wavefront nearest neighbor pattern commun comput ratio vari 1 100 ratio 1 1 figur 8 plot total execut time program mpisimtg predict order demonstr better impact comput granular valid figur 8 plot percentag variat predict time compar measur valu seen figur predict accur ratio comput commun larg typic mani realworld applic amount comput granular program decreas simul incur larger error expect measur error task time estim error becom rel signific nevertheless graph show predict valu differ 15 measur valu even small commun comput ratio valid sampl measur vs predict optim origin 2k5001500001 00125 00167 0025 commun comput ratio time second wvfrntmeasur nnmeasur figur 8 valid sampl origin 2000 percent variat measur time predict time515001 003 010 030 050 070 090 commun comput ratio differ wvfrnt nn figur 9 effect commun comput ratio predict accuraci mpisimtg larg comput commun ratio 5 error indic slightli higher error observ tomcatv sweep3d na sp must due presenc small comput commun ratio 53 expand simul larger system problem size main benefit use compilergener code decreas memori requir simplifi applic code sinc simul use least much memori applic decreas amount memori applic decreas simul memori requir thu allow us simul larg problem size system number processor total memori use total memori use memori reduct factor sweep 3d 44255 per proc problem size 4900 2884mb 30mb 96 sweep 3d 661000 per proc problem size 6400 215gb 122mb 1762 tomcatv 20482048 4 236mb 1184kb 1993 tabl 1 memori usag mpisimd mpisimtg benchmark tabl 1 show total amount memori need mpisim use analyt mpisimtg direct execut mpisimd model sweep3d 4900 target processor analyt model reduc memori requir two order magnitud 44255 per processor problem size similarli 661000 problem size memori requir target configur 6400 processor reduc three order magnitud three order magnitud reduct also achiev tomcatv smaller reduct achiev sp dramat reduct memori requir model allow us simul much larger target architectur b show signific improv execut time simul illustr improv scalabl achiev simul compilerderiv analyt model consid sweep3d paper studi small subset problem interest applic develop repres 20 million cell total problem size divid 44255 77255 2828255 per processor problem size need run 4900 1600 100 processor respect scalabl simul 44255 problem size seen figur 10 memori requir direct execut model restrict largest target architectur could simul 2500 processor analyt model possibl simul target architectur 10000 processor sinc applic predict runtim 10000 processor 110955 second runtim simul configur 148118 second simul slowdown 1335 note instead scale system size could scale problem size instead increas memori requir per process order simul much larger problem valid scalabl sweep3d 4x4x255proc26101 10 100 1000 10000 number processor runtim sec measur figur 10 scalabl sweep3d 44255 per processor size ibm sp 54 perform mpisim benefit compileroptim simul evid memori reduct also improv perform character perform simul four way 1 perform gain use messag optim mpisimtgmo mpisimtg compar mpisimd 2 absolut perform ie total simul time mpisimtg vs mpisimd vs applic 3 parallel perform mpisimtg term absolut rel speedup 4 perform mpisimtg simul larg system given parallel host system effect optim simul perform illustr perform improv mpisimd mpisimtg take advantag local optim mpisimtgmo addit optim messag sent conduct experi three benchmark case sweep3d compar perform three version simul given number host processor avail problem size per processor fix number target processor experi increas studi demonstr abil simul effici simul larg problem size na sp sinc problem size applic given class c fix number target processor vari number host processor avail simul studi illustr rel perform simul also abil use comput resourc figur 11 12 13 show perform mpisimtgmo mpisimtg mpisimd simul sweep3d three size per processor size 77255 1414255 2828255 simul use host processor simul 4900 target processor improv perform mpi simd mpisimtg size averag 397 6728 8807 respect problem size per processor grow larger amount comput per processor increas thu amount comput abstract away increas result runtim save 7x7x255 per processor size 64 host processors20060010 100 1000 10000 target processor runtim sec mpisimtgmo figur 11 sweep3d 7x7x255 per processor size mpisimtgmo mpisimtg messag optim 14x14x255 per processor size 64 hosts2006001000 target processor runtim sec mpisimtgmo figur 12 sweep3d 1414255 per processor size 28x28x255 per processor size 64 host procs200600100010 100 1000 10000 target proc runtim sec mpisimtgmo figur 13 sweep3d 2828255 per processor size although biggest perform gain comput optim reduc size messag sent possibl benefici simul mpisimtgmo run faster simul optim comput mpisimtg improv size 77255 1414255 2828255 2804 3123 139 respect benefit messag optim limit sweep3d applic use larg number barrier synchron well collect oper mpiallreduc oper either take data singl data item also observ great perform improv na sp benchmark class c largest size avail suit figur 14 15 show perform mpisimtg mpisimtgmo two target processor configur 16 64 simul run varieti host processor 1 64 first mpisimtg mpisimtgmo ran faster actual applic measur runtim applic execut 16 processor 262338 second wherea run 64 processor 79067 second addit figur 14 15 illustr simul run order magnitud faster mpisim messag optim use figur 14 jump runtim mpisimtg 1 2 host processor due larg commun cost size messag sent processor 605161 doubl therefor cost send messag increas consider one processor use 2 host processor use increas cost compens increas comput power howev number host processor increas better perform achiev sinc size larg messag reduc 0 mpisimtgmo simul commun overhead significantli reduc simul perform substanti better mpisimtg number target processor increas 64 figur 15 size messag simul reduc 370441 target processor code still use messag optim result order magnitud decreas simul runtim absolut perform local code optim compar absolut perform mpisim gave simul mani processor avail applic host processor target processor class c2006001000 host processor runtim sec mp imtg mp imtgmo figur 14 16 target processor simul na sp class c run variou number host processor processor na sp c lass c100300500700 host processor runtim second mp imtg mp imtgmo figur 15 64 target processor simul na sp class c run variou number host processor figur show absolut perform sweep3d total problem size 150 3 mpisimd averag 28 time slower actual applic measur figur howev mpisimtg initi faster measur applic start 13 time faster run 4 processor gradual becom 22 time faster processor final twice slow applic run 64 processor messag optim present mpisimtgmo decreas simul runtim averag 18 compar mpisimtg mpisimtg mpisimtgmo alway faster averag 185 time faster respect mpisimd show clear benefit compil optim howev number processor increas amount commun rel comput increas thu expos overhead simul commun make mpisimtg mpisim tgmo slower applic cube sweep3d total problem size1010000 number processor runtim second measur mpisimtgmo figur absolut perform mpisim fix total problem size sweep3d vertic scale logarithm figur 17 show runtim applic measur runtim two version simul run na sp class observ mpisimd run twice slower applic predict howev mpisimtg abl run much faster applic even though detail simul commun still perform best case 36 processor run 25 time faster 100 processor run 15 time faster rel perform mpisimtg decreas number processor increas amount comput applic decreas increas number processor thu save abstract comput decreas absolut perform mpisim na sp206010014030 50 70 90 number processor runtim second measur figur 17 absolut perform mpisim na sp benchmark class even dramat result obtain tomcatv runtim mpisimtg exceed 2 second processor configur compar runtim applic rang 130 second figur 18 due abil compil abstract away comput simul need directli execut skeleton code control flow comput commun pattern absolut perform mpisim tomcatv20601001400 number processor runtim second applic figur absolut perform mpisim tomcatv 2048x2048 parallel perform evalu parallel perform simul studi well take advantag increas system resourc processor solv given problem fix total problem size figur 14 15 indirectli demonstr perform simul illustr perform better speedup achiev 16 target configur depict figur 19 although mpisimtgmo smaller runtim mpisimtg scale well 8 host processor number host processor increas commun overhead host begin domin runtim hand mpi simtg send larg messag suffer one host use abl distribut overhead among processor 16target na sp class c05152535 number host processor mpisimtgmo figur 19 speedup mpisim na sp clearli perform simul better larger system simul 64target processor case figur 15 runtim decreas steadili number processor increas howev use host processor actual increas simul runtim 64 target class c could run singl processor due memori constraint direct speedup comparison possibl better scalabl seen sweep3d applic figur 20 show perform mpisimtg mpisimd simul 150 3 sweep3d run 64 target processor number host processor vari 1 64 data singl processor mpisimd simul avail simul exce avail memori clearli mpisimd mpisimtg scale well speedup mpisimtg also shown figur 21 steep slope curv 8 processor indic good parallel effici 8 processor speedup impress reach 15 64 processor due decreas comput commun ratio applic still runtim mpisimtg averag 54 time faster mpisimd runtim imu lator vs applic 150x150x150 sweep3d 64target proc100300500700 number host processor runtim sec mp isim de mp isim tg measur figur 20 parallel perform mpisim speedup mpisimtg 150cube sweep3d 64 target processors515 berofprocessor speedup mpisimtg figur 21 speedup mpisimtg sweep3d perform larg system quantifi perform improv mpisimtg compar run time simul predict perform larg system case want simul billioncel problem sweep3d applic develop envis problem util 20000 processor correspond 661000 per processor problem size figur 22 show run time simul function number target processor 64 host processor use problem size fix per processor problem size increas increas number processor figur clearli show benefit optim best case perform 1600 processor simul correspond 576 million problem size runtim optim simul nearli half runtim origin simul howev even optim memori requir still larg abl simul desir target system mpisim runtim 6x6x1000 per processor size host processors20060010000 500 1000 1500 2000 2500 3000 number target host processor runtim second figur 22 perform mpisim simul sweep3d larg system 6 conclus work develop scalabl approach detail perform evalu commun behavior messag pass interfac mpi high perform fortran hpf program approach base use compil analysi identifi portion comput whose result signific impact program perform therefor simul detail compil build intermedi static task graph represent program enabl identifi program valu impact perform also enabl deriv scale function comput task compil use program slice determin portion comput need determin perform final compil abstract away part comput code correspond data structur replac simpl analyt perform estim also flag messag data transfer perform within simul commun code retain compil simul detail mpisim experiment evalu show approach introduc rel small error predict program execut time benefit achiev significantli reduc simul time typic factor 2 greatli reduc memori usag two three order magnitud give us abil accur simul detail perform behavior system problem size 10100 time larger possibl current stateoftheart simul techniqu current work also explor number altern combin model techniqu exampl use detail simul sequenti task instead analyt model measur allow get accur estim task execut time also enabl us studi applic perform processor memori architectur differ current avail platform within poem aim support combin analyt model simul model measur sequenti task commun code static task graph provid conveni program represent support flexibl model environ 5 one potenti limit work benefit would larg applic parallel commun pattern depend extens intermedi result comput particular socal irregular applic may properti evalu benefit applic requir research perhap refin techniqu develop anoth interest direct whether techniqu describ extend type distribut applic ie nonscientif applic use network commun intens fast simul techniqu could develop applic could prove extrem valuabl control runtim optim decis object migrat load balanc adapt qualityof servic requir critic decis mani distribut applic acknowledg work support darpaito contract n6600197c8533 endtoend perform model larg heterogen adapt paralleldistribut computercommun system httpwwwcsutexaseduuserspoem work also support part asci asap program doellnl subcontract b347884 darpa rome laboratori air forc materiel command usaf agreement number f306029610159 wish thank member poem project valuabl contribut would also like thank lawrenc livermor nation laboratori use ibm sp work perform adv sakellari comput scienc depart rice univers r asci sweep3d benchmark code use integ set dataparallel program analysi optim poem endtoend perform design larg parallel adapt comput system compil synthesi task graph parallel system perform model environ applic represent multiparadigm perform model environ parallel system perform predict larg parallel applic use parallel simul parsec parallel simul environ complex system na parallel benchmark 20 proteu highperform parallelarchitectur simul optimist simul parallel architectur use program execut distribut simul case studi design verif distribut program condit event approach distribut simul rice parallel process testb multiprocessor simul trace use tango poem endtoend perform design larg parallel adapt comput system distribut memori laps parallel simul messagepass program parallel direct execut simul messagepass parallel program fast function algorithm simul testb function algorithm simul fast multipol method architectur implic improv accuraci vs speed tradeoff simul sharedmemori multiprocessor ilp processor interprocedur slice use depend graph transpar implement conserv algorithm parallel simul languag reduc synchron overhead parallel simul adapt synchron method unpredict commun pattern dataparallel program parallel simul data parallel program mpisim use parallel simul evalu mpi program asynchron parallel simul parallel program wisconsin wind tunnel virtual prototyp parallel comput tr rice parallel process testb interprocedur slice use depend graph proteu highperform parallelarchitectur simul wisconsin wind tunnel distribut memori laps reduc synchron overhead parallel simul optimist simul parallel architectur use program execut parallel direct execut simul messagepass parallel program transpar implement conserv algorithm parallel simul languag use integ set dataparallel program analysi optim poem mpisim perform predict larg parallel applic use parallel simul asynchron parallel simul parallel program improv lookahead parallel discret event simul largescal applic use compil analysi parsec poem adapt synchron method unpredict commun pattern dataparallel program compil synthesi task graph parallel program perform predict parallel simul data parallel program fast improv accuraci vs speed tradeoff simul sharedmemori multiprocessor ilp processor ctr yasuharu mizutani fumihiko ino kenichi hagihara fast perform predict masterslav program partial task execut proceed 4th wsea intern confer softwar engin parallel distribut system p17 februari 1315 2005 salzburg austria