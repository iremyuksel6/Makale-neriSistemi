logist regress adaboost bregman distanc give unifi account boost logist regress learn problem cast term optim bregman distanc strike similar two problem framework allow us design analyz algorithm simultan easili adapt algorithm design one problem problem give new algorithm explain potenti advantag exist method algorithm iter divid two type base whether paramet updat sequenti one time parallel also describ parameter famili algorithm includ sequenti parallelupd algorithm special case thu show sequenti parallel approach unifi algorithm give converg proof use gener formal auxiliaryfunct proof techniqu one sequentialupd algorithm equival adaboost provid first gener proof converg adaboost show algorithm gener easili multiclass case contrast new algorithm iter scale algorithm conclud experiment result synthet data highlight behavior old newli propos algorithm differ set b introduct give unifi account boost logist regress show learn problem cast term optim bregman distanc frame work two problem becom extrem similar real differ choic bregman distanc unnorm rel entropi boost binari rel entropi logist regress fact two problem similar framework allow us design analyz algorithm si multan abl borrow method maximumentropi literatur logist regress appli exponenti loss use adaboost especi convergenceproof techniqu convers easili adapt boost method problem minim logist loss use logist regress result famili new algorithm problem togeth converg proof new algorithm well adaboost adaboost logist regress attempt choos paramet weight associ given famili function call featur weak hypothes adaboost work sequenti updat paramet one one wherea method logist regressionmost notabl iter scale 9 10 iter updat paramet parallel iter first new algorithm method optim exponenti loss use parallel updat seem plausibl parallelupd method often converg faster sequentialupd method provid number featur larg make parallel updat infeas preliminari experi suggest case second algorithm parallelupd method logist loss although parallelupd algorithm well known function updat deriv new preliminari experi indic new updat may also much faster unifi treatment give exponenti logist loss function abl present prove converg algorithm two loss simultan true algorithm present paper well next describ analyz sequentialupd algorithm two loss function exponenti loss algorithm equival adaboost algorithm freund schapir 13 view algorithm frame work abl prove adaboost correctli converg minimum exponenti loss function new result although kivinen warmuth 16 mason et al 19 given converg proof adaboost proof depend assumpt given minim problem may hold case proof hold gener without assumpt unifi view lead instantli sequentialupd algorithm logist regress minor modif adaboost similar one propos duffi helmbold 12 like adaboost algorithm use conjunct classif algorithm usual call weak learn algorithm accept distribut exampl return weak hypothesi low error rate respect distribut ever new algorithm provabl minim logist loss rather arguabl less natur exponenti loss use adaboost anoth potenti import advantag new algorithm weight place exampl bound 0 1 suggest may possibl use new algorithm set boost algorithm select exampl present weak learn algorithm filter stream exampl larg dataset point watanab 22 domingo watanab 11 possibl adaboost sinc weight may becom extrem larg provid modif adaboost purpos weight truncat 1 new algorithm may viabl cleaner altern next describ parameter famili iter algorithm includ parallel sequentialupd algorithm also interpol smoothli two extrem converg proof give hold entir famili algorithm although paper consid binari case two possibl label associ exampl turn multiclass case requir addit work algorithm converg proof give binari case turn directli applic multiclass case without modif comparisonw also describ gener iter scale algorithm darroch ratcliff 9 rederiv procedur set abl relax one main assumpt usual requir algorithm paper organ follow section 2 describ boost logist regress model usual formul section 3 give background optim use bregman distanc section 4 describ boost logist regress cast within framework section 5 give parallelupd algorithm proof converg section 6 give sequentialupd algorithm converg proof parameter famili iter algorithm describ section 7 extens multiclass problem given section 8 section 9 contrast method iter scale section 10 give preliminari experi previou work variant sequentialupd algorithm fit gener famili arc algorithm present breiman 4 3 well mason et al anyboost famili algorithm 19 informationgeometr view take also show algorithm studi includ adaboost fit famili algorithm describ 1967 bregman 2 satisfi set constraint work base directli gener set laf ferti della pietra della pietra 18 one attempt solv optim problem base gener bregman distanc gave method deriv analyz parallelupd algorithm set use auxilliari function algorithm converg proof base method work build sever previou paper compar boost approach logist regress fri man hasti tibshirani 14 first note similar boost logist regress loss function deriv sequentialupd algorithm logitboost logist loss howev unlik algorithm requir weak learner solv leastsquar problem rather classif problem anoth sequentialupd algorithm differ relat problem propos cesabianchi krogh warmuth 5 duffi helmbold 12 gave condit loss function give boost algorithm show minim logist loss lead boost algorithm pac sens suggest algorithm problem close may turn also pac boost properti lafferti 17 went studi relationship logist regress exponenti loss use famili bregman distanc howev set describ paper appar extend precis includ exponenti loss use bregman distanc describ import differ lead natur treatment exponenti loss new view logist regress work build heavili kivinen warmuth 16 along lafferti first make connect adaboost inform geometri show updat use adaboost form entropi project howev bregman distanc use differ slightli one chosen normal rel entropi rather unnorm rel entropi adaboost fit model quit complet particular converg proof depend assumpt hold gener kivinen warmuth also describ updat gener bregman distanc includ one exampl bregman distanc use captur logist regress boost logist model loss function set train exampl instanc x belong domain instanc space x label assum also given set realvalu function follow convent maxent literatur call function featur boost literatur would call weak base hypothes studi problem approxim use linear combin featur interest problem find vector paramet 2 r n f good approxim measur good approxim vari task mind classif problem natur tri match sign f attempt minim true 0 otherwis although minim number classif error may worthwhil goal gener form problem intract see instanc 15 therefor often advantag instead minim nonneg loss function instanc boost algorithm adaboost 13 20 base exponenti loss exp gammay verifi eq 1 upper bound eq 2 howev latter loss much easier work demonstr adaboost briefli seri round adaboost use oracl subroutin call weak learn algorithm pick one featur weak hypothesi h j associ paramet j updat note breiman 3 4 variou later author step done way approxim caus greatest decreas exponenti loss paper show first time adaboost fact provabl effect method find paramet minim exponenti loss assum weak learner alway choos best h j also give entir new algorithm minim exponenti loss round paramet updat parallel rather one time hope parallelupd algorithm faster sequentialupd algorithm see section 10 preliminari experi regard instead use f classif rule might instead postul gener stochast function x attempt use f x estim probabl associ label natur wellstudi way pass f logist function use estim likelihood label occur sampl gammay maxim likelihood equival minim log loss model gammay gener improv iter scale 9 10 popular parallelupd method minim loss paper give altern parallelupd algorithm compar iter scale techniqu preliminari experi section 10 section give background optim use bregman distanc form unifi basi studi boost logist regress particular setup follow taken primarili lafferti della pietra della pietra 18 r continu differenti strictli convex function defin close convex set bregman distanc associ f defin instanc bf unnorm rel entropi du shown gener everi bregman distanc nonneg equal zero two argument equal natur optim problem associ bregman distanc name find vector closest given vector q 0 2 subject set linear constraint constraint specifi theta n matrix vector vector p satisfi constraint problem find arg min convex dual problem give altern formul problem find vector particular form closest given vector p form vector defin via legendr transform written simpli v ffi q f clear context use calculu seen equival instanc bf unnorm rel entropi verifi use calculu v eq 6 use note given theta n matrix vector q 0 2 consid vector obtain take legendr transform linear combin column vector q 0 vector set dual optim problem state problem find arg min q closur q remark fact two optim problem solut moreov solut turn uniqu point intersect p q take statement theorem lafferti della pietra della pietra 18 result appear due csisz ar 6 7 topso 21 proof case normal rel entropi given della pietra della pietra lafferti 10 see also csisz ar survey articl 8 theorem 1 let q assum bf 1 exist uniqu 2 bf delta 3 4 q moreov one four properti determin q uniqu theorem extrem use prove converg algorithm describ show next section boost logist regress view optim problem type given part 3 theorem prove optim need show algorithm converg point p q regress revisit return boost logist regress problem outlin section 2 show cast form optim problem outlin recal boost goal find exp gammay minim precis minimum attain finit seek procedur find sequenc caus function converg infimum shorthand call exploss problem view problem form given section 3 0s 1s vector follow final take f eq 4 bf unnorm rel entropi note earlier case v ffi q given eq 7 particular mean furthermor trivial see du du delta equal eq 10 thu minim du equival minim eq 10 theorem 1 equival find satisfi constraint logist regress reduc optim problem form nearli way recal goal find sequenc minim shorthand call logloss problem defin exactli exponenti loss vector q 0 still constant defin 121 space restrict 0 1 minor differ howev import differ choic function f name result bregman distanc trivial choic f verifi use calculu v delta equal eq 13 minim db equival minim eq 13 find q 2 q satisfi constraint eq 12 section describ new algorithm exploss logloss problem use iter method weight j updat iter algorithm shown fig 1 algorithm use function f satisfi certain condit describ particular see use choic f given section 4 thu realli singl algorithm use lossminim problem set paramet appropri note without loss gener assum section instanc algorithm simpl iter vector ffi comput shown ad paramet vector assum algorithm input infinitevalu updat never occur algorithm new minim problem optim method exploss notabl adaboost paramet assumpt 1 2 bf lim ffl ffl updat paramet figur 1 parallelupd optim algorithm gener involv updat one featur time parallel updat method logloss well known see exam ple 9 10 howev updat take differ form usual updat deriv logist model use point distribut q t1 simpl function previou distribut q eq 8 give exploss logloss respect prove next algorithm given fig 1 converg optim either loss prove abstractli matrix vector q 0 function f satisfi follow assumpt assumpt 1 v 2 r q 2 assumpt 2 c 1 set bound show later choic f given section 4 satisfi assumpt allow us prove converg exploss logloss prove convergencew use auxiliaryfunct techniqu della pietra della pietra lafferti 10 roughli idea proof deriv nonneg lower bound call auxiliari function much loss decreas iter sinc loss never increas lower bound zero auxiliari function must converg zero final step show auxiliari function zero constraint defin set p must satisfi therefor theorem 1 must converg optim formal defin auxiliari function sequenc continu function satisfi two condit prove converg specif algorithm prove follow lemma show roughli sequenc auxiliari function sequenc converg optimum point q thu prove converg specif algorithm reduc simpli find auxiliari function auxiliari function matrix assum q lie compact subspac q q eq 9 particular case assumpt 2 hold bf lim proof condit 18 bf delta nonincreas sequenc bound zero therefor sequenc differ bf must converg zero condit 18 mean aq must also converg zero assum q lie compact space sequenc q must subsequ converg point continu p eq 5 hand q limit sequenc point q theorem 1 argument uniqu q show singl limit point q suppos entir sequenc converg q could find open set b contain q fq 1 contain infinit mani point therefor limit point must close set must differ q alreadi argu imposs therefor entir sequenc converg q appli lemma prove converg algorithm fig 1 theorem 3 let f satisfi assumpt 1 2 assum bf 1 let sequenc gener algorithm fig 1 lim q eq 9 lim proof let w claim function auxiliari function clearli continu nonposit upper bound chang delta round aq follow eq 20 21 follow eq 16 assumpt 1 respect eq 22 use fact x j jensen inequ appli convex function e x eq 23 use definit w tj w gamma tj eq 24 use choic ffi inde ffi chosen specif minim eq 23 j q thu auxiliari function theorem follow immedi lemma 2 appli theorem exploss logloss prob lem need verifi assumpt 1 2 satisfi exploss assumpt 1 hold equal logloss first second equal use eq 14 15 respec tive final inequ use 1 assumpt 2 hold trivial logloss sinc bound exploss du clearli defin bound subset r 6 sequenti algorithm section describ anoth algorithm minim problem describ section 4 howev unlik algorithm section 5 one present updat weight one featur time parallelupd algorithm may give faster converg mani featur sequentialupd algorithm use larg number featur use oracl select featur updat next instanc adaboost essenti equival sequentialupd algorithm exploss use assum weak learn algorithm select weak hypothesi ie one featur sequenti algorithm present logloss use exactli way algorithm shown fig 2 theorem 4 given assumpt theorem 3 algorithm fig 2 converg optim sens theorem 3 proof theorem use auxiliari function paramet fig 1 output fig 1 ae ff els ffl updat paramet figur 2 sequentialupd optim algorithm function clearli continu nonposit eq 27 use convex e gammaff x eq 29 use choic ff chose ff minim bound eq 28 thu auxiliari function theorem follow immedi lemma 2 mention algorithm essenti equival adaboost specif version adaboost first present freund schapir 13 adaboost iter distribut train exampl comput weak learner seek weak hypothesi low error respect distribut algorithm present section assum space weak hypothes consist featur h learner alway succe select featur lowest error accur error farthest 12 translat notat weight assign exampl adaboost exactli equal q ti z weight error tth weak hypothesi equal theorem 4 first proof adaboost alway converg minimum exponenti loss assum ideal weak learner form note theorem also tell us exact form lim howev know limit behavior q know limit behavior paramet whether q also present section new algorithm logist regress fact algorithm one given duffi helmbold 12 except choic ff practic term littl work would requir alter exist learn system base adaboost use logist loss rather exponenti lossth differ manner q comput even system base confidencer boost 20 ff j chosen togeth round minim eq 26 rather approxim express use algorithm fig 2 note proof theorem 4 easili modifi prove converg algorithm use auxiliari 7 parameter famili iter algorithm previou section describ separ parallel sequentialupd algorithm section describ parameter famili algorithm includ parallel updat algorithm section 5 well sequentialupd algorithm differ one section 6 famili algorithm also includ algorithm may appropri either certain situat explain algorithm shown fig 3 similar parallelupd algorithm fig 1 round quantiti tj w gamma tj comput vector comput ffi comput fig 1 howev vector ad directli instead anoth vector select provid scale featur vector chosen maxim measur progress restrict belong set allow form scale vector given set paramet algorithm restrict vector satisfi constraint parallelupd algorithm fig 1 obtain choos assum equival make assumpt choos paramet fig 1 r n mthetan satisfi condit output fig 1 ffl tj 2 ffl j ffl updat paramet figur 3 parameter famili iter optim algorithm obtain sequentialupd algorithm choos set unit vector ie one compon equal 1 other equal 0 assum ij 2 j updat becom ae tj els anoth interest case assum natur choos ensur maxim solv analyt give updat idea gener easili case dual norm p q final case restrict scale vector ie choos r case maxim problem must solv choos linear program problem n variabl constraint prove converg entir famili algorithm theorem 5 given assumpt theorem 3 algorithm fig 3 converg optim sens theorem 3 proof use auxiliari function j theorem 3 function continu nonposit bound chang use techniqu given theorem 3 tj tj tj final j 0 sinc everi j exist 2 j 0 impli appli lemma 2 complet theorem section show result extend multiclass case gener preced result see new algorithm need devis new converg proof need prove case rather preced algorithm proof directli appli multiclass case multiclass case label set cardin k featur form h logist regress use model e f xy 6y e f xgammaf xy loss train set e f transform framework follow let fy gg vector p q etc work r 1mdimension index pair b let convex function f use case defin space result bregman distanc 6y clearli shown v assumpt 1 verifi note i2b let 1k1 plug definit give delta equal eq 31 thu algorithm section 5 6 7 use solv minim problem correspond converg proof also directli applic sever multiclass version adaboost ada boostm2 13 special case adaboostmr 20 base loss function i2b exp loss use similar set except choic f instead use i2b fact actual f use binari adaboost mere chang index set b thu i2b v choos multiclass logist regress bf delta equal loss eq 33 thu use preced algorithm solv multiclass problem well particular sequentialupd algorithm give adaboostm2 adaboostmh 20 anoth multiclass version ada boost adaboostmh replac b index set exampl label 2 defin ae loss function adaboostmh exp let use f binari adaboost q obtain multiclass version adaboost 9 comparison iter section describ gener iter scale gi procedur darroch ratcliff 9 comparison algorithm larg follow descript gi given berger della pietra della pietra 1 multiclass case make comparison stark possibl present gi notat prove converg use method develop previou section also abl relax one key assumpt tradit use studi gi adopt notat setup use multiclass logist regress section 8 knowledg analog gi exponenti loss consid case logist loss also extend notat defin q iy q defin moreov verifi q defin eq 30 gi follow assumpt regard featur usual made section prove gi converg second condit replac milder one name sinc multiclass case constant ad featur h j without chang model loss function sinc featur scale constant two assumpt consid clearli made hold without loss gener improv iter scale algorithm della pietra della pietra lafferti 10 also requir milder assumpt much complic implement requir numer search newton raphson featur iter gi work much like parallelupd algorithm section 5 f q 0 defin multiclass logist regress section 8 differ comput vector updat ffi gi requir direct access featur h j specif gi ffi defin clearli updat quit differ updat describ paper use notat section 5 8 reformul framework follow theta h j i2b prove converg updat use usual auxiliari function method theorem 6 let f q 0 modifi gi algorithm describ converg optim sens theorem 3 proof show auxilliari function vector q comput gi clearli continu usual nonneg properti unnorm rel entropi impli eq 35 h w thu impli constraint q proof theorem 3 remain shown introduc notat rewrit gain follow use eq 32 plug definit first term eq 38 written next deriv upper boundon second term eq 38 09train loss seq seq2 par 403050709train loss seq seq2 par figur 4 train logist loss data gener noisi hyperplan mani left right relev featur eq 40 follow log bound ln x x gamma 1 eq 42 use eq 25 assumpt form h j eq 43 follow definit updat ffi final combin eq 36 38 39 44 give eq 37 complet proof clear differ gi updat given paper stem eq 38 deriv ith term sum choic c effect mean log bound taken differ point ln 1 gener case bound exact vari c vari bound taken therebi vari updat section briefli describ experi use synthet data experi preliminari intend suggest possibl algo rithm practic valu systemat experi clearli need use realworld synthet data compar new algorithm commonli use procedur first test effect method minim logist loss train data first ex periment gener data use noisi hyperplan specif first gener random hyperplan 100dimension space repres vector w 2 r 100 chosen uniformli random unit sphere chose 300 point x 2 r 100 point normal distribut x n0 next assign label point depend whether fell chosen hyperplan ie label chosen perturb point x ad random amount n0 08 effect caus label point near separ hyperplan noisi point farther featur identifi coordin x ran parallel sequentialupd algorithm section 5 6 denot par seq figur data also ran sequentialupd algorithm special case parameter famili describ section 7 denot seq2 final ran iter scale algorithm describ section 9 result experi shown left fig 4 show plot logist loss train set four method function number iter loss normal 1 method well comparison iter scale parallelupd method clearli best follow close second sequentialupd algorithm parallelupd method much time faster term number iter iter scale right fig 4 shown result similar experi four compon w forc zero word four relev variabl featur experi sequentialupd algorithm perform kind featur select initi signific advantag test error log seq exp seq log par exp par figur 5 test misclassif error data gener noisi hyperplan boolean featur parallelupd algorithm eventu overtaken last experi test effect new competitor adaboost minim test misclassif error experi chose separ hyperplan w first experi howev chose 1000 point x uniformli random boolean hypercub fgamma1 1g 100 label comput label chosen flip coordin point x independ probabl 005 nois model effect caus exampl near decis surfac noisier far experi use parallel sequenti updat algorithm section 5 6 denot par seq case use variant base exponenti loss exp logist loss log case sequentialupd algorithm section 6 7 identi cal fig 5 show plot classif error separ test set 5000 exampl larg differ perform exponenti logist variant algorithm howev parallelupd variant start much better although eventu method converg roughli perform level acknowledg mani thank manfr warmuth first teach us bregman distanc mani comment earlier draft thank also nigel duffi david helmbold raj iyer help discuss suggest research done yoram singer att lab r della pietra relax method find common point convex set applic solut problem convex program arc edg predict game arc classifi gener iter scale loglinear model induc featur random field scale boost base learner via adapt sampl potenti booster decisiontheoret gener onlin learn applic boost addit logist regress statist view boost boost entropi project addit model statist learn algorithm base bregman di tanc function gradient techniqu combin hypoth se improv boost algorithm use confidencer predict inform theoret optim techniqu comput learn theori discoveri scienc tr ctr stefan riezler new develop pars technolog comput linguist v32 n3 p439442 septemb 2006 nir kraus yoram singer leverag margin care proceed twentyfirst intern confer machin learn p63 juli 0408 2004 banff alberta canada hoiem alexei efro martial hebert automat photo popup acm transact graphic tog v24 n3 juli 2005 zhihua zhang jame kwok dityan yeung surrog maximizationminim algorithm adaboost logist regress model proceed twentyfirst intern confer machin learn p117 juli 0408 2004 banff alberta canada cynthia rudin ingrid daubechi robert e schapir dynam adaboost cyclic behavior converg margin journal machin learn research 5 p15571595 1212004 steven j phillip miroslav dudk robert e schapir maximum entropi approach speci distribut model proceed twentyfirst intern confer machin learn p83 juli 0408 2004 banff alberta canada amir globerson terri koo xavier carrera michael collin exponenti gradient algorithm loglinear structur predict proceed 24th intern confer machin learn p305312 june 2024 2007 corvali oregon zhihua zhang jame kwok dityan yeung surrog maximizationminim algorithm extens machin learn v69 n1 p133 octob 2007 tane mielikinen evimaria terzi panayioti tsapara aggreg time partit proceed 12th acm sigkdd intern confer knowledg discoveri data mine august 2023 2006 philadelphia pa usa joshua goodman sequenti condit gener iter scale proceed 40th annual meet associ comput linguist juli 0712 2002 philadelphia pennsylvania stefano merler bruno capril cesar furlanello parallel adaboost weight dynam comput statist data analysi v51 n5 p24872498 februari 2007 gokhan tur extend boost larg scale spoken languag understand machin learn v69 n1 p5574 octob 2007 hoiem alexei efro martial hebert recov surfac layout imag intern journal comput vision v75 n1 p151172 octob 2007 w john wilbur lana yeganova kim synergi pav adaboost machin learn v61 n13 p71103 novemb 2005 heinz h bauschk dualiti bregman project onto translat cone affin subspac journal approxim theori v121 n1 p112 march joseph turian dan melam advanc discrimin pars proceed 21st intern confer comput linguist 44th annual meet acl p873880 juli 1718 2006 sydney australia michael collin paramet estim statist pars model theori practic distributionfre method new develop pars technolog kluwer academ publish norwel 2004 michael collin terri koo discrimin rerank natur languag pars comput linguist v31 n1 p2570 march 2005 ron meir gunnar rtsch introduct boost leverag advanc lectur machin learn springerverlag new york inc new york ny