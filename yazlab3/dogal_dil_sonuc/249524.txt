runtim parallel schedul processor load balanc abstractparallel schedul new approach load balanc parallel schedul processor cooper schedul work parallel schedul abl accur balanc load use global load inform compiletim runtim provid highqual load balanc paper present overview parallel schedul techniqu schedul algorithm tree hypercub mesh network present algorithm fulli balanc load maxim local runtim commun cost significantli reduc compar exist algorithm b introduct static schedul balanc workload runtim appli problem predict structur call static problem dynam schedul perform schedul activ concurr runtim appli problem unpredict structur call dynam problem static schedul util knowledg problem characterist reach wellbalanc load 1 2 3 4 howev abl balanc load dynam problem addit requir larg memori space store task graph restrict scalabl static schedul dynam schedul gener approach suitabl wide rang applic 5 6 7 adjust load distribut base runtim system load inform howev runtim schedul algorithm util neither characterist inform applic problem global load inform load balanc decis system stabil usual sacrific qualiti quick load balanc parallel schedul promis techniqu processor load balanc parallel schedul ing processor cooper schedul work parallel schedul util global load inform abl accur balanc load provid highqual scalabl load balanc parallel schedul algorithm introduc 8 9 10 11 parallel schedul appli static problem exist schedul algorithm static problem run singl processor scalabl massiv parallel comput store task graph requir larg memori space speed schedul relax demand memori space static schedul parallel kwok ahmad develop parallel algorithm 12 wu parallel mcp algorithm 13 parallel schedul also appli dynam problem parallel schedul appli runtim becom increment collect schedul appli whenev load becom unbalanc processor collect schedul workload system describ 11 start system phase schedul initi task follow user comput phase execut schedul task possibl gener new task next system phase old task execut schedul togeth newli gener task system phase parallel schedul algorithm appli balanc load paper discuss parallel schedul methodolog paper devot particularli kind schedul schedul readi job task object schedul set job task readi execut schedul algorithm tree hypercub mesh network present algorithm primarili design dynam problem randomli arriv dynam gener job task algorithm fulli balanc load maxim local significantli reduc commun overhead compar exist algorithm paper organ follow section 2 discuss optim schedul problem parallel schedul algorithm tree hypercub mesh topolog present section 3 properti algorithm describ section 4 perform present section 5 previou work discuss section 6 section 7 conclud paper 2 optim schedul problem object schedul schedul work processor work load thu need estim task execut time estim applic specif lead less gener approach sometim estim difficult obtain due difficulti task presum requir equal execut time object algorithm becom schedul task processor number task inaccuraci caus grainsiz variat correct next system phase algorithm estim time task could improv load balanc extent howev sinc algorithm complex schedul overhead increas may overwrit benefit 11 schedul problem describ follow parallel system n comput node connect given topolog node w task parallel schedul appli schedul algorithm redistribut task number task node equal assum sum w node evenli divid n averag number task w avg calcul node w avg task execut schedul algorithm w node must determin send task commun step mani commun perform simultan time spent load balanc activ depend number commun step time taken step parallel schedul algorithm util global inform number commun step order log n n number processor 14 averag time commun step depend total number task migrat travel distanc object function minim number taskhop x e k number task transmit edg k gener problem convert minimumcost maximumflow problem 15 follow edg treat bidirect arc given tupl capac cost capac capac edg cost cost edg set edg processor network add sourc node edg node w w avg sink node edg j node j w cost capac cost minimumcost maximumflow algorithm yield solut problem figur 1 show load distribut eightnod hypercub network graph construct figur 1 given figur 2 w 8 minimum cost algorithm 15 gener solut shown figur 3 complex minimum cost algorithm 2 v n number node v desir flow valu 15 complex correspond parallel algorithm n node least onv high complex realist runtim schedul certain topolog tree complex reduc olog n n node topolog tree need find heurist algorithm 3 parallel schedul algorithm section present parallel schedul algorithm tree hypercub mesh topolog common featur algorithm total number task obtain figur 1 load distribut 3dimension hypercub edg figur 2 graph optim schedul problem figur 1 figur 3 optim solut figur 1 parallel reduct oper averag number task per node calcul 14 node send task node unless number task exce averag therefor necessari task migrat discuss individu algorithm differ topolog give gener algorithm shown figur 4 first step collect global inform use sum reduct 14 step 2 averag number task per node calcul number task evenli divid number node remain r task evenli distribut first r node one task other valu w avg r avail node step 3 node calcul quota node know overload underload quota subset node also comput particular topolog step 4 task exchang meet quota minimum commun differ algorithm design differ topolog let w number task node 1 global inform collect perform sum reduct w comput inform total number task 2 averag load calcul w 3 quota calcul node comput quota q w avg otherwis quota subset node also comput 4 task exchang overload node determin send excess task figur 4 gener parallel schedul algorithm follow subsect present three parallel schedul algorithm tree walk algorithm twa cube walk algorithm cwa mesh walk algorithm mwa tree algorithm optim algorithm term number taskhop hypercub mesh algorithm heurist algorithm 31 tree walk algorithm network topolog tree complex optim schedul reduc tree walk algorithm twa shown figur 5 essenti one present 11 step 1 total number task count parallel reduct oper node record number task subtre children subtre step 2 root calcul averag number task per node broadcast number everi node step 3 subtre root node calcul quota q indic mani task schedul subtre q calcul directli follow node keep record q q j node j node child step 4 workload exchang end system phase node number task quota tree walk algorithm twa assign node order accord preorder travers n number node subtre total number node system node parent node p also child vector c i0 c i1 c im gamma1 give children node number 1 global inform collect perform sum reduct w 2 averag load calcul 3 quota calcul quota node q comput also quota subtre comput 4 task exchang node comput 41 node j l receiv task node p 42 node receiv task node c ij 43 node j l task node p 44 node ij task node c ij figur 5 tree walk algorithm lemma 1 execut twa number task node equal quota proof assum node j child node c j node j equal gammaj r il node thu j l receiv jj l task node similarli j r il node equal gammaj l j node j thu j r receiv jj r task node j therefor execut twa number task node j child j child j child j child j child step 1 2 spend 2m commun step depth tree commun step step 4 distanc leaf node anoth leaf node 2m therefor total number commun step algorithm 4m balanc tree number commun step parallel algorithm n node ologn exampl 1 exampl shown figur 6 node tree number preorder travers begin schedul node w task readi schedul valu w calcul step 1 root calcul valu w avg r node calcul valu q step 3 valu w shown follow figur exampl tree walk algorithm number task exchang node shown figur 6 end schedul node 04 five task node 58 four task 32 cube walk algorithm subsect studi two algorithm design hypercub topolog dem algorithm 8 9 propos cube walk algorithm cwa dem small domain balanc first combin form larger domain ultim entir system balanc integ version dem describ figur 7 node pair first dimens whose address differ least signific bit balanc load next node pair second dimens balanc load forth node balanc load neighbor number commun step dem algorithm 3d number dimens 16 dem node exchang node j current valu w w j w task node j w task node j updat valu w figur 7 dem algorithm exampl 2 dem algorithm illustr figur 8 load distribut execut b c figur 8 exampl dem algorithm dem algorithm shown figur 8a first step node exchang load inform balanc load dimens 0 shown figur 8b load balanc dimens 1 shown figur 8c load balanc dimens 2 figur 8d final result shown figur 8e load fulli balanc integ number task transmit node total 33 taskhop wherea optim schedul shown figur 3 21 taskhop execut dem algorithm load differ dimens hypercub 17 figur 9 show exampl 4dimension hypercub figur 9 exampl show number task differ 4 result dem dem algorithm simpl low complex load balanc step node pair exchang load inform global inform collect without global load inform imposs node make correct decis mani task sent node pair attempt averag number task anyway node may send excess task neighbor dem unabl fulli balanc load minim commun cost good heurist algorithm design util global load inform present new parallel schedul algorithm hypercub topolog algorithm call cube walk algorithm cwa shown figur 10 let w 0 number task node algorithm appli first step collect system load inform exchang valu w k obtain valu w k1 node record w vector w k total number task kdimension subcub kdimension subcub node defin node whose number gamma kbit prefix node valu node equal total number task entir cube step 2 node calcul averag number task per node quota vector q calcul step 3 node know kdimension subcub overload underload vector q comput directli follow bitwis bitwis ffi vector differ w q stand number task sent receiv subcub cube walk algorithm cwa assum cube dimens number node let phi denot bitwis exclus bitwis 1 global inform collect perform sum reduct node comput w vector 2 averag load calcul 3 quota calcul node comput vector q k 4 task exchang 41 node comput number task sent task well vector node phi 2 k updat w ffi vector 42 node receiv task well vector node phi 2 k updat w ffi vector dimens figur 10 cube walk algorithm step 4 task exchang conduct among dimens start cube dimens gamma 1 recurs partit cube dimens k two subcub dimens gamma1 node ni pair correspond node ni subcub particular step exchang task ni ni 0 send task one direct overload subcub way overload node necessarili commit send task sinc may postpon action decis made global within subcub calcul vector everi node overload subcub calcul local oper without commun valu ffi n 0 calcul fl vector record number task reserv subcub lower dimens follow lemma show end algorithm node number task quota lemma 2 execut cwa number task node equal quota proof show iter 0 number task node equal quota q 0 need show iter k kdimension subcub q k task subcub need send ffi k task subcub task sent one direct number task sent overload subcub underload subcub must equal ffi k proven show three case assign valu case 1 henc case 2 j1 sinc sinc henc case 3 sinc sinc henc algorithm step 1 spend 2d commun step exchang load inform dimens cube step 4 spend commun step load balanc therefor total number commun step algorithm 3d exampl 3 run exampl cwa shown figur 11 begin schedul node w 0 task readi schedul valu w k calcul step 1 valu w avg r follow node calcul valu q k step 3 everi node quota vector step 4 2 subcub f0123g overload one valu w k follow node d0 d1 d2 thu node 0 send six task node 4 node 1 send three task node 5 load subcub f0 1 2 3g f4 5 6 7g balanc subcub task subcub f01g f45g overload valu w k follow node d0 d1 thu node 0 send five task node 2 node 5 send two task node 7 load subcub f0 1g f2 3g f4 5g f6 7g balanc subcub task overload valu w k follow b c figur 11 run exampl cwa node d0 final node 3 send one task node 2 node 5 send two task node 4 node 6 send two task node 7 result balanc load node eight task total number taskhop 21 33 mesh walk algorithm parallel schedul algorithm mesh topolog name mesh walk algorithm mwa shown figur 12 first scan partial vector w along everi row node record w vector w 0 node mod calcul sum ij scan oper perform along node keep anoth vector c valu w avg r calcul node n gamma 1 spread node mod consequ node spread valu w avg igamman 2 along row vector q 0 ffi calcul node well q 0 igamman 2 valu q calcul directli mod r mod r otherwis step 4 first iter load among row node calcul valu j i0l j i0r receiv jj i0l j task row r gamma 1 receiv jj i0r j task row 1 j i0r 0 submesh row 0 row overload j i0r task need sent row r 1 similarli submesh row underload j i0l task need sent row r gamma 1 vector calcul determin mani task node need sent calcul j local oper without commun variabl fl ijl indic mani task reserv previou j node row variabl j ijl tell mani task remain sent valu w ffi updat iter 0 mesh walk algorithm mwa assum n 1 theta n 2 mesh number node 1 global inform collect perform scan oper comput w vector 2 averag load calcul 3 quota calcul comput vector q k 4 task exchang let initi 42 receiv task well vector node n l updat w vector 43 calcul number task sent ij j l task well vector node n l updat w ffi vector figur 12 mesh walk algorithm balanc load row follow lemma show end algorithm node number task quota lemma 3 execut mwa number task node equal quota proof iter 1 ij 2 larger equal 0 1 3 v 1 assum 1 equal 0 assum exist ffi 0 therefor 1 2 fl 4 3 6 7 8 j iv 1 1 0r j receiv jj i0l j task node receiv jj i0r j task node therefor iter 1 weight w 0 updat iter 0 therefor iter 0 number task row algorithm step 1 spend n 2 commun step collect load inform along row n 1 commun step collect load inform across row broadcast spread oper spend commun step step 4 spend n 1 commun step load balanc therefor total commun step algorithm 3n 1 exampl 4 run exampl mwa shown figur 13 total number task comput parallel reduct valu w avg r calcul everi node q 0 8 32 respect valu also calcul valu w 0 list follow node row valu w 1 list follow 9 3 b c figur 13 run exampl mwa iter 1 everi j i0l 0 therefor task sent one direct exampl valu ffi 0 list follow row 0 node 1 send three task node 5 node 3 send six task node 7 node row 1 receiv task vector updat ffi calcul vector node 4 send nine task node 8 node 7 send three task node 11 final node row 2 updat ffi calcul vector node 8 send three task node 12 node 9 send two task node 13 task exchang shown figur 13b number task row equal quota q 1 32 iter 0 w 0 calcul updat valu w valu q valu j vector follow node exchang task shown figur 13c accord valu equal j result balanc load node eight task total number taskhop 48 4 properti schedul algorithm section discuss schedul qualiti local commun cost twa cwa mwa algorithm next theorem show algorithm abl fulli balanc load number task equal divid number node node equal number task otherwis number task node differ one theorem 1 differ number task node one execut twa cwa mwa proof lemma 1 2 3 number task node equal quota execut twa cwa mwa sinc quota either w avg w avg 1 differ number task node one 2 algorithm also maxim local local task task migrat node nonloc task migrat node maximum local impli maximum number local task minimum number nonloc task lemma 4 5 theorem 2 3 assum number task evenli divid n number node evenli divid n algorithm nearlyoptim follow lemma give minimum number nonloc task lemma 4 reach balanc load minimum number nonloc task proof node w must receiv w task node balanc load therefor total task must migrat nodesth next theorem prove three algorithm maxim local theorem 2 number nonloc task twa cwa mwa algorithm proof time execut twa cwa mwa algorithm number task node less minw w avg twa node receiv task send task cwa mwa node send task weight larger w avg w task sent thu node least task local therefor number nonloc task n theta w avg gamma w state lemma 4 algorithm minim number nonloc task maxim local 2 twa optim schedul algorithm next theorem prove twa minim number taskhop commun theorem 3 twa algorithm minim total number taskhop total number commun proof edg k connect subtre parent minimum number task transmit parent subtre similarli minimum number task transmit subtre parent therefor total number taskhop minim subtre q 6 w least one commun subtre parent minimum number commun q commun subtre parent therefor total number commun minim 2 cwa mwa heurist algorithm gener abl minim commun cost howev system less equal four node algorithm minim commun cost lemma 5 cwa mwa algorithm minim commun cost system two four node proof commun cost system minim neg cycl 15 system two node cycl system four node path consist least three edg form neg cycl either cwa mwa longest path two edg therefor neg cycl 2 dem algorithm minim commun cost four node may path consist three edg 5 perform studi twa optim algorithm minim commun maxim local balanc load optim heurist algorithm cwa mwa need studi simul purpos consid test set load distribut test set load processor randomli select mean equal specifi averag number task number processor vari 4 256 averag number task averag weight per processor vari 2 100 averag weight made integ load fulli balanc first studi cwa compar perform dem cwa fulli balanc load dem case tabl show percentag fullybalanc case dem algorithm run dem algorithm differ number processor differ weight result 1000 test case number processor increas less fullybalanc case 32 processor case 64 processor fullybalanc case test set import measur schedul algorithm local cwa algorithm send necessari task processor maxim local dem algorithm result unnecessari task migrat studi local dem algorithm dem abl fulli balanc load case fullybalanc case select result averag fullybalanc case 1000 test case normal local tabl percentag fullybalanc case dem number averag weight processor 4 7430 7530 7570 7510 7450 7460 measur tdem total number nonloc task dem algorithm topt minimum number nonloc task figur 14 show normal local 4 8 16 processor fullybalanc case exist 16 processor report 0 5 10 15 20 30 40 normal local weight processor processor processor figur 14 normal local dem next compar load balanc overhead dem simpl runtim overhead load balanc decis small howev unnecessari task migrat lead larg commun overhead compar time spent load balanc decis commun time domin factor cwa hand although need time make accur load balanc decis involv less commun overhead normal commun cost measur copt normal cost weight dem 4 processor 0 5 10 15 20 30 normal cost weight dem b 8 processor 0 5 10 15 20 30 40 45 50 normal cost weight dem c processor figur 15 normal commun cost dem cwa normal cost weight 64 processor 0 5 10 15 20 30 40 45 normal cost weight b 256 processor figur normal commun cost cwa copt cdem ccwa copt number taskhop dem cwa optim algorithm respect figur 15 compar normal commun cost 4 8 processor result averag dem fullybalanc case 1000 test case number taskhop cwa four processor minimum seen commun cost dem much larger cwa figur 16 show normal commun cost cwa 64 256 processor data present averag 100 differ test case method assumpt use perform studi mwa algorithm cwa algorithm mwa abl fulli balanc load maxim local howev commun minim case normal commun cost mwa respect optim algorithm measur cmwa copt number taskhop mwa optim algorithm spectiv mention lemma 3 number taskhop mwa two four processor minimum figur 17 show normal commun cost 8 256 processor mesh organ either theta theta m2 data present averag 100 differ test case small mesh mwa provid nearli optim result cost increas number processor 0 1 2 3 4 5 6 7 8 9 normal cost weight processor processor processor 8 16 processor 0 5 10 15 20 30 40 45 50 normal cost weight processor 128 processor processor b 64 128 256 processor figur 17 normal commun cost mwa 6 previou work parallel schedul static schedul share common idea 1 2 3 4 18 util global inform achiev high qualiti load balanc parallel schedul differ static schedul three aspect first schedul activ perform runtim therefor deal dynam problem second possibl load imbal caus inaccur grain size estim correct next turn schedul third elimin requir larg memori space store task graph schedul conduct increment fashion lead better scalabl massiv parallel machin larg size applic larg research effort direct toward process alloc distribut system 7 5 6 19 20 21 22 23 recent comparison studi dynam load balanc strategi highli parallel comput given willebeeklemair reev 16 eager et al compar senderiniti algorithm receiveriniti algorithm 6 work similar assumpt includ gradient model develop lin keller 24 random alloc algorithm develop differ author quit simpl effect 25 5 26 27 adapt contract within neighborhood acwn 22 receiveriniti diffus rid 16 effect algorithm runtim parallel schedul similar dynam schedul certain degre method schedul task runtim instead compiletim schedul decis principl depend adapt runtim system inform howev substanti differ make appear two separ categori first system function user comput mix togeth dynam schedul clear cutoff system user phase runtim parallel schedul potenti offer easi manag low overhead second placement task dynam schedul basic individu action processor base partial system inform wherea parallel schedul schedul activ alway aggreg oper base global system inform categori schedul sometim refer preschedul close relat idea present paper preschedul schedul workload accord problem input fore problem whose load distribut depend input balanc static schedul balanc preschedul appli preschedul period load balanc runtim fox et al first adapt preschedul applic problem geometr structur 28 29 work also deal type problem 30 31 32 project parti autom preschedul nonuniform problem 33 dimens exchang method dem appli applic problem without geometr structur 9 conceptu design hypercub system may appli topolog kari ncube 34 balanc load independ task equal grain size method extend willebeeklemair reev 16 algorithm run increment correct unbalanc load caus vari grain size nicol propos direct map algorithm comput total number task use sumreduct 10 howev minim commun cost elimin commun conflict increment schedul nbodi simul present 35 task graph reschedul period correct load imbal howev runtim schedul yet parallel 7 conclus recent research demonstr runtim parallel schedul provid lowoverhead load balanc global load inform parallel schedul synchron approach remov stabil problem abl balanc load quickli accur parallel schedul combin advantag static schedul dynam schedul processor cooper collect load inform exchang workload parallel parallel schedul ing possibl obtain high qualiti load balanc fullybalanc load maxim local commun cost reduc significantli three algorithm tree hypercub mesh network present paper difficult develop algorithm kari ncube combin cwa mwa algorithm acknowledg author would like thank xin help discuss r schedul parallel program task onto arbitrari target machin hypertool program aid messagepass system pyrro static task schedul code gener messagepass multiprocessor applic perform analysi compiletim optim approach list schedul algorithm distribut memori multiprocessor adapt load share homogen distribut system comparison receiveriniti senderiniti adapt load share load distribut local distribut sy tem program hypercub multicomput dynam load balanc distribut memori multiprocessor commun effici global load balanc runtim increment parallel schedul rip distribut memori comput parallel approach multiprocessor schedul effici parallel schedul algorithm vector model dataparallel comput network matroid strategi dynam load balanc highli parallel comput analysi graph color base distribut load balanc algorithm dynam criticalpath schedul effect techniqu alloc task graph multiprocessor simul three adapt decentr control job schedul algorithm analysi three dynam distribut loadbalanc strategi vari global inform requir load share distribut system adapt dynam process schedul distribut memori parallel comput schedul multithread comput work steal gradient model load balanc method fine grain concurr comput random parallel branchandbound procedur random load balanc tree structur comput solv problem concurr processor parallel hierarch nbodi method lowcost hypercub load balanc algorithm partit strategi nonuniform problem multipro cessor dynam load balanc vortex calcul run multiprocessor parti parallel runtim system gener dimens exchang method load balanc kari ncube variant experi graph schedul map irregular scientif comput tr ctr chen xime lu xianliang runtim increment concentr schedul nownric acm sigop oper system review v34 n2 p8496 april 2000 hwakyung rim juwook jang sungchun kim simpl reduct nonuniform dynam load balanc quantiz load hypercub multiprocessor hide balanc overhead journal comput system scienc v67 n1 p125 august janez brest viljem umer milan ojsterek dynam schedul pc cluster proceed 1999 acm symposium appli comput p496500 februari 28march 02 1999 san antonio texa unit state wan yeon lee sung je hong jong kim sunggu lee dynam load balanc switchbas network journal parallel distribut comput v63 n3 p286298 march heejun park byung kook kim optim task schedul algorithm cyclic synchron task gener multiprocessor network journal parallel distribut comput v65 n3 p261274 march 2005 k antoni j garofalaki mourto p spiraki hierarch adapt distribut algorithm load balanc journal parallel distribut comput v64 n1 p151162 januari 2004 arnaud legrand hlne renard yve robert frdric vivien map loadbalanc iter comput ieee transact parallel distribut system v15 n6 p546558 june 2004 chingjung liao yehch chung treebas parallel loadbalanc method solutionadapt finit element graph distribut memori multicomput ieee transact parallel distribut system v10 n4 p360370 april 1999 yehch chung chingjung liao donlin yang prefix code match parallel loadbalanc method solutionadapt unstructur finit element graph distribut memori multicomput journal supercomput v15 n1 p2549 jan 2000