teach strategi memorybas control combin differ machin learn algorithm system produc benefit beyond either method could achiev alon paper demonstr genet algorithm use conjunct lazi learn solv exampl difficult class delay reinforc learn problem better either method alon class class differenti game includ numer import control problem aris robot plan game play area solut differenti game suggest solut strategi gener class plan control problem conduct seri experi appli three learn approach lazi qlearn knearest neighbor knn genet algorithm particular differenti game call pursuit game experi demonstr knn great difficulti solv problem lazi version qlearn perform moder well genet algorithm perform even better result motiv next step experi hypothes knn difficulti good exampl common sourc difficulti lazi learn therefor use genet algorithm bootstrap method knn creat system provid exampl experi demonstr result joint system learn solv pursuit game high degre accuraci outperform either method alon rel small memori requir b introduct two peopl learn task togeth benefit differ skill bring tabl result learn better would likewis machin learn method abl work togeth learn solv difficult problem paper describ lazi learn algorithm genet algorithm work togeth produc better solut either method could produc explor hypothesi two learn algorithm work togeth outperform either individu focus particular problem agent must perform task task requir sever step accomplish limit feedback well agent perform end task sever learn algorithm appli famili problem call delay reinforc problem widrow 1987 atkeson 1990 watkin 1989 barto sutton watkin 1990 millan torra 1992 moor atkeson 1993 littl done evalu power combin differ type learn algorithm problem one way character delay reinforc problem learn solv markov decis problem van der wal 1981 markov decis problem agent develop map set state set action possibl differ one state optim strategi given state depend current state action direct toward achiev goal perform task payoff penalti action award immedi delay reinforc problem appli zero payoff intermedi state appli actual reward end sequenc one class problem frequent model markovian decis problem class differenti game differenti game requir player make long sequenc move behavior strategi model differenti equat find solut differenti game consist comput valu game term expect payoff determin optim strategi player yield valu differenti game difficult solv yet import solv wide varieti multiag task widespread applic militari entertain industri recent system intellig highway air traffic control railroad monitor ship rout use differenti game theori assist agent optim often compet goal gener strategi solv game use plan intellig agent thu make approach discuss applic broader domain control problem studi begin consid differenti game involv one agent tri pursu captur anoth ie pursuit game earlier research show least one implement task known evas maneuv grefenstett ramsey schultz 1990 solv genet algorithm ga develop lazi learn approach use knearest neighbor knn task hope demonstr lazi learn could perform well better ga made task substanti harder studi limit lazi learn method class problem complic task describ section 32 also resembl complic plan task agent satisfi sever goal simultan chapman 1987 experi show success develop method solv difficult reinforc learn task key idea behind success combin use lazi learn ga observ compar two lazi method knn adapt qlearn genet algorithm lazi method learn solv task depend good exampl databas later found best learn agent first use ga gener exampl switch knn reach certain perform threshold experi demonstr signific improv perform lazi learn overal accuraci memori requir result use techniqu combin system also perform better ga alon demonstr two learn algorithm work togeth outperform either method use alon previou work recent consider work done appli learn algorithm markov decis problem date littl done appli algorithm differenti game one except grefenstett samuel system use genet algorithm addit evas maneuv task grefenstett 1991 appli samuel aerial dogfight target track ramsey grefenstett 1994 use casebas method initi samuel popul solut depend current environ use ga jump start lazi learner ramsey grefenstett use lazi learner jump start ga suggest combin strategi lazi learner ga transmit inform direct could power combin relat research gordon subramanian 1993a 1993b use approach similar explan base learn ebl incorpor advic genet algorithm use samuel ga multistrategi apprach spatial knowledg base highlevel strateg guidanc human teacher encod use rule compil operation rule encod form suitabl samuel use samuel use refin advic genet algorithm idea use lazi learn method delay reinforc task recent studi small number research atkeson 1990 employ lazi techniqu train robot arm follow prespecifi trajectori moor 1990 took advantag improv effici provid store exampl kdtree use lazi approach learn sever robot control task recent moor atkeson 1993 develop priorit sweep algorithm interest exampl q tabl focu updat mccallum 1995 develop nearest sequenc memori algorithm lazi algorithm solv control problem plagu hidden state hidden state artifact perceptu alias map state percept onetoon whitehead 1992 mccallum show algorithm lazi method reduc effect perceptu alias append histori inform state inform sinc approach store complet sequenc minim effect hidden state anoth studi aha salzberg 1993 use nearestneighbor techniqu train simul robot catch ball studi provid agent knew correct behavior robot therefor provid correct action robot made mistak approach typic nearestneighbor applic reli determin good action store exampl case idea exampl good need approach determin exampl one popular approach reinforc learn use neural network learn algorithm often error backpropag algorithm use simpl multistep control problem widrow 1987 nguyen widrow 1989 use knowledg correct control action train network millan torra 1992 use reinforc learn algorithm embed neural net control variabl permit vari continu address problem teach robot navig around obstacl consider research perform use form reinforc learn call tempor differ learn sutton 1988 tempor differ method appli reinforc throughout sequenc action predict futur reinforc appropri action perform task specif predict refin process identifi differ result tempor success action two popular tempor differ algorithm aceas barto sutton anderson 1983 barto et al 1990 qlearn watkin 1989 origin work barto et al 1983 demonstr cart pole problem could solv use method clous utgoff 1992 later use aceas separ teacher cart pole prob lem appli qlearn navig race track lin 1991 use qlearn teach robot navig hall classroom build plug wall socket recharg batteri describ lazi variant qlearn show also capabl learn complex control task addit dorigo colombetti 1994 colombetti dorigo 1994 describ approach use reinforc learn classifi system teach robot approach pursu target approach use separ reinforc program monitor perform robot provid feedback perform learn occur genet algorithm appli classifi fit determin reinforc program recent michael littman 1994 observ reinforc learn appli multiag activ context markov game littman expand watkin qlearn algorithm cover two player simplifi game soccer embed linear program determin optim strategi prior play appli modifi account competit goal player updat estim expect discount reward player recent work learn strategi game play begun deal issu colearn superfici level strateg game chess othello pell 1993 develop environ deriv strategi call symmetr chess like game metagam focus translat rule constraint game strategi use declar formul game characterist metagam appli chess checker nought cross ie tictacto go yield perform intermedi level game smith gray appli call coadapt genet algorithm learn play othello coadapt ga genet algorithm fit valu member popul depend fit member popul found abl control develop nich popul handl sever differ type oppon final tesauro use tempor differ learn tesauro 1992 neural network tesauro sejnowski 1989 train backgammon program call tdgammon backgammon stochast compon move determin part roll dice distinguish determinist game chess despit addit complex tdgammon current play master level 3 problem reinforc learn rl challeng part delay take action receiv reward penalti typic agent take long seri action reward hard decid action respons eventu payoff lazi eager approach reinforc learn found literatur common eager approach use temporaldiffer learn neural network barto et al 1983 1990 clous utgoff 1992 tesauro 1992 advantag lazi approach threefold first minim comput time requir train sinc train consist primarili store exampl tradit lazi approach knearest neighbor second lazi method shown good functionapproxim continu state action space atkeson 1992 see becom import task learn play differenti game third tradit eager approach reinforc learn assum task markov decis problem task nonmarkovian eg histori signific inform must append state encapsul prior state inform order approxim markov decis problem sinc lazi approach store complet sequenc nonmarkovian problem treat similar fashion markovian problem class rl problem studi also studi field differenti game theori differenti game theori extens tradit game theori game follow sequenc action continu state space achiev payoff isaac 1963 sequenc model set differenti equat analyz determin optim play player also interpret differenti game version optim control theori player posit develop continu time goal optim compet control law player friedman 1971 31 differenti game theori origin earli 1960 isaac 1963 framework formal analysi competit game differenti game dynam game ie behavior player model system first order differenti equat form dt set action taken p player time k vector real euclidean nspace denot posit play ie state game h j histori game jth dimens state space word differenti equat model action taken player game chang state game time game initi state game k 0 given object analyz differenti game determin optim strategi player game determin valu game ie expect payoff player assum player follow optim strategi detail see sheppard salzberg 1993 pursuit game special type differenti game two player call pursuer p evad e evad attempt achiev object frequent escap fix play arena pursuer attempt prevent evad achiev object exampl includ simpl game children game call tag popular video game pacman much complic predatorprey interact natur exampl illustr common featur pursuit gamesth pursuer evad differ abil differ speed differ defens mechan differ sens abil one classic pursuit game studi differenti game theori homicid chauffeur game game think play field open park lot singl pedestrian cross park lot singl car driver car chauffeur tri run pedestrian although car much faster pedestrian pedestrian chang direct much quickli car typic formul car pedestrian travel fix speed car fix minimum radiu curvatur pedestrian abl make arbitrarili sharp turn ie radiu curvatur zero basar olsder 1982 analyz game turn solut rel simpl depend four parametersth speed car speed pedestrian radiu curvatur car lethal envelop car ie distanc car pedestrian consid close enough hit pedestrian isaac 1963 show assum optim play player abil p captur e convers e escap depend ratio player speed p radiu curvatur intuit optim play p turn randomli line e turn sharpli otherwis optim strategi e head directli toward p insid radiu curvatur turn sharpli sinc es strategi interest focu learn evad similar game 32 evas maneuv task evas maneuv task differenti game variat homicid chauffeur game even though solut homicid chauffeur game intuit actual surfac character solut highli nonlinear thu reason expect surfac extens problem discuss paper difficult character grefenstett et al 1990 studi evas maneuv task demonstr abil genet algorithm solv complex sequenti decis make problem twodimension simul singl aircraft attempt evad singl missil initi implement pursuit game grefenstett et al later extend make substanti difficult game play occur rel coordin system center evad e rel frame refer search space reduc game determin start posit p use fix control law attempt captur e e must learn evad p even basic game difficult homicid chauffeur game pursuer variabl speed evad nonzero radiu curvatur extend version includ second pursuer make problem much harder unlik singlepursu problem twopursu problem known optim strategi imado ishihara 1993 initi state possibl escap second gave evad addit capabl onepursu game e control turn angl time step thu basic zigzag back forth make seri sharp turn path p escap twopursu game gave e abil chang speed also gave bag smoke bomb limit time help hide e pursuer definit twopursu task pursuer p1 p 2 ident maneuv sens abil use control strategi anticip futur locat e aim locat captur fewest time step begin game random locat fixedradiu circl center evad e initi speed p1 p2 much greater speed e lose speed maneuv direct proport sharp turn make maximum speed reduct 70 scale linearli turn reduct speed maximum turn angl allow 135 regain speed travel straight ahead limit fuel speed p1 p2 drop minimum threshold e escap win game e also win success evad pursuer 20 time step ie p1 p2 run fuel path either p1 p2 ever pass within threshold rang es path game e lose ie pursuer grab e figur 1 use term game includ complet simul run begin initi placement player end either win lose 20 time step later play one pursuer capabl e ident simul aircraft use grefenstett et al one pursuer e control turn angl suffici play game well two pursuer p1 p2 game e addit inform oppon inform includ 13 featur describ state game includ es speed angl previou turn game clock angl defin p 1ep 2 rang differ p1 p 2 also eight featur measur p1 p2 individu speed bear head distanc bear measur posit pursuer rel direct e face eg e face north p1 due east bear would 3 oclock head angl es direct pursuer direct flee two pursuer e adjust speed turn angl time step also period releas evad pursuer pursuer evad caught1 12334 figur 1 game e caught smoke bomb introduc nois sensor read p1 p 2 smoke releas turn angl pursuer shift random factor 50 current turn angl sever turn increas potenti effect smoke 4 learn algorithm follow section discuss detail experi three learn algorithm motiv need learn strategi combin eager learn teacher lazi learn perform explor sever algorithm determin applic lazi learn control problem gener pursuit game particular began examin abil qlearn learn play evas maneuv game adapt qlearn larg continu state space result lazi variant standard qlearn tri tradit lazi learn approach knearest neighbor final experi eager learn method genet algorithm compar two lazi method 41 lazi qlearn evas maneuv qlearn solv delay reinform learn problem use tempor differ learn rule watkin 1989 td method usual assum featur space variabl predict discret sutton 1988 tesauro 1992 qlearn repres problem use lookup tabl contain state natur caus problem larg continu state space encount differenti game therefor develop method predict reward stateact pair without explicitli gener result algorithm lazi version qlearn rather construct complet lookup tabl implement qlearn store exampl similar set instanc produc lazi method knn begin gener set action random particular game action result success evas instead algorithm appli payoff function defin determin reward sequenc stateact pair initi store actual payoff valu pair gener first set pair learn proce follow first assum neighbor state requir similar action specifi two distanc paramet one state one action 1 respect note distanc normal purpos paramet guid search instanc databas system begin evas maneuv game initi simul simul pass first state state matcher locat state databas within 1 current state state matcher fail find nearbi state action compar select action random otherwis action compar examin expect reward associ state select action highest expect reward result action pass simul game continu termin also probabl 03 gener random action regardless find tabl permit fill databas ie explor state space learn pass result action simul game continu termin point simul determin payoff q function updat databas use complet game end game system examin stateact pair game store databas stateact pair new along reward game pair alreadi exist predict reward updat follow qx predict reward state x correspond action j learn rate ae actual reward fl discount factor ey maximum q valu action associ state state state follow action appli state x reward determin use payoff function grefenstett et al 1990 name evad pursuer captur time pair game compar pair databas distanc store state action less 1 2 respect stateact pair game store stateact pair q valu updat 42 knn evas maneuv lazi learn classic approach machin learn pattern recognit commonli form knearest neighbor algorithm knn rare use markov decis problem repres pursuit game format amen algorithm success lazi approach must databas full correctli label exampl knn expect exampl label class name difficulti determin correct action store state formul markov decis problem classif problem let state variabl correspond featur exampl action correspond class typic classif task assum small set discret class assign requir quantiz state space action space instead use interpol action produc knn classifi order know correct action store state must least wait determin outcom game decid label step one exampl ad time step howev even success game e evad p sure action everi time step correct one gener construct initi databas instanc simul gener action randomli evad p complet game correspond stateact pair engag store point knn use futur game state pass simul classifi search databas k nearest neighbor select action averag associ action knn fail produc game end success evas game replay exampl gener randomli select action play end evas evas occur correspond sequenc state action ie complet game store databas evas usual occur 20 time step sinc rare lazi learner pursuer speed drop threshold thu store game typic consist 20 stateact pair implement use euclidean distanc find k nearest neighbor arithmet mean control valu determin appropri action distanc comput follow instanc attrib nearest neighbor determin simpli 8instanc e fail evad use store instanc reset game start posit gener action randomli e succe also gener random action probabl 001 regardless perform result set exampl ad databas initi experi use knearest neighbor vari k 1 5 determin yield best perform complet surpris averag control valu k 1 tend cancel valu extrem exampl three instanc indic turn 90 degre left 5 degre right 85 degre right select action would turn cours averag cyclic valu exampl 359 degre close 1 degre improv averag process might enabl k 1 perform better exampl consist randomli gener game result success e thu could assum least es action correct random game everi action taken e random databas check nearbi neighbor 43 ga evas maneuv grefenstett et al demonstr genet algorithm perform well solv singl pursuer game typic ga use rule call classifi simpl structur term anteced consequ repres binari attribut booker goldberg holland 1989 holland 1975 knowledg evas maneuv problem requir rule term numer valu therefor modifi standard ga represent oper problem use formul similar grefenstett et al 1990 call set rule plan ga plan consist 20 rule gener low 1 state 1 high 1 low n state n high n claus anteced compar state variabl lower upper bound dont care condit gener set correspond rang maxim gen eral map rule form chromosom ga store attribut bound follow action exampl suppos follow rule singl pursuer problem previou turn 90 chromosom correspond rule would associ rule rule strength associ plan plan fit popul may contain fifti plan compet ga system strength fit valu describ determin winner competit initi rule maxim gener result rule match state one rule select uniform probabl follow train game rule fire gener special ga use hillclimb modifi upper lower limit test state variabl follow lb ub lower upper bound respect rule fire state fi learn rate current state within bound predic bound shift closer state base learn rate 01 studi hand state outsid bound nearer bound adjust shift toward valu state follow game strength rule fire updat base payoff receiv game payoff use qlearn given payoff function strength rule fire game updat use profit share plan grefenstett 1988 follow c profit share rate experi ae payoff receiv estim mean strength rule oe estim varianc rule strength plan fit calcul run plan set randomli gener game comput mean payoff set test test plan highest fit use control e heart learn algorithm lie applic two genet oper mutat crossov rule within plan select mutat use fit proport select goldberg 1989 name probabl select determin strength r 8s2rule strength rule set rule plan r rule interest probabl select plan determin similarli use plan fit rather rule strength detail implement see sheppard salzberg 1993 44 result algorithm variat evas maneuv game ran ten experi produc learn curv combin result ten experi averag algorithm perform regular interv estim accuraci algorithm test result train 100 randomli gener game result qlearn experi encourag led next phase studi appli tradit lazi learn method knearest neighbor knn evas maneuv task found knn work well consid eager learn algorithm genet algorithm choic motiv previou work grefenstett et al indic ga capabl solv type task fact abl replic result onepursu problem scale ga still work quit well twopursu game 441 perform lazi qlearn onepursu task qlearn extrem well initi figur 2 reach 80 evas within first 5000 game perform flatten peak perform experi stop 90 appar plateau 5000 game 30000 game perform remain rang 8085 perform jump anoth plateau 90 remaind experi qlearn perform twopursu task also encourag reach 60 evas within 5000 game continu improv reach plateau 80 plateau maintain throughout remaind experi sinc implement qlearn use form lazylearn result led us believ might percent success store game one pursuer two pursuer figur 2 perform qlearn one twoplay pursuit games2060100 percent success store game one pursuer two pursuer figur 3 perform knn one twoplay pursuit game possibl design tradit lazi method ie knn solv evas task first howev hypothesi support see next section 442 perform knn figur 3 show well knn perform two version evas maneuv game number train exampl game increas figur compar perform two problem respect number game store game contain 20 stateact pair experi indic problem escap singl pursuer rel easi solv knn develop set exampl 95 success store approxim 1500 game eventu reach almost perfect perform distanc p e start game guarante escap alway possibl howev result disappoint e given task learn escap two pursuer fact lazi learn approach difficulti achiev level perform 45 demonstr two pursuer problem significantli difficult knn one possibl reason knn poor perform twopursu task presenc irrelev attribut known caus problem nearest neighbor algorithm aha 1992 salzberg 1991 experi method similar stepwis forward select devijv kittler 1982 determin set relev attribut howev determin relev attribut dynam environ difficult reason determin good exampl difficult know attribut use mani success exampl gener anoth possibl reason poor perform knn two pursuer task size search space onepursu problem state space contain 75 theta 10 15 point wherea twopursu evas state space 29 theta 10 33 point one pursuer game show good perform 5700 game achiev similar coverag state space twopursu game would requir roughli 54 theta 10 22 game assum similar distribut game train data like reason knn troubl conclud gener bad exampl earli phase game state lazi learner need correct action someth close store almost everi state memori strategi collect exampl play random game first store game e succeed escap howev mani action taken random game incorrect e might escap one two particularli good action game last 20 time step 20 stateact pair store sinc lazi learn approach way firstse section 52 throw away exampl collect mani bad exampl could get stuck forev low level perform 443 perform ga show result ga experi figur 4 knn ga perform well face one pursuer fact achiev near perfect perform 15000 game good perform 90 5000 game number game somewhat inflat ga evalu 50 plan gener thu count one gener 50 game fact simul ran 500 gener ie 25000 game experi strike differ perform knn genet algorithm ga learn excel strategi twopursu problem nearest neighbor percent success game one pursuer two pursuer figur 4 perform genet algorithm one twoplay pursuit game tabl 1 compar learn evas maneuv task converg algorithm one pursuer two pursuer knn 969 423 qlearn 933 817 ga 996 945 qlearn perform though much better knn still inferior ga inde ga achiev 90 success 16000 game 320 gener success rate continu improv reach approxim 95 444 compar one twopursu evas figur 5a show sampl game e evad singl pursuer give intuit strategi e learn essenti e keep turn sharpli p unabl match chang direct although three algorithm well task closer examin result reveal interest differ knn eventu reach success evas rate 9798 reach 93 evas 10000 game superior qlearn asymptot perform knn perform better ga 5000 game cours ga eventu achiev near perfect perform qlearn also learn rapidli begin exceed ga abil first 3000 game learn slow consid abli fact point ga perform nearli perfectli qlearn perform around 85 twice mani game ga qlearn achiev 91 evas still perform consider poorer ga knn tabl 1 show result compar three algorithm two evas task pursuer evad pursuer pursuer evad b two pursuer one evad one pursuer one evad figur 5 sampl game e success evad converg consid algorithm converg show improv 500 game knn qlearn 100 gener ga recogn difficulti twopursu task rel onepursu task see profound differ perform three approach see figur 5b sampl game e evad two pursuer ga start slowli outperform knn qlearn 3000 game ga began improv rapidli pass knn almost immedi catch qlearn addit 5000 game end result show ga surpass qlearn margin 11 knn margin 52 strike result though poor perform knn twopursu game next set improv figur 5 combin ga lazi learn initi surpris knn perform twopursu task attempt improv perform consid provid good exampl knn base hypothesi primari caus poor perform poor qualiti train experi lazi learn work effect control task store exampl must high probabl good one ie action associ store state correct nearli correct credit assign problem difficulti task design initi train difficult algorithm gll init popul run genet algorithm run ga one gener select best plan determin perform ga perf evalu perform experi best plan ga evad evad store exampl store 20 exampl evalu lazi test 100 game figur lazi learner contrast ga initi search wide varieti solut problem studi tend learn rapidli earli stage observ suggest twophas approach adopt first train ga use provid examplar bootstrap knn 51 bootstrap nearest neighbor bootstrap idea requir one algorithm train time commun learn second algorithm point second algorithm take later first algorithm add addit exampl altern continu combin system reach asymptot limit ga learn much better twopursu game select first learner knn second detail commun teach phase given figur 6 use approach exampl continu accumul genet algorithm learn task result train knn use ga teacher shown figur 7 call system gll first use ga use lazi learn algorithm ie knn point shown graph averag 10 trial first threshold set 0 meant ga provid exampl knn begin train second threshold set 50 permit ga achiev level success approxim equal best perform knn thu plan achiev least 50 evas allow transmit exampl knn final threshold set 90 limit exampl knn game highli train ga made decis exampl store almost immedi reach level equal best perform percent success exampl ga0 ga50 ga90 figur 7 result ga teach knn knn around 45 improv somewhat errat steadili reach perform approxim 97 success figur show perform plot number exampl store number exampl store higher number exampl store knn alon halt learn 50000 exampl consist earlier knn experi perform would 85 rang still enorm improv knn perform better ga start perform high level 70 quickli exce 90 success 50000 exampl gll obtain success rate 95 individu trial random set 100 game achiev 100 success addit learn curv much smoother indic knn probabl store mani bad exampl confirm part earlier hypothesi knn fundament problem storag bad exampl store exampl bad action take bad action later perform continu poor whenev new state similar one bad exampl final perform alway superb exceed ga 90 success rate first set exampl gll converg nearperfect perform 10000 exampl one strike observ gll perform better ga throughout learn exampl achiev 5080 success ga still achiev 210 success gll remain ahead ga throughout train even achiev 98100 evas ga still achiev around 95 evas neither ga knn abl obtain high success rate number trial 52 reduc memori size bootstrap algorithm gll perform well even small number exampl provid ga even outperform teacher ga train amount knowledg requir ga perform well task quit smallonli 20 rule store singl plan number exampl use though small comparison knn still requir significantli space time rule ga consequ decid take studi one step attempt reduc size memori store lazi learn phase gll zhang 1992 skalak 1994 pattern recognit literatur eg dasarathi 1991 algorithm reduc memori size known edit method howev lazi learn usual appli control task abl find edit method specif tie type problem therefor modifi known edit algorithm problem call result system gle ga plu lazi learn plu edit gll perform quit well describ would like reduc memori requir without significantli affect perform earli work wilson 1972 show exampl could remov set use classif suggest simpli edit would frequent improv classif accuraci way prune improv decis tree minger 1989 wilson algorithm classifi exampl data set k nearest neighbor point incorrectli classifi delet exampl set idea point probabl repres nois tomek 1976 modifi approach take sampl 1 data classifi sampl remain exampl edit proce use wilson approach ritter et al 1975 describ anoth edit method differ wilson point correctli classifi discard ritter method similar hart 1968 basic keep point near boundari class elimin exampl midst homogen region edit approach took combin edit procedur ritter et al sampl idea tomek devijv 1986 began gener ten exampl set set consist singl set exampl ga select set best perform 10000 test game case obtain nearli perfect accuraci 1700 exampl next edit memori base classifi exampl use exampl set phase use five nearest neighbor point correctli classifi delet probabl 025 probabl select arbitrarili use show perform chang edit occur prior edit pass data exampl set test use 1nn percent evas exampl figur 8 result edit exampl provid genet algorithm knn 10000 random game one complic classifi point edit class actual threedimension vector three differ action two realvalu turn angl speed one binari emit smoke clear exact match would strict constraint therefor specifi rang around 3vector within system would consid two class addit three valu normal equal effect rang measur result run gle 1700 exampl summar figur 8 logarithm scale use xaxi highlight fact accuraci decreas slowli almost exampl edit read right left graph show accuraci decreas number exampl decreas 11 exampl gle achiev better 80 evas substanti better best ever achiev knn alon 21 exampl compar size plan ga gle achiev 86 evas perform remain high level greater 90 success 66 exampl thu clear small well chosen set exampl yield excel perform difficult task furthermor small memori base guarante onlin perform knn quit fast 6 discuss conclus studi consid approach develop strategi learn play differenti game particular examin sever method learn evas strategi pursuit game optim strategi differenti game determin solv system differenti equat even simpl game result strategi complex mani game known closedform solut illustr complex differenti strategi pursuit game experi demonstr abil three algorithm ga knn lazi q learn perform well simpl task one pursuer show increas difficulti ad second pursuer advers affect two algorithm knn lazi qlearn knn particular consider difficulti scale complex task thu left question whether even possibl lazi learn techniqu knearest neighbor perform well problem type motiv second phase studi use gabas teacher train knn twopursu task experi report show possibl use genet algorithm conjunct lazi learn produc agent perform well difficult delay reinforc learn problem experi also demonstr clearli power teacher sourc good exampl lazi learn method complex control task teacher probabl necessari compon lazi memorybas method experi show genet algorithm use learn plan control law complex domain train lazi learner use learn rule gener good exampl result hybrid system outperform parent system hybrid approach cours appli mani way exampl standard qlearn notori slow converg approach could use acceler one surpris result perform gll outperform ga point train hypothes best exampl given gener pass knn rather experi ga gener fact gll outperform ga right away indic perhap could use teach ga instead way around addit found edit exampl set produc rel small set exampl still play game extrem well make sens sinc edit serv identifi strongest exampl databas given poor exampl still like includ earli stage learn might possibl care edit reduc size memori even question relat theoret work salzberg et al 1991 studi question find minimals train set use help teacher explicitli provid good exampl help teacher similar oracl use clous utgoff 1992 except provid theoret minim number exampl requir learn 7 next step current implement take first step toward truli combin learn system two learner would assist learn task approach use one algorithm start learn process hand result first algorithm second algorithm continu learn task envis gener architectur differ learn algorithm take turn learn depend one learn effect given time architectur expand capabl learn algorithm tackl increasingli difficult control problem one possibl futur direct use genet oper method directli exampl lazi learn approach rather produc rule begin set exampl mutat directli use genet oper evolv databas ie exampl set perform task one approach might examin frequenc exampl use success evad select frequent use exampl exampl convert plan ga specifi rang attribut exampl result new set rule suffici construct plan new plan seed popul ga use gener problem determin optim strategi differenti game complex solv game involv solv system differenti equat learn solut game involv simultan learn player mean player must learn highli dynam environ rather player learn counter singl constant strategi player must adapt strategi chang strategi oppon environ one must avoid prematur converg fix solut studi problem build environ analyz learn algorithm multiag environ specif wish explor effect sequenti decis make sever agent learn time explor abil agent appli one approach learn evas tactic anoth agent use perhap differ approach develop pursuit strategi also pit strategi studi whether singl learn algorithm develop multipl solut reactiv control task acknowledg thank david aha anonym review special issu mani valuabl comment earlier draft paper also thank diana gordon john grefenstett simon kasif sk murthi help comment idea form stage work materi base upon work support part nation scienc foundat grant no iri9116843 iri9223591 r learn catch appli nearest neighbor algorithm dynam control task toler noisi use local model control movement neuronlik adapt element solv difficult learn control problem learn sequenti decis make gabriel dynam noncoop game theori classifi system genet algorithm plan conjunct goal train agent perform sequenti behavior edit rate multiedit algorithm pattern recognit statist approach robot shape develop autonom agent learn differenti game genet algorithm search credit assign rule discoveri system base genet algo rithm lamarkian learn multiag environ learn sequenti decis rule use simul model competit condens nearest neighbor rule differenti game mathemat theori applic warfar topic program robot use reinforc learn teach markov game framework multiag reinforc learn reinforc connectionist approach robot path find nonmazelik environ empir comparison prune method decis tree induct effici memorybas learn robot control priorit sweep reinforc learn less data less time truck backerupp exampl self learn neural network strategi gener evalu metagam play case base reason paper distanc metric instancebas learn learn help teacher prototyp featur select sampl random mutat hill climb algorithm learn predict method tempor differ practic issu tempor differ learn parallel network learn play backgammon experi edit nearestneighbor rule stochast dynam program learn delay reward reinforc learn adapt control percept action asymptot properti nearest neighbor rule use edit data select typic instanc instancebas learn tr edit rate multidit algorithm plan conjunct goal credit assign rule discoveri system base genet algorithm parallel network learn play backgammon classifi system genet algorithm use local model control movement adapt natur artifici system practic issu tempor differ learn reinforc connectionist approach robot path find nonmazelik environ teach method reinforc learn select typic instanc instancebas learn toler noisi irrelev novel attribut instancebas learn algorithm reinforc learn adapt control percept action priorit sweep train agent perform sequenti behavior robot shape genet algorithm search optim machin learn empir comparison prune method decis tree induct learn sequenti decis rule use simul model competit learn predict method tempor differ distanc metric instancebs learn ctr john w sheppard colearn differenti game machin learn v33 n23 p201233 novdec 1998