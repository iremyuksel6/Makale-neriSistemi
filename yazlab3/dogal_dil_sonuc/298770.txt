coevolut success learn backgammon strategi follow tesauro work tdgammon use 4000 paramet feedforward neural network develop competit backgammon evalu function play proce roll dice applic network legal move select posit highest evalu howev backpropag reinforc tempor differ learn method employ instead appli simpl hillclimb rel fit environ start initi champion zero weight proceed simpli play current champion network slightli mutat challeng chang weight challeng win surprisingli work rather well investig peculiar dynam domain enabl previous discard weak method succeed prevent suboptim equilibria metagam selflearn b introduct took great chutzpah gerald tesauro start wast comput cycl tempor differ learn game backgammon tesauro 1992 let machin learn program play hope becom expert inde dream comput master domain selfplay introspect around sinc earli day ai form part samuel checker player samuel 1959 use donald michi menac tictacto learner michi 1961 selfcondit system later gener abandon field due problem scale weak nonexist intern represent moreov self play learner usual develop eccentr brittl strategi appear clever fare poorli expert human comput player yet tesauro 1992 result show selfplay approach could power refin million iter selfplay tdgammon program becom one best backgammon player world tesauro 1995 deriv weight view corpor signific enough intellectu properti keep trade secret except leverag sale minor oper system intern busi machin 1995 other replic td result backgammon research purpos boyan 1992 commerci purpos reinforc learn limit success area zhang dietterich 1996 crite barto 1996 walker et al 1994 respect goal selforgan learn machin start minim specif rise great sophist tdgammon stand alon success understood explain replic domain hypothesi success tdgammon princip due backpropag reinforc temporaldiffer technolog inher bia dynam game backgammon coevolutionari setup train task dynam chang learn progress test hypothesi use much simpler coevolutionari learn method backgammon name hillclimb 2 implement detail use standard feedforward neural network two layer sigmoid set fashion tesauro 1992 4 unit repres number player piec 24 point plu 2 unit indic mani bar board addit ad one unit report whether game reach endgam race situat make total 197 input unit fulli connect 20 hidden unit connect one output unit judg posit includ bia hidden unit make total 3980 weight game play gener legal move convert proper network input pick posit judg best network start weight set zero initi algorithm hillclimb 1 add gaussian nois weight 2 play network mutant number game 3 mutant win half game select next gener nois set step would 005 rm distanc euclidean distanc divid surprisingli work reason well network evolv improv rapidli first sank mediocr problem perceiv compar two close backgammon player like toss bias coin repeatedli may take dozen even hundr game find sure better replac welltest champion danger without enough inform prove challeng realli better player lucki novic rather burden system much comput instead introduc follow modif algorithm avoid buster dougla effect 2 firstli game play pair order play revers random seed use gener dice roll game wash unfair due dice roll two network close particular ident result would alway one win though admittedli make differ move earli game good dice roll particular move one game may turn bad roll correspond move parallel game secondli challeng win contest rather replac champion challeng instead make small adjust direct champion idea similar inertia term backpropag rumelhart et al 1986 introduc assumpt small chang weight would lead small chang decisionmak evalu function bite ear challeng ad champion current decis preserv would less like catastroph replac champion lucki novic challeng initi stage evolut two pair parallel game play challeng requir win 3 4 game although would like rank player player use neurogammon gammontool avail us figur 1 show first 35000 player rate pubev moder good publicdomain player train tesauro use human expert prefer three thing note 1 percentag win pubev increas 0 33 20000 gener 2 frequenc success challeng increas time player improv 3 epoch eg start 20000 perform pubev begin falter first fact show simpl self 2 buster dougla world heavyweight box champion 9 month 1990 play hillclimb capabl learn second fact quit counterintuit expect player improv would harder challeng true respect uniform sampl 4000 dimension weight space true sampl neighborhood given player player good part weight space small chang weight lead mostli similar strategi one make mostli move situat howev game use determin rel fit increas rate chang allow system drift may account subsequ degrad performanceto counteract drift decid chang rule engag evolut proce accord follow anneal schedul 10000 gener number game challeng requir win increas 3 4 5 6 70000 gener increas 7 8 cours bout abandon soon champion one game make averag number game per gener consider less 8 number 10000 70000 chosen ad hoc basi observ frequenc success challeng buster dougla effect particular run later experi show determin anneal schedul principl manner see section 32 100000 game use simpl hillclimb develop surpris player capabl win 40 game pubev network sampl everi 100 gener order test perform network gener 1000 10000 100000 extract use benchmark figur 2 show percentag win sampl player three benchmark network note three curv cross 50 line 1 10 100 respect show gener improv time endgam backgammon call bearoff use anoth yardstick progress learn bearoff occur player piec home board first 6 point dice roll use remov piec figur 1 percentag win first 35000 gener player pubev match consist 200 game gener win board test network abil endgam set race board two piec player 1 7 point one piec 8 point graph figur 3 show averag number roll bearoff network play use fix set 200 random dicestream note pubev stronger 166 roll discuss strength tesauro 1992 result section 5 figur 2 percentag win benchmark network 1000 upper 10000 middl 100000 lower show noisi nearli monoton increas player skill evolut proce win gener gener figur 3 averag number roll bearoff gener sampl 200 dicestream pubev averag 166 roll task 3 analysi 31 learnabl unlearn learnabl formal defin time constraint search space hard randomli pick 4000 floatingpoint weight make good backgammon evalu simpli imposs hard find weight better current initi weight random quit easi play improv would expect get harder harder perhap similar probabl tornado construct 747 junkyard howev search neighborhood current weight find mani similar player make mostli move capit other slightli differ choic expos weak tournament note differ point tesauro origin made feedforward neural network could exploit similar posit although set paramet initi run involv guesswork larg set player examin tri understand phe nomenon take champion network gener 1000 10000 100000 run sampl random player neighborhood differ rm distanc find like find win challeng thousand random neighbor 11 differ rm distanc play 8 game correspond champion figur 4 plot fraction game challeng function rm distanc graph show player improv time probabl find good challeng neighborhood increas account frequenc success challeng goe 3 success challeng requir 3 number good challeng neighborhood go algorithm falter nonetheless sever factor requir studi may due gener growth weight less variabl strategi among matur player less abil simpli tell expert player apart game figur 4 distanc versu probabl random challeng win champion gener 1000 10000 100000 distanc champion 100k win challeng take small step chang move champion order beat hope coevolut appar unlearn becom learnabl convert singl question continu stream question one depend previou answer 32 replic experi first success run tri evolv ten player use paramet anneal schedul 10000 70000 found one ten player even competit closer examin suggest nine run fail anneal earli frequenc success challeng reach appropri level prematur anneal made task challeng even harder challeng success rate fell even lower therefor abandon fix anneal schedul instead anneal whenev challeng success rate exceed 15 averag 1000 gener ten player evolv regim competit though quit good origin player appar benefit extra induct bia due tailormad anneal schedul refin heurist schedul could lead superior player goal 33 rel versu absolut expertis backgammon allow rel expertis absolut optim strategi theoret exist perfect polici backgammon would deliv minimax optim move posit perfect polici could exactli rate everi player linear scale practic especi without run 10000 game verifi seem mani rel cycl help prevent earli converg cellular studi iter prison dilemma follow axelrod 1984 stabl popul tit tat invad cooper allow exploit defect kind relativeexpertis dynam seen clearli simpl game rockpaperscissor littman 1994 might initi seem bad selfplay learn look like advanc might actual lead cycl mediocr small group champion domin circl aris hold tempor oligopoli prevent advanc hand may basic form instabl prevent format suboptim oligopoli allow learn progress problem specif nonzerosum game zero sum game appropri use selfplay shown converg optim play parti 4 discuss believ evid success learn backgammon use simpl hillclimb rel fit environ indic reinforc tempor differ methodolog use tesauro 1992 paper led tdgammon provid advantag essenti success rather major contribut came coevolutionari learn environ dynam back gammon result thu similar bia found mitchel et al packard evolut cellular automata edg chao packard 1988 mitchel et al 1993 obvious suggest 11 hillclimb advanc machin learn techniqu other bring mani task without intern cognit oppon behavior coevolut usual requir popul therefor must someth domain help permit td learn hillclimb succeed selfplay would clearli fail problemsolv task scale section discuss issu coevolutionari learn dynam backgammon may critic learn success 41 evolut versu coevolut tdgammon major mileston kind evolutionari machin learn initi specif model far simpler expect learn environ specifi implicitli emerg result coevolut learn system train environ learner embed environ respond improv hope neverend spiral though elus goal achiev practic coevolutionari effect seen popul model complet unexpect 11 hillclimb evolut coevolut explor sort network problem hilli 1992 tictacto strategi game angelin pollack 1994 rosin belew 1995 schraudolph et al 1994 predatorprey game cliff miller 1995 reynold 1994 classif problem intertwin spiral problem juill pollack 1995 howev besid tesauro tdgammon date view instanc coevolutionari learn sim artifici robot game sim 1994 domain complex backgammon substanti success sinc weak player sometim defeat strong one theori possibl network learn backgammon static evolutionari environ play fix oppon rather coevolutionari one play cours interest acheiv learn without expert hand tdgammon simpli learn neurogammon wouldnt startl result order isol contribut coevolutionari learn modifi train setup origin algorithm appropri selfplay new setup current champion mutant play number game oppon call foil dicestream weight adjust champion lose game mutant win number pair game initi set 1 increment whenev challeng success rate exceed 15 averag 1000 gener lower three plot figur 5 track perform algorithm three benchmark network origin experi act foil seem show relationship learn rate probabl win weak foil 1k learn fast initi probabl win around 50 taper probabl increas strong foil 100k learn slow initi probabl win small speed increas toward 50 evolutionari run outperform coevolutionari version foil algorithm coev champion network play role foil coevolut seem maintain high learn rate throughout run automat provid new gener player oppon appropri skill level keep probabl win near 50 moreov weak foil less like bia learn process automat correct coevolut proce see also section 43 42 dynam backgammon gener problem learn selfplay discov repeatedli earli ai ml learner could keep play kind game explor narrow region strategi space miss critic area game would vulner program human expert problem particularli preval determinist game chess tictacto tesauro 1992 point featur backgammon make suitabl approach involv selfplay random initi condit unlik chess draw imposs game play untrain network make random move eventu termin though may take much longer game compet player moreov random dice roll lead selfplay much larger part search space would like explor determinist game work use popul get around limit selfplay angelin pollack 1994 schraudolph et al 1994 ad nondetermin game go choos move accord boltzmann distribut statist mechan other fogel 1993 expand explor forc initi move epstein 1994 studi mix train use selfplay random test play expert order better understand aspect game learn gener figur 5 perform pubev player evolv play benchmark network origin run gener 1k 10k 100k compar coevolutionari variant algorithm plot averag four run perform origin algorithm includ comparison origin coev 100k believ enough add random game forc explor altern train paradigm someth critic dynam backgammon set apart game random element like monopoli name outcom game continu uncertain contact broken one side clear advantag monopoli earli advantag purchas properti lead accumul return mani observ find excit backgammon help novic sometim overcom expert number situat one dice roll improb sequenc dramat revers player expect win order quantifi revers effect collect statist game play 100000th gener network n 0 120 collect 100 differ game still contact move n n6 100 game reach race stage move n still move number standard deviat 12005move number contact race game probabl figur standard deviat probabl win contact posit race posit contact race figur b probabl game still contact race stage move n figur 7 smooth distribut probabl win function move number contact posit left race posit right densiti densiti probabl win move number move probabl progress estim probabl win 100 posit play 200 differ dicestream figur 6 show standard deviat probabl assum mean 05 function n well probabl game still contact race stage move n figur 7 show distribut probabl win function move number symmetr smooth convolut gaussian function data indic probabl win tend hover near 50 earli stage game gradual move play proce typic remain within rang 15 85 long still contact thu allow reason chanc revers number could differ player less revers stronger player perhap weaker one believ effect remain integr part game dynam regardless expertis conjectur dynam facilit learn process provid almost everi situat nontrivi chanc win nontrivi chanc lose therefor potenti learn consequ current move deep contrast mani domain earli blunder could lead hopeless situat learn virtual imposs reward alreadi becom effect unat tainabl seem featur backgammon may also share task tdlearn success zhang dietterich 1996 crite barto 1996 walker et al 1994 43 avoid suboptim equilibria metagam learn learn system view interact teacher student teacher goal expos student weak correct student goal placat teacher avoid correct build model teacherstud interact formal game call metagam learn mgl avoid confus game learn metagam teacher present student sequenc question prompt respons r student backgammon domain question respons would legal posit roll move receiv payoff process attempt maxim choic question answer limit abil selfmodif gener assum goal learn prepar student interact complex environ e provid object measur perfor manc 4 e thu play similar role assum ident question find payoff matrix enabl perform continu improv measur e reward close correl may tempt ask question easi anticorrel exampl te question might difficult either case hard learn see section 41 4 gener theori evolut selforgan e necessari attract solut problem two student play role teacher inde singl student act teacher thu provid question alway appropri level difficulti dynam mgl selfteach coevolutionari situat would hope lead continu spiral improv may instead get bog antagonist collus dynam depend payoff structur hillclimb setup may think mutant teacher tri gain advantag adjust weight exploit weak champion champion student tri avoid adjust allow weak exploit sinc student teacher approxim equal abil advantag student narrow scope search thu limit domain within teacher abl look weak game chess tictacto student could achiev aim draw instead win alway play particular style game draw allow teacher student may figur way collud exampl throw altern game angelin 1994 make suboptim sequenc earli move effect selflearn system may appear earli converg evolutionari algorithm narrow scope draw collus teacher student fact nash equilibria mgl call mediocr stabl state 5 hypothesi certain featur backgammon oper format mediocr stabl state mgl backgammon ergod sens posit reach posit 6 sequenc move dice roll appar creat enough random prevent either player follow strategi narrow scope game appreci moreov earli suboptim move unlik provid oppon easi win see section 42 collus throw altern game prevent mediocr stabl state also aris human educ system exampl student get answer right reward teacher posit teach evalu ask harder question work hope appli kind mgl equilibrium analysi issu human educ 5 conclus tdgammon remain tremend success machin learn caus success well understood fundament research tesauro 1992 paper basi tdgammon reportedli beat sun gammontool 60 65 time depend number hidden unit achiev pariti neurogammon 10 follow semin 1992 paper tesauro incorpor number handcraft expertknowledg featur eventu engin network achiev world 5 mss follow maynard smith ess maynard smith 1982 6 except race situat posit piec play master level play tesauro 1995 featur includ concept like exist prime probabl blot hit probabl escap behind oppo nent barrier evalu function also improv use multipl pli search best player weve abl evolv win 45 time pubev believ level tesauro 1992 network tesauro never compar 1992 network pubev use gammontool heurist endgam rate level play achiev player somewhat murki test procedur play game network becom race use gammontool algorithm move side end also penal td net learn rather poorli race phase gamep 272 compar network perform pubev must note use network weak endgam rather substitut much stronger expert system like gammontool gerald tesauro commentari issu gracious clear matter compar pubev 1992 result differ somewhat conclus two phenomena fom 1992 paper relev work perform 248posit race test set reach 65 substanti wors race specialist describ previou section p train time order 50000 train game network game 20hidden unit net 200000 game 40hidden unit net p 273 achiev similar level skill observ phenomena train endgam weak converg believ achiev result substanti similar tesauro 1992 result without advanc learn algorithm could make stronger player tune learn paramet ad input featur point claim 100000th gener player anywher near good current enhanc version tdgammon readi challeng best human surprisingli good consid humbl origin hillclimb rel fit measur tune paramet ad input featur would make power player point studi also claim anyth wrong td learn hillclimb good reinforc learn gener cours isnt point environ represent refin work well machin learn method benchmark weakest possibl algorithm credit learn power properli distribut notic sever weak player stem train yet reward punish doubl tripl cost associ sever loss gammon backgammon take account gambl process doubl continu develop player sensit issu game interest player challeng 100000th network use web browser home page conclus replic tesauro 1992 tdgammon success much simpler learn paradigm find reinforc tempor differ method primari caus success rather dynam backgammon combin power coevolutionari learn isol featur backgammon domain enabl coevolutionari reinforc learn work well may lead better understand condit nece sari gener complex selforgan acknowledg work support onr grant n000149610418 krasnow foundat postdoctor fellowship thank gerri tesauro provid pubev subsequ mean calibr jack laurenc pablo fune develop www front end evolv player comment brandei demo group anonym refere justin boyan tom dietterich lesli kaelbl brendan kitt michael littman andrew moor rich sutton wei zhang r competit environ evolv better solut complex task altern interpret iter prison dilemma evolut nonmutu cooper evolut cooper modular neural network learn contextdepend game strategi track red queen measur adapt progress coevolutionari simul improv elev perform use reinforc learn massiv parallel genet program markov game framework multiag reinforc learn algorithm sequenti decis make revisit edg chao evolv cellular automata perform comput adapt toward edg chao studi machin learn use game checker tempor differ learn posit evalu game go evolv 3d morpholog behavior competit learn predict method tempor differ connectionist learn expert prefer comparison train practic issu tempor differ learn tempor differ learn tdgammon tempor differ tr ctr gerald tesauro comment coevolut success learn backgammon strategi machin learn v32 n3 p241243 sept 1998 david b fogel beyond samuel evolv nearli expert checker player advanc evolutionari comput theori applic springerverlag new york inc new york ny ji grim petr somol pavel pudil probabilist neural network play learn tictacto pattern recognit letter v26 n12 p18661873 septemb 2005 multiag system integr reinforc learn bid genet algorithm web intellig agent system v1 n34 p187202 decemb multiag system integr reinforc learn bid genet algorithm web intellig agent system v1 n34 p187202 march gerald tesauro program backammon use selfteach neural net artifici intellig v134 n12 p181199 januari 2002 elizabeth sklar mathew davi multiag simul learn environ proceed fourth intern joint confer autonom agent multiag system juli 2529 2005 netherland yeo keun kim jae yun kim yeongho kim tournamentbas competit coevolutionari algorithm appli intellig v20 n3 p267281 mayjun 2004 elizabeth sklar mathew davi min san tan co sime simul educ multi agent system proceed third intern joint confer autonom agent multiag system p9981005 juli 1923 2004 new york new york edwin de jong maxsolv algorithm coevolut proceed 2005 confer genet evolutionari comput june 2529 2005 washington dc usa jordan b pollack hod lipson gregori hornbi pablo fune three gener automat design robot artifici life v7 n3 p215223 summer 2001 pablo fune jordan pollack evolutionari bodi build adapt physic design robot artifici life v4 n4 p337357 octob 1998 fran oliehoek edwin de jong niko vlassi parallel nash memori asymmetr game proceed 8th annual confer genet evolutionari comput juli 0812 2006 seattl washington usa jordan b pollack hod lipson sevan ficici pablo fune greg hornbi richard watson evolutionari techniqu physic robot creativ evolutionari system morgan kaufmann publish inc san francisco ca 2001 edwin de jong monoton archiv paretocoevolut evolutionari comput v15 n1 p6193 spring 2007 john cartlidg seth bullock combat coevolutionari disengag reduc parasit virul evolutionari comput v12 n2 p193222 june 2004 stephan k chalup alan blair increment train first order recurr neural network predict contextsensit languag neural network v16 n7 p955972 septemb edwin de jong jordan b pollack ideal evalu coevolut evolutionari comput v12 n2 p159192 june 2004 cooper multiag learn state art autonom agent multiag system v11 n3 p387434 novemb 2005 dars bill lourd pea jonathan schaeffer duan szafron learn play strong poker machin learn play game nova scienc publish inc commack ny 2001