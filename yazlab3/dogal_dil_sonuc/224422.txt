hpf compil ibm sp2 describ phpf research prototyp hpf compil ibm sp seri parallel machin compil accept input fortran 90 fortran 77 program augment hpf direct sequenti loop automat parallel compil support symbol analysi express allow paramet number processor unknown compiletim without significantli affect perform commun schedul comput guard gener parameter form compiletim sever novel optim improv version wellknown optim implement phpf exploit parallel reduc commun cost optim includ elimin redund commun use dataavail analysi use collect commun new techniqu map scalar variabl coarsegrain wavefront commun reduct multidimension shift commun present experiment result wellknown benchmark routin result show effect compil gener effici code hpf program b introduct fortran alway synonym fast execut high perform fortran hpf 13 8 defin set direct extens fortran facilit perform portabl fortran program compil largescal multiprocessor architectur preserv sharedaddress space program model unfortun hpf compil appear rapidli origin hope accept high qualiti compil hpf evolv time experi complex compil hpf result ffl abil perform commun optim essenti high perform singl innerloop commun result signific loss perform hpf perform approach handcod perform sophist optim implement ffl implement everi featur fortran affect distribut data across differ address space fortran data primarili static hpf data must dynam alloc sinc array redistribut even without redistribut number processor gener known compiletim size local array partit known static thu addit loop parallel hpf requir ibm tj watson research po box 704 yorktown height ny 10598 author reach email fmguptamidkiffschnbrgshieldskywchingtangogwatsonibmcom correspond author reach schnbrgwatsonibmcom ibm softwar solut divis 1150 eglinton ave east north york ontario canada m3c 1v7 author reach email seshadrivnetibmcom enhanc runtim model new implement featur common block data statement block data array address ffl hpf extrem highlevel languag amount generatedexecut code per sourc line gener larger common program languag pose challeng code gener runtim librari design tool develop ffl addit gener scalabl code hpf compil must gener code good uniprocessor node perform particular hpfspecif transform must inhibit uniprocessor optim spmd overhead must small paper describ phpf hpf compil ibm sp2 architectur develop past two year 1 phpf consist extens hpf subset 13 8 exist fortran 90 7 compil paper focu novel aspect optim experi set benchmark although phpf still develop featur describ paper fulli implement sever hpfrelat compil effort previous describ 11 23 17 19 3 15 18 compil uniqu follow combin featur ffl phpf exploit data parallel fortran 90 array languag also perform program analysi effect parallel loop fortran 77 code ffl phpf perform symbol analysi gener effici code presenc static unknown paramet like number processor array size rather leav task determin commun schedul runtim phpf gener closedform parameter express repres commun data set commun processor 2 ffl along wellknown commun optim like messag vector 11 23 exploit collect commun 16 9 phpf also perform aggress optim like elimin redund commun reduc number messag multidimension shift commun coarsegrain wavefront phpf also use novel techniqu map scalar variabl expos parallel reduc commun cost rest paper organ follow section 2 present overview architectur entir compil spmdizer detail section 3 describ set optim perform spmdizer section 4 give perform result set benchmark section 5 describ compil hpf similar languag discuss literatur final section 6 present conclus idea futur work compil framework phpf implement incorpor new spmdizer compon exist fortran 90 compil shown figur 1 addit frontend extens requir process hpf direct impact hpf compon fortran 90 compil small scalar requir signific modif 1 prototyp develop project ibm researchphpf current support block cyclic blockcycl distribut array languag scalar hpf spmdizer local optim optim backend fortran 90hpf frontend depend analyz loop transform hpf runtim figur 1 architectur phpf compil 21 architectur overview phpf nativ compil preprocessor preprocessor portabl easier build nativ compil gener prefer obtain highlytun perform also easier write applic debugg nativ compil preprocessor compil process summar consid output phase figur 1 ffl front end produc highlevel intermedi represent includ intern form fortran 90 array languag hpf direct ffl scalar compil intern array languag intern fortran 77 scalar form ffl spmdizer interpret hpf direct produc spmd singlenod program data comput partit commun code resolv nonloc data refer ffl local optim perform loop reorder transform uniprocessor program better util cach regist ffl back end perform tradit optim uniprocessor program scalar inlin fortran 90 intrins function whenev possibl inlin intrins function possibl elimin extra array temporari copi achiev perform fortran 90 program compar fortran 77 program hpf extens hpf spmdizer data partit preprocess commun analysi dataflow depend analyz loop transform comput partit commun code gener data partit postprocess figur 2 architectur phpf spmdizer scalar ensur inform parallel implicit fortran 90 construct reduct oper lost inlin record addit inform need later scalar also assign suitabl distribut array temporari creat scalar izat placement spmdizer scalar signific spmdizer process fortran 77 program scalar fortran 90 program uniform manner similarli local optim process spmdize uniprocessor input program uniform way modular function simplifi implement without sacrif perform spmdizer place commun outsid loop legal local optim subsequ abl reorder inner communicationfre loop better cach util fur thermor spmdize form new loop bound parallel loop facilit easi identif conform loop enabl loop fusion transform improv data local distribut array spmdizer shrink distribut dimens addressspac hole data would otherwis reduc spatial local compil includ dataflow analysi data depend analysi loop transform modul enabl varieti optim perform differ compil stage exampl scalar use data depend inform elimin array temporari otherwis need preserv fortran 90 array languag semant 7 data depend analysi loop transform also critic local commun optim structur spmdizer shown figur 2 illustr compil process particular differ phase spmdizer help exampl integ a100 integ b100100 figur 3 fortran 90 exampl spread 511001 611001 figur 4 scalar spread program 22 hpf compil exampl figur 3 show simpl fortran 90 spread program hpf direct figur 4 show result scalar program naiv scalar spread oper would result gener 2dimension temporari array runtim call librari spread routin scalar gener effici inlin code spread oper without creat new temporari variabl figur 5 show pseudocod output produc spmdizer scalar program shown figur 4 use exampl illustr spmdizer phase data partition distribut array may static commonblock array transform fortran 90 pointer alloc dynam librari routin hpf alloc array shrunk accord processor local block size adjust bound local array within global address space exampl 4 processor bound second processor 2650 use fortran 90 dynam pointer alloc alloc shrunk array spmdizer need perform globaltoloc address remap unlik variou hpf preprocessor 15 18 3 form output code gener enough repres miss compiletim inform exampl even though global array bound b given number processor given processorloc block size henc local array bound known compiletim believ numberofprocessor loopbound arraybound inform commonli unknown compiletim phpf design import compil paramet treat gener express rather constant storag map array alway dynam alloc minim observ perform loss commun analysi gener read refer program commun analysi phase determin whether requir interprocessor commun com 3 cyclic distribut requir subscript modif except integ pointer call hpfgetnumprocs2numprocspid call hpfallocateb globalbound blocksiz call hpfallocatea commun call hpfallocatecomputationbufferbuffercbsect pid2 le 99 blocksize2 pid1 le 99 blocksize1 call hpfbcastsectionasendsect buffer i8iownlbound2min0iownubound21001 i9iownlbound1min0iownubound11001 call deallocatebuff figur 5 spmdize spread program munic need analysi determin placement commun ii commun primit carri data movement processor grid dimens phpf extend analysi describ 9 handl scalar incorpor addit possibl regard distribut array dimens name replic map constant processor posit code gener identifi processor data element need particip commun gener call runtim commun routin accord commun requir differ grid dimens figur 4 assign row b sinc align first row b commun analysi select broadcast along first dimens processor grid data processor particip commun specifi array section use globaladdressspac coordin array section cb section specifi bound comput buffer store nonloc data receiv anoth processor array send section specifi processorloc section broadcast send processor comput buffer bound also specifi global address space comput partit comput partit shrink loop bound introduc statement guard necessari distribut comput among differ processor ownercomput rule 11 gener use except reduct comput assign scalar condit describ section 3 hpf runtim librari runtim librari alloc partit array perform local data movement commun provid high level interfac compil specifi commun base global indic data indic correspond array data distri bution processor data processor repres multidimension section possibl nonunit stride buffer alloc pack unpack need linear data commun perform runtim routin runtim system also perform optim like overlap unpack oper nonblock receiv wait complet multipl receiv runtim librari portabl across differ basic commun system current support ibm mpl mpi librari runtim system also provid detail perform statist trace inform debug hand trace gener program visual tool 12 optim spmdizer phpf perform sever optim reduc commun cost overhead guard introduc comput partit optim wellknown discuss literatur 11 23 17 19 3 mani other uniqu phpf messag vector move commun outsid loop amort startup cost send messag recogn extrem import optim phpf use depend inform determin outermost loop level commun place applic optim improv ffl loop distribut preliminari analysi commun comput partit guard use guid select loop distribut phpf use data control depend inform first identifi program structur maxim loop distribut align di ai 1 commun di commun di i1n figur enabl messag vector loop distribut form strongli connect compon scc program depend graph sinc unnecessari loop distribut hurt cach local also redund messag elimin current recogn redund messag differ loop nest phpf identifi scc loop distribut expect improv perform scc identifi innerloop commun move loop distribut mutual differ local iter set obtain comput partit case loop distribut reduc commun cost overhead comput partit exampl figur 6 commun refer di move outsid iloop result loop distribut ffl exploit independ direct compil assum loopcarri depend loop mark independ programm often allow commun move outsid loop static analysi inform imprecis collect commun phpf use techniqu 9 identifi highlevel pattern collect data movement given refer inform use recogn opportun use collect commun primit like broadcast reduct gener effici sendrec code special commun pattern like shift effect analysi improv ffl idiomrecognit reduct current phpf recogn sum product min max reduct fortran 77 code sinc fortran 90 reduct oper handl inlin inform reduct oper gather idiomrecognit inlin repres uniformli commun analysi exploit inform gener parallel code local reduct follow global reduct effici commun ffl symbol analysi sinc data distribut paramet block size often unknown eg number processor specifi static compil use symbol analysi oper like check equal express check one express integr multipl anoth express enabl phpf gener effici code exampl detect absenc commun symbol test strictli synchron properti 9 array refer elimin redund commun uniqu featur phpf analysi avail data processor owner 10 enabl detect redund align aij bij align di bi1 figur 7 exampl redund commun program ref comm ref redund elim redund elim redund comm grid block block 15 11 267 ncar block block 45 cmslow 44 21 523 tabl 1 result optim elimin redund commun commun compil infer data commun alreadi avail intend receiv due prior commun exampl figur 7 commun refer ai1n statement 2 made redund commun refer ai1n 1 implement simplifi version analysi present 10 elimin redund commun find redund commun within singl loop nest advantag analysi perform high level henc larg independ commun code gener tabl 1 summar result obtain phpf benchmark program describ section 4 tabl show static count number refer requir commun optim elimin redund commun show compil quit success identifi redund commun optim nearestneighbor shift commun phpf compil employ sever techniqu optim shift commun occur frequent mani scientif applic scalar optim number temporari array introduc handl fortran 90 shift oper follow optim perform reduc commun cost figur 8 exampl nearestneighbor shift commun figur 9 exampl wavefront comput ffl messag coalesc consid program segment figur 8 commun two rh refer signific overlap neither complet cover augment data commun bi1j1 extra data need bi 1j separ commun bi1j avoid phpf commun analysi identifi data movement bi1j shift first dimens intern data movement idm second dimens data movement bi1j1 shift dimens recogn commun bi1j1 domin due interprocessor commun instead idm second dimens phpf drop commun bi1j augment domin commun drop commun data set case extend upper bound date commun second dimens bi1j1 gener implement redund commun elimin messag coalesc well ffl multidimension shift commun given array refer shift commun processor grid dimens processor ignor boundari case send data receiv data 2 gamma 1 processor exampl commun bi1j1 figur 8 requir processor send data receiv 3 processor phpf compil use optim reduc number messag exchang either direct 2 gamma 1 accomplish compos commun step augment data commun suitabl step experi show notic perform improv optim even shift commun two dimens coars grain wavefront consid program segment shown figur 9 basic depend base algorithm would place commun aij1 insid jloop commun ai1j insid iloop due true depend refer aij align ai bcd align ef privat z without align figur 10 differ align privat scalar lead pipelin parallel across grid dimens regardless loop order one dimens extrem finegrain parallel high commun overhead phpf perform special analysi finegrain pipelin commun take place insid loop nest identifi maxim fullypermut inner loop nest 22 statement insid loop nest comput partit pipelin commun correspond array dimens block distribut move outsid fullypermut loop nest follow two observ first fullypermut loop nest tile arbitrarili 22 second loop blockdistribut array dimens pipelin commun travers tile onto outer loop travers processor appear separ loop spmd code inner loop travers local space processor commun move loop thu exampl figur 9 phpf abl move commun ai1j aij1 outsid jloop lead wavefront parallel low commun overhead associ coarsegrain pipelin futur experi loop stripmin control grainsiz pipelin map scalar wellknown replic scalar variabl processor often lead ineffici code exampl replic variabl x figur 10 would unnecessarili lead processor execut first statement loop everi iter valu b1n c1n broadcast processor scalar variabl privat phpf choos among align produc refer ii align consum refer iii align first explain map chosen phpf scalar variabl figur 10 describ gener algorithm use determin map scalar privatiz variabl align produc refer ie rh refer ai statement comput scalar valu avoid commun need produc refer statement variabl x align consum refer ie lh refer di2 use scalar valu comput x align instead produc refer bi ci commun x owner di2 would requir commun insid iloop depend definit x use x insid loop align x di2 commun need two refer bi ci commun move outsid iloop final variabl z use valu replic array element ei fi comput real a100100 iiown lbounda1i iown ubounda1i 1 end figur 11 guard optim exampl explicitli align refer processor execut iter iloop comput partit determin partit statement loop own comput temporari valu z loop iter phpf use static singl assign ssa represent 5 associ separ map decis assign scalar choos replic default map definit scalar loop privat without copyout base defus analysi reach use identifi reach definit phpf align scalar refer partit array rh statement rh refer statement avail processor scalar variabl explicitli mark align scalar valu need commun owner consum refer phpf determin separ pass desir chang align commun scalar scalar referenc comput consum refer instead chang done new align show lower estim cost result commun move outsid loop scalar comput reduct oper sum carri across processor grid dimens handl special manner addit privat copi scalar creat hold result local reduct comput initi perform processor global reduct oper combin valu local oper optim statement guard statement guard need enforc ownercomput rule howev innerloop guard inhibit parallel significantli degrad perform sever guard optim perform ffl statement within loop local iter set loop loop bound shrink use perform comput partit otherwis guard introduc individu statement ffl guard introduc comput partit hoist loop nest far possibl given statement local iter set differ loop guard differ processor grid dimens handl independ increas abil float guard inner loop sinc type move far possibl exampl figur 11 guard need first processor grid dimens iloop bound reduc local iter set guard second dimens base condit invari insid iloop henc move outsid iloop mpl version 100 100 200 398 789 1578 3043 tabl 2 speedup grid program section discuss preliminari experiment result set benchmark program util optim discuss previou section 41 experiment setup chose four program hpf benchmark suit develop appli parallel research inc publicli avail program vari degre challeng present compil first benchmark program grid iter 2d 9point stencil program featur regular nearest neighbor commun follow global reduct oper grid program littl comput benchmark version program take logarithm 9point stencil comput exponenti valu averag artifici boost computationcommun ratio second program tomcatv origin spec benchmark mesh gener thompson solver third program ncar shallow water atmospher model nontrivi 2d stencil program contain nearest neighbor commun gener commun last program x42 explicit model system use fourth order differenc use valu 2 grid side center point benchmark routin use serial perform program compil use ibm xlf compil baselin perform compil hpf program number physic processor specifi compil time perform result obtain run object code differ number processor 1 2 4 8 16 processor speedup program calcul divid baselin time parallel time p experi sequenti parallel run done 36processor ibm sp2 thinnod 4 widenod program compil optim flag o3 grid ncar shallowwat x42 time handoptim version program use messag pass librari mpl also shown 42 result grid threedimension array align templat distribut onto twodimension processor grid benchmark result obtain cycl compil achiev linear speedup case note previou section phpf success elimin redund commun program column mark grid tabl 2 obtain specifi number processor compiletim show phpf capabl gener high qualiti code number processor unknown compil time tabl 3 speedup tomcatv program program speedup ncar block block 100 101 171 374 675 1220 1932 mpl version 100 114 228 453 882 1662 3110 tabl 4 speedup ncar shallow water program tomcatv array tomcatv distribut columnwis onto 1d processor grid array size 514x514 program first comput residu requir nearest neighbor commun next maximum valu residu determin final tridiagon system solv parallel comput iter maximum valu residu converg idiom recognit identifi reduct oper comput maximum residu commun onethird refer initi identifi need commun recogn redund elimin optim use includ align scalar consum replac induct variabl optim enabl bound main comput loop nest shrunk guard need statement insid loop nest although compil achiev 55 ideal speedup 32 processor result quit good compar result seen hpf compil ncar shallow water benchmark result tabl 4 comput base 512x512 array distribut block compil reli independ direct loop could otherwis recogn parallel due static unknown constant appear subscript small number processor speedup good scale well number processor increas array distribut block block number refer need commun increas 25 45 perform 2d distribut better 1d distribut howev cost extra messag less save send shorter messag redund commun elimin also effect 2d distribut x42 benchmark version apr time part program comput wavefield array distribut use 1d block distribut redund commun elimin remov 8 19 static commun summari compil hpf program mani inher perform overhead result perform less highlytun handcod program benchmark perform x42 block block 100 118 203 381 699 1336 2196 mpl version 100 100 198 385 750 1377 2470 tabl 5 speedup x42 program result report repres combin effect optim built compil symbol analysi abil compil maintain level perform number processor known compil time 5 relat work sever group look problem compil hpflike languag distribut memori machin 11 23 19 3 4 15 18 work also benefit earli project like kali 14 fortran compil 11 perform sever optim like messag vector use collect commun exploit pipelin parallel also perform analysi elimin partial redund commun irregular comput 21 current version fortran compil requir number processor known compil time support partit singl dimens array superb compil 23 develop univers vienna repres second gener compil pioneer techniqu like messag vector use overlap region shift commun compil support block distribut array dimens put special emphasi perform predict guid optim datapartit decis 6 fortran 90d compil 3 exploit parallel fortran 90 construct gener spmd messagepass program attempt parallel sequenti fortran 77 program work focuss consider support parallel io handl outofcor program 20 paradigm compil 19 2 target fortran 77 program provid option automat data partit regular comput also support exploit function parallel addit dataparallel adaptor system 4 support hpf subset perform optim handl fortran 90 array construct improv cach local addit reduc commun cost adaptor determin commun schedul runtim suif compil 1 perform loop transform increas parallel enhanc uniprocessor perform compil also support automat data partit report result messagepass machin 6 conclus describ hpf compil ibm sp seri parallel machin compil phpf uniqu abil effici support fortran 90 array oper sequenti fortran 77 loop hpf program handl static unknown paramet like number processor usual perform degrad use symbol analysi resort runtim determin commun schedul phpf make sever contribut optim commun elimin redund commun use dataavail analysi deal problem map scalar variabl comprehens manner perform specialpurpos optim like coarsegrain wavefront reduc number messag multidimension shift commun present experiment result indic optim lead effici code gener futur plan appli optim commun across procedur boundari interprocedur analysi plan support blockcycl distribut array dimens also provid effici support irregular comput also investig compil strategi use remot memori copi oper like get put basic primit transfer data across processor acknowledg thank rick lawrenc joefon jann provid perform result mpl version benchmark program would also like thank alan adamson lee nackman support r overview compil scalabl parallel machin overview paradigm compil distributedmemori multicomput compil approach fortran 90dhpf compil distribut memori mimd comput adaptor compil system dataparallel fortran program effici comput static singl assign form control depend graph static paramet base perform predict tool parallel program ansi fortran 90 standard committe high perform fortran forum methodolog highlevel synthesi commun multicomput unifi dataflow framework optim commun compil fortran mimd distributedmemori machin visual execut high perform fortran hpf program compil global namespac parallel loop distribut exe cution appli parallel research xhpf system compil communicationeffici program massiv parallel chine process decomposit local refer pghpf portland group autom parallel regular comput distribut memori multicomput paradigm compil compil runtim support outofcor hpf program loop transform theori algorithm maxim parallel compil distributedmemori system tr process decomposit local refer effici comput static singl assign form control depend graph compil fortran mimd distributedmemori machin methodolog highlevel synthesi commun multicomput high perform fortran handbook static paramet base perform predict tool parallel program giventakemyampersandmdasha balanc code placement framework paradigm compil distributedmemori multicomput compil communicationeffici program massiv parallel machin compil global namespac parallel loop distribut execut loop transform theori algorithm maxim parallel visual execut high perform fortran hpf program overview compil scalabl parallel machin compil approach fortran 90d hpf compil unifi dataflow framework optim commun ctr manish gupta edith schonberg static analysi reduc synchron cost dataparallel program proceed 23rd acm sigplansigact symposium principl program languag p322332 januari 2124 1996 st petersburg beach florida unit state combin compiletim runtimedriven proactiv data movement softwar dsm system proceed 7th workshop workshop languag compil runtim support scalabl system p16 octob 2223 2004 houston texa shankar ramaswami sachin sapatnekar prithviraj banerje framework exploit task data parallel distribut memori multicomput ieee transact parallel distribut system v8 n11 p10981116 novemb 1997 gerald roth ken kennedi loop fusion high perform fortran proceed 12th intern confer supercomput p125132 juli 1998 melbourn australia daniel j rosenkrantz lenor r mullin harri b hunt iii minim materi arrayvalu temporari acm transact program languag system topla v28 n6 p11451177 novemb 2006 jo e moreira samuel p midkiff fortran 90 cse case studi ieee comput scienc engin v5 n2 p3949 april 1998 vijay menon keshav pingali highlevel semant optim numer code proceed 13th intern confer supercomput p434443 june 2025 1999 rhode greec b di martino briguglio celino g fogaccia g vlad v rosato briscolini develop larg scale high perform applic parallel compil practic parallel comput nova scienc publish inc commack ny 2001 gerald roth john mellorcrummey ken kennedi r gregg brickner compil stencil high perform fortran proceed 1997 acmiee confer supercomput cdrom p120 novemb 1521 1997 san jose ca shuo yang ali r butt charli hu samuel p midkiff trust verifi monitor remot execut program progress correct proceed tenth acm sigplan symposium principl practic parallel program june 1517 2005 chicago il usa soumen chakrabarti manish gupta jongdeok choi global commun analysi optim acm sigplan notic v31 n5 p6878 may 1996 vikram adv john mellorcrummey use integ set dataparallel program analysi optim acm sigplan notic v33 n5 p186198 may 1998 steven j deitz bradford l chamberlain sungeun choi lawrenc snyder design implement parallel array oper arbitrari remap data acm sigplan notic v38 n10 octob daniel chavarramiranda john mellorcrummey effect commun coalesc dataparallel applic proceed tenth acm sigplan symposium principl practic parallel program june 1517 2005 chicago il usa manish gupta edith schonberg harini srinivasan unifi framework optim commun dataparallel program ieee transact parallel distribut system v7 n7 p689704 juli 1996 ayon basumallik rudolf eigenmann optim irregular sharedmemori applic distributedmemori system proceed eleventh acm sigplan symposium principl practic parallel program march 2931 2006 new york new york usa dhruva r chakrabarti nagaraj shenoy alok choudhari prithviraj banerje effici uniform runtim scheme mix regularirregular applic proceed 12th intern confer supercomput p6168 juli 1998 melbourn australia john mellorcrummey vikram adv simplifi control flow compilergener parallel code intern journal parallel program v26 n5 p613638 octob 1998 rudolf eigenmann jay hoefling david padua automat parallel perfect benchmarks174 ieee transact parallel distribut system v9 n1 p523 januari 1998 christoph barton clin casav georg almsi yili zheng monts farrera siddhartha chatterj jo nelson amar share memori program larg scale machin acm sigplan notic v41 n6 june 2006 bradford l chamberlain steven j deitz lawrenc snyder compar studi na mg benchmark across parallel languag architectur proceed 2000 acmiee confer supercomput cdrom p46e novemb 0410 2000 dalla texa unit state mahmut kandemir alok choudhari prithviraj banerje j ramanujam nagaraj shenoy minim data synchron cost oneway commun ieee transact parallel distribut system v11 n12 p12321251 decemb 2000 kandemir p banerje choudhari j ramanujam n shenoy global commun optim techniqu base dataflow analysi linear algebra acm transact program languag system topla v21 n6 p12511297 nov 1999 johan cockx kristof denolf bart vanhoof richard stahl sprint tool gener concurr transactionlevel model sequenti code eurasip journal appli signal process v2007 n1 p213213 1 januari 2007 ken kennedi charl koelbel han zima rise fall high perform fortran histor object lesson proceed third acm sigplan confer histori program languag p71722 june 0910 2007 san diego california jack dongarra ian foster geoffrey fox william gropp ken kennedi linda torczon andi white refer sourcebook parallel comput morgan kaufmann publish inc san francisco ca