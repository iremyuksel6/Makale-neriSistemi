model select error estim studi model select strategi base penal empir loss minim point tight relationship error estim databas complex penal good error estim may convert databas penalti function perform estim govern qualiti error estim consid sever penalti function involv error estim independ test data empir vc dimens empir vc entropi marginbas quantiti also consid maxim differ error first half train data second half expect maxim discrep close relat capac estim calcul mont carlo integr maxim discrep penalti function appeal pattern classif problem sinc comput equival empir risk minim train data label flip b introduct consid follow predict problem base random observ one estim 2 predict rule measur bound loss function data consist sequenc independ ident distribut sampl distribut x n independ x goal choos predict rule f n restrict class f loss lf close possibl best possibl loss inmum taken predict rule empir risk minim evalu perform predict rule f 2 f term empir loss b l n provid estim whose loss close optim loss l class f sucient larg loss best function f close l ii sucient small nding best candid f base data still possibl two requir clearli con ict tradeo best understood write rst term often call estim error second approxim error often f larg enough minim l possibl distribut x f larg empir risk minimiza tion case common x advanc sequenc smaller model whose union equal f given data n one wish select good model one class problem model select denot b f k function f k minim empir risk one hope select model class fk excess error el close min idea structur risk minim also known complex regular izat add complex penalti b f k compens overt eect penalti usual close relat distributionfre upper bound sup f2f k penalti elimin eect overt thu structur risk minim nd best tradeo approxim error distributionfre upper bound estim error unfortun distributionfre upper bound may conserv specic distribut critic led idea use datadepend penalti next section show approxim upper bound error includ datadepend bound use dene possibl data depend complex penalti c n k model select algorithm excess error close min section 3 give sever applic perform bound section 2 section 31 consid estim provid independ test sampl disadvantag cost data section 32 consid distributionfre estim base vc dimens datadepend estim base shatter coecient unfortun dicult comput section 33 brie consid marginbas error estim view easili comput estim quantiti analog shatter coecient section 34 look estim provid maxim discrep error rst half sampl second half classic estim conveni comput simpli minim empir risk half label ip section 35 look complex estim expect maximum discrep estim calcul mont carlo integr lead better perform bound section 4 review concentr inequ central proof final section 5 oer experiment comparison propos method clariti includ tabl 1 notat use throughout paper work complex regular see akaik 1 barron 23 bar ron birg massart 4 barron cover 5 birg massart 89 buescher kumar 1112 devroy gyor lugosi 14 gallant 16 geman hwang 17 kearn mansour ng ron 20 krzyzak linder 23 lugosi nobel 25 lugosi zeger 27 26 mallow 28 meir 33 modha masri 34 rissanen 35 schwarz 37 shaw taylor bartlett williamson anthoni 38 shen wong 39 vapnik f predict rule set predict rule model class f union model class f k f k element f k minim loss element f k minim empir loss predict rule f minim loss loss minim loss function f k l b l n empir loss r nk estim high condenc upper bound loss l b penal loss estim l loss optim predict rule tabl 1 notat 42 vapnik chervonenki 46 yang barron 50 51 datadepend penalti studi bartlett 6 freund 15 kolt chinskii 21 koltchinskii panchenko 22 lozano 24 lugosi nobel 25 massart 30 shawetaylor bartlett williamson anthoni 38 penal error estim class f k let b f k denot predict rule select f k base data goal select among rule one approxim minim loss key assumpt analysi true loss b f k estim k assumpt 1 everi n posit number c k estim r nk l b avail satis ce 2m 2 1 notic c might depend sampl size n dene databas complex penalti r log k last term requir technic reason becom appar shortli typic small dierenc r nk simpli estim right amount penal l b final dene predict rule r log k follow theorem summar main perform bound f n theorem 1 assum error estim r nk satisfi 1 posit constant c 0 2ce 2m 2 moreov k b minim empir loss model class f k r logc second part theorem 1 show predict rule minim penal empir loss achiev almost optim tradeo approxim error expect complex provid estim r nk complex base approxim upper bound loss particular knew advanc class f k contain optim predict rule could use error estim r nk obtain upper bound el upper bound would improv bound theorem 1 rang loss function innit set inmum empir loss might achiev case could dene b f k suitabl good approxim inmum howev conveni assum throughout minimum alway exist suce variou proof assum n close proof breviti introduc notat 0 sup union bound r log j denit ce 2m assumpt 1 ce sinc prove second inequ k decompos lf n l k rst term may bound standard integr tail inequ shown see eg 14 page 208 e logce2m choos f k lf k second term may bound directli denit minim empir loss f k last step follow fact e sum obtain bound term yield k logce2m impli second statement theorem sometim bound tighter assumpt 1 avail assumpt bound may exploit decreas term log km denit complex penalti assumpt 2 everi n posit number c k estim r nk l b avail satis ce 2 dene modi penalti dene predict rule trivial modic proof theorem 1 obtain follow result theorem 2 assum error estim r nk satisfi assumpt 2 posit constant c 0 moreov k b minim empir loss model class f k log2ec far concentr expect loss penal estim howev easi modic proof obtain exponenti tail inequ work one inequ scenario theorem 1 theorem 3 assum error estim r nk satisfi 1 posit constant c k b minim empir loss model class f k 0 r log k proof note r log k r log k sup r log k rst inequ theorem 1 r log k union bound denit r log k minim empir loss f k e 2n log kn hoed inequ conclud proof exampl shown concentr expect loss penal empir error minim tail probabl estim may obtain case simpl applic theorem applic 31 independ test sampl assum independ sampl pair avail simpli remov sampl train data cours attract may small rel n case estim l b appli hoed inequ show assumpt 1 satis appli theorem 1 give follow result corollari 1 assum model select algorithm section 2 perform holdout error estim 3 min r log k word estim achiev nearli optim balanc approxim error quantiti may regard amount overt inequ recov main result lugosi nobel 25 much simpler estim fact bound corollari may substanti improv main result 25 squar root bound corollari 1 remov increas penalti term small constant factor use bernstein inequ place hoed follow choos modi estim r nk 1 1 posit constant bernstein inequ see eg 14 yield thu 2 satis replac 3m1 8 therefor dene obtain perform bound 32 estim complex remain exampl consid error estim r nk avoid split data simplic concentr section case classic 01 loss dene 0 argument may carri gener case well recal basic vapnikchervonenki inequ 45 43 sup n empir shatter coecient f k number dierent way n point classi element f k easi show inequ impli estim r log es k x 2n assumpt 1 need estim quantiti log es k x 2n simplest way use fact es k x 2n vc dimens f k substitut theorem min r log 4 r rn type distributionfre result mention introduct interest result involv estim es k x 2n k x n theorem 4 assum model select algorithm section 2 use r min r 12e log k x n r log k key ingredi proof concentr inequ 10 random vc entropi log 2 k x n proof need check valid assumpt 1 shown 10 n satis condit theorem 9 first note es k x 2n log es k x 2n log last inequ theorem 9 therefor r 3e log k x n sup r log es k x 2n use vapnikchervonenki inequ 4 follow r r 3e log k x n r 12 log k x n r 3e log k x n r 12 log k x n r 3e log k x n last term may bound use theorem 9 follow r r 3e log k x n log expb 9 expb 9 log 2c exp summar therefor assumpt 1 satis appli theorem 1 nish proof 33 eectiv vc dimens margin practic may dicult comput valu random shatter coecient k x n altern way assign complex may easili obtain observ k x n empir vc dimens class f k vc dimens restrict point immedi estim r log 4 assumpt 1 way estim theorem 4 fact care analysi possibl get rid log n factor price increas constant unfortun comput k gener still dicult lot eort devot obtain upper bound k simpl comput bound handi framework sinc upper bound may immedi convert complex penalti particular marginsbas upper bound misclass probabl neural network 6 support vector machin 38 7 44 13 convex combin classier 36 29 immedi give complex penalti theorem 1 perform bound recal fact basi theori support vector machin see bartlett shawetaylor 7 cristianini shawetaylor 13 vapnik 44 refer therein model class f call class gener linear classier exist function f class linear classier r p class predict rule form w 2 r p weight vector satisfi much theori support vector machin build fact eectiv vc dimens gener linear classier minim distanc correctli classi data point separ hyperplan larger certain margin may bound independ linear dimens p function margin constant say linear classier correctli classi x margin recal follow result lemma 1 bartlett shawetaylor 7 let f n arbitrari possibl data depend linear classier form w n 2 r p weight vector satisfi kw 0 posit random variabl let k n posit integ valu random variabl k x k r correctli classi k n data point x margin 0 sn assum b f minim empir loss class f gener linear classier correctli classi least n k data point margin applic lemma show take sn obtain f sn r2m log sn use inequ inequ show model class f k class gener linear classier class error estim r nk dene condit 1 satis theorem 1 may use result obtain follow perform bound theorem 5 sn kk r log k k r k random variabl k correspond class f k import result lie fact give comput feasibl way assign datadepend penalti linear classier hand estim r nk may much inferior estim studi previou section 34 penal maxim discrep section propos altern way comput penalti improv perform guarante new penalti may still dicult comput ecient better chanc obtain good approxim quantiti dene solut optim problem assum simplic n even divid data two equal halv dene predictor f empir loss two part l 2 use notat section 2 dene error estim r nk l 2 loss function 01 loss ie 0 0 maximum discrep l 2 may comput use follow simpl trick rst ip label rst half data thu obtain modi data set x 0 next nd f k 2 f k minim empir loss base 0 l 2 clearli function f k maxim discrep therefor algorithm use comput empir loss minim b may use nd f k comput penalti base maximum discrep appeal although empir loss minim often comput dicult approxim optim algorithm use nding predict rule estim appropri penalti particular algorithm approxim minim empir loss class f k minim proper subset f k theorem still applic et al 47 consid similar quantiti case pattern classic motiv bound similar 5 elf n b dene eectiv vc dimens obtain choos valu vc dimens give best bound experiment estim elf n b show linear classier xed dimens varieti probabl distribut good suggest model select strategi estim elf n use bound follow theorem justi direct approach use discrep train data directli rather use discrep rang sampl size estim eectiv vc dimens show independ test sampl necessari similar estim consid 49 although error bound present 49 theorem 34 nontrivi maximum discrep neg theorem 6 penalti dene use maximumdiscrep error estim 6 min l 2 r log k proof check assumpt 1 appli theorem 1 introduc ghost sampl x 0 n independ data distribut denot empir loss base sampl proof base simpl observ k thu k l 2 sup l 2 sup l 2 sup l 2 dierenc supremum maximum satis condit mcdiarmid inequ see theorem 8 c probabl exp 2 2 n9 thu assumpt 1 satis proof nish 35 random complex estim section introduc altern way estim quantiti may serv eectiv estim complex model class f maximum discrep estim previou section split data two halv oer altern allow us deriv improv perform bound consid expect random split data two set maxim discrep koltchinskii 21 consid similar estim prove bound analog theorem 7 improv bound theorem sequenc iid random variabl pf 1and independ data n introduc quantiti sup n use nk measur amount overt class f k note nk known may comput arbitrari precis montecarlo simul case pattern classic comput integr involv minim empir loss sampl randomli ip label oer two dierent way use estim model select rst base theorem 1 second slight modic theorem 2 start simpler version theorem 7 let dene error estim r nk choos f n minim penal error estim r log k r log k proof introduc ghost sampl proof theorem 6 recal symmetr trick gine zinn 18 sup sup n sup sup sup rest proof assumpt 1 follow easili concentr equal k sup sup sup last step use mcdiarmid inequ easi verifi dierenc supremum nk satis condit theorem 8 c assumpt 1 hold theorem 1 impli result concentr inequ concentrationofmeasur result central analysi inequ guarante certain function independ random variabl close mean recal three inequ use proof theorem 8 mcdiarmid 31 let x independ random variabl take valu set assum f n r satis sup c mcdiarmid inequ conveni f varianc situat varianc f much smaller follow inequ might appropri theorem 9 boucheron lugosi massart 10 suppos independ random variabl take valu set r exist function r x 0 moreov log 5 experiment comparison empir penal criteria 51 learn problem section report experiment comparison propos model select rule setup propos kearn mansour ng ron 20 toy problem x drawn uniform distribut interv 0 1 class f k dene class function f 2 f k exist partit 0 1 f constant interv straightforward check vcdimens f k k1 follow 20 assum target function f belong f k unknown k label exampl x obtain ip valu denot nois level clearli function g make simpl learn problem especi conveni experiment studi fact comput minima empir loss min f2f k perform time log n use dynam program algorithm describ 20 lozano 24 also report experiment comparison model select method problem paper studi sever penal model select techniqu holdout crossvalid method base independ test sampl penal base empir vc entropi maximum discrep estim random complex estim investig learn problem easi see empir vc entropi log 2 k x n class f k almost sure constant equal therefor penal base empir vc entropi essenti equival guarante risk minim grm procedur propos vapnik 44 thu investig empir method note lozano 24 compar grm procedur method base rademach penalti similar random complex estim nd rademach penalti systemat outperform grm procedur 20 grm compar minimum descript length principl independ test sampl techniqu regard simpli crossvalid techniqu main messag 20 penal techniqu take account empir loss structur properti model compet crossvalid sampl size contrari conclus base experi databas penalti perform favor compar penalti base independ test data gure shown report experi three method 1 holdout method holdout base select independ sampl describ section 31 2 maximum discrep md method select model accord method section 34 3 rademach penal rp perform random complex method propos section 35 use maximum discrep section 34 experi penalti were2 l 2 found multipli penalti dene section 34 12 provid superior perform use random complex estim sec tion 35 penalti sup n note experi log k log k term omit penalti reason comparison perform oracl select also shown pictur method select model minim true loss l b among empir loss minim b f k class f k train error minim algorithm describ 20 implement use templat prioriti queue doubli link list provid leda librari 32 52 result result illustr gure gener conclus may observ gener error ie true loss obtain method mdp rp favor compar holdout even sampl size 500 1000 datadepend penal techniqu perform well holdout data depend penal techniqu exhibit less varianc holdout main messag paper good error estim procedur provid good model select method hand except holdout method datadepend penal method tri estim directli l b gure show accur nois level high becom rather inaccur nois level decreas strong incent explor datadepend penal techniqu take account fact part f k equal elig minim empir loss acknowledg thank vincent mirelli alex smola fruit convers thank anonym review use suggest r new look statist model identi logic smooth densiti estim complex regular applic arti minimum complex densiti estim sampl complex pattern classi gener perform support vector machin pattern classi sharp concentr inequ applic random combinator learn learn canon smooth estima tion learn canon smooth estima tion introduct support vector machin bound learn algorithm nonlinear statist model nonparametr maximum likelihood estim method siev experiment theoret comparison model select method rademach penalti structur risk minim rademach process bound risk function learn radial basi function network complex regular function learn model select use rademach penal adapt model select use empir com plexiti nonparametr estim via empir risk minim concept learn use complex regulariza tion comment c p improv gener explicit optim margin applic concentr inequ stati tic method bound di platform combinatori geometr comput perform bound nonlinear time seri predict minimum complex regress estim weakli depend observ univers prior integ estim minimum descript length boost margin estim dimens model structur risk minim datadepend hierarchi converg rate siev estim best constant khintchin inequ concentr measur isoperimetr inequ product space estim depend base empir data natur statist learn theori statist learn theori uniform converg rel frequenc event probabl theori pattern recognit measur vcdimens learn machin weak converg empir process asymptot properti model select criteria tr ctr david anguita sandro ridella fabio rivieccio rodolfo zunino quantum optim train support vector machin neural network v16 n56 p763770 june clayton scott robert nowak learn minimum volum set journal machin learn research 7 p665704 1212006 leila mohammadi sara van de geer asymptot empir risk minim journal machin learn research 6 p20272047 1212005 peter l bartlett shahar mendelson rademach gaussian complex risk bound structur result journal machin learn research 3 312003 andr anto balz kgl tam linder gbor lugosi datadepend marginbas gener bound classif journal machin learn research 3 p7398 312003 joel ratsabi complex hyperconcept theoret comput scienc v363 n1 p210 25 octob 2006 ron meir tong zhang gener error bound bayesian mixtur algorithm journal machin learn research 4 1212003 joel ratsabi learn multicategori classif sampl queri inform comput v185 n2 p298327 septemb 15 magali fromont model select bootstrap penal classif machin learn v66 n23 p165207 march 2007 shie mannor ron meir tong zhang greedi algorithm classificationconsist converg rate adapt journal machin learn research 4 p713741 1212003 ron meir gunnar rtsch introduct boost leverag advanc lectur machin learn springerverlag new york inc new york ny