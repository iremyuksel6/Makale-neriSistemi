dual formul regular linear system convex risk paper studi gener formul linear predict algorithm includ number known method special case describ convex dualiti class method propos numer algorithm solv deriv dual learn problem show dual formul close relat onlin learn algorithm furthermor use dualiti show new learn method obtain numer exampl given illustr variou aspect newli propos algorithm b introduct consid statist learn problem find paramet random observ minim expect loss risk given loss function lff x n observ independ drawn fix unknown distribut want find ff minim expect loss x z assumpt underli distribut x natur method solv 1 use limit number observ empir risk minim erm method cf 13 choos paramet ff minim observ riskn specif consid follow type regular linear system convex loss shall assum f g convex function appropri chosen posit regular paramet balanc two term typic choos gw 0 function penal larg w formul natur aris mani statist learn applic exampl order appli formul linear regress let howev formul also interest exampl robust estim one interest use order appli formul purpos train linear classifi choos f decreas function fdelta 0 exampl includ support vector machin logist regress one interest theoret result vc analysi concern primal formul dimension independ gener error obtain 14 vc analysi extend 16 howev turn care nonvc analysi 17 actual suitabl particular problem primal form dual form introduc later paper paper studi numer learn aspect dual form 3 certain attract properti compar primal formul dualiti certain learn problem gener gametheoret sens also lead discoveri new learn methodolog paper organ follow section 2 deriv dual formul 3 propos relax algorithm solv dual problem section 3 gener deriv includ equal constraint section 4 provid applic propos algorithm machin learn section 5 provid numer exampl section 6 studi learn aspect dual formul conclus final remark made section 7 dual formul sinc 3 convex program problem involv linear transform primal variabl w dual form obtain introduc auxiliari variabl data point sup n kdelta dual transform fdelta assum f lower semicontinu see 11 well known k convex switch order inf w sup valid minimax convexconcav program problem proof interchang ie strong dualiti given appendix obtain n w minim 5 fix n use rg denot gradient g respect w note main bodi paper assum gradient convex function exist requir case constraint need dealt introduc appropri lagrangian multipli section 3 main reason treatment numer consider howev mathemat proof strong dualiti appendix direct numer consequ use gener definit convex function dualiti 11 without assum differenti exampl gener case constraint convex function cz regard modif cz cz 1 z satisfi constraint substitut 6 5 obtain n st rgw n simplifi notat consid dual transform gdelta hdelta also convex function follow thu rewrit 7 follow dual formul n optim solut w primal problem obtain n system nondegener also see section 3 give follow solut n comparison also follow equat 4 f differenti note deriv equat 8 use two legendr transform k dual f h dual g write x matrix data x row legendr transform point view dual transform xw respect f k dual transform w 1 respect g h consequ regard x interact dual variabl w interact linear interact simplest possibl interact w paper propos follow gener relax algorithm solv dual problem 8 denot dimens w algorithm 1 dual gaussseidel find delta approxim minim updat v n delta updat inner iter algorithm 1 algorithm essenti fix dual variabl j j 6 find perturb delta dual variabl reduc object function 8 sinc object function reduc step algorithm alway converg exact optim use converg true optim solut 8 also hessian 8 exist wellbehav around optim solut 8 local approxim quadrat function follow asymptot converg rate algorithm 1 linear minim exactli 4 rate converg depend spectrum hessian matrix 8 optim solut main attract featur algorithm 1 simplic one might also consid precondit conjug gradient acceler 4 algorithm 1 precondition howev mani interest case kdelta nonsmooth contain simpl bound henc cg might help interest aspect dual formul dual variabl correspond data point x algorithm 1 step one look one data point similar onlin updat algorithm differ algorithm 1 keep dual variabl comput far typic onlin algorithm inform kept actual difficult convert algorithm 1 onlin learn method set use appropri scale factor n step minimax style mistak bound also deriv accordingli howev fullscal studi relat issu left anoth report paper shall provid simpl analysi mistak bound empir risk estim appendix b illustr basic idea demonstr theoret connect dual batch formul onlin learn note primal form 3 dual form 8 strike similar besid connect onlin algorithm sever reason interest studi dual form exampl later see paper g quadrat regular function algorithm 1 especi simpl form anoth reason f nonsmooth g smooth onedimension optim problem dual gaussseidel algorithm easier onedimension optim problem primal gaussseidel algorithm furthermor even primal problem infinit dimension infin dual problem still finit dimension assum finit sampl size elimin primal dimension import gener perform consequ machin learn see section 6 section 7 final dual form 8 relationship 9 gener kernel form svm origin investig vapnik 14 constraint order dual formul 8 valid equat 9 solut howev certain circumst possibl 8 singular henc solut right hand side lie rang rgw w 2 r impos constraint x x matrix row consist data x simplic shall discuss case equal constraint inequ constraint handl similarli sinc origin problem convex optim problem therefor dual problem also convex optim problem impli equal constraint dual variabl must linear constraint situat usual aris gw flat linear transform w c matrix vector sinc crgw j therefor 9 correspond constraint impos dual variabl follow form equat 10 modifi n order preserv structur dual formul 8 employ gaussseidel updat algorithm 1 propos use augment lagrangian method 3 method modifi dual hv primal regular term gw 8 lagrangian multipli vector correspond constraint 13 14 small posit penalti paramet follow algorithm solv 8 constraint 13 util modifi dual h cf 3 page 292 algorithm 2 dual augment lagrangian solv 8 hdelta replac h delta use algorithm 1 use current v initi point updat v els delta one nice properti augment lagrangian approach augment dual problem solv algorithm 1 suffici high accuraci step 4 execut finit number time mean upon termin bound away zero next would like briefli discuss situat primal problem 3 convex constraint cw 0 compon cdelta convex function let correspond lagrangian multipli primal problem rewritten well known 0 3 compon c j w 0 indic cw convex thu regard regular term similar gw select appropri lagrangian multipli essenti solv problem regular term gw replac gw learn problem exact valu lagrangian either known exampl entropi regular probabl distribut investig later noncruci sinc appropri way determin lagrangian regular paramet kind crossvalid anyway also mani case constraint primal variabl w becom flat segment dual variabl case appli unmodifi algorithm 1 comput relationship obtain w modifi accordingli regard primal inequ constraint regular term observ paramet increas correspond c j w decreas optim solut thu also possibl use algorithm suggest paper inner solut engin adjust j appropri examin c j w optim solut idea similar modifi lagrangian method propos deal dual constraint 13 although modif augment lagrangian method appli mani case usual recommend due varieti reason subtl method employ howev due limit space also issu noncruci learn problem shall skip discuss 41 use dual pair section list exampl convex legendr dualiti relev learn problem use pu denot primal function primal variabl u use qv denot dual function dual variabl v 1 assum k symmetr posit definit oper 2 dual pair 1p 3 dual pair 1p dual pair 4 set posit prior 5 note pu flat dimens pu vector ident compon therefor v satisfi constraint dual transform contain free lagrangian paramet 7 0 nonneg lagrangian paramet 8 9 dual pair 1p note mani exampl contain constraint dual also primal variabl case also ignor constraint consid correspond function constraint satisfi last exampl interest relev classif problem also use note dual linear transform either primal variabl primal convex function easili comput assum nonsingular linear transform 42 regular term section briefli discuss regular condit 3 interest squar regular one import regular condit squar penalti k symmetr posit definit oper mani applic one choos ident matrix case henc algorithm 1 replac minim note system 16 particularli simpl actual reduc small problem constant size solv constant time inner product b comput larg problem comput b actual precomput domin pnorm regular let interest data qnorm bound sinc x w bound gener perform also dimension independ 16 17 case approxim newton method employ solv newton iter lambda usual one may use one newton iter need comput deriv part second deriv part ij mani case comput costli evalu b squar regular formul howev trick employ allevi problem sinc accur estim second deriv part ij less import safe use good upperbound second deriv note essenti constraint v order obtain w equat 10 modifi similarli essenti constraint regular term given interest data 1norm entropi bound typic let larg number case approxim newton method requir deriv part requir evalu addit second deriv part requir evalu ij entropi regular usual interest either data infinitenorm bound weight vector w give probabl distribut case unlik use 1norm regular condit lead gener perform degrad logarithm dimens gener perform entropi regular dimension independ 16 17 first consid normal entropi probabl distribut prior distribut dual choos lagrangian paramet deriv term hv second deriv term hv n n therefor two term correspond expect varianc x distribut w note second deriv varianc vanish small posit number use regular solut consid nonnorm entropi posit weight w one easili deal gener situat ad neg weight part w dual newton approxim straightforward remark practic absolut reason need choos regular term base simpl primal form perfectli reason highli recommend design learn algorithm base simpl choic dual function hv whether dual gw complic irrelev far dual algorithm concern exampl consid dual huber function simpl form hv 2 v 2 jvj 1 solv rel easili regular condit gw good replac 1norm regular gener one choos hv piecewis linear quadrat function would like w concentr around 0 appropri form hv also shape concentr around 0 howev uncertainti princip freedom dual dimens invers proport freedom primal dimens easili seen case squar regular sinc kernel k gamma1 dual problem invers kernel k primal problem hand would like design algorithm w bias toward nonzero prior case entropi regular one need use hv monoton increas function easili construct use piecewis quadrat function 43 loss term regress first consid standard squareloss regress regular term 15 line search step algorithm 1 solv analyt robust estim interest case regular term 15 line search step algorithm 1 solv analyt robust natur explain dual updat sinc bound therefor contribut one data point final weight w well control principl use directli design dual form robust regress without resort primal form distribut estim consid modif maximumentropi method given normal entropi regular 19 formul becom maximumentropi method 0 howev conjectur modif advantag standard maximumentropi method reason softmargin svm formul often prefer optimalsepar hyperplan method practic sens choos nonzero regular paramet svm theoret advantag appropri nonzero regular paramet demonstr 17 even linearli separ classif problem possibl achiev exponenti rate converg appropri nonzero regular param eter similar result known optimalsepar hyperplan method fact conjectur 17 gener perform optimalsepar hyperplan method slower exponenti worst case case modifi maximumentropi method newton updat correspond line search step algorithm 1 given note 0 standard maximumentropi method updat fail reason quadrat penalti method becom illcondit penalti paramet close zero easi remedi use modifi lagrangian method alreadi discuss likelihood consid mixtur model estim problem want find distribut w loglikelihood maxim case normal entropi regular 19 note initi valu set zero anymor sinc shall thu start posit valu 1 newton updat correspond line search step algorithm 1 given w correspond nice behav probabl densiti vector random variabl r n instead entropi regular 2norm base regular appropri kernel fisher kernel also util next consid logist regress classif problem deriv maximumlikelihood estim given 15 case newton step correspond algorithm 1 becom start nonzero initi valu 2 0 1 exampl 05 good choic binari classif binari classif typic choos nonneg decreas f f0 0 one exampl logist regress investig section examin exampl let choos gw 15 case exact analyt solut obtain fu replac squar regular condit normal entropi regular newton updat becom shall chosen base class label maxim margin see section 5 x shall data vector inclass member neg data vector outofclass member algorithm correspond normal winnow exponenti gradient updat 9 posit weight way support vector machin correspond perceptron updat 10 better understand observ perceptron updat correspond squar norm regular winnow updat correspond entropi regular exampl see proof method 5 comparison exponenti gradient versu gradient descent 8 optim margin svm linearli separ problem modifi perceptron minim 2norm margin constraint correspond minim entropi margin constraint winnow exponenti gradient famili algorithm softmargin svm modifi optim margin method introduc decreas loss function f squar regular 3 case winnow algorithm translat choic entropi regular soft margin svmlike loss function f updat rule addit use form fu standard one use standard svm formul discuss shortli algorithm point view relationship svm perceptron algorithm mention 12 chapter 12 platt compar svm dual updat rule relat 29 perceptron updat algorithm analog 28 winnow exponenti gradient famili algorithm readili observ due normal exponenti form rhdelta shall note mani discuss exponenti gradient algorithm emphas proper match loss function see 6 exampl formul care choic match loss replac proper regular condit combin loss function also notic nonnorm entropi use regular condit obtain algorithm correspond nonnorm exponenti gradient updat svm choos f let gw given 15 updat exact solut mention platt deriv updat rule 29 12 howev deriv employ bregman techniqu 1 suitabl specif problem particular techniqu convex dualiti adopt paper gener bregman approach also platt comment 29 maxim margin true fact found statist signific differ method standard svm applic although special care taken avoid potenti zerodenomin problem see discuss end section 5 implement consider shall also interest mention 7 author alreadi adopt updat 29 applic worthwhil mention standard svm shift b non regular absorb b last compon w append constant featur typic 1 regular condit gw symmetri gw c vector one last compon zero elsewher case algorithm 2 employ correspond dual constraint 13 exact solut modifi introduct nonregular shift b standard svm formul creat complex optim although significantli illustr section 5 also seem advers effect gener analysi logn factor note expect gener perform o1n full regular see section 6 17 achiev nonregular dimens well known nonregular dimens vcdimens 1 contribut logarithm factor gener perform term o1n worst case tight exampl see 14 howev increas varianc may compens potenti reduc bia practic differ use nonregular shift versu regular shift unclear 5 experi goal section illustr propos algorithm exampl reader develop feel converg rate propos algorithm context exist algorithm due broad scope paper theoret natur illumin provid exampl everi specif instanc learn formul mention section 4 thu provid two exampl one algorithm 1 one algorithm 2 instanc algorithm similar behavior sinc research interest mainli natur languag process shall illustr algorithm text categor exampl standard data set compar text categor algorithm reuter set news stori publicli avail httpwwwresearchattcomlewisreuters21578html use reuters21578 modapt split partit document train valid set data contain 118 class 9603 document train set document test set experi use word stem without stopword remov also use inform gain criterion 15 select 1000 inform wordcount featur use binari valu select featur indic word either appear appear document text categor perform usual measur precis recal rather classif error posit true posit posit theta 100 posit true posit neg theta 100 sinc linear classifi contain threshold paramet adjust tradeoff precis recal convent report breakeven point precis equal recal sinc document reuter dataset multipli categor common studi dataset separ binari classif problem correspond categori overal perform measur microaverag precis recal breakeven point comput overal confus matrix defin sum individu confus matric correspond categori follow exampl use top ten categori remain categori typic small experi done pentium ii 400 pc linux time includ train use train data featur select test test data pars document winnow versu regular winnow exampl studi perform regular winnow correspond updat 28 standard winnow algorithm posit weight use learn rate 0001 standard winnow algorithm approxim optim exampl predict wrong shrink weight expgamma0001x data x class multipli weight exp0001x data x class predict rule w x impli outofclass w inclass w normal predefin paramet regular winnow algorithm 28 let outofclass data sign data revers train phase ff paramet attempt maxim decis margin inclass outofclass data fix 01 exampl illustr intuit correspond let start point weight updat inclass data ff start point weight updat outofclass data goal achiev margin size 2ff regular paramet 28 fix 10 gamma4 run algorithm 100 iter data comparison note kwk data x f0 1g compon thu w x 1 also taken sparsiti data account reason us pick 03 reflect good rang threshold choic 01 microaverag breakeven point winnow 830 cpu time 23 second microaverag breakeven point regular winnow 863 cpu time 26 second 03 microaverag breakeven point winnow 816 cpu time 23 second microaverag breakeven point regular winnow 850 cpu time 26 second better feel report time one note c45 decis tree induc even 500 featur 1000 featur handl take hour finish train partli spars structur util smo algorithm svm one fast text categor algorithm 2 requir three minut tabl 1 show breakeven point algorithm ten categori 01 regular winnow consist superior differ statist sig nific compar differ svm perceptron algorithm indic tabl 2 categori winnow regular winnow acq 828 854 moneyfx 587 628 grain 784 847 crude 781 787 trade 653 711 interest 617 725 ship 637 750 wheat 764 831 corn 655 804 microaverag 830 863 tabl 1 breakeven point winnow versu regular winnow although result good svm achiev state art perform text categor comparison unfair sinc allow posit weight implement winnow style algorithm indic tri find indic word particular topic ignor word even though appear document may strongli suggest document belong topic util addit featur need regular version standard winnow algorithm posit neg weight convers obtain vector version dualiti formul weight w matrix dual data point vector sinc extens investig work shall thu skip comparison perceptron versu svm exampl compar perceptron algorithm propos svm method 30 smo method svm describ chapter 12 12 current prefer method solv svm problem textcategor 2 regular paramet svm fix 10 gamma3 learn rate perceptron 0001 predict wrong updat weight data x class 0001x data x class predict rule w x 0 impli outofclass w x 0 impli inclass also normal weight kwk end iter data enhanc perform total run time perceptron 100 iter train data 13 second faster speed compar winnow algorithm indic winnow algorithm spend time normal step 1 propos formul 30 exactli implement describ use 50 iter call algorithm 1 4 iter algorithm 2 therefor total use 200 iter train data run time 33 second smo algorithm tricki implement made best judgment tradeoff among intern paramet suggest given 12 start point run time particular implement 191 second tabl 2 includ comparison breakeven point three algorithm ten categor microaverag breakeven point observ svm consist better perceptron algorithm howev statist insignific random discrep among svm due differ converg criteria shall also use point j order 10 gamma6 end algorithm 2 ten categori impli dual constraint larg satisfi result similar svm microaverag breakeven 91 obtain use 27 100 iter algorithm 1 total run time 19 second case constant featur valu 1 append data point categori perceptron smo algorithm 2 acq 924 955 950 moneyfx 687 698 754 grain 879 913 886 crude 810 815 841 trade 686 735 735 interest 634 748 771 ship 820 820 809 wheat 778 831 847 corn 772 875 839 microaverag 885 910 912 tabl 2 breakeven point perceptron versu svm would like mention although exampl faith implement suggest algorithm import stick specif formul given paper exampl magic reason start valu 50 decreas constraint valu updat 4 algorithm 2 also recommend use updat valu delta smaller quantiti implement winnow normal frequent inner iter avoid numer instabl suggest newton method gain better numer stabil exampl observ algorithm 1 27 29 use use small fraction exact deltaupd given equat especi import 29 sinc denomin x zero caus larg chang one replac regular version 1 smaller increment allevi problem heavi oscil phenomenon less problem 30 sinc small give larg denomin 1 6 learn consequ differenti convex function h defin distanc function h w w 0 also call bregman diverg 1 properti h w w strictli convex h w w consid n sampl x n 1 x let partit n sampl batch subsampl batch xi contain n data let denot optim solut 8 1 let denot solut approxim minim 8 ith batch subsampl denot empir expect respect ith batch n sampl 8 ffl posit approxim error control practic check dualiti gap primal problem 3 let restraint ith batch subsampl n h h h note first order condit 8 optim solut impli follow estim equat anoth form 11 thu obtain follow inequ similar analysi primal problem 17 shall ignor contribut simplic thu obtain follow fundament inequ dual shall defin v x 33 bound converg v v term hbregman diverg converg v also interest compar inequ primal form 17 approxim empir risk minim w n primal problem satisfi g w w use dualiti bregman diverg 17 relationship 11 note 33 give better constant fact asymptot tight constant compar asymptot estim given 16 17 although learn bound primal formul investig 17 insight studi implic term dual variabl one bound given vapnik term number supportvector ie number nonzero dual variabl see 14 although vapnik bound interest two fundament drawback one drawback bound asymptot correct sens give expect gener error bound slower o1n n sampl sinc number support vector usual grow unbound howev demonstr 17 expect gener error grow rate o1n gener case furthermor misclassif error train linear classifi problem linearli separ exponenti n note later situat clearli vapnik bound intend therefor bound asymptot far inferior correct rate drawback vapnik bound handl situat number support vector small although indic number support vector larg gener perform becom poor bound tend lead statist learn commun toward think obtain good gener perform somewhat desir reduc number support vector exampl clearli case design support vector machin howev analysi lead correct larg sampl converg rate indic minim number support vector import addit drawback support vector concept also character learn problem predict lead small number support vector therefor nontrivi bound sole reli empir evid entir satisfactori theori goal section studi learn aspect dual problem care character gener perform term dual variabl analysi complement primal analysi studi 17 let n 1 33 x n 1 chosen way approxim underli distribut let denot optim solut limit regard random variabl respect shall assum solut continu version 8 exist assum n simplic denot n take expect ex nover n randomli chosen data x n 1 obtain denot empir estim data x n 1 n denot approxim solut 8 error ffl 31 random variabl usual wellbehav continu case estim equat 32 given relationship 11 deriv exist subgradi otherwis see appendix w optim solut primal problem note right hand side 34 becom converg empir expect random variabl mean estim standard probabl techniqu illustr 17 typic h squarelik right hand side variancelik thu converg rate o1n detail case studi given 17 shall repeat addit expect hbregman diverg also obtain expect exponenti lim use independ assumpt x n 1 take limit obtain 8ff 0 use obtain larg deviat type exponenti probabl bound moment bound also similarli obtain see 17 exampl 34 36 obtain expect gener error bound larg deviat style probabl bound use techniqu 17 point earlier primal estim dual estim rather similar constant factor 2 shall regard nonsignific purpos deriv gener bound therefor shall repeat analysi exampl given 17 order see expect error bound 33 improv consid 34 linear loss quadrat regular case solv primal problem obtain kex dual problem j b becom equal sinc local g usual approxim quadrat regular f linear loss b x depend shall intent remov quadrat term f expans correspond drop k term 33 therefor asymptot 33 tight 7 conclud remark paper introduc dual formul class regular linear learn method convex risk new formul relat onlin learn algorithm stochast gradient descent method dual form also lead gener kernel formul support vector machin kernel formul one usual substitut primaldu relationship 10 primal formul 3 approach emphas complet differ dual risk 8 dual formul lead new learn algorithm well new insight certain learn problem intermedi primaldu formul relat minimax onlin mistak bound framework howev pose problem pac style batch set relationship thu bridg onlin learn mistak bound analysi pac analysi numer point view abl obtain new batch learn method dual formul importantli method deriv systemat way mention possibl transform deriv batch learn method onlin learn algorithm shall leav anoth report appendix b demonstr basic idea obtain mistak bound use 33 natur ask gener dualiti machin learn primaldu formul view learn problem gametheoret set learner choos primal weight variabl w minim certain risk oppon choos dual variabl control random sampl behavior maxim risk although strong dualiti convexconcav program difficult extend gener problem gametheoret point view still adopt conjectur even strong dualiti violat may still possibl design dual formul appropri learn method howev mani open issu area requir futur studi also studi learn aspect dual formul 8 specif abl obtain gener bound better asymptot tight constant primal analysi 17 interest consequ dual analysi evid number nonzero dual variabl support vector signific may surpris sinc number stabl even slight perturb loss function gener perform bound base number support vector asymptot suboptim especi linearli separ classif problem bound intend dual formul also provid valuabl insight learn problem exam ple sinc number dual variabl alway independ dimension primal problem therefor appropri regular assumpt dimension primal problem appear learn bound suggest frequent mention notion curs dimension realli issu mani learn problem might seem surpris first understand implic consid high dimension densiti estim problem standard argument curs dimension high dimens order approxim densiti function one need fill number point box exponenti dimension howev reason partial valid appropri way measur whether two distribut similar compar expect bound function respect two distribut therefor instead pointwis converg criterion shall consid converg weak topolog close two densiti measur close action set test function impli proper question ask given fix number data mani test function util obtain stabl densiti estim methodolog use maximum entropi method note appropri regular condit number test function use dimension independ howev sinc choos weak topolog densiti estim need determin topolog test function given famili test function need find appropri structur hierarchi approxim famili proper norm alway defin test function dimension independ partit obtain induc dual metric densiti use measur converg densiti estim exampl typic partit bound test function base logexponenti criterion induc entropi metric densiti space see analysi 17 dualiti test function densiti weight essenti dualiti investig paper furthermor method measur complex learn problem dual vc point view directli measur complex paramet space exampl densiti function estim advantag dual point view reli specif parametr function famili one choos importantli mani learn problem complex measur converg rate random test function mean test space see 17 indic wellestablish probabl tool appli obtain complex measur dimension independ proof strong dualiti would like show valid interchang order inf w sup primaldu formul 4 assum gw well function involv analysi take valu 1 equival dataindepend constraint also assum solut primal problem exist uniqu simplic notat purpos let goal show exist sup wellknown exampl see 11 dualiti gap sup 37 valid find w case w call saddl point follow demonstr exist saddl point construct consid w minim primal problem known 11 page 264 exist subgradi gener gradient concept convex function see definit 11 section 23 denot delta 2 thatn reader familiar convex analysi equat becom follow estim equat empir risk differenti wn n relationship subgradi dualiti see 11 page 218 w achiev minimum n also definit gamma subgradi fdelta relationship subgradi dualiti 11 page 218 achiev maximum finish proof convent subgradi set denot delta howev notat conveni use delta denot member subgradi set proof onlin mistak bound consid follow scenario given sampl primal variabl w weight obtain exact solut dual problem 8 v dual v project v onto first v project v onto ith sampl case 33 proof 33 inequ fundament bridg onlin learn formul batch dual formul refer paper see inequ import note h squar like distanc usual true right hand side order o1k 2 impli kd h v small standard onlin learn telescop techniqu appli deriv mistak bound case done fashion similar parallel exampl studi 17 howev gener case treat anoth dedic report follow shall mere provid illustr use squar regular term 15 draw conclus v k w v k make assumpt bregman diverg l squar like true smooth loss function use estim equat 32 v k sum v k give typic mistak bound correct growth order logn standard assumpt exampl assum c c vs k gamma1 norm bound b well known logarithm growth optim see consid onedimension easi verifi let x larg k w henc growth rate logn achiev simpli sum equal k note logarithm factor indic correct batch learn rate o1n obtain typic random techniqu exampl see 8 modifi onlin algorithm mistak bound batch algorithm also import note match loss function concept cf 6 import analysi allow us analyz problem loss function role match loss function correspond proper choic regular term analysi data depend rather loss function depend r relax method find common point convex set applic solut problem convex program induct learn algorithm represent text categor practic method optim matrix comput gener converg result linear discrimin updat rel loss bound singl neuron discrimin framework detect remot protein homolog addit versu exponenti gradient updat linear predict learn quickli irrelev attribut abound new linearthreshold algorithm mit press convex analysi smola editor advanc kernel method estim depend base empir data statist learn theori compar studi featur select text categoriza tion analysi regular linear function classif problem primal formul regular linear system convex risk tr ctr fred j damerau tong zhang sholom weiss nitin indurkhya text categor comprehens timedepend benchmark inform process manag intern journal v40 n2 p209221 march 2004 tong zhang fred damerau david johnson text chunk base gener winnow journal machin learn research 2 312002 zhang yue pan tong zhang focus name entiti recognit use machin learn proceed 27th annual intern acm sigir confer research develop inform retriev juli 2529 2004 sheffield unit kingdom tong zhang leaveoneout bound kernel method neural comput v15 n6 p13971437 june ron meir tong zhang gener error bound bayesian mixtur algorithm journal machin learn research 4 1212003 tong zhang cover number bound certain regular linear function class journal machin learn research 2 p527550 312002 qiang wu yime ying dingxuan zhou multikernel regular classifi journal complex v23 n1 p108134 februari 2007 ron meir gunnar rtsch introduct boost leverag advanc lectur machin learn springerverlag new york inc new york ny