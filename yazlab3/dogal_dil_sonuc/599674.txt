pac analogu perceptron winnow via boost margin describ novel famili pac model algorithm learn linear threshold function new algorithm work boost simpl weak learner exhibit sampl complex bound remark similar known onlin algorithm perceptron winnow thu suggest wellstudi onlin algorithm sens correspond instanc boost show new algorithm view natur pac analogu onlin pnorm algorithm recent studi grove littleston schuurman 1997 proceed tenth annual confer comput learn theori pp 171183 gentil littleston 1999 proceed twelfth annual confer comput learn theori pp 111 special case algorithm take p equal 2 p equal obtain natur boostingbas pac analogu perceptron winnow respect p equal case algorithm also view gener improv sampl complex bound jackson craven pacmodel boostingbas algorithm learn spars perceptron jackson craven 1996 advanc neural inform process system 8 mit press analysi gener error new algorithm reli techniqu theori larg margin classif b introduct one fundament problem comput learn theori learn unknown linear threshold function label exampl mani differ learn algorithm problem consid past sever decad particular recent year mani research studi simpl onlin addit multipl updat algorithm name perceptron winnow algorithm variant thereof 3 5 8 14 15 16 25 26 27 28 33 36 paper take differ approach describ natur parameter famili boostingbas pac algorithm learn linear threshold function weak hypothes use linear function strong classifi obtain linear threshold function although new algorithm support part nsf graduat fellowship nsf grant ccr9504436 onr grant n000149610550 conceptu algorithm differ perceptron winnow establish perform bound new algorithm remark similar perceptron winnow thu refer new algorithm pac analogu perceptron winnow hope analysi new algorithm yield fresh insight relationship boost onlin algorithm give unifi analysi perceptron winnow analogu includ mani algorithm well grove littleston schuurman 16 shown perceptron version winnow view case gener onlin pnorm linear threshold learn algorithm p 2 real number present pacmodel boostingbas analogu onlin pnorm algorithm valu 2 p 1 pacmodel perceptron winnow analogu mention respect case gener algorithm case algorithm also view gener jackson craven pacmodel algorithm learn spars perceptron 20 algorithm boost use weak hypothes singl boolean lit eral similar case algorithm analysi case gener algorithm deal realvalu rather boolean input variabl yield substanti stronger sampl complex bound establish 20 section 2 paper contain preliminari materi includ overview onlin pnorm algorithm 15 16 section 3 present simpl pacmodel pnorm algorithm prove weak learn algorithm 2 section 4 appli techniqu theori larg margin classif show learn algorithm boost strong learn algorithm small sampl complex final section 5 compar pac algorithm analog onlin algorithm extend algorithm case discuss relationship case algorithm jacksoncraven algorithm learn spars perceptron 11 relat work sever author studi linear threshold learn algorithm work combin weak predictor freund schapir 14 describ algorithm combin intermedi perceptron algorithm hypothes use weight major vote final classifi depth2 threshold circuit prove bound gener error result classifi algorithm use boost combin perceptron hypothes rather weight accord surviv time ji 21 propos randomsearchandtest approach find weak classifi linear threshold function combin simpl major vote thu also obtain depth2 threshold circuit approach closest jackson craven 20 use boost combin singl liter strong hypothesi linear threshold function describ section 5 case algorithm strengthen gener result gener also note freund schapir 12 schapir 32 exhibit close relationship boost onlin learn start geometr definit point denot pnorm name 1norm x kxk qnorm dual pnorm 1 henc 1norm 1norm dual 2 norm dual paper p q alway denot dual norm follow fact well known eg 37 pp h older inequ ju delta minkowski inequ ku throughout paper exampl space x subset linear threshold function x function f function signz take valu 1 z 0 take valu note standard definit linear threshold function allow nonzero threshold ie real number howev linear threshold function gener form n variabl equival linear threshold function threshold 0 definit incur real loss gener write kxk p denot sup x2x kxk use symbol ux denot quantiti x2x measur separ exampl x hyperplan whose normal vector u assum throughout paper kxk p 1 ie set x bound ffi nonzero lower bound separ hyperplan defin u exampl x 21 pac learn denot exampl oracl queri provid label exampl x drawn accord distribut x say algorithm strong learn algorithm u x satisfi follow condit function x distribut x make mffl ffi call exu probabl least output hypothesi h 1g pr x2d hx 6 signu delta say hypothesi h fflaccur hypothesi function mffl ffi x sampl complex algorithm main result describ strong learn algorithm care analyz sampl complex must consid algorithm satisfi strong learn properti still capabl gener hypothes slight advantag random guess socal weak learn algorithm first consid kearn valiant 24 let finit sequenc label exampl x let distribut say say algorithm 12 gamma flweak learn algorithm u follow condit hold finit set describ distribut given input output hypothesi 12 gamma flapproxim u thu purpos weak learn algorithm one alway find hypothesi outperform random guess fix sampl 22 onlin learn pnorm algorithm onlin model learn take place sequenc trial throughout learn process learner maintain hypothesi h map x fgamma1 1g trial proce follow upon receiv exampl x 2 x learn algorithm output predict associ label learn algorithm given true label 2 fgamma1 1g algorithm updat hypothesi h base new inform next trial begin perform onlin learn algorithm exampl sequenc measur number predict mistak algorithm make grove littleston schuurman 16 gentil littleston 15 studi famili onlin algorithm learn linear threshold function see figur 1 refer algorithm parameter real valu onlin pnorm algorithm like wellknown perceptron algorithm onlin pnorm algorithm updat hypothesi make addit chang weight vector z howev shown step 45 figur 1 pnorm input paramet real number p 2 initi weight vector posit valu 0 1 set 2 exampl avail 3 get unlabel exampl 4 5 predict 6 get label 2 fgamma1 1g 7 8 set 9 enddo figur 1 onlin pnorm algorithm algorithm use z vector directli predict rather predict use vector w transform version z vector name w w henc onlin 2norm algorithm perceptron algo rithm 16 shown p 1 onlin pnorm algorithm approach version winnow algorithm precis follow theorem 16 give mistak bound onlin pnorm algorithm theorem 1 let sequenc label exampl everi exampl hx yi 2 2 0 onlin pnorm algorithm invok input paramet p z mistak bound exampl sequenc b 2 mistak bound z 0 c let suppos describ part b mistak bound given b converg ux log log 23 onlin pac learn variou gener procedur propos 1 18 22 automat convert onlin learn algorithm pacmodel algorithm procedur sampl complex result pac algorithm depend mistak bound origin onlin learn algorithm strongest gener result type term minim sampl complex result pac algorithm longestsurvivor convers due kearn li pitt theorem 2 let onlin learn algorithm guarante make mistak pacmodel learn algorithm 0 use logffi log exampl output fflaccur hypothesi probabl theorem 1 2 yield sampl complex bound gener pacmodel convers onlin pnorm algo rithm describ complet differ pacmodel algorithm remark similar sampl complex bound 3 pacmodel pnorm weak pnorm weak learn algorithm motiv follow simpl idea suppos collect label exampl replac neg exampl equival posit exampl obtain new collect 0 exampl let averag locat exampl 0 ie z center mass everi exampl 0 must lie side hyperplan vector u clear z must also lie side hyper plane one might even hope z relat vector point approxim direct vector u pnorm weak learn algorithm call wla present figur 2 onlin pnorm algorithm wla transform vector z vector w use map show simpl algorithm fact weak learner theorem 3 wla 12 gamma flweak learn algorithm 1 littleston 27 give convers procedur yield pac sampl complex bound offl although improv result 22 log factor littleston procedur requir exampl space x finit stronger assumpt make paper input paramet real number p 2 sequenc label exampl distribut 1 set 1 2 return hypothesi hx j figur 2 pnorm weak learn algorithm wla proof let sequenc label exampl x 2 x x everi let distribut show hypothesi h wlap return see h map x gamma1 1 note holder inequ impli show inequ 1 section 21 hold thus2 wk q thu suffic show wk q first note x ja henc lefthand side desir inequ equal second equal use fact p gamma p consequ lefthand side simplifi kzk p thu goal show kzk p ffi x ja last line follow holder inequ theorem prove shown simpl wla algorithm weak learn algorithm halfspac learn problem section use techniqu boost larg margin classif obtain strong learn algorithm small sampl complex 41 boost achiev high accuraci seri import paper schapir 31 freund 10 11 given boost algorithm transform weak learn algorithm strong one paper use adaboost algorithm 13 shown figur 3 notat algorithm similar 34 35 input adaboost sequenc label exampl weak learn algorithm wl two paramet given distribut output hypothesi h adaboost success gener new distribut use wl obtain hypothes h ultim output final hypothesi linear threshold function h 13 freund schapir prove algorithm wl 12 gamma flweak learn algorithm ie call wl adaboost gener hypothesi h ffl fraction exampl misclassifi final hypothesi h given result one straightforward way obtain strong learn algorithm halfspac learn problem draw suffici larg specifi sampl exampl oracl exu run adaboost use wla weak learn algorithm fl given theorem 3 choic ensur adaboost final hypothesi make error moreov sinc hypothesi gener wla form h v final hypothesi use wellknown fact vc dimens class zerobia input paramet sequenc label exampl weak learn algorithm wl gamma1 1 two real valu 1 set log 1 2 3 4 let h output wld 5 set ffl 7 normal factor t1 distribut 9 enddo 10 output final hypothesi hx j signfx figur 3 adaboost algorithm linear threshold function n n main result impli probabl least 1 gamma ffi final hypothesi h fflaccur hypothesi u provid jsj cffl gamma1 n logffl c 0 analysi though attract simpl yield rather crude bound sampl complex depend particular learn problem ie u x rest section use recent result adaboost abil gener largemargin classifi gener abil largemargin classifi give much tighter bound sampl complex learn algorithm 42 boost achiev larg margin suppos classifi form say margin h label exampl hx yi yfx note quantiti nonneg h correctli predict label associ x follow theorem extens theorem 5 34 show adaboost gener larg margin hypothes theorem 4 suppos adaboost run exampl sequenc use weak learn algorithm wl gamma1 1 valu 0 theorem state 34 cover case wl map fgamma1 1g need gener version weak hypothes theorem 3 map gamma1 1 rather fgamma1 1g proof theorem 4 given appendix result section 3 impli wla use learn algorithm adaboost valu ffl alway 12 gamma fl upper bound theorem 4 becom 1gamma2fl 1gamma 12fl 1 easi lemma prove appendix b 14 set appli lemma upper bound theorem 4 becom obtain follow corollari 6 adaboost run sequenc label exampl drawn exu use wla learner fl defin theorem 3 hypothesi h adaboost gener margin least fl2 everi exampl proof bound caus greater 2 log 1 consequ upper bound theorem 4 less 1jsj next subsect use corollari 6 theori larg margin classif establish bound gener error h term sampl size 43 larg margin gener let f collect realvalu function set x finit set fx said shatter f real number r b function f b 2 f 0 fatshatt dimens f scale denot fat f size largest set shatter f finit infin otherwis fatshatt dimens use us follow theorem 4 theorem 7 let f collect realvalu function x let distribut x theta fgamma1 1g let sequenc label exampl drawn probabl least 1 gamma ffi choic classifi hx j signfx f 2 f margin least 0 everi exampl pr log 8em 8m note section 41 final hypothesi h adaboost output must form x invoc wla gener hypothesi form x kv k q 1 impli vector must satisfi kvk q 1 consid class function ae x 7 oe bound fat f given sampl size theorem 7 immedi yield correspond bound pr x2d hx 6 signu delta x halfspac learn prob lem follow theorem prove appendix c give desir bound fat f theorem 8 let x bound region n let f class function x defin 2 fat f 2 log 4n combin theorem 3 corollari 6 theorem 7 follow algorithm use sampl size probabl least 1 gamma ffi hypothesi h gener satisfi pr x2d log n log 2 log thu establish follow onot hide log theorem 9 algorithm obtain appli adaboost wla use paramet set describ corollari 6 strong learn algorithm u x sampl complexit sampl complex boostingbas pnorm pac learn algorithm remark similar pac transform onlin pnorm algorithm section 21 log factor set bound depend linearli ffl gamma1 quadrat kuk q kxk p ffi compar bound detail see onlin variant describ part theorem 1 extra factor bound present sampl complex algo rithm variant offer advantag though user need know valu quantiti kxk p kuk q advanc order run algorithm turn part b theorem 1 see paramet set appropri onlin algorithm onlin bound differ pac algorithm bound extra factor z 0 ignor log factor part c theorem 1 show even z 0 chosen also note omegagamma557 n gentil littleston 15 given altern express onlin pnorm bound term kxk1 use entir similar analysi bound algorithm analog rephras case well 51 sinc case onlin pnorm algorithm precis perceptron algorithm case algorithm view natur pacmodel analogu onlin perceptron algorithm note upper bound given lemma 12 appendix c strengthen delta kxk 2 see lemma 13 4 theorem 41 2 proof mean fatshatt dimens upper bound theorem 8 improv 2 remov log factor bound theorem 9 howev bound still contain variou log factor log term theorem 7 52 algorithm extrem defin natur algorithm consid vector z w comput weak learn algorithm wla let r number coordin z z jz lim wk q ae signz r jz henc natur consid version wla denot wla 0 vector w defin take wise analysi continu hold minor modif describ appendix obtain strong learn algorithm theorem 9 hold place wla close relationship work jackson craven learn spars perceptron 20 note one coordin z jz wla 0 hypothesi kxk1 sign variabl strongli correl distribut valu signu delta similar weak learn algorithm use jackson craven 20 take singl bestcorrel liter hypothesi break ing tie arbitrarili proof bestsingleliter algorithm use 20 weak learn algorithm due goldmann hastad razborov 17 howev proof 17 assum exampl space x f0 1g n target vector u integ coeffici thu note jackson craven 20 algorithm learn spars perceptron appli learn problem defin discret input domain contrast algorithm appli continu input domain restrict exampl space x target vector u satisfi also observ theorem 9 establish tighter sampl complex bound strong learn algorithm given 20 see let suppos target vector coeffici algorithm 20 appli learn problem ffi ux omegagamma20 let theorem 9 impli learn algorithm sampl complex roughli 2 ffl ig nore log factor substanti improv roughli 4 ffl sampl complex bound given 20 gener sampl complex bound given 20 learn sspars kperceptron roughli ks 4 ffl analysi paper easili extend establish sampl complex bound roughli ks 2 ffl learn spars kperceptron 6 open question result give evid broad util boost algorithm adaboost natur question much util extend simpl boostingbas pac version standard learn algorithm note context kearn mansour 23 shown variou heurist algorithm topdown decis tree induct view instanti boost anoth goal construct power boostingbas pac algorithm linear threshold function algorithm discuss paper invers quadrat depend separ paramet ffi linearprogram base algorithm learn linear threshold function see eg 6 7 9 29 30 depend natur boostingbas pac algorithm linear threshold function perform bound similar linearprogram base algorithm acknowledg warmli thank le valiant help comment suggest r machin learn 2 probabilist method proc 36th symp found comp sci advanc kernel method support vector learn perceptron algorithm fast nonma liciou distribut proc 37th symp found comp sci learnabl vapnikchervonenki dimens proc 38th symp found comp sci boost weak learn algorithm joriti fifth ann work comp learn theori proc ninth ann conf comp learn theori decisiontheoret gener onlin learn applic boost proc eleventh ann conf comp learn theori proc 12th ann conf comp learn theori proc 10th ann conf comp learn theori major gate vs gener weight threshold gate space effici learn algorithm probabl inequ sum bound random variabl advanc neural inform process system 8 combin weak classifi proc fourth int workshop machin learn proc 28th symp theor comp 21st acm symp theor comp proc eighth ann conf comp learn theori learn quickli irrelev attribut abound new linearthreshold algorithm mistak bound logarithm linearthreshold learn algorithm proc fourth ann conf comp learn theori halfspac learn comput learn theori natur learn system volum constraint prospect strength weak learnabl proc twelfth ann conf comp learn theori proc twelfth ann conf comp learn theori boost margin new explan effect vote method proc eleventh ann conf comp learn theori criteria lower bound perceptronlik learn rule advanc calculu tr