multicategori classif support vector machin examin problem discrimin object three class specif investig twoclass discrimin method extend multiclass case show linear program lp approach base work mangasarian quadrat program qp approach base vapnik support vector machin svm combin yield two new approach multiclass problem lp multiclass discrimin singl linear program use construct piecewiselinear classif function propos multiclass svm method singl quadrat program use construct piecewisenonlinear classif function piec function take form polynomi radial basi function even neural network k 2class problem svm method origin propos requir construct twoclass svm separ class remain class similarili k twoclass linear program use multiclass problem perform empir studi origin lp method propos k lp method propos singl qp method origin k qp method discuss advantag disadvantag approach b introduct investig problem discrimin larg realworld dataset two class given exampl point known come k class construct function discrimin class goal select function ecient correctli classifi futur point classif techniqu use data mine pattern recognit exampl unit state postal servic interest ecient yet accur method classifi zipcod actual handwritten digit zipcod collect unit state postal servic use studi digit repres 16 16 pixel grayscal map result 256 attribut sampl number given enorm quantiti mail postal servic sort day accuraci ecienc evalu extrem import paper combin two independ relat research direct develop solv twoclass linear discrimin problem first linear program lp method stem multisurfac method mangasarian 12 13 method later extens robust linear program rlp approach 6 use highli success breast cancer diagnosi system 26 second direct quadrat program qp method base vapnik statist learn theori 24 25 statist learn theori address mathemat problem best construct function gener well futur point problem construct best linear twoclass discrimin pose convex quadrat program linear constraint result linear discrimin known support vector machin svm function subset train data known support vector specif implement gener optim plane gop method proven perform well practic 8 throughout paper refer two dierent approach rlp svm primari focu paper two research direct dier approach solv problem k 2 class origin svm method multiclass problem find k separ twoclass discrimin 23 discrimin construct separ singl class other process requir solut k quadrat program appli k classifi origin multicategori dataset multipli classifi point unclassifi point may occur ambigu avoid choos class point correspond classif function maxim point lp approach directli construct k classif function point correspond class function maxim 5 6 multicategori discrimin method 5 6 construct piecewiselinear discrimin k class problem use singl linear program call method mrlp sinc direct extens rlp approach show two dierent approach combin two yield two new method krlp msvm section 2 provid background exist rlp svm method kclass case quit dierent twoclass linear discrimin method svm rlp almost ident dier regular term use object use regular form rlp propos 3 equival svm except dierent norm use regular term twoclass linear discrimin rlp gener equal well comput ecient svm rlp exploit fact stateoftheart lp code far ecient reliabl qp code primari appeal svm simpli elegantli appli nonlinear discrimin minor chang svm method construct wide class twoclass nonlinear discrimin solv singl qp 24 basic idea point map nonlinearli higher dimension space dual svm problem use construct linear discrimin higher dimension space nonlinear origin attribut space use kernel function dual svm problem svm ecient eectiv construct mani type nonlinear discrimin function includ polynomi radial basi function machin neural net work success polynomialtim nonlinear method base lp use multistep approach method roy et al 20 19 18 use cluster conjuct lp gener neural network polynomi time anoth approach recurs construct piecewiselinear discrimin use seri lp 13 2 15 approach could also use svm limit discuss nonlinear discrimin construct use svm kerneltyp approach introduct exist multiclass method mrlp k svm show idea use mrlp adapt construct multiclass svm use singl quadrat program adapt problem formul similar twoclass case twoclass case initi problem construct linear discrimin data point transform higher dimension featur space linear discrimin construct higher dimens space result nonlinear classif function origin featur space section 3 k 2 class case begin construct piecewiselinear discrimin function regular term ad avoid overfit method extend piecewis nonlinear classif function section 4 variabl map higher dimension space piecewiselinear discrimin function construct new space result piecewisenonlinear discrimin origin space section 5 extend method piecewis insepar dataset call final approach multicategori support vector machin msvm depend choic transform piec may polynomi radial basi function neural network etc concentr research polynomi classifi leav comput investig classif function futur work figur 1 show piecewisesecond degre polynomi separ three class two dimens msvm requir solut larg quadrat program transform data point higher dimens featur space number figur 1 piecewisepolynomi separ three class two dimens variabl grow exponenti exampl second degre polynomi classifi two dimens requir origin variabl x 1 x 2 well variabl x 2 2 x 1 x 2 primal problem problem size explod degre polynomi increas dual problem howev remain tractabl number dual variabl k 1 time number point regardless transform select dual problem transform appear inner product high dimension space inexpens techniqu exist comput inner product dual variabl correspond point origin featur space point correspond posit dual variabl refer support vector goal maintain high accuraci use small number support vector minim number support vector import gener also reduc comput time requir evalu new exampl section 6 contain comput result compar two lp approach k rlp mrlp two qp approach ksvm msvm method compar term gener test set accuraci number support vector comput time follow notat use throughout paper mathemat abstract problem follow given element set 1 k ndimension real space r n construct discrimin function determin separ point distinct region region contain point belong almost class let j set point ndimension real space r n cardin j let j j n matrix whose row point j th point j th row j denot j let e denot vector one appropri dimens scalar 0 vector zero repres 0 thu x r n x 0 impli x 0 similarli n set minim fx set denot arg min xs fx vector x r n x denot vector r n compon x n step function x denot vector 0 1 n compon n vector x r n matrix r nm transpos x denot x respect dot product two vector x denot x background section contain brief overview rlp svm method cla sific first discuss twoclass problem use linear classifi svm two class defin rlp review final piecewiselinear function use multicategori classif mrlp review 21 two class linear discrimin commonli method discrimin two class point involv determin linear function consist linear combin attribut given set simplest case linear function use separ two set shown figur 2 function separ plane x psfrag replac figur 2 two linearli separ set separ plane w normal plane distanc origin let 1 2 two set point ndimension real space r n cardin 1 2 respect let 1 1 n matrix whose row point 1 let 2 2 n matrix whose row point 2 let x r n point classifi follow two set point 1 2 linearli separ e vector one appropri dimens two class linear separ infinit mani plane separ two class goal two choos plane gener best futur point mangasarian 12 vapnik chervonenki 25 conclud best plane separ case one minim distanc closest vector class separ plane separ case formul mangasarian multisurfac method pattern recognit 13 vapnik optim hyperplan 24 25 similar 3 concentr optim hyperplan problem sinc basi svm valid theoret statist learn theori 24 accord statist learn theori optim hyperplan construct linear psfrag replac class 1 class 2 figur 3 two support plane result optim separ plane discrimin high dimension space without overfit reader consult 24 full detail statist learn theori cover paper problem canon form vapnik 24 becom determin two parallel plane margin distanc two plane maxim margin seper two support plane 2 w exampl plane shown figur 3 problem find maximum margin becomes24 min gener alway possibl singl linear function complet separ two given set point thu import find linear function discrimin best two set accord error minim criterion bennett mangasarian 4 minim averag magnitud misclassif error construct follow robust linear program problem rlp min subject z 2 w misclassif cost avoid null solut cardin 1 2 respect rlp method eectiv practic function gener rlp gener well mani realworld problem addit comput time reason small solut involv singl linear program note howev rlp method longer includ notion maxim margin statist learn theori indic maxim margin essenti good gener svm approach 8 23 multiobject quadrat program minim absolut misclassif error maxim separ margin minim w 2 min fix constant note problem 6 equival rlp addit regular term linear program version 6 construct replac norm use minim weight w 3 recal svm object minim squar 2norm w w 1norm w w use instead absolut valu function remov introduc variabl constraint w svm object modifi substitut e w w optim k result lp min wyz refer problem rlp sinc yield origin rlp method svm method rlp method minim averag distanc misclassifi point relax support plane maximum classif error main advantag rlp method svm problem rlp linear program solvabl use robust algorithm simplex method 17 svm requir solut quadrat program typic much comput costli size problem 3 rlp method found gener well linear svm much less comput cost ecient comput solv dual rlp svm prob lem dual rlp problem min uv e paper use 1 2 may posit weight misclassif cost dual svm problem extens nonlinear discrimin given next section 22 nonlinear classifi use support vector machin primari advantag svm 6 rlp 7 dual form use construct nonlinear discrimin use polynomi separ radial basi function neural network etc basic idea map origin problem higher dimension space construct linear discrimin higher dimension space correspond linear discrimin origin space exampl construct quadrat discrimin two dimension problem input attribut x 1 x 2 map linear discrimin function construct new fivedimension space two exampl possibl polynomi classifi given figur 4 dual svm appli map point regular term primal object help avoid overfit higher dimension space dual svm provid practic comput approach use gener inner product kernel figur 4 two exampl second degre polynomi separ two set dual svm follow follow min formul nonlinear case conveni rewrit problem summat notat let set point 1 2 defin 2 total number point let x construct nonlinear classif function origin data point x transform higher dimens featur space function x n dot product origin vector x replac dot product transform vector x first term object function written sum use notat simplifi problem becom min st support vector machin svm vapnik replac inner product inner product hilbert space kx x symmetr function kx x must satisfi theorem 53 23 theorem ensur inner product featur space choic kx x determin type classifi construct possibl choic includ polynomi classifi figur 4 kx x degre polynomi radial basi function machin k xx x x distanc two vector width param ter twolay neural network kx x sigmoid function 23 variant svm 10 proven quit success paractic 21 22 7 note number variabl program 10 remain constant increas dimension addit object function remain quadrat thu complex problem increas fact size problem depend number nonzero dual variabl point x correspond variabl call support vector accord statist learn theori best solut given misclassif error use minimum number support vector final classif function gener kernel function kx x support vector psfrag replac 3 figur 5 piecewiselinear separ set 1 2 3 convex piecewiselinear function fx 23 multicategori discrimin multicategori classif piecewiselinear separ use discrimin point examin two method accomplish first use svm 24 two construct discrimin function separ one class remain k 1 class process repeat k time separ case linear discrimin class must satisfi follow set inequ find w classifi new point x comput f one clearli point belong class one f x 0 class ambigu thu gener rule class point x determin w find maxim figur 5 show piecewiselinear function r separ three set note either svm 10 rlp use construct k twoclass discrimin clariti call method use svm 10 k svm denot method use rlp 8 ksvm advantag ksvm use piecewisenonlinear discrimin krlp limit piecewiselinear discrimin ksvm krlp attain perfect train set accuraci follow inequ must satisfi inequ use definit piecewiselinear separ definit 21 piecewiselinear separ set point 1 k repres matric piecewis linearli separ exist w equival definit 21 find piecewiselinear separ involv solv equat w e rewritten 0 w e figur 6 show exampl piecewiselinear separ three class two dimens linear separ function repres quantiti psfrag replac 3 figur three class separ piecewiselinear function w 1 k mrlp method 1 propos investig 5 6 use find w min mrlp 15 optim object valu zero dataset piecewiselinearli separ dataset piecewis linearli separ posit valu variabl ij l proport 1 method origin call multicategori discrimin magnitud misclassifi point plane x w program 15 gener twoclass rlp linear program 5 multicategori case like origin rlp 5 mrlp includ term maxim margin directli permit use gener inner product kernel allow extens nonlinear case next section show mrlp svm combin includ margin maxim gener inner product mrlp 3 formul msvm piecewiselinear separ case propos construct piecewiselinear piecewisenonlinear svm use singl quadrat program analog two class case start formul optim piecewiselinear separ separ case assum k set point piecewiselinearli separ ie exist class point x determin w find maxim piecewiselinearli separ problem infinit mani w exist satisfi 16 intuit optim w provid largest margin classif approach analog two class support vector machin svm approach add regular term dash line figur 7 repres margin piec w piecewiselinear separ function margin separ class j ie distanc 2 would like minim w also add regular term 1k object piecewiselinearli separ problem get follow min st w simplifi notat formul piecewiselinear svm rewrit matrix notat see appendix complet matrix definit gener k three class problem follow matric let psfrag replac w 1 3 w 1 w 2 w 1 w 1 figur 7 piecewiselinear separ margin three class r nn ident matrix let vector one use notat fix k 2 program becom min w k dual problem written u elimin variabl w problem first show matrix c nonsingular proposit 31 nonsingular c invers matrix c k 2 nk1 n 1 nk1 n 1 nk1 n 1 nk1 n n indic n n ident matrix proof show c nonsingular k 2 calcul invers matrix c defin appendix size n 1 kn recal n indic dimens featur space k 1i n size kn kn therefor kn kin kin simpl calcul shown invers matrix nk1 n 1 nk1 n 1 nk1 n 1 nk1 n use proposit 31 follow relationship result 22 follow problem 20 equat 22 u 23 use relationship elimin w dual problem addit remov simplif new dual problem becom u construct multicategori support vector machin conveni problem summat notat let dual vector u 1k u kk1 result dual problem piecewiselinear dataset l li l l l 0 number point class recal piecewiselinear classif function class point x determin find maxim equat 23 u solv w summat notat get therefor 4 formul msvm piecewisenonlinearli separ case like twoclass case msvm gener piecewis nonlinear function construct separ function f x higher dimens featur space origin data point x transform 8 function f x relat sum dot product vector higher dimens featur space accord 23 symmetr function kx x theorem 9 replac dot product x x mercer theorem guarante eigenvalu j expans kx x posit sucient condit function kx x defin dot product higher dimens featur space therefor let kx x return dual problem 25 object function contain sum dot product j two point origin featur space transform point j p higher dimens featur space replac dot product result msvm piecewiselinearli separ dataset l li l l l l 0 point l correspond nonzero dual variabl u ij l refer support vector possibl l correspond figur 8 piecewisepolynomi separ three class two dimens support vector indic circl one nonzero variabl l figur 8 support vector repres circl around point point doubl circl indic two dual variabl u ij l 0 complementar within kkt condit 14 l l w consequ support vector locat closest separ func tion fact remaind point support vector necessari construct separ function result nonlinear classif problem point x find classif function support vector support vector maxim 5 formul msvm piecewis insepar case proceed section provid formul piecewiselinearli piecewisenonlinear separ case construct classif function piecewis linearli insepar dataset must first choos error minim crite rion techniqu use preceed section formul msvm piecewiselinearli separ dataset combin 1norm error criterion use problem 15 bennett mangasarian 6 result msvm piecewiselinearli insepar problem use matrix notat section 3 add term 1 object problem 15 result primal problem follow min wy 13 solv dual substitut u simplifi produc follow problem u shown proposit 51 problem 30 maxim concav quadrat object bound polyhedr set thu exist local optim solut global optim proposit 51 concav object function u e 1 u concav proof matrix alway posit semidefinit symmetr thu hessian matrix 1 neg semidefinit therefor object concav function problem 30 ident problem 24 piecewiselinearli separ case except dual variabl bound 1 therefor transform data point l proceed ident section 4 use function denot dot product featur space final msvm result l li l l 1 section 3 4 class point x determin find maximum function support vector support vector determin threshold valu solv primal problem w fix aw transform higher dimens featur space problem follow min l st r l l r li r r l l r r l 0 right side constraint constant thu problem 33 linear program easili solv 6 comput experi section present comput result compar msvm 32 rlp 15 ksvm use svm 10 krlp use rlp 8 sever experi realworld dataset report descript dataset follow paragraph method implement use mino 54 17 solver quadrat program problem svm ksvm solv use nonlinear solver implement mino 54 solver use reducedgradi algorithm conjunct quasinewton method msvm ksvm mrlp select valu given better solut may result dierent choic addit alli necessari valu use method kernel function piecewisenonlinear msvm ksvm method degre desir polynomi wine recognit data wine dataset 1 use chemic analysi wine determin cultivar 178 point 13 featur three class dataset distribut follow 59 point class 1 71 point class 2 48 point class 3 dataset avail via anonym file transfer protocol ftp uci repositori machin learn databas domain theori 16 ftpftpicsuciedupubmachinelearningdatabas glass identif databas glass dataset 11 use identifi origin sampl glass chemic analysi dataset compris six class 214 point 9 featur distribut point class follow 70 float process build window 17 float process vehicl window 76 nonfloat process build window 13 contain 9 tablewar 29 headlamp dataset avail via anonym file transfer protocol ftp uci repositori machin learn databas domain theori 16 ftpftpicsuciedupubmachinelearningdatabas us postal servic databas usp databas 10 contain zipcod sampl actual mail databas compris separ train test set 7291 sampl train set 2007 sampl test set sampl belong one ten class integ 0 9 sampl repres 256 featur two experi perform first dataset normal 1 1 10fold cross valid use estim gener futur data second experi conduct two subset unit state postal servic usp data data contain handwrit sampl integ 0 9 object dataset quickli eectiv interpret zipcod data separ train test set consist 10 integ class compil two individu train subset usp train data first subset contain 1756 exampl belong class 3 5 8 call set usps1 train data second subset contain 1961 exampl belong class 4 6 7 call set usps2 train data similarli two subset creat test data dataset data valu scale 1 test set accuraci report four method total number uniqu support vector result classif function msvm ksvm method given tabl contain result mrlp krlp msvm ksvm wine glass dataset anticip ad regular term msvm 9719 9719 9775 9663 9663 glass mrlp 6495 ksvm 4346 5561 6495 7056 7243 tabl 1 percent test set accuraci total number support vector msvm ksvm05 krlp msvm ksvm degre one problem msvm produc better test gener mrlp wine dataset wine dataset piecewiselinearli separ therefor mrlp method infinit mani optim solut ever test accuraci msvm degre one glass data much lower mrlp accuraci may indic choic larg howev degre increas accuraci msvm method improv exce mrlp result ksvm method gener surprisingli well test accuraci report ksvm wine dataset higher msvm linear krlp method perform well quadrat ksvm program wine dataset better msvm mrlp method glass data degre creas method msvm ksvm improv dramat test accuraci use higher degre polynomi msvm ksvm method surpass accuraci mrlp krlp demonstr potenti polynomi piecewisepolynomi classif function linear piecewiselinear function tabl contain result four method usp data subset similar observ made dataset piecewis linearli separ solut mrlp found dataset test significantli lower method ksvm method gener slightli better msvm krlp method report similar accuraci ksvm method addit solv linear program rather quadrat program comput train time significantli smaller method chang paramet may improv gener aliz msvm method consist find classif function use fewer support vector ksvm fewer support vector sam msvm 9126 9187 9228 9207 9228 ksvm 9167 9228 9289 9268 9248 msvm 9458 9497 9536 9497 9400 ksvm 9613 9652 9613 9516 9458 tabl 2 percent test set accuraci total number support vector msvm svm05 ksvm degre tabl 3 total comput train time second mrlpkrlp msvm ksvm usps1 ple classifi quickli sinc dotproduct sampl support vector must comput thu msvm would good method choos classif time critic cpu time train four method usps1 dataset report tabl 3 time dataset list program run use batch system cluster machin time reliabl howev trend clear krlp method significantli faster method msvm ksvm method degre increas comput time would decreas certain degre reach would increas degre polynomi start increas vari dataset surprisingli usp dataset ksvm method faster mrlp method case wine glass dataset mrlp method faster train time ksvm dataset time report ibm rs6000 model 590 workstat 128 mb ram conclus examin four method solut multicategori discrimin problem base lp method mangasarian qp method svm vapnik twoclass method rlp svm dier norm regular term past two dierent approach use k 2 class case method call ksvm construct k twoclass discrimin use k quadrat program result classifi piecewiselinear piecewis nonlinear discrimin function depend kernel function use svm origin multicategori rlp k class construct piecewiselinear discrimin use singl linear program propos two new hybrid approach like ksvm method k rlp use lp construct k twoclass discrimin also formul new approach msvm began formul ad regular term mrlp like ksvm piecewisenonlinear discrimin nonlinear piec found map origin data point higher dimens featur space transform appear dual problem inner product two point higher dimens space gener inner product use make problem tractabl new msvm method requir solut singl quadrat program perform comput studi four method four dataset gener found k svm krlp gener howev msvm use fewer support vector counterintuit result sinc twoclass class statist learn theori predict fewer support vector result better gener theoret justif better gener ksvm krlp svm mrlp open question krlp method provid accur ecient result piecewiselinear separ dataset ksvm also test surprisingli well requir solut k quadrat program thu provid solut smaller classif time piecewis linearli insepar dataset polynomi piecewisepolynomi classifi provid improv mrlp krlp method dataset krlp method found solut gener best nearli best less comput time matrix represent multicategori support vector machin appendix contain definit matric use gener kclass svm formul 18 min let 0 0 0 0 0 0 0 0 0 r nn ident matrix matrix c n 0 0 0 0 0 0 0 0 0 0 matrix 0 0 0 0 0 0 0 0 0 0 vector one matrix r comparison classifi high dimension set decis tree construct via linear program geometri learn neural network train via linear program multicategori discrimin via linear program serial parallel multicategori discrimin support vector network method mathemat physic rule induct forens scienc linear nonlinear separ pattern linear program nonlinear program mathemat program machin learn uci repositori machin learn databas mino 54 user guid algorithm gener radial basi function rbflike net classif problem polynomi time algorithm construct train class multilay perceptron pattern classif use linear program ming incorpor invari support vector machin compar support vector machin gaussian kernel radial basi function classifi natur statist learn theori natur statist learn theori theori pattern recognit multisurfac method pattern separ medic diagnosi appli breast cytolog tr polynomi time algorithm construct train class multilay perceptron algorithm gener radial basi function rbflike net classif problem natur statist learn theori network featur minim within decis tree featur select via concav minim support vector machin incorpor invari support vector learn machin comparison viewbas object recognit algorithm use realist 3d model compar support vector machin gaussian kernel radial basi function classifi ctr tieyan liu yime yang hao wan huajun zeng zheng chen weiy support vector machin classif largescal taxonomi acm sigkdd explor newslett v7 n1 p3643 june 2005 kobi crammer yoram singer algorithm implement multiclass kernelbas vector machin journal machin learn research 2 312002 rong jin jian zhang multiclass learn smooth boost machin learn v67 n3 p207227 june 2007 glenn fung l mangasarian multicategori proxim support vector machin classifi machin learn v59 n12 p7797 may 2005 ryan rifkin aldebaro klautau defens onevsal classif journal machin learn research 5 p101141 1212004 yiguang liu zhisheng lipe cao novel quick svmbase multiclass classifi pattern recognit v39 n11 p22582264 novemb 2006 ping zhong masao fukushima secondord cone program formul robust multiclass classif neural comput v19 n1 p258282 januari 2007 andrea albrecht chakkuen wong approxim boolean function local search comput optim applic v27 n1 p5382 januari 2004 isabel guyon jason weston stephen barnhil vladimir vapnik gene select cancer classif use support vector machin machin learn v46 n13 p389422 2002 fabien lauer ching suen grard bloch trainabl featur extractor handwritten digit recognit pattern recognit v40 n6 p18161824 june 2007