comparison predict accuraci complex train time thirtythre old new classif algorithm twentytwo decis tree nine statist two neural network algorithm compar thirtytwo dataset term classif accuraci train time case tree number leav classif accuraci measur mean error rate mean rank error rate criteria place statist splinebas algorithm call polyclsss top although statist significantli differ twenti algorithm anoth statist algorithm logist regress second respect two accuraci criteria accur decis tree algorithm quest linear split rank fourth fifth respect although splinebas statist algorithm tend good accuraci also requir rel long train time polyclass exampl third last term median train time often requir hour train compar second algorithm quest logist regress algorithm substanti faster among decis tree algorithm univari split c45 indcart quest best combin error rate speed c45 tend produc tree twice mani leav indcart quest b introduct much current research machin learn statist commun algorithm decis tree classifi often emphasi accuraci algorithm one studi call statlog project michi spiegelhalt taylor 1994 compar accuraci sever decis tree algorithm nondecis tree algorithm larg number dataset studi smaller scale includ brodley utgoff 1992 brown corrubl pittard 1993 curram minger 1994 shavlik mooney towel 1991 recent comprehens tree structur receiv attent comprehens typic decreas increas tree size complex two tree employ kind test predict accuraci one fewer leav usual prefer breslow aha 1997 survey method tree simplif improv comprehens third criterion larg ignor rel train time algorithm statlog project find algorithm uniformli accur dataset studi instead mani algorithm possess compar accuraci algorithm excess train time may undesir hand 1997 purpos paper extend result statlog project follow way 1 addit classif accuraci size tree compar train time algorithm although train time depend somewhat impl mentat turn larg differ time second versu day differ attribut implement alon 2 includ decis tree algorithm includ statlog project name splu tree clark pregibon 1993 holt maass 1995 oc1 murthi kasif salzberg 1994 lmdt brodley utgoff 1995 quest loh shih 1997 3 also includ sever newest splinebas statist algorithm classif accuraci may use benchmark comparison algorithm futur 4 studi effect ad independ nois attribut classif accuraci appropri tree size algorithm turn except possibl three algorithm other adapt nois quit well 5 examin scalabl promis algorithm sampl size increas experi compar twentytwo decis tree algorithm nine classic modern statist algorithm two neural network algorithm mani dataset taken univers california irvin uci repositori machin learn databas merz murphi 1996 fourteen dataset reallif domain two artifici construct five dataset use statlog project increas probabl find statist signific differ algorithm number dataset doubl addit nois attribut result total number dataset thirtytwo section 2 briefli describ algorithm section 3 give background dataset section 4 explain experiment setup use studi section 5 analyz result issu scalabl studi section 6 conclus recommend given section 7 2 algorithm short descript algorithm given detail may found cite refer algorithm requir class prior probabl made proport train sampl size 21 tree rule cart use version cart breiman friedman olshen stone 1984 implement cart style ind packag buntin caru ana 1992 gini index divers split criterion tree base 0se 1se prune rule denot ic0 ic1 respec tive softwar obtain httpicwwwarcnasagovicprojectsbayesgroupindindprogramhtml splu tree variant cart algorithm written languag becker chamber wilk 1988 describ clark pregibon 1993 employ devianc split criterion best tree chosen tenfold crossvalid prune perform ptree function treefix librari venabl ripley 1997 statlib archiv httplibstatcmuedu 0se 1se tree denot st0 st1 respect c45 use releas 8 quinlan 1993 quinlan 1996 default set includ prune httpwwwcseunsweduauquinlan tree construct c45 rule induct program use produc set rule tree denot c4t rule c4r fact fast classif tree algorithm describ loh vanichs takul 1988 employ statist test select attribut split node use discrimin analysi find split point size tree determin set stop rule tree base univari split split singl attribut denot ftu base linear combin split split linear function attribut denot ftl fortran 77 program obtain httpwwwstatwisceduloh quest new classif tree algorithm describ loh shih 1997 quest use univari linear combin split uniqu featur attribut select method neglig bia attribut uninform respect class attribut approxim chanc select split node tenfold crossvalid use prune tree univari 0se 1se tree denot qu0 qu1 respect correspond tree linear combin split denot ql0 ql1 respect result paper base version 1710 program softwar obtain httpwwwstatwiscedulohquesthtml ind due buntin 1992 use version 21 default set ind come sever standard predefin style compar four bayesian style paper bay bay opt mml mml opt denot ib ibo im imo respect opt method extend nonopt method grow sever differ tree store compact graph struc ture although time memori intens opt style increas classif accuraci oc1 algorithm describ murthi et al 1994 use version 3 httpwwwcsjhuedusalzbergannounceoc1html compar three style first one denot ocm default use mixtur univari linear combin split second one option denot ocu use univari split third one option denot ocl use linear combin split option kept default valu lmdt algorithm describ brodley utgoff 1995 construct decis tree base multivari test linear combin attribut tree denot lmt use default valu softwar httpyakeecnpurdueedubrodleysoftwarelmdthtml cal5 fraunhof societi institut inform data process germani muller wysotzki 1994 muller wysotzki 1997 use version 2 cal5 design specif numericalvalu attribut howev procedur handl categor attribut mix attribut numer categor includ studi optim two paramet control tree construct predefin threshold signific level ff randomli split train set two part stratifi class twothird use construct tree onethird use valid set choos optim paramet configura tion employ cshell program come cal5 packag choos best paramet vari ff 010 090 095 step 005 best combin valu minim error rate valid set chosen tree construct record train set use chosen paramet valu denot cal t1 onelevel decis tree classifi exampl basi one split singl attribut holt 1993 split categor attribut 5with b categori produc b reserv miss attribut valu hand split continu attribut yield j leav j number class one leaf reserv miss valu softwar obtain httpwwwcsiuottawacaholtelearningothersiteshtml 22 statist algorithm lda linear discrimin analysi classic statist method model instanc within class normal distribut common covari matrix yield linear discrimin function qda quadrat discrimin analysi also model class distribut normal estim covari matrix correspond sampl covari matrix result discrimin function quadrat detail lda qda found mani statist textbook eg johnson 1992 use sa proc discrim sa institut inc 1990 implement lda qda default set nn sa proc discrim implement nearest neighbor method pool covari matrix use comput mahalanobi distanc log logist discrimin analysi result obtain poli tomou logist regress see eg agresti 1990 fortran 90 routin written first author httpwwwstatwiscedulimtlogdiscr fda flexibl discrimin analysi hasti tibshirani buja 1994 gener linear discrimin analysi cast classif problem one involv regress mar friedman 1991 nonparametr regress procedur studi use splu function fda mda librari statlib archiv two model use addit model degree1 denot fm1 model contain firstord interact degree2 penalty3 denot fm2 pda form penal lda hasti buja tibshirani 1995 design situat mani highli correl attribut classif problem cast penal regress framework via optim score pda implement splu use function fda methodgenridg mda stand mixtur discrimin analysi hasti tibshirani 1996 fit gaussian mixtur densiti function class produc classifi mda implement splu use librari mda pol polyclass algorithm kooperberg bose stone 1997 fit polytom logist regress model use linear spline tensor product provid estim condit class probabl use predict class label pol implement splu use function polyfit polyclass librari statlib archiv model select done tenfold crossvalid 23 neural network lvq use learn vector quantiz algorithm splu class librari venabl ripley 1997 statlib archiv detail algorithm may found kohonen 1995 ten percent train set use initi algorithm use function lvqinit train carri optim learn rate function olvq1 fast robust lvq algorithm addit finetun learn perform function lvq1 number iter ten time size train set olvq1 lvq1 use default valu 03 003 ff learn rate paramet olvq1 lvq1 respect rbf radial basi function network implement sa tnn3sa macro sarl 1994 feedforward neural network httpwwwsascom network architectur specifi archrbf argument studi construct network one hidden layer number hidden unit chosen 20 total number input output unit 25 5 hidden unit dna dna dataset 10 5 hidden unit tae tae dataset memori storag limit although macro perform model select choos optim number hidden unit util capabl would taken long dataset see tabl 6 therefor result report algorithm regard lower bound perform hidden layer fulli connect input output layer direct connect input output layer output layer class repres one unit take valu 1 particular categori 0 otherwis except last one refer categori avoid local optima ten preliminari train conduct best estim use subsequ train detail radial basi function network found bishop 1995 ripley 1996 3 dataset briefli describ sixteen dataset use studi well modif made experi fourteen real domain two artifici creat thirteen uci breast cancer bcw one breast cancer databas uci collect univers wisconsin w h wolberg problem predict whether tissu sampl taken patient breast malign benign two class nine numer attribut 699 observ sixteen instanc contain singl miss attribut valu remov analysi result therefor base 683 record error rate estim use tenfold crossvalid decis tree analysi subset data use fact algorithm report wolberg tanner loh 1987 wolberg tanner loh 1988 wolberg tanner loh 1989 dataset also analyz linear program method mangasarian wolberg 1990 contracept method choic cmc data taken 1987 nation indonesia contracept preval survey sampl marri women either pregnant know pregnant time interview problem predict current contracept method choic use longterm method shortterm method woman base demograph socioeconom characterist lerman molyneaux pangemanan iswarati 1991 three class two numer attribut seven categor attribut 1473 record error rate estim use tenfold crossvalid data obtain httpwwwstatwiscedupstatftppublohtreeprogsdataset statlog dna dna uci dataset molecular biolog use project splice junction point dna sequenc su perfluou dna remov process protein creation higher organ problem recogn given sequenc dna boundari exon part dna sequenc retain splice intron part dna sequenc splice three class sixti categor attribut four categori sixti categor attribut repres window sixti nucleotid one four categori middl point window classifi one exonintron boundari intronexon boundari neither 3186 exampl databas divid randomli train set size 2000 test set size 1186 error rate estim test set statlog heart diseas hea uci dataset cleveland clinic foundat courtesi r detrano problem concern predict presenc absenc heart diseas given result variou medic test carri patient two class seven numer attribut six categor attribut 270 record statlog project employ unequ misclassif cost use equal cost algorithm allow unequ cost error rate estim use tenfold crossvalid boston hous bo uci dataset give hous valu boston suburb harrison rubinfeld 1978 three class twelv numer attribut one binari attribut 506 record follow loh vanichs takul 1988 class creat attribut median valu owneroccupi home follow class otherwis error rate estim use tenfold crossvalid led display led artifici domain describ breiman et al 1984 contain seven boolean attribut repres seven lightemit diod ten class set decim digit attribut valu either zero one accord whether correspond light digit attribut valu ten percent probabl valu invert class attribut integ zero nine inclus c program uci use gener 2000 record train set 4000 record test set error rate estim test set bupa liver disord bld uci dataset contribut r forsyth problem predict whether male patient liver disord base blood test alcohol consumpt two class six numer attribut 345 record error rate estim use tenfold crossvalid pima indian diabet pid uci dataset contribut v sigillito patient dataset femal least twentyon year old pima indian heritag live near phoenix arizona usa problem predict whether patient would test posit diabet given number physiolog measur medic test result two class seven numer attribut 532 record origin dataset consist 768 record eight numer attribut howev mani attribut notabl serum sulin contain zero valu physic imposs remov serum insulin record imposs valu attribut error rate estim use tenfold cross valid statlog satellit imag sat uci dataset give multispectr valu pixel within 3 theta 3 neighborhood satellit imag classif associ central pixel neighborhood aim predict classif given multispectr valu six class thirtysix numer attribut train set consist 4435 record test set consist 2000 record error rate estim test set imag segment seg uci dataset use statlog project sampl databas seven outdoor imag imag handseg creat classif everi pixel one brickfac sky foliag cement window path grass seven class nineteen numer attribut 2310 record dataset error rate estim use tenfold crossvalid algorithm t1 could handl dataset without modif program requir larg amount memori therefor t1 algorithm discret attribut except attribut 3 4 5 one hundr categori attitud toward smoke restrict smo survey dataset bull 1994 obtain httplibstatcmuedudatasetscsb problem predict attitud toward restrict smoke workplac prohibit restrict unrestrict base bylawrel smokingrel sociodemograph covari three class three numer attribut five categor attribut divid origin dataset train set size 1855 test set size 1000 error rate estim test set thyroid diseas thi uci anntraindatacontribut r werner problem determin whether patient hyperthyroid three class normal hyperfunct subnorm function six numer attribut fifteen binari attribut train set consist 3772 record test set 3428 record error rate estim test set statlog vehicl silhouett veh uci dataset origin ture institut glasgow scotland problem classifi given silhouett one four type vehicl use set featur extract silhouett vehicl view mani angl four model vehicl doubl decker bu chevrolet van saab 9000 opel manta 400 four class eighteen numer attribut 846 record error rate estim use tenfold crossvalid congression vote record vot uci dataset give vote member u hous repres 98th congress sixteen issu problem classifi congressman democrat republican base sixteen vote two class sixteen categor attribut three categori yea nay neither 435 record rate estim tenfold crossvalid waveform wav artifici threeclass problem base three wave form class consist random convex combin two waveform sampl integ nois ad descript gener data given breiman et al 1984 c program avail uci twentyon numer attribut 600 record train set error rate estim independ test set 3000 record evalu tae data consist evalu teach perform three regular semest two summer semest 151 teach assist ta assign statist depart univers wisconsinmadison score group three roughli equals categori low medium high form class attribut predictor attribut whether nativ english speaker bi nari ii cours instructor 25 categori iii cours 26 categori iv summer regular semest binari v class size numer dataset first report loh shih 1997 differ dataset two categor attribut larg number categori result decis tree algorithm cart employ exhaust search usual take much longer train algorithm cart evalu 2 split categor attribut c valu error rate estim use tenfold crossvalid data obtain httpwwwstatwiscedupstatftppublohtreeprogsdataset summari attribut featur dataset given tabl 1 tabl 1 characterist dataset last three column give number type ad nois attribut dataset notat n01 denot standard normal distribut uimn denot uniform distribut integ n inclus u01 denot uniform distribut unit interv train origin attribut type nois attribut data sampl num categor total numer categor total set size class 2 3 4 5 25 26 dna 2000 3 led 2000 bld pid smo thi veh 846 4 vot 435 2 tae 4 experiment setup algorithm design categor attribut case categor attribut convert vector 01 attribut categor attribut x take k valu fc g replac 1dimension vector otherwis vector consist zero affect algorithm statist neural network algorithm well tree algorithm ftl ocu ocl ocm lmt order increas number dataset studi effect nois attribut algorithm creat sixteen new dataset ad independ nois attribut number type nois attribut ad given right panel tabl 1 name new dataset origin dataset except addit symbol exampl bcw dataset nois ad denot bcw dataset use one two differ way estim error rate algorithm larg dataset size much larger 1000 test set size least 1000 use test set estim error rate classifi construct use record train set test test set twelv thirtytwo dataset analyz way remain twenti dataset use follow tenfold crossvalid procedur estim error rate 1 dataset randomli divid ten disjoint subset contain approxim number record sampl stratifi class label ensur subset class proport roughli whole dataset 2 subset classifi construct use record classifi test withheld subset obtain crossvalid estim error rate 3 ten crossvalid estim averag provid estim classifi construct data algorithm implement differ program languag languag avail platform three type unix workstat use studi workstat type implement languag algorithm given tabl 2 rel perform workstat accord spec mark given tabl 3 float point spec mark show task take one second dec 3000 would take 14 08 second sparcstat 5 ss5 sparcstat 20 ss20 respect therefor enabl comparison train time report term 3000equival secondsth train time record ss5 ss20 divid 14 08 respect 5 result error rate train time algorithm given separ tabl dataset appendix tabl also report error rate naiv plural rule ignor inform covari classifi everi record major class train sampl 51 exploratori analysi error rate present formal statist analysi result help studi summari tabl 4 mean error rate algorithm dataset given second row minimum maximum error rate tabl 2 hardwar softwar platform algorithm workstat dec 3000 alpha model 300 dec sun sparcstat 20 model 61 ss20 sun sparcstat 5 ss5 algorithm platform algorithm platform tree rule st1 splu tree 1se dec qu0 quest univari 0se decf90 lmt lmdt linear decc qu1 quest univari 1se decf90 cal cal5 ss5c ql0 quest linear 0se decf90 singl split decc ql1 quest linear 1se decf90 ftu fact univari decf77 statist linear decf77 lda linear discrimin anal decsa c4t c45 tree decc qda quadrat discrimin anal decsa c4r c45 rule decc nn nearestneighbor decsa ib ind bay style ss5c log linear logist regress decf90 ibo ind bay opt style ss5c fm1 fda degre 1 ss20 im ind mml style ss5c fm2 fda degre 2 ss20 imo ind mml opt style ss5c pda penal lda ss20 ic0 ind cart 0se ss5c mda mixtur discrimin anal ss20 ic1 ind cart 1se ss5c pol polyclass ss20 ocu oc1 univari ss5c ocl oc1 linear ss5c neural network ocm oc1 mix ss5c lvq learn vector quantiz ss20 st0 splu tree 0se dec rbf radial basi function network decsa tabl 3 spec benchmark summari workstat specfp92 specint92 sourc dec dec 3000 model 300 915 662 spec newslett 150mhz vol 5 issu 2 june 1993 ss20 sun sparcstat 20 1028 889 spec newslett model 61 60mhz vol 6 issu 2 june 1994 ss5 sun sparcstat 5 473 570 spec newslett 70mhz vol 6 issu 2 june 1994 plural rule given dataset last three column let p denot smallest observ error rate row ie dataset algorithm error rate within one standard error p consid close best indic p tabl standard error estim follow p independ test set let n denot size test set otherwis p crossvalid estim let n denot size train set standard error p estim formula pn algorithm largest error rate within row indic x total number p xmark algorithm given third fourth row tabl follow conclus may drawn tabl tabl 4 minimum maximum naiv plural rule error rate dataset p mark indic algorithm error rate within one standard error minimum dataset xmark indic algorithm worst error rate dataset mean error rate algorithm given second row decis tree rule statist algorithm net error rate naiv mean bcw cmc pp pp dna dna hea bo led bld pid seg smo thi ppp ppp pp ppp pp vot tae x tae pp 1 algorithm pol lowest mean error rate order algorithm term mean error rate given upper half tabl 5 2 algorithm also rank term total number p xmark criterion accur algorithm pol fifteen p tabl 5 order algorithm mean error rate mean rank error rate mean pol log mda ql0 lda ql1 pda ic0 fm2 ibo imo error 195 204 207 207 208 211 213 215 218 219 219 rate c4r im lmt c4t qu0 qu1 ocu ic1 ib ocm st0 mean pol fm1 log fm2 ql0 lda qu0 c4r imo mda pda rank 83 122 122 122 124 137 139 140 140 143 145 c4t ql1 ibo im ic0 ftl qu1 ocu ic1 st0 st1 error 145 146 147 149 150 154 166 166 169 170 177 rate lmt ocm ib rbf ftu qda lvq ocl cal nn mark xmark eleven algorithm one xmark rank increas order number xmark parenthes ftl1 ocm1 st11 fm21 mda1 fm12 ocl3 qda3 nn4 lvq4 t111 exclud remain algorithm rank order decreas number p mark parenthes pol15 log13 ql010 lda10 pda10 ql19 ocu9 1 qu08 qu18 c4r8 ibo8 rbf8 c4t7 imo6 im5 ic15 st05 ftu4 ic04 cal4 ib3 lmt1 top four algorithm 1 also rank among top five upper half tabl 5 3 last three column tabl show algorithm sometim less accur plural rule nn cmc cmc smo bld qda smo thi thi ftl tae st1 tae 4 easiest dataset classifi bcw bcw vot vot error rate lie 003 009 5 difficult classifi cmc cmc tae minimum error rate greater 04 6 two difficult dataset smo smo case smo t1 margin lower error rate plural rule algorithm lower error rate plural rule smo 7 dataset largest rang error rate thi thi rate rang 0005 0890 howev maximum 0890 due qda qda ignor maximum error rate drop 0096 8 six dataset one p mark bld pol sat lvq sat fm2 seg ibo veh veh qda time 9 overal addit nois attribut appear increas significantli error rate algorithm 52 statist signific error rate 521 analysi varianc statist procedur call mix effect analysi varianc use test simultan statist signific differ mean error rate algorithm control differ dataset neter wasserman kutner 1990 p 800 although make assumpt effect dataset act like random sampl normal distribut quit robust violat assumpt data procedur give signific probabl less 10 gamma4 henc hypothesi mean error rate equal strongli reject simultan confid interv differ mean error rate obtain use tukey method miller 1981 p 71 accord procedur differ mean error rate two algorithm statist signific 10 level differ 0058 visual result figur 1a plot mean error rate algorithm versu median train time second solid vertic line plot unit right mean error rate pol therefor algorithm lie left line mean error rate statist significantli differ pol algorithm seen form four cluster respect train time cluster roughli delin three horizont dot line correspond train time one minut ten minut one hour figur 1b show magnifi plot eighteen algorithm median train time less ten minut mean error rate statist significantli differ pol 522 analysi rank avoid normal assumpt instead analyz rank algorithm within dataset dataset algorithm lowest error rate assign rank one second lowest rank two etc averag rank assign case tie lower half tabl 5 give order algorithm term mean rank error rate pol first last note howev mean rank pol 83 show far uniformli accur across dataset compar two method order tabl 5 seen pol log ql0 lda algorithm consist good perform three algorithm perform well one criterion mda fm1 fm2 case mda low mean error rate due excel perform four dataset veh veh wav wav mani algorithm poorli domain concern shape identif dataset contain numer mean error rate median sec ftu c4r ib ibo im imo ocu ocl ocm st0 st1 lda qda pda mda pol rbf 1hr 10min 1min thirtythre method mean error rate median sec ftu c4r ib im ocu lda pda mda 1min 5min b less 10min accuraci sig differ pol figur 1 plot median train time versu mean error rate vertic axi logscal solid vertic line plot divid algorithm two group mean error rate algorithm left group differ significantli 10 simultan signific level pol minimum mean error rate plot b show algorithm statist significantli differ pol term mean error rate median train time less ten minut attribut mda gener unspectacular rest dataset reason tenth place rank term mean rank situat fm1 fm2 quit differ low mean rank indic fm1 usual good perform howev fail miser seg seg dataset report error rate fifti percent algorithm error rate less ten percent thu fm1 seem less robust algorithm fm2 also appear lack robust although lesser extent worst perform bo dataset error rate fortytwo percent compar less thirtyf percent algorithm number xmark algorithm tabl 4 good predictor errat poor perform mda fm1 fm2 least one xmark friedman 1937 test standard procedur test statist signific differ mean rank experi give signific probabl less 10 gamma4 therefor null hypothesi algorithm equal accur averag reject differ mean rank greater 87 statist signific 10 level holland wolf 1999 p 296 thu pol statist significantli differ twenti algorithm mean rank less equal 170 figur 2a show plot median train time versu mean rank algorithm algorithm lie left vertic line statist significantli differ pol magnifi plot subset algorithm significantli differ pol median train time less ten minut given figur 2b algorithm differ statist significantli pol term mean error rate form subset differ pol term mean rank thu rank test appear power analysi varianc test experi fifteen algorithm figur 2b may recommend use applic good accuraci short train time desir 53 train time tabl 6 give median dec 3000equival train time algorithm rel train time within dataset owe larg rang train time order rel fastest algorithm dataset report fastest algorithm indic 0 algorithm 10 xgamma1 time slow indic valu x exampl case dna dataset fastest algorithm c4t t1 requir two second slowest algorithm fm2 take three million second almost forti day henc 10 6 10 7 time slow last two column tabl give fastest slowest time dataset tabl 7 give order algorithm fastest slowest accord median train time overal fastest algorithm c4t follow close ftu ftl lda two reason superior speed c4t compar decis tree algorithm first split categor attribut mean rank median sec ftu c4r ib ibo im imo ocu ocl ocm lda qda pda mda pol rbf 1hr 10min 1min thirtythre method mean rank median sec c4r im ocu lda pda mda 1min 5min b less 10min accuraci sig differ pol figur 2 plot median train time versu mean rank error rate vertic axi logscal solid vertic line plot divid algorithm two group mean rank algorithm left group differ significantli 10 simultan signific level pol plot b show algorithm statist significantli differ pol term mean rank median train time less ten minut tabl 6 dec 3000equival train time rel time algorithm second third row give median train time rank algorithm entri x subsequ row indic algorithm time slower fastest algorithm dataset fastest algorithm denot entri 0 minimum maximum train time given last two column h denot second minut hour day respect decis tree rule statist algorithm net cpu time median cpu 32m 32m 59m 59m 7s 8s 5s 20 34 275m 34 339m 52 47 46 149m 137m 151m 144m 57m 13h 36 10 15 20 4m 156m 38h 56 3m 32h 11m 113h 5s rank tabl 7 order algorithm median train time 5s 7s 8s 10 15 20 20 34 34 36 46 47 52 56 11m 3m 32m 32m 4m 57m 59m 59m ocm 137m 144m 149m 151m 156m 275m 339m 13h 32h 38h 113h mani subnod number categori therefor wast time form subset categori second prune method requir crossvalid increas train time sever fold classic statist algorithm qda nn also quit fast expect decis tree algorithm employ univari split faster use linear combin split slowest algorithm pol fm2 rbf two splinebas one neural network although ic0 ic1 st0 st1 claim implement cart algorithm ind version faster splu version one reason ic0 ic1 written c wherea st0 st1 written languag anoth reason ind version use heurist buntin person commun instead greedi search number categori categor attribut larg appar tae dataset categor attribut twentysix categori case ic0 ic1 take around forti second versu two half hour st0 st1 result tabl 4 indic ind classif accuraci advers affect heurist see aroni provost 1997 anoth possibl heurist sinc t1 onelevel tree may appear surpris faster algorithm c4t produc multilevel tree reason split continu attribut j number class hand c4t alway split continu attribut two interv therefor j 2 t1 spend lot time search interv 54 size tree tabl 8 give number leav tree algorithm dataset nois attribut ad case error rate obtain tenfold cross valid entri mean number leav ten crossvalid tree tabl 9 show much number leav chang addit nois attribut mean median number leav classifi given last column two tabl ibo imo clearli yield largest tree far apart t1 necessarili short design algorithm shortest tree averag ql1 follow close ftl ocl rank algorithm univari split increas median number leav t1 ic1 st1 qu1 ftu ic0 st0 ocu qu0 c4t algorithm c4t tend produc tree mani leav algorithm one reason may due underprun although error rate quit good anoth unlik binarytre algorithm c4t split categor attribut mani node number categori addit nois attribut typic decreas size tree except c4t cal tend grow larger tree imo seem fluctuat rather wildli result complement oat jensen 1997 look effect sampl size number leav decis tree algorithm found signific relationship tree size train sampl size c4t observ tree algorithm employ costcomplex prune better abl control tree growth 6 scalabl algorithm although differ mean error rate pol mani algorithm statist signific clear error rate sole criterion pol would method choic unfortun pol one computeintens 1algorithm see train time increas sampl size small scalabl studi carri algorithm qu0 ql0 ftl c4t c4r ic0 log fm1 pol train time measur algorithm train set size four dataset use gener samplessat smo tae new larg uci dataset call adult two class six continu seven categor attribut sinc first three dataset larg enough experi bootstrap resampl employ gener train set n sampl randomli drawn replac dataset avoid get mani replic record valu class attribut sampl case randomli chang anoth valu probabl 01 new valu select pool altern equal probabl bootstrap sampl carri adult dataset 32000 record instead nest train set obtain random sampl without replac time requir train algorithm plot loglog scale figur 3 except pol fm1 log logarithm train time seem increas linearli logn nonmonoton behavior pol fm1 puzzl might due random use crossvalid model select errat behavior log adult dataset caus converg problem model fit mani line figur 3 roughli parallel suggest rel comput speed algorithm fairli constant rang sampl size consid ql0 c4r two except cohen 1995 observ c4r scale well 7 conclus result show mean error rate mani algorithm suffici similar differ statist insignific differ also probabl insignific practic term exampl mean error rate top rank algorithm pol log ql0 differ less 0012 small differ import real applic user may wish select algorithm base criteria train time interpret classifi unlik error rate huge differ train time algorithm pol algorithm lowest mean error rate take fifti time long train next accur algorithm ratio time roughli equival hour versu minut figur 3 show maintain wide rang sampl size larg applic time factor may advantag use one quicker algorithm interest old statist algorithm lda mean error rate close best surpris design binaryvalu attribut categor attribut transform 01 vector prior applic lda ii expect effect class densiti cpu time cpu time tae cpu time adult cpu time figur 3 plot train time versu sampl size loglog scale select algorithm multimod fast easi implement readili avail statist packag provid conveni benchmark comparison futur algorithm low error rate log lda probabl account much perform better algorithm exampl pol basic modern version log enhanc flexibl log employ splinebas function automat model select although strategi comput costli produc slight reduct mean error rateenough bring top pack good perform ql0 may similarli attribut lda quest linearsplit algorithm design overcom difficulti encount lda multimod situat appli modifi form lda partit data partit repres leaf decis tree strategi alon howev enough higher mean error rate ftl show latter base fact algorithm precursor quest one major differ quest fact algorithm former employ costcomplex prune method cart wherea latter result suggest form bottomup prune may essenti low error rate purpos construct algorithm data interpret perhap decis rule tree univari split suffic except cal t1 differ mean error rate decis rule tree algorithm statist signific pol ic0 lowest mean error rate qu0 best term mean rank c4r c4t far behind four algorithm provid good classif accuraci c4t fastest far although tend yield tree twice mani leav ic0 qu0 c4r next fastest figur 3 show scale well ic0 slightli faster tree slightli fewer leav qu0 howev loh shih 1997 show cartbas algorithm ic0 prone produc spuriou split situat acknowledg indebt p auer c e brodley w buntin hasti r c holt c kooperberg k murthi j r quinlan w sarl b schulmeist w taylor help advic instal comput program also grate j w molyneaux provid 1987 nation indonesia contracept preval survey data final thank w cohen f provost review mani help comment suggest r categor data analysi increas effici data mine algorithm breadthfirst marker propag new languag neural network pattern recognit classif regress tree simplifi decis tree survey multivari versu univari decis tree multivari decis tree comparison decis tree classifi backpropag neural network multimod classif problem analysi attitud toward workplac smoke restrict learn classif tree introduct ind version 21 recurs partit fast effect rule induct neural network multivari adapt regress spline discuss use rank avoid assumpt normal implicit analysi varianc construct assess classif rule hedon price demand clean air discrimin analysi gaussian mixtur penal discrimin analysi flexibl discrimin analysi optim score nonparametr statist method simpl classif rule perform well commonli use dataset appli multivari statist analysi polychotom regress cancer diagnosi via linear program uci repositori machin learn databas machin learn automat construct decis tree classif decisiontre algorithm cal5 base statist approach split algorithm system induct obliqu decis tree appli linear statist model effect train set size decis tree complex improv use continu attribut c4 pattern recognit neural network neural network statist model sa institut symbol neural learn algorithm empir comparison modern appli statist splu diagnost scheme fine needl aspir breast mass fine needl aspir breast mass diagnosi statist approach fine needl aspir diagnosi breast mass tr appli multivari statist analysi symbol neural learn algorithm c45 program machin learn simpl classif rule perform well commonli use dataset multivari decis tree selforgan map saset user guid version 6 neural network pattern recognit effect train set size decis tree complex multivari versu univari decis tree simplifi decis tree survey ctr ganesan velayathan seiji yamada behaviorbas web page evalu proceed 15th intern confer world wide web may 2326 2006 edinburgh scotland ganesan velayathan seiji yamada behaviorbas web page evalu proceed 2006 ieeewicacm intern confer web intellig intellig agent technolog p409412 decemb 1822 2006 samuel e buttrey ciril karo use knearestneighbor classif leav tree comput statist data analysi v40 n1 p2737 28 juli 2002 kwekumuata oseibryson evalu decis tree multicriteria approach comput oper research v31 n11 p19331945 septemb 2004 richi nayak lauri buy jan loviekitchin data mine conceptualis activ age proceed fifth australasian confer data mine analyst p3945 novemb 2930 2006 sydney australia xiangyang li nong ye supervis cluster algorithm comput intrus detect knowledg inform system v8 n4 p498509 novemb 2005 laura elena raileanu kilian stoffel theoret comparison gini index inform gain criteria annal mathemat artifici intellig v41 n1 p7793 may 2004 nong ye xiangyang li scalabl increment learn algorithm classif problem comput industri engin v43 n4 p677692 septemb 2002 jonathan eckstein peter l hammer ying liu mikhail nediak bruno simeon maximum box problem applic data analysi comput optim applic v23 n3 p285298 decemb 2002 sattar hashemi mohammad r kangavari parallel learn use decis tree novel approach proceed 4th wsea intern confer appli mathemat comput scienc p18 april 2527 2005 rio de janeiro brazil khale badran peter rockett role divers preserv mutat prevent popul collaps multiobject genet program proceed 9th annual confer genet evolutionari comput juli 0711 2007 london england nigel william sebastian zander grenvil armitag preliminari perform comparison five machin learn algorithm practic ip traffic flow classif acm sigcomm comput commun review v36 n5 octob 2006 sorin alex peter l hammer acceler algorithm pattern detect logic analysi data discret appli mathemat v154 n7 p10501063 1 may 2006 ruggieri effici c45 ieee transact knowledg data engin v14 n2 p438444 march 2002 md zahidul islam ljiljana brankov framework privaci preserv classif data mine proceed second workshop australasian inform secur data mine web intellig softwar internationalis p163168 januari 01 2004 dunedin new zealand efstathio stamatato gerhard widmer automat identif music perform learn ensembl artifici intellig v165 n1 p3756 june 2005 niel landwehr mark hall eib frank logist model tree machin learn v59 n12 p161205 may 2005 karann toh quoclong tran dipti srinivasan benchmark reduc multivari polynomi pattern classifi ieee transact pattern analysi machin intellig v26 n6 p740755 june 2004 abraham bernstein foster provost shawndra hill toward intellig assist data mine process ontologybas approach costsensit classif ieee transact knowledg data engin v17 n4 p503518 april 2005 rich caruana alexandru niculescumizil empir comparison supervis learn algorithm proceed 23rd intern confer machin learn p161168 june 2529 2006 pittsburgh pennsylvania nicola baskioti michl sebag c45 compet map phase transitioninspir approach proceed twentyfirst intern confer machin learn p10 juli 0408 2004 banff alberta canada gabriela alex peter l hammer span pattern logic analysi data discret appli mathemat v154 n7 p10391049 1 may 2006 ingolf geist framework data mine kdd proceed 2002 acm symposium appli comput march 1114 2002 madrid spain anthoni j lee yaot wang effici data mine call path pattern gsm network inform system v28 n8 p929948 decemb kwekumuata oseibryson postprun decis tree induct use multipl perform measur comput oper research v34 n11 p33313345 novemb 2007 friedhelm schwenker han kestler gther palm unsupervis supervis learn radialbasisfunct network selforgan neural network recent advanc applic springerverlag new york inc new york ny 2001 chenfu chien wenchih wang jenchieh cheng data mine yield enhanc semiconductor manufactur empir studi expert system applic intern journal v33 n1 p192198 juli 2007 irma becerrafernandez stelio h zanaki steven walczak knowledg discoveri techniqu predict countri invest risk comput industri engin v43 n4 p787800 septemb 2002 zhiwei fu bruce l golden shreevardhan lele raghavan edward wasil diversif better classif tree comput oper research v33 n11 p31853202 novemb 2006 rueyshiang guh hybrid learningbas model onlin detect analysi control chart pattern comput industri engin v49 n1 p3562 august 2005 krzysztof krawiec genet programmingbas construct featur machin learn knowledg discoveri task genet program evolv machin v3 n4 p329343 decemb 2002 huimin zhao sudha ram combin schema instanc inform integr heterogen data sourc data knowledg engin v61 n2 p281303 may 2007 karann toh train reciprocalsigmoid classifi featur scalingspac machin learn v65 n1 p273308 octob 2006 asparoukhov w j krzanowski nonparametr smooth locat model mix variabl discrimin statist comput v10 n4 p289297 octob 2000 r chandrasekaran young u ryu varghes jacob sungchul hong isoton separ inform journal comput v17 n4 p462474 octob 2005 chingpao chang chihp chu defect prevent softwar process actionbas approach journal system softwar v80 n4 p559570 april 2007 toni van gestel johan k suyken bart baesen stijn viaen jan vanthienen guido deden bart de moor joo vandewal benchmark least squar support vector machin classifi machin learn v54 n1 p532 januari 2004 elena barali silvia chiusano essenti classif rule set acm transact databas system tod v29 n4 p635674 decemb 2004 siddharth pal david j miller extens iter scale decis data aggreg ensembl classif journal vlsi signal process system v48 n12 p2137 august 2007 man cheang kwong sak leung kin hong lee genet parallel program design implement evolutionari comput v14 n2 p129156 june 2006 foster provost pedro domingo tree induct probabilitybas rank machin learn v52 n3 p199215 septemb johann gehrk wieyin loh raghu ramakrishnan classif regress money grow tree tutori note fifth acm sigkdd intern confer knowledg discoveri data mine p173 august 1518 1999 san diego california unit state vasant dhar dashin chou foster provost discov interest pattern invest decis make glower xcirca genet learner overlaid entropi reduct data mine knowledg discoveri v4 n4 p251280 octob 2000 perlich foster provost jeffrey simonoff tree induct vs logist regress learningcurv analysi journal machin learn research 4 p211255 1212003 foster provost venkateswarlu kolluri data mine task method scalabl handbook data mine knowledg discoveri oxford univers press inc new york ny 2002 gabriela alex sorin alex tibriu bonat alexand kogan logic analysi data vision peter l hammer annal mathemat artifici intellig v49 n14 p265312 april 2007 krzysztof j cio lukasz kurgan clip4 hybrid induct machin learn algorithm gener inequ rule inform scienc intern journal v163 n13 p3783 14 june 2004 foster provost venkateswarlu kolluri survey method scale induct algorithm data mine knowledg discoveri v3 n2 p131169 june 1999 b kotsianti zaharaki p e pintela machin learn review classif combin techniqu artifici intellig review v26 n3 p159190 novemb 2006