effici learn equilibrium introduc effici learn equilibrium ele norm approach learn noncoop set ele learn algorithm requir equilibrium addit learn algorithm must arriv desir valu polynomi time deviat prescrib ele becom irrat polynomi time prove exist ele desir valu expect payoff nash equilibrium paretoel object maxim social surplu repeat game perfect monitor also show ele alway exist imperfect monitor case final discuss extens result generalsum stochast game b introduct reinforc learn context multiag interact attract attent research cognit psycholog experiment econom machin learn artici intellig relat eld quit time 13 6 much work use repeat game 5 8 stochast game 16 15 12 2 model interact literatur learn game game theori 8 mainli concern understand learn procedur adopt dierent agent converg end equilibrium correspond game game may known idea show simpl dynam lead ration behavior prescrib nash equilibrium learn algorithm requir satisfi ration requir converg adopt agent equilibrium face uncertainti game play gametheorist adopt bayesian approach typic assumpt approach exist probabl distribut possibl game commonknowledg notion equilibrium extend context game incomplet inform treat appropri solut concept context agent assum ration agent adopt correspond bay nash equilibrium learn issu major claim gametheoret approach line goal multiag reinforc learn research ai must modi first bayesian approach use model partial inform line common approach theoret comput scienc comput learn deal uncertainti second descript motiv underli learn research gametheori dier consider norm motiv learn research ai dierenc import ramic explain issu detail first consid bayesian model partial inform date work machin learn particular work singleag reinforc learn taken dierent approach motiv larg work onlin algorithm comput scienc distribut assum uncertain entiti instead goal approach behavior agent complet inform close quickli inde ai research adopt nonbayesian approach work learn game look algorithm converg appropri equilibrium game class relev game follow suit howev research multiag reinforc learn choos adopt assumpt made gametheorist despit fact dierenc much fundament work learn game start descript motiv mind goal show peopl use simpl heurist rule updat behavior multiag set ie game eventu adopt behavior correspond appropri equilibrium behavior case econom model base equilibria concept sens justi assumpt agent use learn rule justi fact agent involv peopl ie design similarli ai concern descript model human behavior interest design artici agent except case cooper system reason believ agent design dierent design employ learn algorithm moreov one view design choic learn algorithm agent fundament decis follow norm criteria inde ai perspect choic learn algorithm basic action take game play agent design anoth relat point gametheorist adopt descript stanc concern quickli learn rule lead converg age evolv behavior agent design want agent learn quickli care agent ospr thu ai speed converg paramount import better align research methodolog multiag reinforc learn ai perspect present paper nonbayesian norm approach learn game approach make assumpt distribut possibl game may play make ectiv set studi machin learn ai spirit work onlin algorithm comput scienc treat choic learn algorithm game specic adopt framework repeat game view learn algorithm strategi agent repeat game strategi take action stage base previou observ initi inform ident game play given follow natur requir learn algorithm provid agent 1 individu ration learn algorithm equilibrium irrat agent deviat learn algorithm long agent stick algorithm regardless actual game 2 ecienc deviat learn algorithm singl agent other stick algorithm becom irrat ie lead situat deviat payo improv polynomi mani stage b agent stick prescrib learn algorithm expect payo obtain agent within polynomi number step close valu could obtain nash equilibrium agent known game outset tupl learn algorithm satisfi properti given class game said ecient learn equilibrium ele notic learn algorithm satisfi desir properti everi game given class despit fact actual game play initi unknown assumpt typic work machin learn borrow game theori literatur criterion ration behavior multiag system take individu ration associ notion equilibrium also take equilibrium actual initi unknown game benchmark success wish obtain correspond valu although initi know game play idea constitut major conceptu contribut paper remain section formal notion ecient learn equilibrium show devoid content ie prove exist ele gener class game class repeat game perfect monitor also show class game ele exist gener result context paretoel wish obtain maxim social surplu also discuss extens result generalsum stochast game technic speak result prove reli novel combin socal folk theorem econom novel ecient algorithm punish deviat game initi unknown ecient learn equilibrium denit section develop denit ecient learn equilibrium context twoplay repeat game gener nplayer repeat game immedi requir addit notat present extens stochast game appear section 6 game model multiag interact game set player choos action perform given set action result player combin choic outcom obtain describ numer form payo vector ie vector valu one player common descript twoplay game bimatrix call game strateg form row matrix correspond player 1s action column correspond player 2s action entri row column j game matrix contain reward obtain player player 1 play th action player 2 play j th action make simplifi assumpt size action set player ident extens set dierent size trivial repeat game rg player play given game g repeatedli view repeat game respect game g consist innit number iter player select action game g play iter player receiv appropri payo dictat game matrix move next iter eas exposit normal player payo game g nonneg real 0 posit constant r max denot interv possibl payo perfect monitor set set possibl histori length set possibl histori h union set possibl histori 0 empti histori name histori time consist histori action carri far correspond payo obtain player henc perfect monitor set player observ action select payo obtain past know game matrix start imperfect monitor setup player observ follow perform action payo obtain action select player player observ player payo even constrain set strict imperfect monitor player observ action payo alon denit possibl histori agent imperfect monitor set follow natur given rg polici player map h set possibl histori set possibl probabl distribut henc polici determin probabl choos particular action possibl histori notic learn algorithm view instanc polici dene valu player 1 polici prole polici player 1 polici player 2 use expect averag reward criterion follow given rg natur number denot expect iter undiscount averag reward player 1 player follow polici prole u 1 denit player 2 similar dene u 2 polici prole learn equilibrium everi game matrix dene set action possibl payo rst requir learn algorithm treat strategi order individu ration best respons one anoth addit rapidli obtain desir valu ident desir valu may paramet take natur candid nash equilibrium game anoth appeal altern discuss later assum consid game k action g everi repeat game nash equilibrium oneshot game associ denot nv ng expect payo obtain agent equilibrium polici prole ecient learn equilibriumel everi 0 0 1 exist 0 polynomi 1 k everi game matrix g correspond rg u nash equilibrium ng player 1 deviat 0 iter l probabl failur similarli player 2 notic deviat consid irrat increas expect payo spirit equilibrium game theori done order cover case expect payo nash equilibrium equal probabilist maximin valu 1 case denit replac one requir deviat lead decreas valu obtain similar result chosen order remain consist gametheoret literatur equilibrium stochast context notic also deviat consid irrat detriment eect deviat player averag reward manifest near futur exponenti far futur captur insight norm approach learn noncoop set assum initi game unknown agent learn algorithm rapidli lead valu player would obtain nash equilibrium known game moreov mention earlier learn algorithm equilibrium 1 probabilist maximin valu player 1 dene set polici player 1 2 respect denit player 2 similar 3 ecient learn equilibrium exist denit ele lesser interest provid interest exampl ele instanc section prove follow construct result theorem 1 exist ele perfect monitor set describ concret algorithm properti said earlier base combin socal folk theorem econom novel ecient punish mechan ensur ecienc approach folk theorem eg see 9 extend discuss 10 basic idea strategi prole lead payo greater equal secur level probabilist maximin valu agent guarante obtain direct agent use prescrib strategi tell agent punish agent turn deviat behavior punish remain threat follow equilibrium result desir strategi prole execut order use idea set need howev use techniqu punish without initi know payo matrix moreov need devis ecient punish procedur set recal consid repeat game iter g play follow often use term agent denot player use algorithm question term adversari denot player player set possibl action consid follow algorithm term ele algorithm ele algorithm player 1 perform action one time k time parallel player 2 perform sequenc action time player behav accord nash equilibrium reveal game comput player behav accord correspond strategi point sever nash equilibria exist one select use share select algorithm one player refer adversari deviat player refer agent act follow agent replac payo g complement r max adversari payo henc agent treat game constantsum game aim minim adversari payo notic payo might unknown use g refer modi game describ agent go minim adversari payo initi construct follow model 0 repeat game game g replac game g 0 entri game matrix assign reward r addit associ boolean valu variabl jointact fassumedknowng variabl initi valu assum repeat comput act comput optim probabilist maximin g 0 execut observ updat follow joint action follow let action agent perform let 0 adversari action 0 perform rst time updat reward associ 0 g 0 observ mark known 1 ele algorithm adopt player inde ele much proof theorem nontrivi rest show agent abil punish adversari quickli detail present appendix 4 imperfect monitor ele algorithm previou section use agent abil view adversari action payo natur question whether abil requir exist ele section show gener perfect monitor requir special class game ele exist imperfect monitor start gener case theorem 2 ele alway exist imperfect monitor set proof order see consid follow game 1 g1 2 g2 payo obtain joint action g1 g2 ident player 1 dierent player 2 equilibrium g1 player play second action lead 1500 equilibrium g2 player play rst action lead 69 uniqu equilibria sinc obtain remov strictli domin strategi assum ele exist look correspond polici player equilibrium notic order ele must visit entri 69 time game g2 visit entri 1500 time game otherwis player 1 resp player 2 obtain high enough valu g2 resp g1 sinc payo g2 resp g1 lower given ration player 2 deviat pretend game alway g1 behav accord suggest equilibrium polici tell case sinc game might actual g1 player 1 tell dierenc player 2 abl lead play second action player time also game g2 increas payo 9 10 contradict ele approach bayesian exclud possibl agent know particip game particular class thu may class repeat game ele exist particular consid class repeat commoninterest game repeat game underli game g commoninterest game ie game player alway receiv ident payo set denit imperfect perfect monitor denot set player know payo know adversari payo well thu examin case strict imperfect monitor recal set player know action payo theorem 3 exist ele class commoninterest game strict imperfect monitor proof idea quit simpl surprisingli propos complex less ecient approach propos requir knowledg number action avail agent polynomi bound algorithm work follow first agent go seri random play sucient mani time ensur probabl jointact play greater 1 phase agent maintain inform best payo obtain far action use payo rst obtain explor phase agent play best action repeatedli learn equilibria averag reward learn strategi lead maxim averag reward everi agent thu agent motiv deviat ele polynomi number step requir attain averag reward see deviat immedi reduc averag reward agent need polynomi number step approxim obtain maxim averag reward need ok 4 log k 2 step random play ensur jointact play probabl least 1 follow follow given larg enough k get probabl k 2 trial agent play previous unplay joint action approxim e henc get ok 2 log k 2 probabl learn outcom associ new joint action approxim repeat process k 2 time get desir result 5 pareto ele previou section dealt ele perfect imperfect monitor set case interest learn procedur enabl agent obtain expect payo one would obtain nash equilibrium known game ambiti object follow let p b denot payo player game question player 1 play player 2 play b say pair action b econom ecient total payo agent maxim easi see agent choos action g gener way guarante agent behav econom ecient manner due fact may case although b econom ecient behavior perform resp b agent 1 resp 2 irrat may lower probabilist maximin valu agent 1 resp 2 guarante classic approach econom deal econom ineci introduc side monetari payment formal part strategi agent function agent instruct pay certain amount money agent part strategi 2 agent reward p paid c c posit neg zero util assum type util function term quasi linear sum agent monetari payment alway 0 result agent turn use strategi maxim u 1 u 2 also econom ecient dene notion pareto ele pareto ele similar nash ele aim agent behavior econom ecient therefor two distinct aspect pareto ele 1 requir agent abl get close ecient outcom 2 allow side payment part agent behavior 2 denit perfect monitor case denit imperfect monitor case similar suppos consid game k action everi repeat game let econom ecient joint action oneshot game associ denot pv payo obtain agent joint action polici prole also allow side payment pareto ecient learn equilibrium everi 0 0 1 exist 0 polynomi k everi game matrix g dene action correspond rg u 1 player 1 deviat 0 iter l u 1 probabl failur similarli player 2 theorem 4 exist paretoel perfect monitor set proof consid follow algorithm dene polici side payment agent player 1 perform k iter action parallel player 2 perform sequenc action time game known agent comput probabilist maximin valu agent 1 agent 2 denot probabilist maximin valu agent payo get econom ecient solut e without loss choos r paid rbi player 1 ecient solut play player total payo least high probabilist maximin easi see examin two case e 2 agent adopt ecient behavior sidepay follow iter sever econom ecient behavior exist predetermin select algorithm use case one player adversari deviat either explor stage follow state player agent punish case nash ele play payo game complement r max adversari payo proof follow step proof exist ele case imperfect monitor result respect nashel hold theorem 5 pareto ele alway exist imperfect monitor set 6 stochast game stochast game provid gener model repeat multiag interact stochast game player may one nite mani state state associ game strateg form joint action state determin payo also determin stochast ident next state agent reach formal let set action avail agent state game associ associ payo p agent j joint action b addit everi 2 probabl next state j joint action b denot p multipl game polici agent associ possibl mix action everi state potenti payment agent polici function histori state agent visit payo observ throughout section assum perfect monitor set sinc imposs result imperfect monitor repeat game immedi rule exist ele gener context stochast game stochast game provid realist also technic challeng set first let us tri understand issu involv rst obstacl face lack gener result exist nash equilibrium averagereward stochast game thu restrict attent case paretoel conceptu requir gener straightforward learn algorithm quickli lead econom ecient polici agent ie polici maxim averag sum reward deviat quickli lead lower reward howev case repeat game equat quick polynomi size game approxim paramet situat stochast game complic paramet typic use assess speed converg learn algorithm stochast game return mix time 14 3 intuit return mix time polici expect time would take agent use polici converg valu close valu polici ideal would like learn algorithm attain optim valu time polynomi return mix time optim polici formal assum xed stochast game let polici prole denot step averag reward polici prole agent start state return mix time minim state us thu polici prole execut step longer agent expect averag sum reward close longterm averag sum reward let polici prole maxim min us let mix return mix time denit pareto ecient learn equilibrium stochast game ident repeat game except must polynomi mix well note game irreduc ie xed polici prole induc markov chain ergod us depend show follow theorem 6 follow assumpt paretoel stochast game exist 1 agent perfect monitor 2 mix known proof intuit idea behind algorithm ident case repeat game elabor new issu first agent run algorithm nding polici prole maxim u next run algorithm nding best accomplish ie assum agent tri minim averag payo point run polici prole adjust appropri side payment agent receiv best accomplish much like case repeat game point agent deviat agent play goal minim agent averag reward first note learn algorithm paretooptim longterm averag sum reward algorithm close optim averag sum reward desir agent incent deviat stage sidepay structur guarante attain least valu could attain show algorithm paretoel also need show valu attain ecient punish perform ecient resort recent result ecient learn xedsum stochast game commoninterest stochast game first comput polici maxim u use algorithm describ 4 refer reader detail need note algorithm learn requir polici prole polynomi time next comput valu agent attain use rmax 3 rmax appropri learn xedsum game first xed sum game reward base agent 1s reward respect xedsum game reward base agent 2s reward note given valu 0 input rmax learn 0 step polici time polynomi 0 game paramet polici optim among polici mix time 0 shall take averag reward polici use comput side payment structur case repeat game case averag reward polici prole suitabl modi includ side payment lower valu agent receiv thu agent deviat know within mix step attain lower averag reward ie punish carri ecient final note standard though imperfect techniqu remov knowledg mix simpli guess progress higher valu mix refer reader 3 implic approach previou work learn game ts one follow two paradigm 1 studi learn rule lead nash equilibrium solut concept game 8 2 studi learn rule predict human behavior noncoop inter action one model repeat game 6 approach taken 2 signic merit descript purpos norm approach learn go beyond recommend behavior eventu lead desir solut major issu one need face 1 learn algorithm agent individu ration 2 learn algorithm ecient converg desir valu employ agent 3 deviat desir learn algorithm becom irrat short period time concept introduc paper address issu ele paretoel provid new basic tool learn noncoop set moreov abl show construct exist result ele paretoel context repeat game perfect monitor also abl show relax perfect monitor assumpt desir properti imposs obtain gener case paretoel appeal concept context stochast game well abl extend result context togeth concept result provid rigor norm approach learn gener noncoop interact use contrast approach import line relat work featur algorithm guarante agent use attain valu approxim equal valu would attain known advanc adversari would play algorithm along line appear eg 11 7 special attent given issu ecienc latter result truli spirit onlin algorithm goal much onlin would abl line case attempt react onlin adversari behavior manner would similar term averag payo best could done known adversari behavior hand result highli valuabl mani reader may notic subtl crucial point treat adversari polici xed sequenc mix strategi probabilist action contrari spirit gametheori realiti adversari adjust polici respons agent behavior imagin exampl follow instanc wellknown prison dilemma game consid follow two adversari polici 1 agent initi play row 1 denot cooper adversari alway play column 1 denot cooper agent initi play row 2 denot defect adversari alway play column 2 denot defect 2 agent initi play cooper adversari alway play defect agent initi play defect adversari alway play cooper clear agent guarante bestrespons valu adversari approach limit view adversari use predetermin sequenc mix strategi bottom line despit practic theoret import result replac concept base notion equilibrium anoth relat work norm guidelin design learn algorithm 1 bowl veloso suggest two criteria learn algorithm rst call ration stipul player polici converg stationari polici learn algorithm converg bestrespons polici second call converg stipul learner necessarili converg stationari polici criteria attract work notion nashequilibrium learn strategi deeper notion ration bestrespons upon conver genc converg though denit desir ignor issu converg rate moreov converg speci well dene inde work bowl veloso consid special wellden case converg selfplay ie agent use algorithm standard notion converg adopt work learn game use fact particular context selfplay investig bowl veloso requir equival requir algorithm converg correl equilibrium common properti pursu learn algorithm gametheori literatur concept ele provid rigor notion individu ration learn strategi moreov believ ecient ie polynomi time converg rate integr part denit ration mani set happen exponenti number iter great interest appli judgment irration well agent make irrat choic lead increas reward near futur decreas reward exponenti number step seem irrat r ration coverg learn stochast game learn coordin ecient model base approach dynam reinforc learn cooper multiag system predict peopl play game reinforc learn game uniqu strategi equilibrium adapt game play use multipl weight theori learn game folk theorem repeat game discount incomplet inform game theori reinforc procedur lead correl equilibrium reinforc learn survey markov game framework multiag reinforc learn stochast game tr dynam reinforc learn cooper multiag system distribut algorithm mechan design friendorfo qlearn generalsum game multiag reinforc learn nearoptim reinforc learn polynomin time use redund improv robust distribut mechan implement rmax gener polynomi time algorithm nearoptim reinforc learn ctr jeffrey shneidman david c park specif faith network ration node proceed twentythird annual acm symposium principl distribut comput juli 2528 2004 st john newfoundland canada david c park jeffrey shneidman distribut implement vickreyclarkegrov mechan proceed third intern joint confer autonom agent multiag system p261268 juli 1923 2004 new york new york yevgeniy vorobeychik michael p wellman satind singh learn payoff function infinit game machin learn v67 n12 p145168 may 2007 rob power yoav shoham thuc vu gener criterion algorithm framework learn multiag system machin learn v67 n12 p4576 may 2007 guido boella leendert van der torr enforc social law proceed fourth intern joint confer autonom agent multiag system juli 2529 2005 netherland vincent conitz tuoma sandholm awesom gener multiag learn algorithm converg selfplay learn best respons stationari oppon machin learn v67 n12 p2343 may 2007