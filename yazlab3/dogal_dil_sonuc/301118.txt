perform predict larg parallel applic use parallel simul accur simul larg parallel applic facilit use direct execut parallel discret event simul paper describ use compass direct executiondriven parallel simul perform predict program includ commun io intens applic simul use predict perform applic distribut memori machin like ibm sp sharedmemori machin like sgi origin 2000 paper illustr use compass versatil perform predict tool use realworld applic synthet benchmark studi applic scalabl sensit commun latenc interplay factor like commun pattern parallel file system cach applic perform also show simul accur predict also effici abil use parallel simul reduc execut time case yield nearlinear speedup b introduct accur effici perform predict exist parallel applic multipl target architectur challeng problem analyt simul approach use success purpos wherea analyt solut advantag effici also suffer limit mani complex system analyt intract although simul wide applic tool major limit extrem long execut time largescal system number simul includ parallel proteu 22 laps 13 simo 30 wisconsin wind tunnel 28 mpisim 26 develop control execut time simul model parallel program simul typic use direct execut reduc cost simul sequenti instruct use parallel discret event simul exploit parallel within simul reduc impact scale target configur simul exist program simul design studi cpuintens parallel program howev inadequ parallel io perform becom signific deterr overal perform mani applic number solut propos improv parallel io perform 10 29 abil includ io cpuintens applic unifi perform predict environ thu appear signific valu develop compass componentbas parallel system simul portabl execut driven asynchron parallel discret event simul use predict perform largescal parallel program includ comput io intens applic target execut sharednoth share memori architectur well smp cluster particular simul modul develop predict perform applic function commun latenc number avail processor machin interest differ cach strategi parallel i0 parallel file system characterist altern implement collect commun i0 command simul use detail program simul within poem project 12 poem perform orient endtoend model system collabor multiinstitut project whose goal creat experiment evalu problem solv environ endtoend perform model complex paralleldistribut system paper describ simul use evalu perform larg scale complex applic function variou system characterist demonstr simul accur also fast due abil run parallel use real world applic howev case use synthet benchmark highlight particular featur simul show simul portabl accuraci valid tool two platform distribut memori ibm sp share memori sgi origin 2000 rang synthet real world applic instanc show predict execut time asci kernel call sweep3d 32 within 5 measur execut time architectur second demonstr scalabl tool major impedi widespread use program simul execut ineffici show compass effect exploit parallel model execut dramat reduc execut time simul model without sacrif accuraci particular show configur applic kernel call sweep3d target machin 64 processor simul reduc slowdown factor 35 use sequenti simul low 25 use parallel simul run 64 processor larger amount memori avail parallel platform allow us conduct scalabl studi target configur least two order magnitud larger obtain sequenti machin instanc sweep3d applic memori constraint sequenti simul would limit us simul target architectur 13 processor 150 3 problem size use memori avail us 128node sp abl predict perform sweep3d 1600 processor establish simul accuraci scalabl demonstr capabl 1 use simul predict scalabl properti applic use standard measur scalabl includ isoeffici scaleup function number processor 2 analyz behavior applic function commun latenc target architectur demonstr applic sweep3d sensit latenc variat impli execut applic network workstat rather massiv parallel system reason altern 3 show compass model new architectur consist cluster smp newest ibm sp even though hardwar smp cluster exist mpi softwar yet avail exploit faster commun avail among processor smp node use compass show applic would perform new architectur fast intranod mpi commun made avail particular use synthet benchmark identifi type applic run 20 faster use four 4way smp rather sixteen processor 4 use synthet benchmark demonstr sensit differ commun pattern variat commun latenc target architectur 5 parallel file system becom complex allow comput ionod cach demonstr variou cach polici affect perform benchmark particular io intens benchmark see network latenc degrad gain cooper cach 10 7 becom neglig next section give brief descript simul section 3 describ benchmark target host architectur use perform studi section 4 present result valid scalabl simul section showcas featur simul describ point 1 5abov section 6 discuss relat work conclud discuss futur research direct 2 compass goal simul enabl simul largescal parallel applic written use mpi mpiio varieti high perform architectur applic program simul refer target program architectur perform predict refer target architectur machin simul execut refer host machin may sequenti parallel simul environ compos sever distinct yet tightli coupl componentsth simul kernel mpi commun librari simul mpisim parallel io simul piosim parallel file system simul pf sim success compon build upon extend capabl previou compon expand breadth depth perform issu may investig simul simul kernel provid framework implement simul protocol provid support schedul execut thread mpisim provid capabl simul individu collect mpi commun routin piosim extend mpisim capabl includ io routin well provid sever implement collect io abil handl user defin data type need support complex io oper simpl io servic time model pfssim complet simul environ provid detail simul parallel file system multipl cach algorithm simul portabl run varieti parallel platformsth ibm sp origin 2000 intel paragon simul kernel heart simul environ gener number processor host machin less number processor target architectur simul simul must support multithread kernel processor schedul thread ensur event processor execut correct timestamp order target thread simul follow local code simul direct execut 8 commun io command trap simul use appropri model predict execut time correspond activ target architectur correspond commun io command also execut consist target program physic time taken execut oper ignor use direct execut simul local code requir processor host target machin similar howev interconnect network parallel io system file system two architectur may differ compass support commonli use mpi commun routin pointtopoint collect commun simul collect commun function implement term pointtopoint commun function pointtopoint commun function implement use set core nonblock mpi function 27 interconnect network model current ignor content network detail model develop given excel valid obtain simpler model varieti benchmark previou work 26 consid seriou limit parallel io compon compass simul individu collect io construct provid mpiio construct includ creat open close delet file data access readwrit oper local datatyp constructor introduc part mpiio specif file system compon compass simul parallel file system use servic io request gener mpiio program compon selfcontain may replac simpl disk access model order speed simul whenev detail system model requir howev use detail model allow studi wide varieti parallel file system configur basic structur function file system compon taken vesta parallel file system highli scalabl experiment file system develop ibm 6 behavior physic disk simul set disk model includ simpl model base seek time rotat latenc data transfer rate well highli detail model develop dartmouth 24 detail system simul slow parallel simul potenti reduc execut time model provid greater amount memori anoth necess larg detail simul simul kernel provid support sequenti parallel execut simul parallel execut support via set conserv parallel simul protocol 26 combin kernel builtin multithread capabl allow simul effect use howev mani host processor avail without limit size type experi may run simul also support number optim base analysi behavior parallel applic among optim made avail program behavior analysi techniqu allow simul protocol describ actual turn elimin costli overhead global synchron submiss 3 benchmark system 31 real world applic benchmark 311 sweep3d sweep3d solver threedimension time independ neutron particl transport comput calcul flux particl given region space flux region depend flux neighbor cell threedimension space xyz discret threedimension cell ijk comput progress wavefront manner eight octant space octant contain six independ angl angl correspond six independ direct flux one face cubecel sweep3d use 2d domain decomposit onto 2d array processor j direct configur sweep progress processor comput flux column cell send outgo flux inform two neighbor processor order improv perform k dimens angl divid block allow processor calcul part valu dimens angl send valu neighbor processor 312 na benchmark na parallel benchmark npb suit parallel scientif benchmark made avail numer aerodynam simul na project nasa ame research center 2 na suit contribut strong core experiment set repres number wellknown differ realworld nonvendorspecif code easili tailor util compass system use npb 22 releas softwar includ varieti applic four bt lu mg sp deem stabl author bt sp lu comput solut system discret navierstok equat mg solv threedimension scalar poisson equat npb distribut provid preconfigur set problem size f77 constraint dynam memori program oper applic problem size increas order b c furthermor program run parallel specif number processor bt sp run 4 9 16 processor lu mg run 4 8 16 npb suit sweep3d origin program fortran mpisim current support c program first translat use f2c 14 subsequ translat code automat local allow simul simul multipl simultan thread target program singl processor host machin local also convert mpi mpii0 call equival call defin within compass librari local fulli autom use success larg applic 32 synthet benchmarkssampl although real world applic kernel like sweep3d na use benchmark simul compass major disadvantag core algorithm difficult understand imposs modifi evalu impact altern type program structur includ comput granular commun pattern program provid mean paramet adjust larg granular chang could made serv need measur perform function specif runtim behavior thu addit use real world benchmark sought write synthet applic allow explicit tune commun comput paramet effort result sampl synthet applic messagepass librari environ c program perform precis changeabl amount calcul messagepass interprocess commun suitabl experiment analysi sampl execut messag pass via call target either compass actual mpi librari facilit valid sampl simpl loop contain two inner loop first pure comput loop whose durat vari adjust number float point divis execut second commun loop implement multipl commun pattern chang frequenc size destin messag sent receiv process messag distribut take wide varieti pattern describ 17 use mpi pointtopoint capabl implement number method wavefront nearest neighbor ring onetoal alltoal commun use predefin metric user easili chang commun comput ratio program 33 io benchmark sinc implement mpii0 standard yet wide avail hard find real world applic stress parallel io simul capabl simul henc set synthet benchmark develop purpos benchmark use n process map uniqu comput node process gener read write request block data given size interarriv time io request sampl normal random distribut given mean block file distribut across io node disk total md disk process issu r request given c first rc request use warm cach paramet easili modifi 34 host target architectur sgi origin 2000 19 multiprocessor system silicon graphic inc origin provid cachecoher numa distribut share memori layout two mip r10000 processor compris process node multiplex hub chip reduc memori latenc increas memori bandwidth origin testb small ten 180 mhz r10000 processor share 320 mb memori due limit number processor memori could complet perform number size experi ibm sp ibm scalabl parallel sp system scalabl multiprocessor condens sever complet rs6000 workstat one system 9 form sharednoth collect process node connect typic bidirect 4ari 2fli multistag interconnect network achiev simultan anytoani connect 21 packetswitch network use altern ip protocol name user space commun subsystem us css provid nearconst latenc bandwidth use us css baselin protocol experi sp2 new gener ibm sp showcas cluster architectur node machin 4way smp exampl machin new ibm sp lawrenc livermor nation laboratori current machin includ 158 comput node four 332 mhz 604e processor share 512 mb memori attach 1gb disk internod commun sp give bandwidth 100mbsecond latenc 35 microsecond use sp high perform switch tb3 current perform possibl applic run one four processor node simul behavior system run mpi applic internod commun handl way sharednoth architectur model commun processor use highperform switch howev intranod commun model use share memori inform implement mpi construct design exploit share memori yet avail ibm sp fact current implement processor node commun use much slower ip compass model base mpi implement sgi origin 2000 22 certainli perform applic depend exact implement allow us demonstr capabl tool enabl studi 4 valid perform compass 41 valid first set experi aim valid predict compass ibm sp sgi origin 2000 figur 1 graph execut time measur sweep3d program compar execut time predict compass curv function number processor use sweep3d number target processor simul compass valid thu limit number physic processor origin 10 compass data taken averag run time multithread combin target processor number instanc eight target processor averag run time taken execut 1 2 4 8 host processor graph seen compass inde accur correctli predict execut time benchmark within 5 ibm sp 3 o2k even multithread oper conduct scalabl studi often case number avail host processor significantli less number target processor result sever simul thread run processor sinc multithread might affect result simul thread might affect other runtim import studi whether effect exist quantifi effect multithread abil simul correctli predict runtim applic simul sweep3d use wide rang host target processor seen figur 2 even rel high degre multithread 8 target mpi process singl host processor variat predict runtim small 2 runtim sweep3don ibmsp 503problems mk10 mmi320601001 2 4 8 processor runtim measuredruntim figur valid compass sweep3d ibm sp number processor time second measur predict figur valid compass sweep3d o2k hostprocessor 2target 4target 8target 16target 64target 128target figur 2 effect compass multithread predict perform ibm sp compass also valid suit na benchmark present result sp bt benchmark origin 2000 mention earlier na program come configur run parallel predetermin number processor predetermin set problem size processor memori constraint rel small o2k restrict us size benchmark figur 3 show result valid experi bt sp class show good valid accuraci within 85 21 respect point 16 processor graph show predict perform sinc 8 host processor avail machin sinc sweep3d na benchmark comput intens also use communicationintens synthet benchmark sampl valid commun model measur predict execut time sampl benchmark also show excel valid compass varieti configur figur 4 show sampl run use wavefront commun pattern computationtocommun ratio 11 1 seen figur compass accur predict run time within 3 percent result similar pattern omit breviti valid nasspandbt o2k05154 9 number processor btmea btpred spmea sppred figur 3 valid na benchmark o2k valid sampleon o2k35452 4 8 number processor measur predict figur 4 valid sampl o2k 42 scalabl simul present number result demonstr rel improv perform simul obtain parallel execut figur 5a show perform compass simul execut sweep3d problem size 50 3 100 3 cell 64 target processor ibm sp seen figur simul effect use addit processor parallel simul 64 processor achiev speedup almost 35 100 3 problem size compar sequenti execut time simul speedup 50 3problem size smaller ultim perform simul bound perform applic anoth metric commonli use evalu perform simul slowdown simul rel target architectur defin slowdownst time simul applic use host processor time execut applic processor figur 5b show slowdown compass simul target problem size 50 3 number host processor equal number target processor simul slowdown factor less 3 host architectur fewer avail processor target machin slowdown get wors overal perform reason thu iratio number target processor number host processor 16 64 target processor 4 host processor slowdown factor 10 speedup compass run 64 target processor sweep3d mk10 mmi3 ibmsp5152535 processor 503 1003 figur 5 speedup compass ibm sp sweep3d slowdownof compass 503sweep3d mk10 host processor avail figur slowdown compass ibm spsweep3d largest configur studi 1600 target processor use 64 host processor iratio 25 yield slowdown 18 consider better slowdown factor report program simul like wwt 23 laps 13 slowdown factor report high 100 comput intens applic figur 6a show speedup attain compass origin 2000 simul processor two problem size sweep3d origin 2000 compass achiev nearlinear speedup number host processor increas reach speedup 7 8 host processor use slowdown graph 8target processor configur shown figur 6b show ratio 1 simul slowdown 2 slowdown 4 host processor slightli 2 show even half desir number processor avail simul run twice slower applic target processor would speedup compassrunningsweep3dwith32 target processor number host processor 503 1003 figur compass sgi origin 2000 sweep3d speedup slowdown compass simul na benchmark also show improv parallel execut albeit lesser degre figur 7a 8a show speedup bt sp applic respect see speedup simul increas progress number host processor increas rate increas well final speedup attain 8 host lower seen previou benchmark simul produc speedup high 545 bt benchmark 438 sp benchmark similarli slowdown curv reach low 142 167 respect applic see figur 7b 8b investig indic applic scale well sweep3d henc differ perform compass directli relat perform target program simul speedup slowdown experi show compass exploit parallel avail applic without ad consider overhead number host processor 8target figur perform compass sgi origin 2000 sweep3d number host processor 4target 9target figur 7a speedup compass sgi origin 2000 bt number host processor 9 target figur 7b slowdown compass sgi origin 2k bt 5 result featur compass scalabl sweep3dth perform studi first evalu scalabl sweep3d function variou paramet includ size problem number processor function network latenc perform studi ibm sp use 64 host processor figur 9a demonstr scalabl sweep3d three problem size 50 3 100 3 150 3 larg problem studi show perform scale well number processor increas almost 1600 although rel improv perform drop beyond 256 processor largest problem size runtim applic shown 125 time smaller run 1600 processor compar run applic 4 processor smaller problem size element perform appear peak 1024 processor subsequ get wors observ strengthen isoeffici analysi effici defin speedup sp number processor isoeffici function determin rate problem size need increas respect number processor maintain fix effici 16 system highli scalabl problem size need increas linearli function number processor total work w time run algorithm singl processor p time run algorithm p processor sum overhead processor give effici need grow fep maintain effici e fep defin isoeffici function speedupof compassrunningsp135 number host processor 4target 9target figur compass origin 2k sp slowdown compassrun sp261014 number processor 9 target figur slowdown compass origin 2k sp performanceof sweep3d mk10 mmi3 ibmsp101000 processor predict runtim 503 1003 figur 9a scalabl sweep3d ibm sp figur 9b show isoeffici function sweep3d variou number effici graph show problem size need maintain given effici 204060 90 given number processor first observ maintain 90 even 60 effici hard howev 40 manag second use larg number processor given problem size effici exampl 500000 22221000 problem size use less 16 processor give best effici 90 use 100 processor result 20 effici sinc run problem processor might result slow runtim tradeoff time effici made 36 processor use result 60 effici figur 9b also demonstr isoeffici hard captur simpl extrapol exampl 40 isoeffici curv flatten 16 million problem size impli give processor applic improv effici isoeffi functionmk1mmi650015000 100 200 300 400 500 processor problem size 20 40 90 figur isoeffici sweep3d ibm sp 51 impact latenc variat perform also studi effect commun latenc perform figur 10 show perform sweep3d latenc network vari problem size 50 3 seen figur faster commun switch signific impact applicationth perform chang 5 variat latenc 0 10x current switch latenc processor 128 larger problem differ neglig howev perform appear suffer significantli latenc increas factor 50 might case applic port network workstat latenc impact much signific small number processor processor contain larger portion comput region caus messag becom larg sensit latenc 52 model smp cluster architectur preced experi evalu applic perform distribut memori architectur new architectur ibm sp cluster architectur use 4way smp node describ section 34 exploit fast memori access share memori system scalabl distribut memori machin next set experi project improv execut time benchmark obtain migrat architectur sinc previou experi show na sweep3d benchmark rel insensit commun latenc hardli surpris appear benefit notic fast intranod commun breviti omit result howev demonstr sampl benchmark applic higher percentag commun new architectur appear offer benefit 503problems mk10 processor runtim second 0xsp sp 50xp figur 10a sensit sweep3d latenc small problem size 1503problems mk10 mmi3 ibmsp2006001000 processor runtim second 0xsp sp 50xsp 100xsp figur 10b sensit sweep3d latenc larg problem size figur 11a show perform sampl fix problem size per processor see simul valid well one processor per node case mea non smp compass also notic predict slightli better perform run smp node would support fast intranod commun compass smp even though current implement mpi commun smp node poor perform mea smp similarli figur 11b show perform sp run sampl function number comput iter time commun 37 total runtim number iter increas ratio comput commun constant see predict smp perform improv averag 20 compar singl processor per node perform see clear drawback use intranod commun support current mea current smp even though mpi sp support fast intranod commun processor smp share main memori might tempt applic develop redesign exist mpi applic use main memori processor node mpi node simul like compass help make decis invest time effort would result better perform numberofprocessor mea nonsmp mea current compass compassfor figur smp perform ibm sp sampl constant comput per comput loop runtim second mea nonsmp mea current compass compass figur smp perform ibm sp sampl increas comput per processor 53 simul common commun pattern anoth set experi involv investig impact differ commun pattern program perform use synthet benchmark sampl scientif program produc wide varieti traffic pattern depend algorithm use sought understand differ type messag dispers affect applic perform sampl benchmark use gener number messagepass scheme studi wavefront pattern involv 2dimension mesh 0th processor resid upperlefthand corner initi commun wave toward lowerrighthand corner use mesh layout nearestneighbor dispers processor send receiv messag four logic adjac processor ring pattern form cycl singl messag token sent around logic ring processor final onetoal pattern processor broadcast messag rout use broadcast tree other perform variou commun pattern evalu function commun latenc number processor host machin select experi origin 2000 8 processor figur 12a show perform sampl function latenc target o2k architectur processor figur 12b show perform function number processor target architectur expect ring pattern sensit latenc processor count messag travers sequenti ring somewhat surpris result rel insensit wavefront ontoal commun howev note pattern block initi processor immedi initi commun correspond process execut next iter henc reason well overlap commun produc observ insensit slight jump predict execut time increas processor attribut chang depth broadcast tree figur 12b sensit commun pattern latency100300500 latenc xorigin nearest neighbor onetoal ring wavefront figur perform commun pattern function latenc scalabilityof communicationpatterns901000 20 40 processor nearestneighbor onetoal ring wavefront figur perform commun pattern function number processor o2k 54 effect latenc parallel file system cach strategi last experi demonstr use simul evalu impact architectur featur io intens program cooper cach techniqu propos improv perform applic larg io requir 710 suggest cach least partial manag global rather entir local manner case comput node cnode io node ionod cach base cach simpli allow node manag cach greedi forward allow ionod cach miss check node cach requir data go fetch disk central coordin cach portion cnode cach collect manag ionod remain portion cnode cach manag local cnode percentag coordin manag cach vari experi global manag cach similar 100 coordin cach except strategi block placement cach modifi allow ionod cach hold data evict cnode cach cach techniqu depend effici access remot memori order improv cach hit rate applic perform perform depend commun latenc network figur 13 show result set experi design measur impact chang network latenc ibm sp cooper cach techniqu support compass benchmark 16 process separ comput node randomli read write 512 byte block data block file distribut across 2 io node 2 disk total 4 disk process issu 10000 request first 5000 request use warm cach 80 request read request graph plot predict execut time benchmark network latenc increas cach perform base cach cooper greedi forward central coordin 40 80 100 percent coordin global manag cach shown network latenc 0 1 10 100 time latenc interconnect absoluteperformanceof cachingtechniques2006000 50 100 networklat xsp latenc base greedi global figur comparison cach techniqu ibm sp understand network latenc increas predict execut time benchmark also increas howev experi also hint extrem sensit cooper cach techniqu increas network latenc may appear cach techniqu even base cach equal affect increas network latenc found case absolut differ predict execut time diminish slightli latenc increas rel differ differ cach techniqu decreas markedli shown figur 13b effect network becom slower benefit use cooper cach lost perform degrad slightli better base cach result import implic use techniqu larg network workstat design hybrid strategi cach manag cooper small region network rather entir network perform cooper cachingrel tobas network latenc x sp latenc base greedi global figur perform cach techniqu rel base cach ibm sp 6 relat work accur effici perform predict exist parallel applic target machin thousand processor challeng problem first gener simul like proteu 4 use sequenti simul slow slowdown factor rang 2 35 process target program led mani effort improv execut time program simul dpsim 25 laps 13 parallel proteu 20 simo 30 wisconsin wind tunnel 28 tango 11 mpisim 2627 design purpos simul typic use direct execut portion code reduc cost simul sequenti instruct typic use variat conserv parallel discreteev simul 5 algorithm exploit parallel within simul reduc impact scale target machin size mani parallel simul use synchron approach simul simul process synchron global fix time interv order maintain program correct interv quantum taken larger commun latenc network simul guarante messag sent one quantum receiv next interv also impli messag process correct order synchron simul proteu parallel architectur simul engin tango share memori architectur simul engin tunnel wwt share memori architectur simul engin simo complet system simul multipl program plu oper system term simul commun two simul engin use approach similar parallel proteu laps distinguish featur compass portabl part due implement use mpi sinc mpi readili avail parallel distribut system simul abl use data movement synchron hand laps design specif run intel paragon use paragon nativ commun primit made laps broad use limit compass also fast slowdown around 2 proteu typic slowdown rang number simul also design simul io oper although tend use sequenti simul set collect io implement compar use starfish 18 simul base proteu 3 hybrid methodolog evalu perform parallel io subsystem describ pio tracedriven io simul use calcul perform io system subset problem evalu analyt model use remaind scalabl distribut memori machin examin 31 use applic kernel investig network perform content librari also develop ppf 15 portabl parallel file system librari design sit top multipl uf instanc provid wide varieti parallel file system capabl cach prefetch data distribut compass environ describ paper use parallel io system simul detail 1 perhap simul combin abil integr interconnect network io file system scalabl studi also use simul data parallel program compil messagepass code 25 addit simul highli scalabl slowdown factor singl digit larg target applic architectur 7 conclus futur research demonstr compass use studi wide rang applic function varieti architectur characterist rang standard scalabl studi network stress test parallel io properti shown compass accur valid multipl applic architectur within percent physic measur also fast achiev excel perform ibm sp well sgi origin 2000 achiev nearlinear speedup highli parallel applic suffer moder slowdown shown use wide rang architectur perform studi combin separ area io parallel file system perform interconnect network commun librari simul compass use detail program simul within poem project collabor poet work develop hybrid perform model combin analyt simul model techniqu also part project compass integr detail memori processor model allow us break away depend requir host processor architectur similar target processor architectur direct execut simul also provid opportun extend use parallel simul techniqu processor memori simul 8 acknowledg work support advanc research project agenc darpacsto contract f3060294c0273 scalabl system softwar measur evalu darpaito contract n6600197c8533 endtoend perform model larg heterogen adapt paralleldistribut computercommun system thank offic academ comput ucla paul hoffman help ibm sp2 well lawrenc livermor nation laboratori use ibm sp mani experi execut 9 r parallel simul parallel file system io program na parallel benchmark 20 methodolog evalu parallel io perform massiv parallel processor proteu highperform parallel architectur simul distribut simul case studi design verif distribut program vesta parallel file system avoid cachecoher problem paralleldistribut file system rice parallel process testb parallel comput architectur hardwaresoftwar approach remot client memori improv file system perform multiprocessor simul trace use tango poem endtoend perform design larg parallel adapt comput system parallel direct execut simul messagepass parallel program f2c fortran c convert ppf high perform portabl parallel file system analysi scalabl parallel algorithm architectur survey introduct parallel comput design analysi algorithm tune starfish sgi origin ccnuma highli scalabl server asci bluepacif ibm rs6000 tr system lawrenc livermor nation laboratori reduc synchron overhead parallel simul mpi perform studi sgi origin 2000 wisconsin wind tunnel ii fast portabl parallel architectur simul galley parallel file system parallel simul data parallel program perform predict parallel program use parallel simul evalu mpi program wisconsin wind tunnel virtual prototyp parallel comput improv parallel io via twophas runtim access strategi use simo machin simul studi complex comput system simul base scalabl studi parallel system asci sweep3d benchmark code tr rice parallel process testb analysi scalabl parallel algorithm architectur introduct parallel comput wisconsin wind tunnel simulationbas scalabl studi parallel system ppf vesta parallel file system galley parallel file system reduc synchron overhead parallel simul parallel direct execut simul messagepass parallel program use simo machin simul studi complex comput system sgi origin poem mpisim parallel simul parallel file system io program parallel comput architectur avoid cachecoher problem paralleldistribut file system parallel simul data parallel program tune starfish proteu highperform parallelarchitectur simul perform predict parallel program ctr sundeep prakash ewa deelman rajiv bagrodia asynchron parallel simul parallel program ieee transact softwar engin v26 n5 p385400 may 2000 clia l kawabata regina h c santana marco j santana sarita bruschi kalinka r l j castelo branco perform evalu cmb protocol proceed 37th confer winter simul decemb 0306 2006 monterey california rajiv bagrodia ewa deelman thoma phan parallel simul largescal parallel applic intern journal high perform comput applic v15 n1 p312 februari 2001 leo yang xiaosong frank mueller crossplatform perform predict parallel applic use partial execut proceed 2005 acmiee confer supercomput p40 novemb 1218 2005 thoma phan rajiv bagrodia optimist simul parallel messagepass applic proceed fifteenth workshop parallel distribut simul p173181 may 1518 2001 lake arrowhead california unit state vikram adv rajiv bagrodia ewa deelman thoma phan rizo sakellari compilersupport simul highli scalabl parallel applic proceed 1999 acmiee confer supercomput cdrom p1e novemb 1419 1999 portland oregon unit state vikram adv rizo sakellari applic represent multiparadigm perform model largescal parallel scientif code intern journal high perform comput applic v14 n4 p304316 novemb 2000 ewa deelman rajiv bagrodia rizo sakellari vikram adv improv lookahead parallel discret event simul largescal applic use compil analysi proceed fifteenth workshop parallel distribut simul p513 may 1518 2001 lake arrowhead california unit state vikram adv rajiv bagrodia ewa deelman rizo sakellari compileroptim simul largescal applic high perform architectur journal parallel distribut comput v62 n3 p393426 march 2002 ihsin chung jeffrey k hollingsworth use inform prior run improv autom tune system proceed 2004 acmiee confer supercomput p30 novemb 0612 2004 vikram adv rajiv bagrodia jame c brown ewa deelman aditya dube elia n housti john r rice rizo sakellari david j sundaramstukel patricia j teller mari k vernon poem endtoend perform design larg parallel adapt comput system ieee transact softwar engin v26 n11 p10271048 novemb 2000 murali k nethi jame h aylor mix level model simul larg scale hwsw system high perform scientif engin comput hardwaresoftwar support kluwer academ publish norwel 2004