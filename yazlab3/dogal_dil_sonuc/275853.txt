schedul blockcycl array redistribut abstractthi articl devot runtim redistribut onedimension array distribut blockcycl fashion processor grid previou studi concentr effici gener commun messag exchang processor involv redistribut focu schedul messag organ messag exchang structur commun step minim content build upon result walker otto solv particular instanc problem deriv optim schedul gener case name move cyclicr distribut pprocessor grid cyclic distribut qprocessor grid arbitrari valu redistribut paramet p q r b introduct runtim redistribut array distribut blockcycl fashion multidimension processor grid difficult problem recent receiv consider attent interest motiv larg hpf 13 program style scientif applic decompos phase phase optim distribut data array onto processor grid typic array distribut accord cyclicr pattern along one sever dimens grid best valu distribut paramet r depend characterist algorithm kernel well communicationtocomput ratio target machin 5 optim valu r chang phase phase one machin anoth think heterogen environ runtim redistribut turn critic oper state 10 21 22 among other basic decompos redistribut problem follow two subproblem messag gener array redistribut effici scan process order build messag exchang processor commun schedul messag must effici schedul minim commun overhead given processor typic sever messag send processor subset term mpi collect oper 16 must schedul someth similar mpi alltoal commun except processor may send messag particular subset receiv subset depend sender previou work concentr mainli first subproblem messag gener messag gener make possibl build differ messag pair processor must commun therebi guarante volumeminim commun phase processor send receiv data need howev question effici schedul messag receiv littl attent one except interest paper walker otto 21 schedul messag order chang array distribut cyclicr p processor linear grid cyclickr grid aim extend walker otto work order solv gener redistribut problem move cyclicr distribut p processor grid cyclic distribut qprocessor grid gener instanc redistribut problem turn much complic particular case consid walker otto howev provid effici algorithm heurist optim schedul commun induc redistribut oper main result follow valu redistribut paramet p q r construct optim schedul schedul whose number commun step minim commun step defin processor sendsrec one messag therebi optim amount buffer minim content commun port construct optim schedul reli graphtheoret techniqu edg color number bipartit graph delay precis mathemat formul result section 4 need sever definit beforehand without loss gener focu onedimension redistribut problem articl although usual deal multidimension array highperform comput problem reduc tensor product individu dimens hpf allow one loop variabl align direct therefor multidimension assign redistribut treat sever independ onedimension problem instanc rest articl organ follow section 2 provid exampl redistribut oper expos difficulti schedul commun section 3 briefli survey literatur redistribut problem particular emphasi given walker otto paper 21 section 4 present main result section 5 report mpi experi demonstr use result final section 6 state conclus futur work direct motiv exampl consid array x0m gamma 1 size distribut accord block cyclic distribut cyclicr onto linear grid p processor number goal redistribut x use cyclic distribut q processor number simplic assum size x multipl qs least common multipl p r qs redistribut pattern repeat slice l element therefor assum even number slice x enabl us without loss gener avoid discuss side effect let l number slice exampl 1 consid first exampl 5 note new grid q processor ident disjoint origin grid p processor actual total number processor use unknown valu 16 32 commun summar tabl 1 refer commun grid note view sourc target processor grid disjoint tabl 1 even may actual case see sourc processor messag processor receiv 7 messag henc need use full alltoal commun scheme would requir 16 step total 16 messag sent per processor precis 15 messag local copi rather tri schedul commun effici ideal could think organ redistribut 7 step commun phase step 16 messag would exchang involv disjoint pair processor would perfect oneport commun machin processor send andor receiv one messag time note may ask someth tri organ step way step 8 involv pair processor exchang messag length approach interest cost step like dictat length longest messag exchang step note messag length may may vari significantli number tabl 1 vari 1 3 singl slice vector vector x length length vari 1000 3000 time number byte need repres one datatyp element schedul meet requir name 7 step 16 disjoint processor pair exchang messag length provid section 432 report solut schedul tabl 2 entri posit p q tabl denot step number g clariti processor p send messag processor q tabl 3 comput cost commun step proport length longest messag involv step total cost redistribut sum cost step elabor model commun cost section 431 tabl 1 commun grid 5 messag length indic vector x size commun grid msg nbr msg 7 7 exampl 2 second exampl show use effici schedul even processor commun everi processor illustr tabl 4 messag length vari ratio 2 7 need organ alltoal exchang step way messag length commun step abl achiev goal see section 432 solut schedul given tabl 5 step number p cost given tabl 6 check 16 step compos messag length exampl 3 third motiv exampl shown tabl 7 commun scheme sever unbalanc processor may differ number messag send andor receiv techniqu abl handl complic situat provid section 44 schedul compos 10 step longer possibl messag length step instanc processor messag length 3 send processor messag length 1 2 achiev redistribut commun step processor sendsrec one messag per step number commun step tabl 8 clearli optim processor send cost schedul given tabl 9 tabl 2 commun step commun step 9 b f g e c tabl 3 commun cost commun cost step b c e f g total cost exampl 4 final exampl p 6 q show size two processor grid need see tabl 10 commun grid unbalanc solut schedul see section 44 compos 4 commun step number optim sinc processor messag receiv note total cost equal sum messag length processor must receiv henc optim 3 literatur overview briefli survey literatur redistribut problem particular emphasi given work walker otto 21 tabl 4 commun grid indic vector x size msg nbr msg 31 messag gener sever paper dealt problem effici code gener hpf array assign statement like array b distribut blockcycl fashion linear processor grid research see stichnoth et al17 van reeuwijk et al19 wakatani wolf 20 dealt princip array distribut use either pure scatter cyclic distribut cyclic1 hpf full block distribut cyclicd n array size p number processor recent howev sever algorithm publish handl gener blockcycl distribut sophist techniqu involv finitest machin see chatterje et al 3 settheoret method see gupta et al 8 diophantin equat see kennedi et al 11 12 hermit form lattic see thirumalai ramanujam 18 linear program see ancourt et al 1 compar survey algorithm found wang et al 22 report power algorithm handl blockcycl distribut effici simpler case pure cyclic fullblock map end messag gener phase processor comput sever differ messag usual store temporari buffer messag must sent set receiv processor exampl section 2 illustr symmetr processor comput number length messag receiv therefor alloc correspond memori space summar messag gener phase complet processor tabl 5 commun step commun step 9 g tabl commun cost commun cost step b c e f cost prepar messag processor must send data processor possess inform regard messag receiv number length origin 32 commun schedul littl attent paid schedul commun induc redistribut oper simpl strategi advoc instanc kaln ni 10 view commun total exchang processor specifi oper compar survey wang et al 22 use follow templat execut array assign statement 1 gener messag tabl post receiv advanc minim oper system overhead 2 pack commun buffer 3 carri barrier synchron tabl 7 commun grid 5 messag length indic vector x size 14 nbr msg nbr msg 6 9 6 6 9 6 6 9 6 6 9 6 6 9 9 4 send buffer 5 wait messag arriv 6 unpack buffer although commun phase describ precis note explicit schedul messag sent simultan use asynchron commun pro tocol approach induc tremend requir term buffer space deadlock may well happen redistribut larg array scalapack librari 4 provid set routin perform array redistribut describ prylli tourancheau 15 total exchang organ processor arrang virtual caterpillar total exchang implement success step step processor arrang pair perform sendrec oper caterpillar shift new exchang pair form even though special care taken implement total exchang attempt made exploit fact processor pair may need commun first paper devot schedul commun induc redistribut walker otto 21 review two main possibl implement commun induc redistribut oper wildcard nonblock receiv similar strategi wang et al describ asynchron strategi simpl implement requir buffer messag receiv henc total amount buffer high total volum data redistribut tabl 8 commun step commun step tabl 9 commun cost commun cost step b c e f g h j total cost synchron schedul synchron algorithm involv commun phase step step particip processor post receiv send data wait complet receiv sever factor lead perform degrad instanc processor may wait other receiv data hot spot aris sever processor attempt send messag processor step avoid drawback walker otto propos schedul messag step processor send one messag receiv one messag strategi lead synchron algorithm effici asynchron version demonstr experi written mpi 16 ibm sp1 intel paragon requir much less buffer space walker otto 21 provid synchron schedul special instanc redistribut problem name chang array distribut cyclicr p processor linear grid cyclickr grid size main result provid schedul compos k step step processor send receiv exactli one messag k smaller p size grid dramat improv tradit alltoal implement tabl 10 commun grid messag length indic vector x size msg nbr msg 2 4 aim articl extend walker otto work order solv gener redistribut problem move cyclicr distribut p processor grid cyclic distribut qprocessor grid retain origin idea schedul commun step step particip processor neither send receiv one messag avoid hot spot resourc content explain 21 strategi well suit current parallel architectur section 431 give precis framework model cost redistribut 4 main result 41 problem formul consid array x0m gamma 1 size distribut accord blockcycl distribut cyclicr onto linear grid p processor number goal redistribut x use cyclic distribut q processor number 1 equival perform hpf assign cyclicr processor grid cyclic qprocessor grid 1 blockcycl data distribut map global index vector x ie element xi onto processor index p block index l item index x local block indic start 0 map gamma p l x may written birc deriv relat 1 gener assign dealt similarli tabl 11 commun step commun step tabl 12 commun cost commun cost step b c total cost similarli sinc distribut cyclic qprocessor grid global index j map get redistribut equat qs least common multipl p r qs element li x initi distribut onto processor l multipl p r henc r divid l p divid l xi r similar reason two element redistribut onto processor word redistribut pattern repeat slice l element therefor restrict discuss vector x length l follow let rq bound equat 3 becom given distribut paramet r grid paramet p q redistribut problem determin messag exchang find valu p q redistribut equat 3 solut unknown l x subject bound equat 4 comput number solut given processor pair p q give length messag start simpl lemma lead handi simplif lemma 1 assum r rel prime proof redistribut equat 3 express equat 3 express solut given processor pair p q delta divid z z deltaz 0 deduc solut redistribut problem r 0 0 p q let us illustr simplif one motiv exampl back exampl 3 note need scale messag length move redistribut oper r rel prime one let us return exampl 3 assum know build commun grid tabl 7 deduc commun grid say keep messag scale length process make sens new size vector slice deltal rather l see tabl 13 result commun grid cours schedul commun remain cost tabl 9 multipli delta 42 commun pattern consid redistribut paramet r p q assum qs commun pattern induc redistribut oper complet alltoal oper proof rewrit equat 5 ps gamma p rl gamma qsm arbitrari multipl g sinc z lie interv 1 gamma whose length r guarante multipl g found within interv convers assum g r exhibit processor pair p q exchang messag inde desir processor pair see note pr gamma g divid p r henc multipl g ad pr gamma qs lie interv 1 gamma therefor messag sent p q redistribut 2 follow aim character pair processor need commun redistribut oper case consid follow function 2 anoth proof see petitet 14 tabl 13 commun indic vector x size 14 nbr msg nbr msg 6 9 6 6 9 6 6 9 6 6 9 6 6 9 9 function f map processor pair p q onto congruenc class pr gamma qs modulo g accord proof lemma 2 p send messag q fp modg let us illustr process use one motiv exampl back exampl 4 exampl proof lemma 2 receiv messag p receiv messag see tabl 10 check character class introduc integ u v r theta extend euclid algorithm provid number rel prime r follow result proposit 1 assum r u mod g proof first see pq inde integ note sinc g divid p r qs divid pq two differ class disjoint definit turn class number element see note k 2 0 integ 0 sinc g class deduc number element class pq next see p q mod final p p r qs divid divid rs deduc pq divid henc processor pair p distinct thu enumer class0 definit 3 consid redistribut paramet r p q assum 1 let lengthp q length messag sent processor p processor q redistribut singl slice vector x size said earlier commun pattern repeat slice valu report commun grid tabl section 2 singl slice equal lengthp q interest repres homogen commun processor pair given class exchang messag length proposit 2 assum qs length vector x redistribut let volk piecewis function given figur 1 k 2 1 gamma recal p q 2 classk send messag q volk volr figur 1 piecewis linear function vol proof simpli count number solut redistribut equat pr easili deriv piecewis linear vol function repres figur 1 know build commun tabl section 2 still deriv schedul way organ commun effici possibl 43 commun schedul 431 commun model accord previou discuss concentr schedul compos sever success step step sender send one messag symmetr receiv receiv one messag give formal definit schedul follow definit 4 consid redistribut paramet r p q ffl commun grid p theta q tabl nonzero entri lengthp q posit p q p send messag q ffl commun step collect pair lengthp commun step complet sender receiv activ incomplet otherwis cost commun step maximum valu entri word maxflengthp ffl schedul success commun step nonzero entri commun grid appear one one step cost schedul may evalu two way 1 number step ns simpli number commun step schedul 2 total cost tc sum cost commun step defin commun grid illustr tabl section 2 summar length requir commun singl slice vector vector size qs motiv evalu schedul via number step via total cost follow ffl number step ns number synchron requir implement sched ule roughli estim commun step involv processor permut measur unit number step good evalu cost redistribut ffl may tri precis step sever messag differ length exchang durat step like relat longest length messag simpl model would state cost step ff ff startup time invers bandwidth physic commun link although express take hot spot link content account proven use varieti machin 4 6 cost redistribut accord formula affin express ff theta ns motiv interest number step total cost 432 simpl case simpl character processor pair class special case r q well p rel prime proposit 3 assum respect denot invers r modulo g proof rel prime qs henc g therefor invers r modulo g well defin comput use extend euclid algorithm appli r g similarli invers modulo g well defin condit easili translat condit proposit simpl case nice solut schedul problem assum first 1 simpli schedul commun class class class compos pq processor pair equal distribut row column commun grid class exactli q send processor per row p receiv processor per column direct consequ proposit 3 note g divid p q hypothesi gcdr schedul class want processor g send messag processor equival look receiv side word processor posit p 0 within block g element must send messag processor posit q 0 within block g element done maxpq complet step messag instanc five block sender three block receiv block sender send messag 3 block receiv use algorithm gener block permut order commun block irrelev alltoal commun scheme illustr exampl 2 schedul class lead algorithm messag length given step 1 case simpli regroup class equival modulo g proceed summar discuss follow result proposit 4 assum schedul class success lead optim commun scheme term number step total cost proof assum without loss gener p q accord previou discuss number class time p number step class commun step step schedul messag class k henc length volk time p commun step compos messag length name process given class k 2 0 remark 1 walker otto 21 deal redistribut shown go r kr simplifi go techniqu describ section enabl us retriev result 21 44 gener case gcd p entri commun grid may evenli distribut row sender similarli entri commun grid may evenli distribut column receiv back exampl 3 5 see tabl 7 row commun grid 5 nonzero entri messag row 10 similarli henc r 3 column commun grid 6 nonzero entri column 10 first goal determin maximum number nonzero entri row column commun grid start analyz distribut class class classk k 2 0 1 processor pair distribut follow ffl p 0 entri per column q 0 column grid none remain column ffl q 0 entri per row p 0 row grid none remain row proof first let us check sinc r rel prime q 0 definit r 0 pq element per class sinc class obtain translat class0 restrict discuss distribut element class formula lemma 1 state r mod mod p take valu multipl 0 r mod q take valu multipl r 0 henc result check total number element note let us illustr lemma 3 one motiv exampl back exampl 3 element class locat p 0 column processor grid let us check class1 instanc inde follow lemma 3 show use schedul base class consid class separ would lead incomplet commun step rather build commun step mix element sever class order use avail processor maximum number element row column commun grid obviou lower bound number step schedul processor send receiv one messag commun step proposit 5 assum otherwis commun grid full use notat lemma 3 1 maximum number mr element row commun grid 2 maximum number mc element column commun grid e proof accord lemma 1 two element classk classk row commun grid interv 0 pq necessarili 0 divid p rel prime u fortiori 0 rel prime u therefor 0 divid share row processor grid congruent modulo 0 induc partit class sinc exactli q 0 element per row class sinc number class congruent valu modulo 0 either b rsgamma1 c rsgamma1 e deduc valu mr valu mc obtain similarli turn lower bound number step given lemma 5 inde achiev theorem 1 assum otherwis commun grid full use notat lemma 3 lemma 5 optim number step ns opt schedul proof alreadi know number step ns schedul greater equal g give construct proof bound tight deriv schedul whose number step maxfmr mc g borrow materi graph theori view commun grid graph set send processor set receiv processor entri p q commun grid nonzero g bipartit graph edg link vertex p vertex q degre g defin maximum degre vertic g accord konig edg color theorem edg color number bipartit graph equal degre see 7 vol 2 p1666 berg 2 p 238 mean edg bipartit graph partit g disjoint edg match construct proof follow repeatedli extract e maximum match satur maximum degre node iter exist maximum match guarante see berg 2 p 130 defin schedul simpli let match iter repres commun step remark 2 proof theorem 1 give bound complex determin optim number step best known maximum match algorithm bipartit graph due hopcroft karp 9 cost ojv j 5 maxp q iter construct schedul procedur ojp j 2 construct schedul whose number step minim 45 schedul implement goal twofold design schedul ffl minim number step schedul ffl minim total cost schedul alreadi explain view commun grid bipartit graph e accur view edgeweight bipartit graph edg edg p q length lengthp q messag sent processor p processor q adopt follow two strategi stepwis specifi number step choos iter maximum match satur node maximum degre sinc free select match natur idea select among match one maximum weight weight match defin sum weight edg greedi specifi total cost adopt greedi heurist select maximum weight match step might end schedul ns opt step whose total cost less implement approach reli linear program framework see 7 chapter 30 let jv j theta jej incid matrix g ae 1 edg j incid vertex sinc g bipartit total unimodular squar submatrix determin 0 1 gamma1 match polytop g set vector x 2 q jej ae intuit select match polyhedron determin equat 7 integr rewrit set vector x 2 q jej find maximum weight match look x c 2 n jej weight vector choos greedi strategi simpli repeat search maximum weight match commun done choos stepwis strategi ensur iter vertic maximum degre satur task difficult vertex v maximum degre posit replac constraint ax translat number maximum degre vertic 2 f0 1g jv j whose entri posit 1 iff ith vertex maximum degre note either case polynomi method match polyhedron integr solv ration linear problem guarante find integ solut see fact greedi strategi better stepwis strategi term total cost consid follow exampl exampl 5 consid redistribut problem 3 commun grid given tabl 14 stepwis strategi illustr tabl 15 number step equal 10 optim total cost 20 see tabl 16 greedi strategi requir step name 12 see tabl 17 total cost see tabl 18 tabl 14 commun grid messag length indic vector x size msg nbr msg 451 comparison walker otto strategi walker otto 21 deal redistribut know go r kr simplifi go appli result section 432 see remark 1 gener case evenli distribut among column commun grid r 1 necessarili among row howev row total number nonzero element 0 divid word bipartit graph regular sinc maximum match perfect match messag length lengthp p q commun grid consequ stepwis strategi lead optim schedul term number step total cost note ns opt k hypothes walker otto use notat lemma 5 note result appli graph regular entri commun grid equal follow theorem extend walker otto main result 21 tabl 15 commun step stepwis strategi stepwis strategi tabl commun cost stepwis strategi stepwis strategi step b c e f g h j total cost proposit 6 consid redistribut problem arbitrari p q schedul gener stepwis strategi optim term number step total cost strategi present articl make possibl directli handl redistribut arbitrari cyclicr arbitrari cyclic contrast strategi advoc walker otto requir two redistribut one cyclicr cycliclcmr second one cycliclcmr cyclic 5 mpi experi section present result run intel paragon redistribut algorithm describ section 4 tabl 17 commun step greedi strategi greedi strategi tabl commun cost greedi strategi greedi strategi step b c e f g h j k l total cost 51 descript experi execut intel paragon xp 5 comput c program call routin mpi librari mpi chosen portabl reusabl reason schedul compos step step gener one send andor one receiv per processor henc use onetoon commun primit mpi main object comparison new schedul strategi current redistribut algorithm scalapack 15 name caterpillar algorithm briefli summar section 32 run schedul algorithm proceed follow 1 comput schedul step use result section 4 2 pack commun buffer 3 carri barrier synchron 4 start timer 5 execut commun use redistribut algorithm resp caterpillar algorithm 6 stop timer 7 unpack buffer maximum timer taken processor emphas take cost messag gener account compar commun cost instead caterpillar algorithm could use mpi alltoallv commun primit turn caterpillar algorithm lead better perform mpi alltoallv experi differ roughli 20 short vector 5 long vector use physic processor input output processor grid result sensit grid disjoint grid sender receiv 52 result three experi present first two experi use schedul present section 432 optim term number step ns total cost tc third experi use schedul present section 44 optim term ns back exampl 1 first experi correspond exampl 1 5 redistribut schedul requir 7 step see tabl 3 sinc messag length theoret improv caterpillar algorithm 16 step 716 044 figur 2 show signific differ two execut time theoret ratio obtain small vector eg size 1200 doubleprecis real result surpris startup time domin cost small vector larger vector ratio vari 056 064 due content problem schedul need 7 step step gener 16 commun wherea 16 step caterpillar algorithm gener fewer commun 6 8 per step therebi gener less content back exampl 2 second experi correspond exampl 2 redistribut schedul requir 16 step total cost 6 caterpillar algorithm requir 16 step step least one processor send messag length proport 7 henc total cost 112 theoret gain 77112 069 expect long vector startup time obtain anyth better 086 content experi ibm sp2 network workstat would like lead favor ratio back exampl 4 third experi correspond exampl 4 experi similar first one redistribut schedul requir much fewer step caterpillar 12 two differ howev p 6 q algorithm guarante optim term total cost instead obtain theoret ratio 412 033 obtain result close 06 explain need take closer look caterpillar algorithm shown tabl 19 6 12 step caterpillar algorithm inde empti step theoret ratio rather 46 066 global size redistribut vector 64bit doubl precision500015000 microsecond caterpillar optim schedul figur 2 compar redistribut time intel paragon tabl 19 commun cost caterpillar schedul caterpillar step b c e f g h j k l total cost 6 conclus articl extend walker otto work order solv gener redistribut problem move cyclicr distribut p processor grid cyclic distribut qprocessor grid valu redistribut paramet p q r construct schedul whose number step optim schedul shown optim term total cost particular instanc redistribut problem includ walker otto work futur work devot find schedul optim term number step total cost arbitrari valu redistribut problem sinc problem seem difficult may prove npcomplet anoth perspect explor use heurist like greedi algorithm introduc assess perform run experi gener optimist result one next releas scalapack librari may well includ redistribut algorithm present articl global size redistribut vector 64bit doubl precision40008000microsecond caterpillar optim schedul figur 3 time measur caterpillar greedi schedul differ vector size redistribut r linear algebra framework static hpf code distribut graph et hypergraph gener local address commun set dataparallel program portabl linear algebra librari distribut memori comput design issu perform softwar librari linear algebra comput high perform comput matrix comput handbook combinator compil array express effici execut distributedmemori machin processor map techniqu toward effici data redistribut effici address gener blockcycl distribut lineartim algorithm comput memori access sequenc dataparallel program steel jr algorithm redistribut method block cyclic decomposit effici blockcycl data redistribut mpi complet refer gener commun array state ment design fast address sequenc gener dataparallel program use integ lattic implement framework hpf distribut array messagepass parallel comput system redistribut blockcycl data distribut use mpi redistribut blockcycl data distribut use mpi runtim perform parallel array assign empir studi tr ctr prashanth b bhat viktor k prasanna c raghavendra blockcycl redistribut heterogen network cluster comput v3 n1 p2534 2000 stavro souravla mano roumelioti pipelin techniqu dynam data transfer multiprocessor grid intern journal parallel program v32 n5 p361388 octob 2004 chinghsien hsu shihchang chen chaoyang lan schedul contentionfre irregular redistribut parallel compil journal supercomput v40 n3 p229247 june 2007 hyungyoo yook myongsoon park schedul genblock array redistribut journal supercomput v22 n3 p251267 juli 2002 chinghsien hsu spars matrix blockcycl realign distribut memori machin journal supercomput v33 n3 p175196 septemb 2005 minyi guo yi pan improv commun schedul array redistribut journal parallel distribut comput v65 n5 p553563 may 2005 minyi guo ikuo nakata framework effici data redistribut distribut memori multicomput journal supercomput v20 n3 p243265 novemb 2001 neungsoo park viktor k prasanna cauligi raghavendra effici algorithm blockcycl array redistribut processor set ieee transact parallel distribut system v10 n12 p12171240 decemb 1999 chinghsien hsu yehch chung donlin yang chyiren dow gener processor map techniqu array redistribut ieee transact parallel distribut system v12 n7 p743757 juli 2001 chinghsien hsu yehch chung chyiren dow effici method multidimension array redistribut journal supercomput v17 n1 p2346 aug 2000 saeri lee hyungyoo yook misoo koo myongsoon park processor reorder algorithm toward effici genblock redistribut proceed 2001 acm symposium appli comput p539543 march 2001 la vega nevada unit state chinghsien hsu kunm yu compress diagon remap techniqu dynam data redistribut band spars matrix journal supercomput v29 n2 p125143 august 2004 emmanuel jeannot frdric wagner schedul messag data redistribut experiment studi intern journal high perform comput applic v20 n4 p443454 novemb 2006 peizong lee wenyao chen gener commun set array assign statement blockcycl distribut distribut memori parallel comput parallel comput v28 n9 p13291368 septemb 2002 antoin p petitet jack j dongarra algorithm redistribut method blockcycl decomposit ieee transact parallel distribut system v10 n12 p12011216 decemb 1999 jihwoei huang chihp chu effici commun schedul method processor map techniqu appli data redistribut journal supercomput v37 n3 p297318 septemb 2006