enlarg margin perceptron decis tree capac control perceptron decis tree typic perform control size prove quantiti relev reduc flexibl combat overfit particular provid upper bound gener error depend size tree margin decis node enlarg margin perceptron decis tree reduc upper bound gener error base analysi introduc three new algorithm induc larg margin perceptron decis tree assess effect larg margin bia oc1 journal artifici intellig research 1994 2 132 murthi kasif salzberg wellknown system induc perceptron decis tree use baselin algorithm extens experiment studi real world data show three new algorithm perform better least significantli wors oc1 almost everi dataset one except oc1 perform wors best marginbas method everi dataset b introduct perceptron decis tree pdt introduc number author differ name 17 6 7 8 10 11 27 18 decis tree intern node associ hyperplan gener posit input space use mani real world pattern classif task good result 7 18 9 given high flexibl featur share standard decis tree one produc c45 20 tend overfit data complex somehow kept control standard approach control complex limit size earli stop prune procedur paper introduc novel approach complex control pdt base concept margin name distanc decis boundari train point control quantiti basi eectiv system vapnik support vector machin 12 adaboost 24 bayesian classifi 13 prove quantiti import treesiz capac control paramet theoret motiv behind approach lie datadepend structur risk minim 25 scale cover use vc theori provid bound gener error depend margin henc hierarchi class chosen respons data cours two complex control criteria use togeth combin prune phase bia toward larg margin obtain better perform result motiv new class pdt learn algorithm aim produc larg margin tree propos three algorithm fat moc1 moc2 compar perform oc1 one best known pdt learn system three largemargin system outperform oc1 real world dataset use indic overfit pdt ecient combat enlarg margin decis boundari train data perceptron decis tree common decis tree node check valu singl attribut could defin axi parallel test associ node equival axisparallel hyperplan input space mani variat simpl model propos sinc introduct system earli 80 involv complex test decis node usual test one attribut decis tree whose node test linear combin attribut propos dierent research dierent name linear combin tree multivari dt 11 obliqu dt 18 perceptron decis tree 27 etc first system propos breiman incorpor packag cart10 test associ node equival hyperplan gener posit partit input space polyhedra illustr figur 1 obvious includ special case common decis tree output system like c45 x x x x x x x x x x x x x x figur 1 perceptron decis tree way split input space extrem flexibl system make particularli expos risk overfit ecient method control express power typic prune techniqu alway use combin standard topdown growth algorithm class function comput pdt formal defin follow definit 21 gener decis tree gdt given space x set boolean function class gdtf gener decis tree f function implement use binari tree intern node label element f leaf label either 1 0 evalu particular tree input x x boolean function associ node assign argument x x argument x valu assum determin uniqu path root leaf intern node left respect right edg child taken output function associ intern node 0 respect 1 path known evalu path valu function x valu associ leaf reach say input x reach node tree node evalu path x follow node intern node binari tree leav extern one exampl given tree bdt gdt given tree cdt gdt kind decis tree defin continu space output common algorithm like c45 cart refer cdt given tree pdt gdt assum input augment coordin constant valu henc implement threshold perceptron pdt gener induc mean topdown growth procedur start root node greedili choos perceptron maxim cost function usual measur impur subsampl implicitli defin split maxim usual hard perform sometim replac random suboptim subsampl map two children node procedur recurs appli node tree grown stop criterion met tree use start point bottomup search perform prune tree impli elimin node redund unabl pay term cost function gener prune overfit tree produc better classifi obtain earli stop sinc make possibl check promis direct fact worth explor local good solut contrari deadend standard topdown algorithm extrem greedi procedur introduct prune possibl lookahead allow discoveri hidden structur capac control pdt henc complet achiev control size tree complex overal classifi propos altern method contrari focus reduc complex node classifi independ tree size possibl thank theoret analysi gener perform function class defin pdt framework vc theori theoret analysi gener gener perform learn machin studi mean uniform converg bound techniqu introduc vapnik chervonenki 30 central concept analysi eectiv capac class hypothes access machin richer class higher risk overfit featur learn machin often refer flexibl capac issu prevent overfit allow right amount flexibl therefor known capac control notion eectiv cardin function class captur growth function boolean class cover number real valu function size cover number depend accuraci cover well function class larger margin less accuraci requir cover follow concern estim capac class pdt see margin aect flexibl hypothesi class treesiz motiv altern techniqu control overfit albeit conceptu similar prune act complex node classifi rather complex overal tree begin definit fatshatt dimens first introduc 15 use sever problem learn sinc 1 4 2 3 definit 31 let f set real valu function say set point x shatter f rel real number r x index x x binari vector b index x function f b f satisfi fat shatter dimens fat f set f function posit real number integ map valu size largest shatter set finit infin otherwis exampl relev subsequ analysi consid class quot follow result 5see also 12 theorem 32 5 let f lin restrict point ball n dimens radiu r origin follow theorem bound gener classifi term fat shatter dimens rather usual vapnikchervonenki pseudo dimens let denot threshold function class function f theorem 33 25 consid real valu function class f fat shatter function bound function afat r n continu right fix r learner correctli classifi independ gener exampl z train error zero confid 1 expect error h bound k 8em import theorem use explain classifi give better gener would predict classic analysi vc dimens essenti expand margin perform automat capac control function class small fat shatter dimens theorem show larg margin achiev work lower vc class stress gener bound obtain better case larg margin observ priori guarante margin occur therefor priori classic vc bound use view correspond lower bound gener error term vc dimens posteriori bound depend favor probabl distribut make actual learn task easier henc result use distribut favor least adversari sens result distribut depend result despit distribut depend tradit sens assumpt distribut made deriv benign behavior distribut automat estim learn process order perform similar analysi perceptron decis tree consid set margin obtain node bound gener function valu turn bound fat shatter dimens pdt view real function classifi dicult therefor direct gener analysi mimick proof theorem 33 take account margin decis node tree definit 34 let x pseudo metric space let subset x 0 set cover everi exist b b da b cover number n minim cardin cover finit cover defin cover number f respect pseudometr measur maximum discrep sampl x respect distanc f number bound follow lemma present histor reason though fact requir slightli gener corollari lemma 35 alon et al 1 let f class function x 0 1 p distribut x choos 0 1 let expect e taken wrt sampl x drawn accord p corollari 36 25 let f class function x b p distribut x choos expect e sampl x drawn accord p posit tackl main lemma bound probabl doubl sampl first half zero error second error greater appropri error interpret dierent classifi output tree order simplifi notat follow lemma assum decis tree k node denot fat f lin fat lemma 37 let perceptron decis tree k decis node margin 1 2 k decis node satisfi k correctli classifi label exampl x gener independ accord unknown fix distribut p support ball radiu r second sampl bound follow probabl less xy tree correctli classifi x fraction misclassifi k k log8m use standard permut argument 30 may fix sequenc xy bound probabl uniform distribut swap permut sequenc satisfi condit state consid gener minim k 2cover xy valu k suppos node tree margin hyperplan w satisfi therefor find xy whose output valu within 2 w consid tree obtain replac node perceptron w correspond f tree perform classif function first half sampl margin node remain larger point second half sampl incorrectli classifi either still incorrectli classifi adapt tree one decis node closer decis boundari k 2 point thu distinguish left hand side point correctli classifi margin greater k 2 node henc point must kept right hand side order condit satisfi henc fraction permut allow one choic function cover 2 must take union bound choic function cover use techniqu 25 number choic bound corollari 36 follow valu lemma statement therefor ensur union bound less lemma 37 appli particular tree specifi number node architectur fat shatter dimens node practic observ quantiti run learn algorithm gener tree henc obtain bound appli practic must bound probabl uniformli possibl architectur dimens aris give theorem give bound requir two result first due vapnik 28 page 168 key bound error probabl term probabl discrep doubl sampl lemma 38 let x set system set x p probabl measur x x second result give bound number dierent tree architectur given number comput node theorem 39 21 number k k node decis tree skeleton combin two result lemma 37 obtain follow theorem theorem 310 suppos abl classifi sampl label exampl use perceptron decis tree suppos tree obtain contain k decis node margin node bound gener error probabl greater less r radiu sphere contain support distribut must bound probabl dierent architectur tree dierent margin first use lemma 38 bound probabl error term probabl discrep perform two halv doubl sampl order appli lemma 37 must consid possibl architectur occur architectur dierent pattern k decis node fix valu k theorem 39 give number decis tree skeleton largest allow valu k fix k bound number possibl count possibl label k1 leaf node henc number applic lemma 37 fix k sinc largest valu k take let sum choos applic lemma 37 ensur probabl statement fail hold less 2 note replac constant 8 order ensur continu right requir applic theorem 33 upperbound log4emk log4em henc appli lemma 38 case probabl statement theorem fail hold less 4 experiment result theori present previou section follow largemargin pdt like gener well bia toward largemargin tree implement number dierent way either postprocess phase exist tree brand new impur measur determin splittingstop criteria topdown growth algorithm facilit comparison implement three algorithm modif one bestknown pdt learn system oc1 18 murthi kasif salzberg freeli avail internet eect largemargin bia henc directli assess run marginarbitrari version algorithm data first algorithm fat accept input pdt construct use oc1 output larg margin version tree two moc1 moc2 dierent impur measur take consider margin three algorithm work multiclass data three system compar oc1 10 benchmark data set result confirm predict theoret model clearli indic gener improv enlarg margin data set use studi 6 data set use origin oc1 paper18 4 data set publicli avail uci data repositori 31 data set studi 18 dim bright wisconsin breast cancer pima indian diabet boston hous iri four addit data set bupa sonar heart wisconsin breast cancer prognosi data set dier greatli subject size number attribut subject data set rang medic astronom size 150 4192 number attribut 4 60 1 detail data set see 18 31 data set singl run 10fold crossvalid carri relev quantiti experi dierenc test accuraci pdt arbitrari margin construct oc1 pdt larg margin data compar learn algorithm drawn extens attent recent 16 14 23 19 singl run 10fold crossvalid reason number data set still prefer 1 number attribut point data set follow bright142462 bupa6345 cer9 682 dim14 4192 heart13 297 hous 13506 iris4 150 pima8 768 prognosis32198 practic approach prone detect dierenc two algorithm basic follow approach recommend 23 rest section first briefli review oc1 system present three larg margin algorithm compar perform oc1 41 review oc1 oc1 18 random algorithm perform random hillclimb search learn perceptron build tree topdown start root node system choos hyperplan minim predefin impur measur eg inform gain 20 gini index 10 two rule 10 18 etc system greedi stage choos best split avail random best split obtain mean exhaust search random hillclimb process throughout studi use two rule impur measur oc1 fat moc1 moc2 use modifi two rule impur measur impur measur also appli fat moc1 without chang moc2 would need minor chang two rule 1 total number instanc current node number class two class problem number instanc left split ie w number instanc right split ie w number instanc categori left split number instanc categori right split good measur rather impur one oc1 attempt maxim split tree growth via minim 1twoingv alu detail random prune split criteria found 18 42 result fat descript algorithm fat algorithm fat use tree produc oc1 start point maxim margin involv find node hyperplan perform split perform oc1 tree maxim margin done consid subsampl reach node perfectli divid two part feed data accordingli relabel algorithm find optim separ separ hyperplan maxim margin linearli separ data optim separ hyperplan place correspond decis node new tree test test data note pdt produc fat tree structur train accuraci origin pdt construct oc1 dier test accuraci use support vector machin svm algorithm 29 find optim separ hyperplan conform definit pdt kernel use svm optim separ hyperplan construct input space algorithm fat 1 construct decis tree use oc1 call oc1pdt 2 start root oc1pdt travers nonleaf node node relabel point node x class right point node class left find perceptron optim separ hyperplan separ class right class left perfectli maxim margin replac origin perceptron new one 3 output fatpdt optim separ hyperplan svm algorithm linearli separ case follow problem solv node find optim separ hyperplan linearli separ data 29 min subject w x correspond class right correspond class left number point reach decis node comput reason usual solv dual problem 2 min subject fatpdt gener error bound theorem 310 observ fat complet reli restrict perceptron decis tree induc oc1 mani case margin split found oc1 small fat littl scope optim gener big margin top split root node fat gener much better impli greedi algorithm oc1 good tree induc fat sens margin need find better nongreedi tree induc fat hand fat provid new approach appli support vector machin multiclass classif task comparison fat oc1 dataset 10fold crossvalid use measur learn abil algorithm fat oc1 pair ttest use test dierenc mean fat oc1 10fold crossvalid result fat vs oc1 oc1 10fold cv averag accuraci fat 10fold averag accuraci signific signific xy figur 2 comparison 10fold cv result fat versu oc1 point line indic 10fold cv mean fat higher oc1 vice versa figur show fat outperform oc1 9 10 data set outperform 1 data set figur 2 see fat outperform oc1 9 10 data set outperform oc1 6 data set studi 18 10fold crossvalid mean dierenc fat oc1 9 data set signific pair ttest appli one data set prognosi oc1 outperform fat dierenc signific also observ except one case prognosi fat perform good better oc1 everi fold 10fold crossvalid fat higher mean oc1 signific small level pair ttest even though dierenc small strong indic perceptron decis tree larg margin gener better 10fold crossvalid mean p valu summar tabl 2 43 result moc1 descript moc1 moc1 margin oc1 variat oc1 modifi object function oc1 consid size margin underli philosophi find separ plane tradeo train accuraci size margin node idea motiv support vector machin linearli nonsepar case minim classif error maxim margin time svm soft margin minim sum misclassif error constant c multipli reciproc soft margin svm tri find split high classif accuraci larg soft margin analag svm minim sum impur measur constant time reciproc hard margin moc1 algorithm minim follow object function object impur measur oc1 studi default two rule use impur measur current margin sum perpendicular distanc hyperplan two nearest point dierent side current separ hyperplan scalar weight 0 1 point current node determin much larg margin weight select split tune could improv perform determin weight margin also take number point current node consider idea constant weight margin node good weight abl adapt posit current node size train exampl current node sinc particularli interest find tree highest possibl accuraci rather demonstr larg margin improv gener tune data set achiev highest possibl accuraci set data set word result moc1 present best result possibl comparison moc1 oc1 previou section use 10fold crossvalid measur learn abil algorithm moc1 oc1 test dierenc mean moc1 oc1 pair ttest use figur 3 see moc1 higher 10fold crossvalid mean oc1 8 10 data set 5 significantli higher oc1 higher mean two data set cancer prognosi dierenc tini signific overal moc1 outperform oc1 6 10 data set good oc1 four six data set studi 18 moc1 outperform oc1 five perform well oc1 final one cancer see tabl 2 respect mean p valu oc1 10cv averag accuraci moc1 averag accuraci signific signific xy figur 3 comparison 10fold cv result moc1 versu oc1 point line indic 10fold cv averag moc1 higher oc1 vice versa figur show moc1 outperform oc1 6 10 data set perform good oc1 four data set 44 result moc2 descript moc2 moc2 use modifi two rule directli incorpor idea larg margin impur measur unlik moc1 moc2 use soft margin treat point fall within margin outsid margin dierent impur measur alter rest standard oc1 algorithm modifi two rule mtr total number instanc current node number class two class problem number instanc left split ie w number instanc right split ie w number instanc categori left split number instanc categori right split number instanc left split w mtr number instanc right split w number instanc categori w number instanc categori w modifi two rule goal node find split fewer point fall within margin accuraci outsid margin good overal accuraci tri achiev balanc classif accuraci size margin want push apart two class separ hyperplan far possibl maintain reason good classif accuraci henc improv gener induc decis tree advantag moc2 free paramet tune comparison moc2 oc1 previou section 10fold crossvalid use measur learn abil algorithm moc2 oc1 pair ttest use test dierenc mean moc2 oc1 figur 4 see moc2 higher mean 9 10 data set slightli lower mean one data set hous 9 higher mean 5 significantli higher one lower mean signific overal moc2 outperform oc1 5 oc1 10cv averag accuraci moc2 averag accuraci 10fold cross valid result moc2 vs oc1 signific signific xy figur 4 comparison 10fold cv result moc2 versu oc1 point line indic 10fold cv mean moc2 higher oc1 data set vice versa figur show moc2 outperform oc1 5 10 data set perform well oc1 5 data set 10 data set perform well oc1 5 six data set studi 18 moc2 outperform oc1 three perform well oc1 three respect mean p valu summar tabl 2 modifi two rule open new way measur good split directli incorpor gener factor measur experi proven use measur 45 tree size fat tree size exactli oc1 sinc fat pdt tree structur oc1 pdt fat replac split node oc1 pdt larg margin perceptron perform exactli split ten data set moc1 induc five smaller tree one size tree four larger tree compar leav depth leav depth leav depth bright 540 280 620 320 570 290 bupa 500 280 210 110 740 360 cancer 250 130 400 250 290 150 heart 610 210 330 200 210 110 hous 1000 420 710 380 640 300 iri 320 210 320 210 300 200 prognosi 360 200 230 120 220 110 sonar 430 260 610 330 590 290 tabl 1 10fold cv averag tree size oc1 fat moc1 moc2 x p valu x p valu x p valu classifi bright 9846 9862 05 9894 10 9882 10 moc1 cancer 9589 9648 05 9560 9589 fat heart 7340 7643 12 7576 21 7778 10 moc2 hous 8103 8320 05 8202 8023 fat iri 9533 9600 17 9533 9600 fat pima 7109 7148 04 7318 08 7253 23 moc1 prognosi 7891 7415 7823 7959 moc2 tabl 2 10fold cv mean oc1 fat moc1 moc2 oc1 moc2 induc five smaller tree five bigger tree compar oc1 find consist pattern tree size tabl 1 list tree size oc1 fat moc1 moc2 46 summari experiment result theori state maxim margin data point side separ hyperplan perceptron decis tree improv error bound perceptron decis tree like gener better theori guarante specif classifi low error rate 10fold crossvalid result 10 data set fat 9 higher mean oc1 significantli higher moc1 7 higher mean 6 significantli higher moc2 8 higher mean 5 significantli higher equal lower mean happen 3 data set cancer moc1 slightli smaller mean oc1 moc2 mean oc1 hous moc2 slightli smaller mean oc1 prognosi fat significantli smaller mean moc1 also slightli smaller mean dierenc signific classifi highest mean fat produc four moc1 moc2 produc three oc1 produc none experi believ pdt larg margin like smaller varianc perform experi case fat moc1 moc2 produc classifi smaller varianc mani significantli smaller though occasion produc classifi significantli larger varianc howev draw confid conclus varianc therefor present studi varianc short experiment result show find separ hyperplan larg margin node perceptron decis tree improv error bound henc pdt like higher averag accuraci ie gener better furthermor believ improv error bound margin maxim learn algorithm perform consist like smaller varianc well conclus experiment result present paper clearli show enlarg margin improv gener bia insert growth algorithm provid tree specif built minim theoret bound gener error tree lose desir featur readabl eas mainten updat flexibl speed furthermor theoret analysi algorithm show dimens input space aect gener perform henc possibl conceiv perceptron decis tree highdimension featur space take advantag kernel marginmaxim support vector machin would provid side eect natur approach multiclass classif support vector machin theoret result exist indic tree size necessarili good measur capac analysi also show take advantag theoret observ design learn algorithm control hypothesi complex act complex nodeclassifi henc whole tree three propos approach postprocess method fat two margin base split criteria moc1 moc2 led signific improv baselin oc1 method open question method best maxim margin consider everi pdt algorithm r function learn interpol gener perform support vector machin pattern classifi robust linear program discrimin two linearli insepar set multicategori discrimin via linear program ming serial parallel multicategori discrimin support vector decis tree databas market olshen r bayesian classifi larg margin hyperplan hilbert space approxim statist test compar supervis classif learn algorithm studi crossvalid bootstrap accuraci estim model select pattern recognit via linear pro gram theori applic medic diagnosi kasif assess relev determin method use delv gener neural network machin learn learn decis tree use minimum descript lenght principl grow prune neural tree network compar classifi pitfal avoid recommend approach boost margin new explan e structur risk minim datadepend hierarchi neural tree new tool classif estim depend base empir data natur statist learn theori uniform converg rel frequenc event probabl univers california tr infer decis tree use minimum descript length principl c45 program machin learn multivari decis tree natur statist learn theori network fatshatt learnabl realvalu function scalesensit dimens uniform converg learnabl gener perform support vector machin pattern classifi approxim statist test compar supervis classif learn algorithm compar classifi grow prune neural tree network boost margin bayesian classifi larg margin hyperplan hilbert space function learn interpol ctr volkan vural jennif g dy hierarch method multiclass support vector machin proceed twentyfirst intern confer machin learn p105 juli 0408 2004 banff alberta canada nello cristianini colin campbel chri burg editori kernel method current research futur direct machin learn v46 n13 p59 2002 laurenc hirsch robin hirsch masoud saeedi evolv lucen search queri text classif proceed 9th annual confer genet evolutionari comput juli 0711 2007 london england martin anthoni gener error fix combin classifi journal comput system scienc v73 n5 p725734 august 2007 martin anthoni gener error bound threshold decis list journal machin learn research 5 p189217 1212004