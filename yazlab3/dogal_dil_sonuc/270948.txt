viennafortranhpf extens spars irregular problem compil abstractvienna fortran high perform fortran hpf data parallel languag introduc allow program massiv parallel distributedmemori machin dmmp rel high level abstract base spmd paradigm main featur includ direct express distribut data comput across processor machin paper use viennafortran gener framework deal spars data structur describ new method represent distribut data dmmp propos simpl languag featur permit user character matrix spars specifi associ represent togeth data distribut matrix enabl compil runtim system translat sequenti spars code explicitli parallel messagepass code develop new compil runtim techniqu focu achiev storag economi reduc commun overhead target program overal result power mechan deal effici spars matric data parallel languag compil dmmp b introduct past year signific effort undertaken academia govern laboratori industri defin highlevel extens standard program languag particular fortran facilit data parallel program wide rang parallel architectur without sacrif perform import result work vienna fortran 10 28 fortran 15 high perform fortran hpf 18 intend becom defacto standard languag extend fortran 77 fortran 90 direct specifi align distribut program data among processor thu enabl programm influenc local comput whilst retain singl thread control global name space lowlevel task map comput target processor framework singleprogrammultipledata spmd model insert commun nonloc access left compil hpf1 origin version high perform fortran focuss attent regular com putat provid set basic distribut block cyclic replic although approv extens hpf2 includ facil express irregular distribut use indirect special support spars data structur propos paper consid specif requir spars comput aris varieti problem area molecular dynam matrix decomposit solut linear system imag reconstruct mani other order parallel sequenti spars code effect three fundament issu must address 1 must distribut data structur typic use code 2 necessari gener represent spars matric singl processor distributedmemori machin way save memori comput also achiev parallel code 3 compil must abl adapt global comput local comput processor resolv addit complex spars method introduc paper present approach solv three problem first new data type introduc vienna fortran languag repres spars matric data distribut explicitli design map data type onto processor way exploit local spars comput preserv compact represent matric vector therebi obtain effici workload balanc minim commun experi parallel spars code hand 2 confirm suitabl distribut also excess amount time spent develop debug stage manual parallel encourag us build compil specifi algorithm highlevel dataparallel languag way new element introduc vienna fortran extend function express irregular problem subsequ compil runtim techniqu develop enabl specif optim handl typic featur spars code includ indirect array access appear array element loop bound result power mechan store manipul spars matric use dataparallel compil gener effici spmd program irregular code kind paper assum represent distribut spars data invari howev fact represent spars data comput runtim simplifi addit support handl complex featur dynam redistribut matrix fillin ie runtim insert addit nonzero element spars matrix rest paper organ follow section 2 introduc basic formal background handl spars matric section 3 present sever data distribut spars problem section 4 describ new direct specif distribut vienna fortran languag section 5 6 respect outlin runtim support compil technolog requir implement featur section 7 8 present experiment result finish section 9 10 discuss relat work conclus repres spars matric distributedmemori machin matrix call spars small number element nonzero rang method develop enabl spars comput perform consider save term memori comput 16 solut scheme often optim take advantag structur within matrix consequ parallel firstli want retain much save possibl parallel code secondli order achiev good load balanc runtim necessari understand achiev term data structur occur spars problem formul section discuss method repres spars matric distributedmemori machin assum reader familiar basic distribut function vienna fortran hpf 10 28 18 name block cyclick throughout paper denot set target processor proc assum data distribut twodimension mesh proc x processor number 0 dimens specif assum vienna fortran hpf code includ follow declar note abstract processor declar impli specif topolog actual processor interconnect network 21 basic notat terminolog array associ index domain denot replicationfre distribut total function map array element processor becom owner element capac store element local memori processor p 2 proc denot p set element local p call local segment p follow assum twodimension real array repres spars matrix declar index domain notat introduc relat without explicitli reflect depend begin defin set auxiliari function definit 1 1 symbol matrix associ total predic ftrue falseg 2 2 ff j fiji 2 ig j specifi number matrix element nonzero valu biject enumer number element consecut order start 1 4 assum enumer select total function tth index associ nonzero element default use enumer follow number element row wise ie assum spars matrix map distributedmemori machin approach requir two kind inform specifi user 1 represent spars matrix singl processor call spars format 2 distribut matrix across processor machin context concept distribut use matrix dens combin spars format distribut call distribut spars represent matrix 22 spars format discuss data distribut strategi spars data must understand data usual repres singl processor numer storag format propos sparsematrix literatur work use commonli use cr compress row storag format approach extend cc compress column storag swap row column text follow simplic consid spars matric real element immedi gener includ element type logic integ complex definit 2 compress row storag cr spars format determin tripl function dacoro total data function defin dat 1 denot set real number 2 co total column function defin cot t2 total row function defin follow let denot arbitrari row number roi least one specifi properti exist otherwis roi roi 1 three function repres obviou way vector ff real number data vector ff column number column vector number rang row vector respect see figur 1b data vector store nonzero valu matrix travers rowwis fashion column vector store column indic element data vector final row vector store indic data vector correspond first nonzero element row element exist storag save achiev approach usual signific instead store n element need locat spars matrix algorithm design cr format typic use nest loop outer loop iter row matrix inner loop iter nonzero row see exampl section 4 matrix element identifi use twodimension index set say ijj denot ith row matrix jj denot jjth nonzero row matrix element refer ijj one row number r column number coroijj nonzero valu store daroijj heavi use indirect access spars represent requir introduc major sourc complex ineffici parallel code distributedmemori machin number optim present later overcom 3 distribut spars represent let denot spars matrix discuss ffi associ distribut distribut spars represent result combin ffi spars format understood follow distribut ffi interpret convent sens ie 2 pair z x number dens fortran array ffi determin local function p 2 proc specifi local segment p p spars matrix distribut spars represent obtain construct represent element p base given spars format da co ro automat convert set vector da p co p ro p p 2 proc henc parallel code save comput storag use mechan appli origin program spars format use cr illustr idea data distribut introduc two differ scheme subsequ section decompos spars global domain mani spars local domain requir 31 multipl recurs decomposit mrd common approach partit unstructur mesh keep neighborhood properti base upon coordin bisect graph bisect spectral bisect 8 19 spectral bisect minim commun requir huge tabl store boundari local region expens algorithm comput graph bisect algorithm less expen sive also requir larg data structur coordin bisect significantli tend reduc time comput partit expens slight increas commun time binari recurs decomposit brd propos berger bokhari 4 belong last categori brd specifi distribut algorithm spars matrix recurs bisect altern vertic horizont partit step mani submatric processor submatrix map uniqu processor flexibl variant algorithm produc partit shape individu rectangl optim respect userdetermin function 7 section defin multipl recurs decomposit mrd gener brd method also improv commun structur code assum processor array declar prime factor decomposit x order way prime factor x sort descend order come first follow factor sort fashion mrd distribut method produc x partit matrix k step recurs perform horizont divis matrix prime factor x vertic one prime factor matrix partit p 1 submatric way nonzero element spread across submatric evenli possibl submatrix partit horizont row nonzero entri uniqu assign either partit includ lower one vertic step column assign right partit submatrix result step i1 partit p submatric use criteria process termin creat q k submatric enumer consecut 0 x use horizont order scheme submatrix number r map processor procsr 2 distribut defin local segment processor rectangular matrix preserv neighborhood properti achiev good load balanc see figur 2 fact perform horizont partit step vertic one reduc number possibl neighbor submatrix may henc simplifi analysi perform compil runtim system combin cr represent local segment mrd distribut produc mrdcr distribut spars represent inmedi gener storag format howev sinc use cr illustr idea refer mrdcr mrd 32 br distribut spars represent second strategi base cyclic distribut see figur 4a retain local access regular case suitabl workload spread evenli across matrix present period densiti matrix vari time mani common algorithm natur includ spars matrix decomposit lu choleski qr wz imag reconstruct algorithm section assum dimens 0 distribut cyclic block length 1 see figur 4b sever variant represent distribut segment context describ literatur includ mm ess bb method 1 consid cr spars format result br block row scatter distribut spars represent similar distribut represent bc block column scatter 26 spars format compress column chang row column vice versa map establish br choic requir complex auxiliari structur translat scheme within compil howev data use togeth cyclicallydistribut dens array structur properli align lead save commun extens support spars matrix comput 41 languag consider section propos new languag featur specif spars data data parallel languag clearli block cyclic distribut offer hpf1 adequ purpos hand indirect distribut 15 28 includ approv extens hpf2 allow specif structur inher distribut spars represent thu introduc unnecessari complex memori consumpt execut time propos make structur explicit appropri new languag element seen provid special syntax import special case userdefin distribut function defin vienna fortran hpf 11 12 new languag featur provid follow inform compil runtim system ffl name index domain element type spars matrix declar done use regular fortran declar syntax array actual appear origin code sinc repres set array name introduc refer specifi distribut ffl annot specifi declar array spars provid inform represent array includ name auxiliari vector order data column row declar explicitli program size determin implicitli matrix index domain ffl dynam attribut use manner analog mean vienna fortran hpf specifi distribut spars represent determin dynam result execut distribut statement otherwis compon distribut spars represent construct time declar process often inform contain file whose name indic annot addit input spars matrix avail compiletim must read file standard format distribut runtim name file may provid compil addit direct concret exampl typic spars code illustr detail syntax well hpf given figur 5 6 42 solut spars linear system wide rang techniqu solv linear system among iter method use success approxim obtain accur solut step conjug gradient cg 3 oldest best known effect nonstationari iter method symmetr posit definit system converg process speed use precondition comput cg includ figur 5 dataparallel code unprecondit cg algorithm involv one matrixvector product three vector updat two inner product per iter input coeffici matrix vector scalar b also initi estim must comput xvec solut vector element initi residu r defin everi iter two inner product perform order updat scalar defin make sequenc fulfil certain orthogon condit end iter solut residu vector updat 43 lanczo algorithm figur 6 illustr algorithm extend hpf tridiagon matrix lanczo method 24 use new direct indic nsd specifi requir declar inform execut distribut direct result comput distribut spars represent point matrix legal access program sever matrixvector vectorvector oper perform comput diagon output matrix 5 runtim analysi base languag extens introduc section show access spars data effici translat vienna fortran hpf explicitli parallel messag pass code context data parallel spmd paradigm rest paper assum input matrix avail compiletim assumpt matrix distribut postpon runtim obvious enforc global local index translat also perform runtim parallel code use indirect address compil typic use inspectorexecutor strategi 22 loop access distribut variabl tranform insert addit preprocess loop call inspector inspector translat global address access indirect processor offset tupl describ locat element comput commun schedul executor stage use preprocess inform fetch nonloc element access distribut data use translat address obviou penalti use inspectorexecutor paradigm runtim overhead introduc inspector stage becom signific multipl level indirect use access distribut array seen frequent case sparsematrix algorithm use compact storag format cr exampl xvecdaroijj refer encount figur 5 requir three preprocess step one access distribut array ro second access da yet third access xvec pay special attent issu section outlin effici solut parallel 51 sar approach though base inspectorexecutor paradigm solut translat cr like spars indic runtim within dataparallel compil significantli reduc time memori overhead compar standard generalpurpos chao librari 23 techniqu call spars array roll sar encapsul small descriptor inform input matrix distribut across processor allow us determin processor offset locat spars matrix element without plod distribut auxiliari array datastructur thu save preprocess time requir intermedi array figur 7 provid overview sar solut approach distribut matrix repres cr format carri partition routin respons comput domain decomposit give output distribut represent well associ descriptor descriptor index translat process use row number nonzero index x locat processor offset matrix element store element found nonloc derefer process assign address local memori element place fetch executor stage use preprocess inform insid coupl gatherscatt routin fetch mark nonloc element place assign locat final loop comput access distribut data use translat address effici translat function memori overhead descriptor larg depend matrix distribut follow section provid detail distribut studi paper 52 mrd descriptor translat mrd distribut map rectangular portion dens index space n theta onto virtual processor space x theta correspond descriptor replic processor consist two part vector parth store row number x horizont partit made two dimension array partv size n theta keep track number nonzero element vertic partit row exampl 1 mrd distribut matrix figur 3 correspond descriptor replic among processor follow parth18 denot horizont partit made row 8 row two vertic partit valu partv912 23 say first section row 9 two nonzero element second section one 3 assum parth01 parthxn1 partvk00 1 k n given nonzero element identifi ijj perform translat mean descriptor determin processor own nonzero element assum processor identifi posit myr myc x theta virtual processor mesh valu myr myc processor own element satisfi follow inequ search right myr myc satisfi inequ requir search space size x theta search optim first check see element local plug local processor valu myr myc assum high degre local check frequent succe immedi fail binari search mechan employ offset element locat xvecpartvimyc thu column number element ijj found coxvecpartvimyc processor myr myc nonzero valu access daxvecpartvimyc processor without requir commun addit preprocess step 53 br descriptor translat unlik mrd br descriptor differ processor processor myrmyc element nx row map onto br descriptor store local row matrix entri everi nonzero element row regardless whether element map local element local entri store local index da nonloc element entri store global column number element origin matrix distinguish local entri nonloc entri swap sign local indic becom neg actual datastructur use crslike twovector represent vector call cs store entri element map local row anoth vector ra store indic row start cs exampl 2 spars matrix partit show figur 4 valu cs ra processor 00 follow cs12 say element 53 store global column 2 nonloc cs21 signifi element 19 map local store local index 1 remain entri similar interpret processor own element rx identifi follow first local row identifi use simpl formula x entri element obtain use csrarjj neg impli element local access dam posit global row column number element impli processor own element save ijj indic list indic mark later retriev processor q executor gather routin send ijj indic q similar translat process repeat time howev element local found sent request processor 6 compil section describ compil implement within vienna fortran compil system vfc input compil viennafortran code extend spars annot describ section 4 compil process result fortran 77 code enhanc messagepass routin well runtim support alreadi discuss previou section tool structur set modul shown figur 8 describ modul separ 61 frontend first modul part tool interact declar part program respons 1 scan pars new languag element present section 4 oper gener abstract syntax tree annot tabl summar compiletim inform extract tabl built spars direct need compil proce remov code 2 insert declar local vector auxiliari variabl target code runtim support util 62 parallel stage compil first scan code search spars refer extract inform avail compiletim ie indirect syntax indic loop condit insid refer etcetera inform organ databas later lookup parallel process done loop decomposit start goal consist distribut workload sourc code evenli possibl among processor task turn particularli complex compil handl spars code mainli frequent use indirect access spars data frequent use spars refer loop bound case multipl queri distribut spars data requir processor order determin iter space lead larg number commun overcom problem address problem differ way rather tri access actual spars valu request loop header appli loop transform determin local iter space also map valu semant equival inform local distribut descriptor approach doubl advantag reus compil auxiliari structur ensur local access perform loop boundari result much faster mechan access data extra memori overhead mrd case exampl array parth partv determin local region data spars matrix base global coordin way loop partit driven similar strategi block differ region differ size similar workload determin runtim descriptor gener runtim support br case solut straightforward let us take exampl conjug gradient cg algorithm figur 5 dens vector distribut dens cyclic spars matrix follow br scheme note cg loop refer dens structur decomposit perform enforc stride loop number processor data dimens travers loop distribut consecut local data cyclic alway separ constant distanc term global coordin howev refer spars vector includ loop fact true first matrix dimens second one actual sparsiti degre matrix determin distanc consecut data term global column sinc becom unpredict compiletim recal assumpt spars matrix pattern avail runtim runtim check defin function br distribut descriptor need insert loop travers second matrix dimens success parallel check eventu move inspector phase executor comput number iter thu decreas overal runtim overhead see transform final code gener figur 10 figur 9 provid code excerpt outlin loop decomposit perform within vfc two spars loop figur 5 ra cs vector br descriptor processor coordin myr myc ra store indic way local ro consid element place global row theta x myr given local row cycliclik approach follow extract local iter first loop ra travers element second loop cs delimit local iter subsequ note differ criteria follow parallel loop first loop wellknown owner comput rule appli second loop though underli idea avoid replic comput first calcul local partial sum given local element accumul valu singl reduct phase way comput distribut base owner everi singl da p valu given index k make match alway processor achiev complet local 63 backend workload assign processor compil enter last stage whose output target spmd code reach goal code transform inspector executor phase loop figur show final spmd code spars loop parallel figur 9 overal next sequenc step carri compil modul 1 inspector loop insert prior loop comput header loop obtain syntax tree parallel statement insid loop gener collect indic distribut array auxiliari vector vector taken input translat process 2 call translat derefer scattergath routin place inspector executor loop complet runtim job 3 refer distribut variabl executor loop sintact chang index translat function produc output inspector see function f g figur 10 4 addit io routin must insert begin execut part merg processor local data descriptor sar scheme done partition routin 7 evalu distribut method choic distribut strategi matrix crucial determin perform control data local load balanc executor preprocess cost inspector memori overhead runtim support section discuss br mrd distribut affect aspect particular case spars loop conjug gradient algorithm account effect differ sparsiti structur chose two differ matric come harwellbo collect 14 identifi psmigr1 bcsstk29 former contain popul migrat data rel dens wherea latter spars matrix use larg eigenvalu problem matrix characterist summar tabl 1 71 commun volum executor tabl 2 show commun volum executor 16 processor 4 theta 4 processor mesh comput spars loop cg algorithm commun necessari accumul local partial product array q oper implement like typic reduct oper local matrix row processor row note two thing first relat commun volum processor mesh configur second balanc commun pattern note comparison commun volum across two matric rel number row gener x theta processor mesh n theta spars matrix commun volum roughli proport nx theta logi thu 8 theta 2 processor mesh 4 time less total commun volum 4 theta 4 mesh br processor accumul exactli amount data mrd minor imbal stem slightli differ size horizont partit see figur 11 commun time executor show black figur 13 72 loop partit workload balanc explain section 62 iter spars loop conjug gradient algorithm map owner da element access iter result perfect workload balanc mrd case sinc processor own equal number nonzero br workload balanc reli random posit element except patholog case result good load balanc tabl 3 show load balanc index br maximum variat averag divid averag 73 memori overhead vector store local submatrix processor requir similar amount memori distribut howev distribut descriptor use runtim support requir substanti differ amount memori tabl 4 summar requir first row indic expect memori overhead next two row show actual overhead term number integ requir overhead column repres memori overhead percentag amount memori requir store local submatrix vector partv cs respons overhead distribut sinc keep track posit nonzero element mrd br respect overhead much higher br cs vector store column number even offprocessor nonzero length vector reduc use processor mesh 8 runtim evalu section describ perform evalu spars loop conjug gradient algorithm parallel use vfc compil br mrd especif intent studi effect distribut choic inspector executor perform within dataparallel compil final manual version applic use baselin determin overhead semiautomat parallel platform intel paragon use nxlib commun librari experi ment account io time read matrix perform distribut 81 inspector cost figur 12 show preprocess cost spars loop mrd br version cg algorithm two matric preprocess overhead reduc increas parallel though effici drop high end also note br incur higher preprocess overhead mrd also scale better understand rel cost br rel mrd recal br translat mechan involv preprocess nonzero local row mrd dereferenc requir binari search distribut descriptor local nonzero though process fewer element size mrd search space proport size processor mesh processor ad translat requir search larger space though shown tabl measur indic br inspector actual faster mrd 64 processor 82 executor time sinc scheme distribut nonzero equal across processor found comput section executor scale well distribut 32 processor commun overhead start reduc effici figur 13 show executor time spars loop two cg version indic good load balanc fact find case superlinear speedup attribut cach effect executor commun time shown dark figur 13 br commun overhead remain essenti invari across processor size suggest overhead extra commun startup offset reduc commun volum maintain total overhead mrd commun much unbalanc lead much poorer scale commun cost inde effect particularli appar bcsstk29 redistribut extrem unbalanc becom sever bottleneck processor size increas 83 comparison manual parallel effici spars code parallel within vfc compil depend larg primari factor ffl distribut scheme select parallel either mrd br ffl sparsiti rate input matrix ffl cost inspector phase figur access pattern hand seen parallel spars loop cg algorithm within vfc lead target code executor perform commun gatherscatt routin consequ full local achiev data distribut local represent loop partit strategi apart actual comput executor contain commun accumul local partial product implement reduct routin exactli programm would thu executor time becom accur estim effici smart programm attain addit cost use automat compil lie intir preprocess time inspector loop plu subsequ runtim call figur 10 figur 14 tri explain impact major factor influenc parallel effici provid comparison manual compilerdriven parallel execut time compil includ cost singl inspector plu executor per iter wherea manual version inspector requir far distribut concern figur 14 show br introduc bigger overhead direct consequ expens inspector slower global local translat process howev even br case overal result quit effici number iter practic converg cg algorithm start exhibit stationari behaviour less one hundr iter time inspector cost alreadi wide amort total compil overhead alway kept 10 regardless input matrix distribut chosen number processor parallel machin respect matrix sparsiti conclud higher degre sparsiti matrix better result produc compil compar manual version overal comparison manual parallel also reflect good scalabl manual gain small number iter summar say cost paid automat parallel worthwhil long algorithm amort inspector cost minimum number iter remain cost conjug gradient algorithm lie multipl loop deal dens array distribut cyclic howev comput weight part never goe 10 total execut time even though compil effici expect improv case influenc minimum lead signific variat full algorithm addit experi demonstr effici scheme develop trena 24 implement manual version lanczo algorithm see figur 6 use pvm routin br scheme 9 relat work program design carri rang spars algorithm matrix algebra outlin 3 code requir optim describ paper effici target code gener parallel system varieti languag compil target distribut memori multiprocessor 28 9 15 18 attempt deal loop aris spars irregular comput one approach origin fortran vienna fortran base indirect data distribut express structur spars data result memori runtim overhead scheme propos paper provid special syntax special class userdefin data distribut propos vienna fortran hpf 12 hand area automat parallel outstand tool know parafras 20 polari 6 intend framework parallel spars algorithm address present work method propos saltz et al handl irregular problem consist endow compil runtim librari 23 facilit search captur data locat distribut memori major drawback approach larg number messag gener consequ access distribut data address tabl associ overhead memori 17 order enabl compil appli optim simplifi task programm bik wijshoff 5 implement restructur compil automat convert program oper dens matric spars code method postpon select data structur compil phase though friendli end user approach risk ineffici result allow programm choos appropri spars structur way deal problem differ defin heurist perform effici map data languag extens describ map data parallel languag 18 28 produc benchmark prototyp compil integr vfc abl gener effici code irregular kernel compil transform insert procedur perform runtim optim implement qualit differ effort cite number import respect particular respect use new data type spars format data distribut distribut spars represent irregular comput basic idea distribut take account way spars data access map data pseudoregular way compil may perform number optim spars code specif pseudoregular distribut allow us describ domain decomposit use small descriptor addit access local save memori overhead distribut tabl well commun cost need lookup gener applic code irregular problem normal code segment loop complex access function advanc analysi techniqu known slice analysi 13 deal multipl level indirect transform code contain refer code contain singl level indirect howev multipl commun phase still remain sar techniqu implement insid spars compil novel abl handl multipl level indirect cost singl translat key attain goal consist take advantag compiletim inform semant relat element involv indirect access conclus paper spars data distribut specif languag extens propos dataparallel languag vienna fortran hpf improv handl spars irregular comput featur enabl translat code use typic spars code techniqu without necess rewrit show detail code may translat result code retain signific featur sequenti spars applic particular save memori comput typic techniqu retain lead high effici run time data distribut design retain data local appropri support good load balanc avoid memori wastag compil time run time support translat structur permit spars represent data processor parallel system languag extens requir minim yet suffici provid compil addit inform need translat optim number typic code kernel shown paper 26 demonstr limit amount effort requir port sequenti code kind extend hpf vienna fortran result demonstr data distribut languag featur propos suppli enough inform store access data distribut memori well perform compil optim bring great save term memori commun overhead lowlevel support spars problem describ propos implement optim compil perform translat compil improv function dataparallel languag irregular comput overcom major weak field runtim techniqu use context inspectorexecutor paradigm howev set lowlevel primit differ use sever exist implement order take advantag addit semant inform avail approach particular runtim analysi abl translat multipl indirect array access singl phase make use expens translat tabl final result optim compil abl gener effici parallel code comput close expect manual parallel much faster comparison exist tool area r schedul spars matrixvector multipl massiv parallel dap comput partit strategi nonuniform problem multiprocessor automat data structur select transform spars matrix comput massiv parallel method engin scienc problem vienna fortran compil system program vienna fortran user defin map vienna tran extend hpf advanc data parallel applic index array flatten program transform user guid harwellbo spars matrix collect fortran languag specif comput solut larg spars posit definit sy tem high perform languag specif numer experi partit unstructur mesh structur parafrase2 advanc parallel compil c fortran data distribut spars matrix vector multipl solver parallel algorithm eigenvalu comput spars matric effici resolut spars indirect dataparallel compil evalu parallel techniqu spars applic vienna fortran languag specif version 11 tr ctr chunyuan lin yehch chung jenshiuh liu effici data compress method multidimension spars array oper base ekmr scheme ieee transact comput v52 n12 p16401646 decemb rongguey chang tyngruey chuang jenq kuen lee effici support parallel spars comput array intrins function fortran 90 proceed 12th intern confer supercomput p4552 juli 1998 melbourn australia gerardo bandera manuel ujaldn emilio l zapata compil runtim support parallel spars matrix updat algorithm journal supercomput v17 n3 p263276 nov 2000 manuel ujaldon emilio l zapata effici resolut spars indirect dataparallel compil proceed 9th intern confer supercomput p117126 juli 0307 1995 barcelona spain roxan adl marc aiguier franck delaplac toward automat parallel spars matrix comput journal parallel distribut comput v65 n3 p313330 march 2005 v blanco p gonzlez j c cabaleiro b hera f pena j j pombo f f rivera perform predict parallel iter solver journal supercomput v28 n2 p177191 may 2004 chunyuan lin yehch chung data distribut scheme spars array distribut memori multicomput journal supercomput v41 n1 p6387 juli 2007 bradford l chamberlain lawrenc snyder array languag support parallel spars comput proceed 15th intern confer supercomput p133145 june 2001 sorrento itali chunyuan lin yehch chung jenshiuh liu effici data distribut scheme ekmrbas spars array distribut memori multicomput journal supercomput v34 n3 p291313 decemb 2005 garz garca approach base permut partit spars matric multiprocessor journal supercomput v34 n1 p4161 octob 2005 thoma l sterl han p zima gilgamesh multithread processorinmemori architectur petaflop comput proceed 2002 acmiee confer supercomput p123 novemb 16 2002 baltimor maryland ali pinar cevdet aykanat fast optim load balanc algorithm 1d partit journal parallel distribut comput v64 n8 p974996 august 2004 bradford l chamberlain steven j deitz lawrenc snyder compar studi na mg benchmark across parallel languag architectur proceed 2000 acmiee confer supercomput cdrom p46e novemb 0410 2000 dalla texa unit state ken kennedi charl koelbel han zima rise fall high perform fortran histor object lesson proceed third acm sigplan confer histori program languag p71722 june 0910 2007 san diego california