topicori collabor crawl major concern implement distribut web crawler choic strategi partit web among node system goal select strategi minim overlap activ individu node propos topicori approach web partit gener subject area crawler assign examin design altern topicori distribut crawler includ creation web page classifi use context approach compar experiment hashbas partit crawler assign determin hash function comput url page content experiment evalu demonstr feasibl approach address issu commun overhead duplic content detect page qualiti assess b introduct crawler program gather resourc web web crawler wide use gather page index web search engin 12 23 may also use gather inform web data mine 16 20 question answer 14 28 locat page specif content 1 9 crawler oper maintain pend queue url crawler intend visit stage permiss make digit hard copi part work person classroom use grant without fee provid copi made distribut profit commerci advantag copi bear notic full citat first page copi otherwis republish post server redistribut list requir prior specif permiss andor fee cikm02 novemb 49 2002 mclean virginia usa crawl process url remov pend queue correspond page retriev url extract page url insert back pend queue futur process perform crawler often use asynchron io allow multipl page download simultan 46 structur multithread program thread execut basic step crawl process concurr other 23 execut crawler thread parallel multiprocessor distribut system improv perform 4 10 23 implement distribut system potenti allow widescal geograph distribut crawl node minim competit local network bandwidth distribut crawler may structur group n local crawler singl local crawler run node nnode distribut sy tem local crawler may multithread pro gram maintain pend queue data structur share local execut thread local crawler may even parallel program execut cluster workstat central control global pend queue paper use term distribut crawler refer system data structur system control fulli distribut local crawler oper entir independ collabor necessari avoid duplic eort sever respect local crawler must collabor reduc elimin number resourc visit one local crawler extrem case collabor local crawler might execut ident crawl visit resourc order overal distribut crawler must partit web local crawler focus subset web resourc target crawl collabor also need identifi deal duplic content 5 13 23 often multipl url use refer site moneycnncom cnnfncom cnnfncnncom case larg set interrel page encount repeatedli crawl crawler avoid visit copi entireti exampl mani copi sun java jdk document found web result crawl use web search system user might present authorit copi resourc copi java document sun web site similarli webbas question answer 14 depend independ assumpt duplic content valid recogn duplic content local crawler might comput hash function content web page 13 23 distribut crawler sucient inform must exchang local crawler allow duplic handl correctli collabor may necessari eectiv implement url order algorithm algorithm attempt order url pend queue desir page retriev first result order might reflect expect qualiti page need refresh local copi import page whose content known chang rapidli cho et al 12 compar sever approach order url within pend queue introduc order base pagerank measur 4 measur page expect qualiti compar breadthfirst order random order order base simpl backlink count report experi demonstr pagerank order like place import page earlier pend queue given page p pagerank rp may determin pagerank page t1 link damp factor whose valu usual rang 08 09 c outdegre total number page link given set page pagerank valu may comput assign page initi valu 1 iter appli formula context crawler pagerank valu may estim url pend queue backlink inform provid page alreadi retriev depend backlink inform local crawler may need exchang inform order accur estim pagerank valu within local pend queue 10 ideal local crawler would oper nearli indepen dentli despit need collabor amount data exchang local crawler mini mize preserv network bandwidth actual download target web resourc scalabl total amount data sent receiv local crawler small constant factor larger amount receiv data actual associ crawler partit synchron local crawler oper one local crawler may delay need commun anoth local crawler avoid avoid synchron especi import local crawler geograph distribut network node failur may disrupt commun final desir output local crawler usabl independ subcollect exampl crawl gener use distribut appli cation distribut search engin may possibl transfer data directli sourc node distribut crawler destin node distribut applic without ever central data output local crawler use independ subcollect exhibit intern cohes link graph contain dens connect compon allow pagerank qualiti metric accur estim 2 4 27 paper present x4 topicori distribut crawl system x4 web partit content local crawler assign broad content categori target crawl page download pretrain classifi appli page determin uniqu content categori local crawler oper independ unless encount boundari page page associ assign categori boundari page queu transfer associ local crawler remot node boundari page arriv remot crawler treat directli download local crawler next section paper provid review relat work section 3 discuss approach collabor crawl view primari altern topicori approach altern web partit hash url page content section 4 5 examin issu design x4 crawler section 6 x4 evalu experiment includ comparison hashbas partit 2 relat research comprehens studi web crawler design cho 2001 stanford phd thesi 10 par ticular chapter 3 thesi along subsequ paper 11 repres studi awar attempt map explor full design space parallel distribut crawler share mani goal work address issu commun band width page qualiti divis work local crawler part work cho propos evalu hashbas approach similar discuss next section suggest occasion exchang link inform node improv accuraci local comput page qualiti estim cho thesi directli address issu duplic content detect distribut context apart cho work design parallel crawler implement form global control pend queue relat data structur maintain central actual download page take place worker node 4 6 one exampl webfountain crawler 20 base clusterofworkst architectur softwar consist three major compon ant duplic detector singl system control control manag system whole maintain global data structur monitor system perform ant respons actual retriev web resourc control assign site ant crawl retriev url site duplic detector recogn ident nearident content major featur webfountain crawler mainten todat copi page content identifi page chang frequent reload need technic detail crawler use commerci web search servic natur regard trade secret publish detail structur implement one signific except merca tor crawler use altavista search servic replac older scooter crawler 1 heydon najork 23 24 describ detail problem associ creat commercialqu web crawler solut use address problem mercat written java mercat achiev scalabl extens larg care softwar engin research focus crawler close connect work describ present paper essenc focus crawler attempt order pend queue page concern specif topic place earlier queue goal creat special collect target topic chakrabarti et al 9 introduc notion focus crawler experiment evalu implement concept use set fairli narrow topic cycl mutual fund hivaid order determin next url access implement use hypertext classifi 8 determin probabl page relev topic distil identifi hub page point mani topicrel page 27 mukherjea 33 present wtm system gather analyz collect relat web page describ focus crawler form part system wtm crawler use vector space similar measur compar download page target vector repres desir topic url page higher similar score placer earlier pend queue mccallum et al 30 diligenti et al 18 recogn target page focus crawl necessarili link directli one anoth describ focus crawler learn identifi appar otop page reliabl lead ontop page menczer et al 31 consid problem evalu compar eectiv strategi use focus crawler intellig crawler propos aggarw et al 1 gener idea focus crawler encompass much prior research area work assum exist predic determin membership target group start arbitrari point crawler adapt learn linkag structur web lead target page consid combin featur includ page content pattern match url ratio link sibl page also satisfi predic 3 hashbas collabor one approach implement distribut crawler partit web comput hash function url page content local crawler extract url retriev page represent first normal convert absolut url necessari translat escap sequenc ascii valu hash function comput normal url assign one n local crawler assign local crawler locat remot node url transfer node url present correct node may ad node pend queue similarli local crawler comput hash function content page download pagecont hash function assign content one local crawler duplic detect postprocess take place hashbas scheme collabor three local crawler may involv process url encount 1 local crawler encount 2 node assign url hash function node content assign page content hash function although mani transfer local crawler avoid local crawler maintain local tabl url page content previous seen prevent unnecessari transfer url hash function need take entir url account exampl url hash function base hostnam ensur url given server assign local crawler download 10 allow load place server better control similarli pagecont hash function base normal content allow neardupl assign node 5 13 use pagecont hash function may forc consider data transfer local crawler n 2 uniform hash function map retriev page remot node download page map local node must export assign node nnode distribut crawler expect ratio export data total data download n 1n data export remot node data import node local crawler local crawler download data rate expect ratio import data total data download also n 1n limit number node increas amount data transfer local crawler twice amount data download web uniform hash function content page map local crawler independ locat page refer number local crawler increas probabl referenc page assign node referenc page decreas proport properti may neg impact qualiti heurist use order pend queue turn eectiv crawl page qualiti heurist use backlink inform may inform local avail accur estim order metric solut propos cho 10 local crawler period exchang backlink inform cho demonstr rel small number exchang substanti improv local page qualiti estim approach increas complex commun local crawler implement approach local crawler must either select queri other backlink inform transfer backlink inform local crawler import paramet collabor crawler probabl p l link page link page assign node hashbas collabor crawler use uniform hash function p l 1n one goal topicori collabor reduc depend p l number node n 4 topicori collabor previou section assum hash valu content specif page independ hash valu page refer section outlin design topicori collabor crawler use text classifi assign page node given content web page classifi assign page one n distinct subject categori subject categori associ local crawler classifi assign page remot node local crawler transfer assign node process topicori collabor crawler may view set broadtop focus crawler partit web breadth subject categori depend valu n n rang 1020 two subject categori might busi sport larger n subject categori narrow invest footbal hockey implement classifi use x4 discuss next section potenti advantag replac simpl pagecont hash function text classifi increas likelihood link page map node link page exampl link page classifi sport may like refer anoth sport page busi page mani potenti benefit topicori collabor crawl deriv assumpt topic local page tend refer page gener topic 17 one immedi benefit topic local reduct bandwidth requir transfer page one local crawler anoth boundari page assign node retriev transfer addit page qualiti metric depend backlink inform might accur estim page group assign categori sinc complet linkag inform may present addit advantag topicori collabora tion output local crawler meaning regard independ subcollect inform broker 21 22 weight output dierent search system accord expect perform dierent queri type may abl take advantag topic focu subcollect topicori approach collabor crawl disadvantag url may independ encount download multipl crawler approach url hash alway retain node encount url encount two node node independ download page transfer common node categor properti may appear repres seriou limit topicori collabor crawl observ page encount multipl node page multipl categori refer situat topic local assumpt tend minim benefit topicori collabor depend accuraci classifi actual intra inter categori linkag pattern found web experiment evalu issu use x4 crawler report section 6 provid context evalu first describ evalu simpl classifi use x4 5 page categor text categor group text document set predefin categori topic often categor base probabl gener train set preclassifi document contain exampl topic document featur word phrase structur relationship extract preclassifi document use train classifi given unclassifi document train classifi extract featur document assign highestprob categori text categor heavili studi subject varieti machin learn inform retriev techniqu appli problem includ rocchio feedback 25 support vector machin 26 expectationmaxim 34 boost 35 yang liu 37 provid recent comparison five wide use method mani techniqu appli categor web page case extend exploit uniqu properti web data much work context focu crawler discuss section 2 chakrabarti et al 8 take advantag web link structur improv web page categor use iter relax label approach page classifi use categori assign neighbor page part featur set dumai chen 19 take advantag larg collect hierarch organ web page provid organ yahoo looksmart develop hierarch categor techniqu base support vector machin select page categor techniqu use x4 sever attribut avail techniqu consid ere first categor base document content content neighbor document consid classifi page neighborhood would retriev crawler encount retriev neighborhood page classifi like increas overlap crawler someth x4 seek avoid second train complet classifi must remain static learn new data sinc categori assign page content everi local crawler must third classifi ecient enough categor page rate download crawl major portion web requir minimum download rate sever mbp classifi abl match rate without requir resourc crawler final accuraci classifi percent page correctli cla sifi high possibl moreov minim number boundari page encount probabl link page classifi categori link page topic local also high possibl mani avail techniqu choos three studi basi simplic potenti ecient implement basic naiv bay classifi 32 classifi base rocchio relev feedback 25 probabilist classifi due lewi 29 data open directori project 2 odp use train test classifi odp selfregul organ maintain volunt expert categor url hierarch class directori similar directori provid yahoo other top level 17 categori volunt examin content url determin categori level hierarchi contain list relev extern link list link subcategori snapshot 673mb data obtain odp purpos experi entir url directori tree collaps top categori two categori given special treatment region cate adult art busi comput game health recreat refer scienc world figur 1 odp categori use topic classifi gori encompass page specif variou geograph area elimin entir sinc believ repres much separ topic altern organi zation page world categori written languag english also ignor initi classifi select phase final x4 classifi nonenglish page handl separ languag iden tifier 16 toplevel categori includ world exclud region list figur 1 ultim becam target categori use x4 classifi categor page preprocess first remov tag script numer inform remain text token term base whitespac punc tuation term convert lower case result term treat document featur requir classifi odp categori 500 document randomli select train 500 test html document contain 50 word preprocess consid candid select perform classifi shown figur 2 overal naiv bay classifi achiev accuraci 607 lewi classifi 582 rocchiotfidf classifi 437 topicori crawl import statist topic local measur proport link page classifi categori link page test topic local 100 web page categori randomli select train data five random link page link page five retriev classifi result shown figur 3 overal naiv bay classifi achiev topic local 624 lewi classifi 623 rocchiotfidf classifi 489 test speed classifi 30000 web page averag length 7738 byte fed naiv bay classifi achiev throughput 1950 pagessecond lewi classifi 289 pagessecond rocchiotfidf classifi 161 pagessecond although naiv bay lewi classifi perform compar term categor accuraci outperform rocchio tfidf classifi six time greater speed naiv bay classifi recommend use x4 crawler topic categor natur languag page written must determin languagespecif classifi may use languag identif partit page two languagespecif topic divis node natur languag languagespecif topic depend number local crawler requir mix web resourc target crawl purpos experi report paper group nonenglish page singl categori world follow odp organ number simpl statist languag identif techniqu shown good perform 3 7 15 x4 identifi englishlanguag page proport common english word appear train test identifi select 500 random page cat egori includ world result languag identifi accuraci 961 902 identifi english nonenglish page respect 6 experiment evalu x4 crawler implement extens multitext web crawler origin develop part multitext project univers waterloo gather data webbas question answer 14 crawler sinc use number extern group share design goal mercat 23 mul titext crawler design highli modular config urabl use gener collect 1tb size ordinari pc crawler maintain download rate million page day includ pre postprocess page core crawler provid dataflow script languag coordin activ independ softwar compon perform actual oper crawl individu compon respons specif crawl task address resolut page download url extract url filter duplic content handl core crawler also provid transact support allow crawler action roll back restart system failur x4 creat ad two new compon multitext crawler one compon topic classifi compon data transfer util data transfer remot node queu local topic classifi period everi 30 minut data transfer util poll remot node download queu data scp use perform actual transfer apart chang standard crawl script add call classifi data transfer util chang multitext crawler requir experiment evalu x4 pend queue maintain breadthfirst order one except breadthfirst order would place excess load singl host defin 02 total crawler activ time period roughli one hour url associ host remov requeu anticip load drop accept level experiment evalu x4 use naiv bay topic classifi train odp data describ previou section preprocess retriev page first check determin unclassifi defin contain fewer 50 term remov tag script preprocess arbitrarili assign unclassifi page categori 0 adult unless unclassifi page languag identifi appli page assign world categori languag identifi topic classifi appli assign page final categori experi local crawler associ 16 categori figur 1 local crawler map onto six node workstat cluster assign three local crawler five node singl adu art bu com gam hea hom kid new rec ref sci shp soc spt1030507090na bay lewi rocchiotfidf categori figur 2 accuraci classifi odp data adu art bu com gam hea hom kid new rec ref sci shp soc spt1030507090na bay lewi rocchiotfidf categori topic local figur 3 topic local classifi odp data categori world one node although multipl local crawler assign node test purpos local crawler act respect execut distinct node gener 137gb experiment crawl june 2001 due limit avail bandwidth gener internet crawler group limit download rate 256kbsecond local crawler limit 16kbsecond due diculti control download rate low speed actual download rate vari 11 kbsecond 16 kbsecond total 89 million page download classifi figur 4 plot distribut data across local crawler show volum download data retain local categor volum data import local crawler topic local achiev local crawler plot figur 5 gener topic local achiev crawl slightli lower seen odp data figur 3 rel topic local achiev topic roughli main except local crawler 0 adult topic local aect arbitrari decis assign unclassifi page associ categori comparison equival valu hashbas collabor p l 116 625 hashbas collabor crawl url hash exchang map url uniqu local crawler prevent multipl crawler download url case topicori collabor crawl sinc page content exchang topicori collabor multipl crawler download url url referenc page one categori show log number page retriev exactli local crawler number page decreas rapidli increas page retriev local crawler gener home page major organ product wwwnytimescom wwwmicrosoftcomi order examin properti subcollect gener local crawler order page subcollect use pagerank algorithm permit direct comparison hashbas collabor redistribut page 16 dierent subcollect use pagecont hash function comput pagerank order final gather page singl collect comput global pagerank order figur 7 compar pagerank order comput subcollect global pagerank order comput combin collect subcol lection figur report kendal rank correl pagerank order comput subcol port categori size figur 4 distribut crawl data lection pagerank order page comput global collect higher correl coe cient indic accur local estim global pagerank order might expect use uniform hash function coecient hashbas subcollect nearli ident case coecient topicori subcollect greater co ecient hashbas subcollect 7 conclus futur work paper propos concept topicori collabor crawl method implement generalpurpos distribut web crawler demonstr feasibl implement experiment evalu ation contrast url hostbas hash approach evalu cho 1011 approach allow duplic page content recogn gener subcollect page relat topic rather locat x4 could extend improv sever way current implement pend queue maintain breadthfirst order instead focus crawl techniqu might use order pend queue place url like refer ontop resourc closer front queue techniqu could substanti increas proport ontop page retriev decreas number transfer local crawler x4 transfer content boundari page node retriev node classifi assign design decis taken consequ desir map content page uniqu local crawler order facilit duplic detect postprocess altern design would transfer url link appear boundari page content boundari page would transfer disadvantag approach content url uniqu associ local crawler everi local crawler encount url boundari page retain copi content advantag approach commun overhead local crawler reduc commun local crawler transfer url extract boundari page retent boundari page content multipl node may advantag topic local impli page like relat topic local crawler encount high backlink count associ boundari page encount mani crawler impli high qualiti current work primari interest web page categor text categor method could explor use x4 techniqu featur select 36 might use improv ecienc accuraci problem associ increment crawl dynam chang content consid examin futur work evalu base rel small crawl 137gb thorough evalu base multiterabyt crawl might reveal issu obviou current experi ment final x4 test abil scale larger number node correspondingli larger number categori 8 r intellig crawl world wide web arbitrari predic author languag tree zip anatomi largescal hypertextu web search engin crawl toward etern enhanc hypertext categor use hyperlink martin van den burg crawl web discoveri mainten largescal web data parallel crawl find replic web collect exploit redund question answer autonom learn construct knowledg base world wide web topic local web focus crawl use context graph hierarch classif web content adapt model optim perform increment web crawler intellig fusion multipl method inform server select mercat scalabl perform limit java core librari probabilist analysi rocchio algorithm tfidf text classif statist learn model text classif support vector machin authorit sourc hyperlink environ scale question answer web evalu phrasal cluster represent text categor task build domainspecif search engin machin learn techniqu evalu topicdriven web crawler machin learn wtm system collect analyz topicspecif web inform text classif label unlabel document use em boost rocchio appli text filter fast categoris larg document collect tr evalu phrasal cluster represent text categor task enhanc hypertext categor use hyperlink syntact cluster web boost rocchio appli text filter method inform server select anatomi largescal hypertextu web search engin effici crawl url order perform limit java core librari reexamin text categor method focus crawl authorit sourc hyperlink environ find replic web collect hierarch classif web content topic local web myampersandldquoauthoritymyampersandrdquo mean qualiti predict expert qualiti rate web document text classif label unlabel document use em learn construct knowledg base world wide web intellig crawl world wide web arbitrari predic adapt model optim perform increment web crawler scale question answer web statist learn learn model text classif support vector machin evalu topicdriven web crawler exploit redund question answer machin learn mercat probabilist analysi rocchio algorithm tfidf text categor focus crawl use context graph crawl web ctr antonio badia tulay muezzinoglu olfa nasraoui focus crawl experi real world project proceed 15th intern confer world wide web may 2326 2006 edinburgh scotland jo exposto joaquim macedo antnio pina albano alv jo rufino geograph partit distribut web crawl proceed 2005 workshop geograph inform retriev novemb 0404 2005 bremen germani weizheng gao hyun chul lee yingbo miao geograph focus collabor crawl proceed 15th intern confer world wide web may 2326 2006 edinburgh scotland