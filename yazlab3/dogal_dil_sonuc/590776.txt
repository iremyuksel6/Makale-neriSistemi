overcom myopia induct learn algorithm relieff current induct machin learn algorithm typic use greedi search limit lookahead prevent detect signific condit depend attribut describ train object instead myopic impur function lookahead propos use relieff extens relief develop kira rendel lsqb10 11rsqb heurist guidanc induct learn algorithm reimplement assist system top induct decis tree use relieff estim attribut select step algorithm test sever artifici sever real world problem result compar well known machin learn algorithm excel result artifici data set two real world problem show advantag present approach induct learn b introduct induct learn algorithm typic use greedi search strategi overcom combinatori explos search good hy pothes heurist function estim potenti successor current state search space major role greedi search current induct learn algorithm use variant impur function like inform gain gain ratio25 giniindex1 distanc ever measur assum attribut condit independ given class therefor domain strong condit depend attribut greedi search poor chanc reveal good hypothesi kira rendel 10 11 develop algorithm call seem power estim qualiti attribut exampl pariti problem variou degre signific number irrelev ran dom addit attribut relief abl correctli estim relev attribut time proport number attribut squar number train instanc reduc limit number iter relief origin relief deal discret continu tribut deal incomplet data limit twoclass problem develop extens relief call relieff improv origin algorithm estim probabl reliabl extend handl incomplet multiclass data set complex remain relieff seem promis heurist function may overcom myopia current induct learn algorithm kira rendel use relief preprocessor elimin irrelev attribut data descript learn ing relieff gener rel effici reliabl enough guid search learn process paper reimplement assist learn algorithm top induct decis tree 4 describ name assist r instead inform gain assistantr use relieff heurist function estim attribut qualiti step tree gener experi seri artifici realworld data set describ result obtain use relieff select criterion compar result ap proach follow approach compar ffl use inform gain select criterion ffl lfc 27 28 tri overcom myopia inform gain limit lookahead ffl naiv bayesian classifi assum condit independ attribut ffl knearest neighbor algorithm paper organ follow next section origin relief briefli describ along interpret extend version relieff section 3 present reimplement assist call assistantr section 41 briefli describ algorithm use experi section 42 describ experiment methodolog section 5 describ experi compar result differ algorithm show assistantr perform least well assist sometim much better conclus potenti breakthrough discuss basi excel result artifici data set final integr compar algorithm propos 2 relieff 21 relief key idea relief estim attribut accord well valu distinguish among instanc near purpos given instanc relief 1 set weight wa 00 2 1 n 3 begin 4 randomli select instanc r 5 find nearest hit h nearest miss 6 1 attribut 7 wa wa diffarhn 8 9 end figur 1 basic algorithm relief search two nearest neighbor one class call nearest hit differ class call nearest miss origin algorithm relief 10 11 randomli select n train instanc n userdefin paramet algorithm given figur 1 function diffattributeinstance1instance2 calcul differ valu attribut two instanc discret attribut differ either 1 valu differ 0 valu equal continu attribut differ actual differ normal interv 0 1 normal n guarante weight w interv gamma1 1 howev normal n unnecessari step w use rel comparison among attribut weight estim qualiti attribut rational formula updat weight good attribut valu instanc class subtract differ differenti instanc differ class ad differ function diff use also calcul distanc instanc find nearest neighbor total distanc simpli sum differ attribut fact origin relief use squar differ discret attribut equival diff experi signific differ result use diff squar differ enc n number train instanc complex algorithm theta n theta attribut 22 interpret relief estim follow deriv show relief estim strongli relat impur function obviou relief estim w attribut approxim follow differ probabl differ valu aj nearest instanc differ class gammap differ valu aj nearest instanc class 1 elimin 1 requir select instanc nearest formula becom differ valu ajdiffer class gammap differ valu ajsam class 2 rewrit equal valu classjequ valu obtain use bay rule sampl replac strict sens follow equal hold use equal obtain const theta highli correl giniindex gain 1 class c valu v attribut differ instead factor giniindex gain use equat 3 show strong relat lief weight giniindex gain probabl two instanc valu attribut eq 3 kind normal factor multivalu tribut impur function tend overestim multivalu attribut variou normal heurist need avoid tendenc eg gain ratio 25 distanc measur 16 binar attribut 4 equat 3 show relief exhibit implicit normal effect anoth defici giniindex gain valu tend decreas increas number class 14 denomin constant factor equat 3 given attribut serv kind normal therefor relief estim exhibit strang behavior giniindex gain deriv elimin nearest instanc condit probabl put back interpret relief estim averag local estim smaller part instanc space enabl relief take account context attribut ie condit depend attribut given class valu detect context local global point view depend hidden due effect averag train instanc exactli make impur function myopic impur function use correl attribut class disregard context attribut use global point view disregard local peculiar exampl data set given tabl 1 illustr differ myopic estim function relief three attribut eight train instanc class valu determin xor function attribut a1 a2 third attribut a3 randomli gener relief equat 1 correctli estim attribut a1 a2 import contribut attribut a3 poor hand wa equat 3 tabl exampl data set estim qualiti attribut function a1 a2 a3 class inform gain 9 0000 0000 0049 gainratio 25 0000 0000 0051 distanc 16 0000 0000 0026 ginigain equat 4 origin giniindex gain 1 inform gain 9 gain ratio 25 distanc measur 16 estim contribut a3 highest attribut a1 a2 estim complet irrelev hong 8 develop procedur similar relief estim qualiti attribut directli emphas use contextu inform differ relief approach use inform nearest miss ignor nearest hit besid hong use normal penal contribut nearest miss far away given instanc 23 extens relief origin relief deal discret continu attribut howev deal incomplet data limit twoclass problem equat 1 crucial import extens relief turn extens relief straightforward unless realiz relief fact approxim probabl extens design way probabl reliabl approxim develop extens relief call relieff improv origin algorithm estim probabl reliabl extend deal incomplet multiclass data set brief descript extens follow reliabl probabl approxima tion paramet n algorithm lief describ section 21 repres number instanc approxim probabl eq 1 larger n impli reliabl approxim obviou choic adopt relieff rel small number train instanc one thousand run outer loop relief avail train instanc select nearest neighbor crucial import relief purpos find nearest neighbor respect import attribut redund noisi attribut may strongli affect select nearest neighbor therefor estim probabl noisi data becom unreli increas reliabl probabl approxim relieff search k nearest hitsmiss instead one near hitmiss averag contribut k nearest hitsmiss shown extens significantli improv reliabl estim attribut qualities13 overcom problem paramet tune experi k set 10 empir give satisfactori result problem significantli better result obtain tune typic major machin learn algorithm incomplet data enabl relief deal incomplet data set function diffattributeinstance1 instance2 relieff extend miss valu attribut calcul probabl two given instanc differ valu given attribut ffl one instanc eg i1 unknown valu ffl instanc unknown valu condit probabl approxim rel frequenc train set nearest neighbor correl coeffici independ att pariti problem figur 2 correl relieff estim intend qualiti attribut data set condit independ strongli depend attribut approach assum condit probabl attributevalu given class applic without context tribut may case naiv howev includ context atribut far ineffici multiclass problem kira rendel relief use estim attribut qualiti data set two class split problem seri 2class problem solut seem unsatisfactori section 41 discuss perform approach compar extens describ use prac tice relief abl deal multiclass problem without prior chang knowledg represent could affect final outcom instead find one near miss differ class relieff search k near miss differ class c averag contribut updat estim w averag weight prior probabl class idea algorithm estim abil attribut separ pair class regardless two class closest normal prior probabl class necessari k near miss differ class would tend exagger influenc class small number case note time complex relieff 2 theta attribut n number train instanc 24 relieff estim attribut qualit estim contribut paramet k nearest hitsmiss relieff estim attribut qualiti kononenko 13 compar intend inform gain attribut estim gener relieff calcul standard linear correl coeffici correl coeffici show intend qualiti estim qualiti attribut relat typic graph data set condit independ attribut strongli depend attribut pariti problem variou de gree shown figur 2 condit independ attribut qualiti estim monoton increas number nearest neighbor conditionali depend tribut qualiti increas maximum later decreas number nearest neighbor exce number instanc belong peak distribut space given class note attribut evalu myopic impur function like giniindex inform gain qualiti estim would high condit independ attribut poor strongli depend attribut correspond estim relieff larg number nearest hitsmiss test effect normal factor eq 3 run relieff also one well known medic data set primari tumor describ 6 author section 53 major differ estim impur function estim relieff primari tumor problem estim two signific attribut inform gain giniindex overestim one attribut 3 valu opinion physician specialist relieff normal version impur function correctli estim attribut less import 3 assistantr assistantr reimplement assist learn system top induct decis trees4 basic algorithm goe back cl concept learn system develop hunt et al 9 reimplement sever author see 25 overview follow describ main featur assist binar attribut algorithm gener binari decis tree decis step binar version attribut select maxim inform gain attribut continu attribut decis point select maxim tribut inform gain discret attribut heurist greedi algorithm use find local best split attribut valu two subset purpos binar reduc replic problem strengthen statist support gener rule decis tree prune preprun postprun techniqu use prune unreli part decis tree preprun ing three userdefin threshold provid minim number train instanc minim attribut inform gain maxim probabl major class current node postprun method develop niblett bratko 22 use use laplac law success estim expect classif error current node commit pruningnot prune subtre incomplet data handl learn train instanc miss valu select attribut weight probabl attribut valu condit class label classif instanc miss valu weight uncondit probabl attribut valu naiv bayesian classifi intern node decis tree eventu third successor appear label attribut valu train instanc avail null leav naiv bayesian formula use calcul probabl distribut leaf use attribut appear path root leaf note calcul done offlin ie learn phase classif null leav alreadi label calcul class probabl distribut use classif manner ordinari leav main differ assist reimplement assistantr reli eff use attribut select addi tion wherev appropri instead rel frequenc assistantr use mestim probabl shown often significantli increas perform machin learn algorithms2 3 prior probabl laplac law success use possibl outcom n number trial n x number trial outcom x prior probabl use mestim condit probabl paramet trade contribut rel frequenc prior probabl experi paramet set set usual use default em piric give satisfactori result 2 3 although tune problem domain better result may expect mestim use naiv bayesian formula 5 postprun instead laplac law success propos cestnik bratko3 relieff estim probabl eq 1 use probabl root tree estim prior probabl lower intern node nt correspond train instanc ajnearest miss ajnearest miss root ajnearest hit ajnearest hit root 4 experiment environ 41 algorithm comparison perform seri experi assistantr compar perform follow algorithm assistanti variant assistantr instead relieff use inform gain select criterion assist ever differ assist remain mestim probabl algorithm enabl us evalu contribut lieff paramet assistanti assistantr fix throughout experi preprun postprun 2 lfc ragavan et al 27 28 use limit lookahead lfc lookahead featur con struction algorithm top induct decis tree detect signific condit depend attribut construct induct show interest result data set reimplement algorithm 29 test perform result present paper show drawback experiment comparison describ ragavan rendel confirm advantag limit lookahead construct induct lfc gener binari decis tree node algorithm construct new binari attribut origin attribut use logic oper conjunct disjunct negat construct binari tribut best attribut select process recurs repeat two subset train instanc correspond two valu select attribut construct induct limit lookahead use space possibl use construct strict due geometr represent condit entropi estim attribut qualiti reduc search space algorithm also limit breadth depth search lfc use lookahead less myopic greedi algorithm assist comparison result may show perform greedi search combin reli eff versu lookahead strategi make result compar assistantr equip lfc prune probabl estim facil describ section 3 test perform default set paramet depth lookahead 3 beam size 20 although domain better result may obtain paramet tune howev higher valu paramet may combinatori increas search space lfc make algorithm impract naiv bayesian classifi classifi use naiv bayesian formula 5 calcul probabl class given valu attribut assum condit independ attribut new instanc classifi class maxim calcul probabl mestim probabl use paramet set 2 experi perform naiv bayesian classifi serv estim condit independ attribut knn knearest neighbor algorithm given new instanc algorithm search nearest train instanc classifi instanc frequent class k instanc knn algorithm distanc measur use relieff see section 21 present result obtain manhattandist result use euclidian distanc practic best result respect paramet k pre sent although fair comparison paramet tune allow train test set select naiv bayesian classifi knn algorithm comparison well known simpl perform well mani realworld problem perform two algorithm may show natur classif problem 42 experiment methodolog experi data set perform time randomli select 70 instanc learn 30 test result averag system use subset instanc learn test order provid experiment condit verifi signific differ use onetail ttest confid level null hypothesi state differ zero5 differ result valu statist threshold consid signific except methodolog experi finit element mesh design problem experiment methodolog dictat previou publish result describ section 54 besid classif accuraci measur also averag inform score15 measur elimin influenc prior probabl appropri treat probabilist answer classifi averag inform score defin test instanc test instanc inform score classif ith test instanc defin class ith test instanc p cl prior probabl class cl probabl return classifi return probabl correct class greater prior probabl inform score posit obtain inform correct interpret prior inform necessari correct classif minu posterior inform necessari correct classi ficat return probabl correct class lower prior probabl inform score neg obtain inform wrong interpret prior inform necessari incorrect classif minu posterior inform necessari incorrect classif main differ classif accuraci inform score illustr follow exampl let prior distribut class p let posterior distribut return classifi p correct class c 1 inform score posit classif accuraci treat given posterior distribut wrong answer correct class c 2 inform score neg classif accuraci treat given posterior distribut correct answer classif accuraci may special case exhibit high varianc inform score much stabl special case data set irrelev attribut exactli 50 instanc one class 50 instanc class leav oneout test probabilist classifi would give approxim accuraci 50 thedefault classifi classifi everi instanc major class accuraci would 0 slight modif distribut train instanc would drastic chang latter accuraci approxim 50 drastic modif distribut say 80 case one class 20 would increas accuraci default classifi 80 accuraci probabilist classifi would approxim 08 theta 0802 theta 02 68 howev classifi inform score would scenario remain approxim 0 bit would indic classifi unabl extract use inform attribut 5 experiment result section give result sever artifici realworld data set present experi divid four part accord four group data set artifici data set control condit depend attribut benchmark artifici data set medic data set realworld data set group give brief descript data set follow result result tabl includ averag sever run standard error 51 artifici data set gener sever data set order compar perform variou algorithm inf1 domain three condit independ inform binari attribut three class three random binari attribut learner detect three attribut inform rel easi task five algorithm abl solv problem inf2 domain obtain inf1 replac inform attribut two attribut whose valu defin valu origin attribut xor relat prob lem learner detect six import attribut fact attribut pairwis strongli condit depend fairli complex problem solv myopic heurist data set show advantag lfc assistantr tree domain whose instanc gener decis tree 6 intern node contain differ binari attribut 5 random binari attribut ad descript instanc problem easi greedi decis tree learn algorithm approach may difficulti due inappropri knowledg represent target concept par2 pariti problem two signific binari attribut 10 random binari tribut 5 randomli select instanc label wrong class problem hard lot attribut equal score evalu myopic evalu function inform gain par3 par2 except three signific attribut pariti relat make problem harder par4 par2 except four signific attribut pariti relat make problem hardest among pariti problem use experi basic characterist artifici data set list tabl 2 characterist includ percentag major class interpret default accuraci class entropi give impress complex classif problem result learn algorithm lfc assistanti assistantr well naiv bayesian classifi knn algorithm given tabl 3 classif accuraci tabl inform score result expect show ffl classifi perform well rel sim ple domain condit independ attribut ffl version assist perform well problem reconstruct decis tree tree classifi significantli wors assistantr lfc abl success solv problem strong condit depend attribut inf2 par2 4 howev two assistantr perform better especi case hardest problem par4 note lfc solv par4 depth lookahead increas tabl basic descript artifici data set domain class att valatt instanc majclass entropybit inf2 3 21 20 200 36 158 tabl 3 classif accuraci learn system artifici data set domain lfc assistanti assistantr naiv bay knn inf1 860sigma51 901sigma35 888sigma38 916sigma31 890sigma36 inf2 671sigma63 554sigma98 687sigma78 321sigma45 568sigma63 tree 758sigma54 792sigma57 788sigma62 690sigma59 682sigma53 936sigma33 749sigma79 957sigma28 567sigma57 794sigma43 howev time complex lookahead increas exponenti depth hand assistantr solv pariti problem equal quickli ffl inform score naiv bayesian classifi problem strong condit depend attribut poor indic classifi fail find regular data set 52 benchmark artifici data set besid artifici data set previou subsect use also follow benchmark artifici data set use author note result author directli compar result experiment condit trainingtest split bool boolean function defin 6 attribut 10 class nois optim recognit rate 90 target function data set use smyth et al 31and report 672sigma17 classif accuraci naiv bay 825sigma11 back propag 859sigma09 rulebas classifi led leddigit problem 10 nois attribut valu optim recognit rate estim 74 smyth et al 31 report 681sigma17 classif accuraci naiv bay 646sigma35 backpropa gation 727sigma13 rulebas classi fier data set obtain irvin database21 krk1 problem legal kingrook king chess endgam posit attribut describ relev relat piec rank adjac file origin data includ five set 1000 exampl 1000 learn 4000 test use test induct logic program algorithms7 report classif accuraci 997sigma01 use one set 1000 exampl ie 700 instanc train krk2 krk1 except avail attribut coordin piec data set use mladenic19 report result 69 accuraci atri system 64 assist basic descript data set provid tabl 5 result given tabl 6 7 interest led domain naiv bayesian classifi knn algorithm reach estim upper bound classif accuraci suggest attribut consid optim classif domain problem attribut condit independ given class therefor good perform naiv bayesian classifi surpris howev three domain perform naiv bayesian classifi poor due strong tabl 4 averag inform score learn system artifici data set domain lfc assistanti assistantr naiv bay knn tabl 5 basic descript benchmark artifici data set domain class att valatt instanc majclass entropybit tabl 6 classif accuraci learn system artifici data set domain lfc assistanti assistantr naiv bay knn led 708sigma23 711sigma24 717sigma22 739sigma21 739sigma21 krk1 987sigma12 986sigma12 986sigma12 916sigma14 922sigma19 krk2 860sigma21 666sigma31 701sigma33 648sigma21 707sigma17 tabl 7 averag inform score learn system artifici data set domain lfc assistanti assistantr naiv bay knn led 213sigma007 211sigma006 212sigma007 233sigma005 222sigma005 condit depend attribut inform score see tabl 7 show naiv bayesian classifi provid averag inform bool krk2 domain perform differ variant assist almost except krk2 domain perform assistanti poor note default accuraci krk2 67 perform assistantr knn algorithm significantli better 9995 confid level howev inform score show assistantr knn success problem expect without construct induct possibl reveal regular chess posit describ coordin piec lfc abl construct import attribut domain enabl achiev significantli better result algorithm 53 medic data set compar perform algorithm sever medic data set ffl data set obtain univers medic center ljubljana slovenia problem locat primari tumour patient metastas prim problem predict recurr breast cancer five year remov tumour brea problem determin type cancer lymphographi lymp diagnosi rheumatolog rheu ffl hepa prognost surviv patient suffer hepat data provid gail gong carnegiemellon univers ffl data set obtain statlog database18 diagnosi diabet diab diagnosi heart diseas heart diab data set ragavan rendel 27report 788 classif accuraci lfc al gorithm also report poor perform tabl 8 basic descript medic data set domain class att valatt instanc majclass entropybit prim 22 17 22 339 25 389 tabl 9 classif accuraci learn system medic data set domain lfc assistanti assistantr naiv bay knn brea 761sigma43 768sigma46 785sigma39 787sigma45 795sigma27 lymp 824sigma52 770sigma55 770sigma59 847sigma42 826sigma57 hepa 790sigma53 772sigma53 823sigma54 861sigma39 826sigma49 heart 773sigma52 754sigma40 776sigma45 845sigma30 829sigma37 tabl averag inform score learn system medic data set domain lfc assistanti assistantr naiv bay knn sever algorithm without construct induct 58 howev result see result statlog project 18 show poor result algorithm domain due lack construct induct experi diab dataset classifi perform equal well except naiv bayesian classifi significantli better basic characterist medic data set given tabl 8 result experi data set provid tabl 9 10 medic data set attribut typic condit independ given class fore surpris naiv bayesian classifi show clear advantag data sets12 interest perform knn algorithm good domain although wors perform naiv bayesian classifi inform score tabl 10 brea data set indic learn algorithm abl solv problem suggest attribut relev version assist similar per formanc except hepa domain assistantr significantli better perform confid level detail analysi show problem relieff discov signific condit interdepend two attribut given class two attribut score poorli consid indepen dentli assistanti abl discov regular data hand attribut avail contain similar inform two attribut togeth reason naiv bayesian classifi perform better tri provid naiv bayesian classifi addit attribut join two condit depend attribut ever perform remain achiev significantli better result two induct algorithm lymp domain construct induct seem use howev lfc perform significantli wors rheu domain domain three induct algorithm perform equal well 54 nonmed realworld data set compar perform algorithm also follow nonmed real world data set soyb iri vote obtain irvin database21 sat obtain statlog databas 18 soyb famou soybean data set use iri well known fisher problem determin type iri flower mesh3mesh15 problem determin number element edg object finit element mesh design problem6 five object expert construct appropri mesh five experi one object use test four learn result averag result report dzeroski 7 variou ilp system 12 classif accuraci foil 22 mfoil 29 golem result report pomp et al 23 28 sfoil descript mesh problem appropri ilp system attribut learner relat ariti 1 ie attribut use describ problem note domain trainingtest split algorithm test methodolog special case leaveoneout therefor result tabl problem standard deviat quinlan 26 report result ilp system achiev 90 domain test posit neg instanc howev result mislead posit instanc ten neg instanc averag therefor 11 copi instanc classif instanc correct least 9 11 copi give 82 classif accuraci classifi alway classifi wrong class mesh3 contain three basic attribut origin databas ignor relat descript object therefor domain attribut learner given less inform ilp learner contain besid 3 origin tribut 12 attribut deriv relat background knowledg prob lem attribut learner advantag alreadi provid addit attribut provid descript object ilp learner actual inform princi ple attribut number addit attribut could deriv extrem cleaver ilp learner relat descript background knowledg ever fairli complex task therefor attribut learner mesh15 data set better chanc ilp learner reveal good hypothesi sat databas consist multiclass spectral valu pixel 3 theta 3 neighborhood satellit imag classif central pixel neighborhood result statlog project18 906 classif accuraci knn algorithm 861 backpropag 850 c45 848 cn2 693 naiv bayesian classifi use rel frequenc mestim probabl vote vote record session 1984 unit state congress smyth et al 31 report 889 classif accuraci naiv bayesian classifi 930 backpropag 949 rulebas classifi basic characterist nonmed real world data set present tabl 11 tabl 12 13 give result soyb iri data set classifi perform equal well result naiv bayesian classifi indic attribut condit rel independ data set agreement previous publish result sat data set knn significantli outperform algorithm agreement result statlog project 18 ever naiv bayesian classifi mestim probabl reach classif accuraci induct learn algorithm result naiv bayesian classifi use 14 author tabl basic descript nonmed realworld data set domain class att valatt instanc majclass entropybit tabl classif accuraci learn system nonmed realworld data set domain lfc assistanti assistantr naiv bay knn iri 950sigma38 957sigma37 952sigma26 966sigma26 970sigma21 tabl averag inform score learn system nonmed realworld data set domain lfc assistanti assistantr naiv bay knn statlog project much wors cestnik 2 shown mestim significantli increas perform naiv bayesian classifi also confirm experi version assist perform data set except sat data set assistantr lfc achiev significantli better result 9995 confid level result confirm relieff estim qualiti attribut better inform gain vote data set naiv bayesian classifi worst version assist compar rule base classifi smyth et al 31 interest result appear domain although attribut learner mesh3 less inform ilp system outperform result ilp system report dzeroski 7 pomp et al 23 12 addit attribut mesh15 result induct learner significantli improv induct learn system significantli outperform naiv bayesian classifi knn algorithm detail analysi show excel result version assist due use naiv bayesian formula calcul class probabl distribut null leav see section 3 name problem often happen test instanc fall null leaf train instanc valu signific attribut test instanc naiv bayesian classifi effici solv problem lfc gener null leav construct attribut strictli binari valu true fals therefor classif object differ valu origin attribut train instanc alway proce branch label fals effect strategi given test instanc correspond leaf contain train instanc similar valu attribut appear path root leaf strategi also work well mesh problem 6 discuss note null leav version assist influenc perform arti ficial data set miss valu data also mesh15 problem perform lfc good although gener null leav therefor use null leav crucial differ assist lfc equat 3 show interest relat relief estim impur func tion relief effici estim continu discret attribut implicit normal eq 3 enabl relief appropri deal multivalu attribut howev assistanti would use eq 3 instead inform gain would still myopic ex ampl par24 problem eq 3 would estim attribut equal nonimport therefor reason success assistantr nearest instanc heurist influenc estim probabil iti heurist enabl relief detect strong condit depend attribut would overlook estim probabl would done randomli select instanc instead nearest instanc relieff effici heurist estim attribut qualiti abl deal data set condit depend independ tribut extens relieff enabl deal noisi incomplet multiclass data set increas number k nearest hitsmiss correl relieff estim impur function also increas unless k greater number instanc peak instanc space studi report 14 show relieff accept bia respect measur estim attribut differ number valu myopia current induct learn system partial overcom replac exist heurist function relieff assistantr variant top induct decis tree algorithm use relieff estim qualiti attribut significantli outperform classifi domain strong condit depend tribut myopia induct learner may caus overlook signific relat easili demonstr artifici data set also shown two real world problem hepa sat data set relieff detect signific condit interdepend attribut result significantli better result assistantr result assistanti one featur relief address paper attribut replic data set replic get estim increas number replic qualiti estim descreas replic attribut affect distanc instanc construct induct lfc use limit lookahead detect signific condit depend attribut lfc show similar advantag algorithm assist r one artifici problem krk2 one real world problem lymp lfc perform significantli better due construct induct howev case construct induct may spoil result case rheu data set lfc perform well prob lem suggest limit lookahead good search strategi realworld prob lem lookahead howev reason limit time complex exponentiali increas lookahead depth although relieff may overcom myopia useless assistantr chang represent requir case construct induct appli exampl krk2 problem assistantr achiev good result improv without construct induct good idea construct induct may use relieff instead combin lookahead naiv bayesian classifi obviou advantag domain condit rel independ attribut medic diagnost problem domain naiv bayesian classifi abl reliabl estim condit probabl also abl use tribut ie avail inform would interest appropri combin power relieff naiv bayesian classifi current ilp system 20 abl use attribut appropri demonstr mesh3 domain attribut learn er outperform exist ilp system enabl ilp system deal attributevalu rep resent combin semi naiv bayesian classifi could use hand current ilp system use greedi search techniqu heurist guid search myopic pomp kononenko 24 implement adapt version relieff foil like ilp system call ilpr prelemi nari experi show similar advantag system ilp system assistantr assistanti 7 conclus relieff effici heurist estim attribut qualiti abl deal data set condit depend independ tribut noisi incomplet multiclass data set myopia current induct learn system partial overcom replac exist heurist function reli eff accept increas comput complex may certain domain payoff eventu discoveri strong condit depend attribut detect use myopic impur measur guid greedi search experiment result indic major real world problem myopia margin effect one may wonder whether myopia realli worth much attent howev face new data set unreason tri myopic algorithm unless know advanc data set strong condit depend attribut seriou applic machin learn new data tri discov much regular data po sibl therefor nonmyop approach one describ paper use indispens tool analys data acknowledg use mestim equat 1 propos bojan cestnik thank matjaz zwitter prim brea data set milan soklic lymp gail gong hepa padhraic smyth bool led saso dzeroski krk1 mesh bob hen eri diab heart sat data set statlog databas strathclyd univers patrick murphi david aha data set irvin databas grate colleagu saso dzeroski matevz kovac matjaz kukar uro pomp tanja urbanc anonym review comment earlier draft significantli improv paper work support slovenian ministri scienc technolog r wadsworth intern group estim probabl crucial task machin learn estim probabl tree prune assist 86 gener statist applic induct logic program finit element mesh de sign handl nois induct logic pro gram use contextu inform featur rank discret experi duction practic approach featur select featur select prob lem tradit method new algorithm induct bayesian learn medic diagnosi bias estim multivalu attribut inform base evalu criterion classifi perform id3 revisit distanc base criterion attribut select learn told learn exampl experiment comparison two method knowledg acquisit context develop expert system soybean diseas diagnosi combinatori optim induct concept learn uci repositori machin learn databas learn decis rule noisi domain linear space induct first order logic relieff induct decis tree minimum descript length principl categor theori lookahead featur construct learn hard concept learn complex realworld conceptsthrough featur construct construct induct decis tree rule induct use inform theori tr ctr xin jin rongyan li xian shen rongfang bie automat web page categor relieff hidden naiv bay proceed 2007 acm symposium appli comput march 1115 2007 seoul korea use contextu inform featur rank discret ieee transact knowledg data engin v9 n5 p718730 septemb 1997 marko robnikikonja igor kononenko theoret empir analysi relieff rrelieff machin learn v53 n12 p2369 octobernovemb david bell hui wang formal relev applic featur subset select machin learn v41 n2 p175195 novemb 2000 llu mrquez llu padr horacio rodrguez machin learn approach po tag machin learn v39 n1 p5991 april 2000 saher esmeir shaul markovitch anytim learn decis tree journal machin learn research 8 p891933 512007 huan liu hiroshi motoda lei yu select sampl approach activ featur select artifici intellig v159 n12 p4974 novemb 2004 foster provost venkateswarlu kolluri survey method scale induct algorithm data mine knowledg discoveri v3 n2 p131169 june 1999